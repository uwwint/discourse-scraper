{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f58132-7bf2-45c4-aea4-b5aae2b5d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d544bed8-fb34-4ef8-84c5-586991f8d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Pages finished: 25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Pages finished: 30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "Pages finished: 35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "Pages finished: 40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "Pages finished: 45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Pages finished: 50\n",
      "Writing PRs to file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58365621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''offset = 21\n",
    "n_pages = 30\n",
    "path = \"../out/\"\n",
    "headers = {'Accept': 'application/json', 'Authorization': 'TOK:<<>>'}\n",
    "prs = \"\"\n",
    "for i in range(n_pages):\n",
    "    i += offset\n",
    "    print(i)\n",
    "    page_path = \"https://api.github.com/repos/galaxyproject/galaxy/pulls?page={}&state=all&per_page=100\".format(i)\n",
    "    r = requests.get(page_path, allow_redirects=True, headers=headers)\n",
    "    loaded_string = json.loads(r.content.decode(\"utf-8\"))\n",
    "    if prs == \"\":\n",
    "        prs = loaded_string\n",
    "    else:\n",
    "        prs.extend(loaded_string)\n",
    "    if i % 5 == 0 and i > 0:\n",
    "        print(\"Pages finished: {}\".format(i))\n",
    "print(\"Writing PRs to file...\")\n",
    "open(path + \"github_pr_page_{}-{}.json\".format(offset, offset+n_pages), 'w').write(json.dumps(prs))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef69ddaf-b072-4dd1-84ed-f08f338153a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 16:46:52,786] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Haystack document store...\n"
     ]
    }
   ],
   "source": [
    "# create CSV file for all PRs\n",
    "import glob,json\n",
    "from haystack import Document\n",
    "from haystack.nodes import PreProcessor\n",
    "import pandas as pd\n",
    "\n",
    "docs = []\n",
    "\n",
    "print(\"Creating Haystack document store...\")\n",
    "\n",
    "# process PRs\n",
    "for json_file in glob.glob(\"../out/github_pr_page_*.json\"):\n",
    "    with open(json_file, \"r\") as fin:\n",
    "        doc_json = json.load(fin)\n",
    "        for pr in doc_json:\n",
    "            pr_text = pr[\"body\"]\n",
    "            if pr_text != None:\n",
    "                useful_text_limit = pr_text.find(\"## How to test the changes\")\n",
    "                if useful_text_limit > 0:\n",
    "                    pr_text = pr_text[:useful_text_limit].strip()\n",
    "                    pr_dict = {\"content\": pr_text, \"meta\": {\"name\": pr[\"number\"]}}\n",
    "                    doc = Document.from_json(json.dumps(pr_dict))\n",
    "                    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e7cf19-72c8-4031-a77b-81705804bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nhi, i have a very ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nhi, i met an error w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nhi, i am attempting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nsubmitting a job to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\ni need a tool which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nhello, . i am workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\ni have been trying t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\ni am trying to follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\nhello. two questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>\\n\\n\\n### Instruction:\\n\\ngood morning! i here...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations\n",
       "0     \\n\\n\\n### Instruction:\\n\\nhi, i have a very ba...\n",
       "1     \\n\\n\\n### Instruction:\\n\\nhi, i met an error w...\n",
       "2     \\n\\n\\n### Instruction:\\n\\nhi, i am attempting ...\n",
       "3     \\n\\n\\n### Instruction:\\n\\nsubmitting a job to ...\n",
       "4     \\n\\n\\n### Instruction:\\n\\ni need a tool which ...\n",
       "...                                                 ...\n",
       "1406  \\n\\n\\n### Instruction:\\n\\nhello, . i am workin...\n",
       "1407  \\n\\n\\n### Instruction:\\n\\ni have been trying t...\n",
       "1408  \\n\\n\\n### Instruction:\\n\\ni am trying to follo...\n",
       "1409  \\n\\n\\n### Instruction:\\n\\nhello. two questions...\n",
       "1410  \\n\\n\\n### Instruction:\\n\\ngood morning! i here...\n",
       "\n",
       "[1411 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process conversations\n",
    "\n",
    "df_conversations = pd.read_csv(\"../data/documents_conversations_galaxy_help.csv\", sep=\"\\t\")\n",
    "df_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfce64e2-45af-4bcc-8118-c35eba1f139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(df_conversations.iterrows()):\n",
    "    conv_dict = {\"content\": item[1][\"conversations\"], \"meta\": {\"name\": idx}}\n",
    "    doc = Document.from_json(json.dumps(conv_dict))\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12cb5ab-307a-4dd2-b1a5-56a8d00aa911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   6%|████████▎                                                                                                                                       | 293/5065 [00:00<00:03, 1496.58docs/s]We found one or more sentences whose split count is higher than the split length.\n",
      "Preprocessing:  39%|███████████████████████████████████████████████████████▊                                                                                       | 1978/5065 [00:01<00:02, 1115.79docs/s]Document a54b46bd20f7cd243598211e91981972 is 27560 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 5298a0ed77b8b25aea6acf3858f36223 is 17560 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 3810/5065 [00:03<00:02, 538.26docs/s]Document 5ef263f2061a357840a5dd1d63cf3376 is 10931 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 4858/5065 [00:05<00:00, 787.17docs/s]Document 16bdd2df7846b3f9903fd6d68b47524 is 24546 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 44a0c1fb33187396b2ad800cbbfd5a5b is 14546 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5065/5065 [00:05<00:00, 881.79docs/s]\n"
     ]
    }
   ],
   "source": [
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0,\n",
    "    language=\"en\",\n",
    ")\n",
    "preprocessed_docs = processor.process(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e1b7d1-d5e1-4fb6-9e5e-8d67b60c533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Document: {'content': 'Follow up to #17553\\nIgnore autogenerated CSS from the visualization build', 'content_type': 'text', 'score': None, 'meta': {'name': 17559, '_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '507bc9e8922085954a7928cb73983ec8'}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5340fad1-04bd-43a9-ba6b-a6f21d83a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93089285-66c3-4678-aaf5-1ed8aea3828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6634/6634 [00:00<00:00, 18692.11 docs/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(preprocessed_docs)\n",
    "\n",
    "#add new comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf5460b-cfd1-49d1-8fa1-4e74f13e76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d683c347-d32f-4598-9b1d-f6c52fbdb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa517eb-bb82-4bf9-a242-a169d1db86c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-22 14:12:37,106] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "<<>> ········\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[1;32m      6\u001b[0m HF_TOKEN \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m qa_template \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m(prompt\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m  Context: {join(documents)};\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m  Question: {query}\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m  [/INST]\"\"\"\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "# a good Question Answering template, adapted for the instruction format\n",
    "# (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
    "from haystack.nodes import PromptNode\n",
    "from getpass import getpass\n",
    "\n",
    "HF_TOKEN = getpass(\"<<>>\")\n",
    "\n",
    "qa_template = PromptTemplate(prompt=\n",
    "  \"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\n",
    "  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\n",
    "  Context: {join(documents)};\n",
    "  Question: {query}\n",
    "  [/INST]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "994c1250-ed84-439e-a9e1-1479a3d46d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "prompt_node = PromptNode(model_name_or_path=model_name, #\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "                         api_key=HF_TOKEN,\n",
    "                         default_prompt_template=qa_template,\n",
    "                         max_length=5500,\n",
    "                         model_kwargs={\"model_max_length\":8000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a798ef-6042-483e-9096-4712192ff491",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8e7feb0-2e06-4f34-b852-bf9cbffd99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print_answer = lambda out: pprint(out[\"results\"][0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2ef6279-f403-4986-8abe-9c3340f68e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I'm sorry, but I don't see any context provided for the question. Can you \"\n",
      " 'please provide more information or clarify what you are asking?')\n"
     ]
    }
   ],
   "source": [
    "print_answer(rag_pipeline.run(query=\"I would suggest installing the refseq_masher package. I checked earlier, and found it in the toolshed. \\\n",
    "Please, this package will help a lot.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4ceb1-812d-4984-ab4f-f7041b7cf27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18fc59-d3a7-4499-8a1b-6d50d839236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0300460-613f-4c1c-92ba-465f2e695388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
