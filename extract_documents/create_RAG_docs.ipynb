{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b529c1ba-0fc8-4cb4-a710-46c1b4cf6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-22 14:10:41,459] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import markdown\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "from haystack import Document\n",
    "from haystack.nodes import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247cdac1-e948-4cce-b494-b4c4711648b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108347/3443310435.py:18: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  plain_text = ''.join(BeautifulSoup(html_content, \"html.parser\").findAll(text=True))\n"
     ]
    }
   ],
   "source": [
    "# Collect from GTN\n",
    "\n",
    "docs = []\n",
    "directory_path = \"../../../gtn-data/\"\n",
    "\n",
    "def read_md_file_1(path):\n",
    "    with open(path) as f:\n",
    "        content = f.read()\n",
    "        return content\n",
    "\n",
    "def read_md_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "    return extract_plain_text_from_md(md_content)\n",
    "\n",
    "def extract_plain_text_from_md(md_content):\n",
    "    html_content = markdown.markdown(md_content)\n",
    "    plain_text = ''.join(BeautifulSoup(html_content, \"html.parser\").findAll(text=True))\n",
    "    return plain_text.strip()\n",
    "    \n",
    "#included_content = \"/topics/statistics/tutorials/intro_deep_learning/\"\n",
    "included_content = \"/topics/\"\n",
    "\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for filename in files:\n",
    "        if fnmatch.fnmatch(filename, '*.md'):\n",
    "            path = os.path.join(root, filename)\n",
    "            if included_content in path:\n",
    "                s_path = path.split(\"/\")[-3:]\n",
    "                tutorial_name = \"_\".join(s_path)\n",
    "                md_plain_text = read_md_file(path)\n",
    "                pr_dict = {\"content\": md_plain_text, \"meta\": {\"name\": tutorial_name}}\n",
    "                doc = Document.from_json(json.dumps(pr_dict))\n",
    "                docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3050016-b331-4faa-ad61-91840bae3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect from PRs\n",
    "\n",
    "# process PRs\n",
    "for json_file in glob.glob(\"../out/github_pr_page_*.json\"):\n",
    "    with open(json_file, \"r\") as fin:\n",
    "        doc_json = json.load(fin)\n",
    "        for pr in doc_json:\n",
    "            pr_text = pr[\"body\"]\n",
    "            if pr_text != None:\n",
    "                useful_text_limit = pr_text.find(\"## How to test the changes\")\n",
    "                if useful_text_limit > 0:\n",
    "                    pr_text = pr_text[:useful_text_limit].strip()\n",
    "                    pr_dict = {\"content\": pr_text, \"meta\": {\"name\": pr[\"number\"]}}\n",
    "                    doc = Document.from_json(json.dumps(pr_dict))\n",
    "                    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf46f3da-46ff-434d-9fa5-8b8510f530fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|                                                                                                                         | 0/5018 [00:00<?, ?docs/s]We found one or more sentences whose split count is higher than the split length.\n",
      "Document 765879b873e5da1f6844b2b004354251 is 15953 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  11%|████████████▌                                                                                                 | 572/5018 [00:02<00:15, 279.66docs/s]Document bd56adb5222bde92e6e6a446ac4125db is 18398 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  13%|██████████████                                                                                                | 640/5018 [00:02<00:14, 304.15docs/s]Document 4632727d14cf65552d3611c3d379e2ff is 71514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f4de4544390feafab7826fb379b6846f is 61514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9ad9ce235fa88af47d09d1b31cc7bb76 is 51514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 7bfd75b67821e92587a537e8b0138361 is 41514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f058db145146309c1ab88d47befcfffa is 31514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 79ba607382954d4ec7a0dc7aa0135820 is 21514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 4791c539a6060c785e4b64a43e11f64 is 11514 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  65%|██████████████████████████████████████████████████████████████████████                                      | 3253/5018 [00:06<00:00, 1953.16docs/s]Document a54b46bd20f7cd243598211e91981972 is 27560 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 5298a0ed77b8b25aea6acf3858f36223 is 17560 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5018/5018 [00:08<00:00, 605.63docs/s]\n"
     ]
    }
   ],
   "source": [
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0,\n",
    "    language=\"en\",\n",
    ")\n",
    "preprocessed_docs = processor.process(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4227350-e8c7-4522-b798-28d7952762c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc33ab6-0daf-4f22-b026-970a937930ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████████████████████████████████████████████████████████████████████████████████| 11966/11966 [00:00<00:00, 15995.83 docs/s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa661ef1-9c36-4f56-85d7-08978bdf2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35ef815-f99f-4a2b-aed4-e9aabba8c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store, top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba63121-1970-426b-9235-c610c299e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "<<>> ········\n"
     ]
    }
   ],
   "source": [
    "# a good Question Answering template, adapted for the instruction format\n",
    "# (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
    "from haystack.nodes import PromptNode\n",
    "from getpass import getpass\n",
    "\n",
    "HF_TOKEN = getpass(\"<<>>\")\n",
    "\n",
    "qa_template = PromptTemplate(prompt=\n",
    "  \"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\n",
    "  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\n",
    "  Context: {join(documents)};\n",
    "  Question: {query}\n",
    "  [/INST]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded67a44-aef2-4956-b938-a78dd6582d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "prompt_node = PromptNode(model_name_or_path=model_name,\n",
    "                         api_key=HF_TOKEN,\n",
    "                         default_prompt_template=qa_template,\n",
    "                         max_length=5500,\n",
    "                         model_kwargs={\"model_max_length\":8000}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eddac64a-caa3-4ebc-97de-ba5b0822102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf02716-333d-4c60-8f04-1cb5204b899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print_answer = lambda out: pprint(out[\"results\"][0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2604c936-bc2b-40d3-90fe-71ad662374e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Exception while running node 'prompt_node': HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\nEnable debug logging to see the data that was passed when the pipeline failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:237\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/utils/requests_utils.py:93\u001b[0m, in \u001b[0;36mrequest_with_retry\u001b[0;34m(attempts, status_codes_to_retry, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# We raise here too in case the request failed with a status code that\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# won't trigger a retry, this way the call will still cause an explicit exception\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/NousResearch/Llama-2-7b-chat-hf",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/pipelines/base.py:567\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    566\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 567\u001b[0m node_output, stream_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m node_output \u001b[38;5;129;01mand\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m node_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/pipelines/base.py:469\u001b[0m, in \u001b[0;36mPipeline._run_node\u001b[0;34m(self, node_id, node_input)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m, node_input: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/base.py:201\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mThe Pipelines call this method when run() is executed. This method in turn executes the _dispatch_run_general()\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mmethod with the correct run method.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run_general\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/base.py:245\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         run_inputs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 245\u001b[0m output, stream \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Collect debug information\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/prompt_node.py:312\u001b[0m, in \u001b[0;36mPromptNode.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, invocation_context, prompt_template, generation_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m invocation_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(\n\u001b[1;32m    309\u001b[0m     query, file_paths, labels, documents, meta, invocation_context, prompt_template, generation_kwargs\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 312\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvocation_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_collector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m prompt_template_resolved: PromptTemplate \u001b[38;5;241m=\u001b[39m invocation_context\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/prompt_node.py:140\u001b[0m, in \u001b[0;36mPromptNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_template\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/prompt_node.py:169\u001b[0m, in \u001b[0;36mPromptNode.prompt\u001b[0;34m(self, prompt_template, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt being sent to LLM with prompt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prompt, kwargs_copy)\n\u001b[0;32m--> 169\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m results\u001b[38;5;241m.\u001b[39mextend(output)\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/prompt_model.py:129\u001b[0m, in \u001b[0;36mPromptModel.invoke\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mTakes in a prompt and returns a list of responses using the underlying invocation layer.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:return: A list of model-generated responses for the prompt or prompts.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_invocation_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:173\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer.invoke\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    172\u001b[0m }\n\u001b[0;32m--> 173\u001b[0m response: requests\u001b[38;5;241m.\u001b[39mResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:254\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceUnauthorizedError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key is invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceError(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFace Inference returned an error.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatus code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mstatus_code,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m: HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_answer(\u001b[43mrag_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI would suggest installing the refseq_masher package. I checked earlier, and found it in the toolshed. \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mPlease, this package will help a lot.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/pipelines/base.py:574\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# If you really want to see it, raise the log level.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_id, node_input)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnable debug logging to see the data that was passed when the pipeline failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    577\u001b[0m queue\u001b[38;5;241m.\u001b[39mpop(node_id)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Exception while running node 'prompt_node': HuggingFace Inference returned an error.\nStatus code: 403\nResponse body: {\"error\":\"The model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\"}\nEnable debug logging to see the data that was passed when the pipeline failed."
     ]
    }
   ],
   "source": [
    "print_answer(rag_pipeline.run(query=\"I would suggest installing the refseq_masher package. I checked earlier, and found it in the toolshed. \\\n",
    "Please, this package will help a lot.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a7395-622f-4353-abf5-d2d0eee0b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22dffe-ee0d-46cd-b364-118c72f5aaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424c6eb-23e9-4ec6-a165-fcc783153172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
