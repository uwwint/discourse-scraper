{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8f95bd-7224-4a1c-b350-5fab83141e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b063814-eadf-4a17-9c83-ed2418defac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] Hi, I met an error when I used t...</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] Hi,\\nI’m attempting to run HISAT...</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] submitting a job to a SGE 8.1.9 ...</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] I need a tool which can change t...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] hi\\ni am working with galaxy for...</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] hello,\\n.\\ni am working with can...</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] I have been trying to create a G...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] I am trying to follow this trans...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] Hello.  Two questions\\n\\nDoes an...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>\\n\\n&lt;s&gt;[INST] Good Morning! I hereby inform yo...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tokens\n",
       "0     \\n\\n<s>[INST] Hi, I met an error when I used t...     478\n",
       "1     \\n\\n<s>[INST] Hi,\\nI’m attempting to run HISAT...     909\n",
       "2     \\n\\n<s>[INST] submitting a job to a SGE 8.1.9 ...    1128\n",
       "3     \\n\\n<s>[INST] I need a tool which can change t...     125\n",
       "4     \\n\\n<s>[INST] hi\\ni am working with galaxy for...     290\n",
       "...                                                 ...     ...\n",
       "1251  \\n\\n<s>[INST] hello,\\n.\\ni am working with can...     214\n",
       "1252  \\n\\n<s>[INST] I have been trying to create a G...     175\n",
       "1253  \\n\\n<s>[INST] I am trying to follow this trans...     250\n",
       "1254  \\n\\n<s>[INST] Hello.  Two questions\\n\\nDoes an...     394\n",
       "1255  \\n\\n<s>[INST] Good Morning! I hereby inform yo...     211\n",
       "\n",
       "[1256 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_dataframe = pd.read_csv(\"../data/conversations-galaxy-q-a.csv\", sep=\"\\t\")\n",
    "conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb32a8a-140d-4161-861d-3c144e46bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c691a856-202c-4937-9ca2-5dc6088aec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(conv_dataframe).train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caad485f-1d7c-4519-8b26-356b7ba99b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "new_model = \"llama-2-3b-galaxy-help\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype #torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#Resize the embeddings\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "#Configure the pad token in the model\n",
    "#model.config.pad_token_id = tokenizer.pad_token_id\n",
    "#model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameter efficient model ...\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "PEFT loading time: 0.15964317321777344 seconds\n",
      "Setting up Training arguments ...\n",
      "Setting up SFTTrainer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1004/1004 [00:00<00:00, 1838.19 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 2263.46 examples/s]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTTrainer setting up time: 0.9094860553741455 seconds\n",
      "Start training ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1255' max='1255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1255/1255 8:47:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.669400</td>\n",
       "      <td>2.488999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.389500</td>\n",
       "      <td>2.275794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.204100</td>\n",
       "      <td>2.205792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.160600</td>\n",
       "      <td>2.175745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.164300</td>\n",
       "      <td>2.156597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.056600</td>\n",
       "      <td>2.137323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.128200</td>\n",
       "      <td>2.126042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.074900</td>\n",
       "      <td>2.117802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.157600</td>\n",
       "      <td>2.108902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.104300</td>\n",
       "      <td>2.103289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.015500</td>\n",
       "      <td>2.098771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.996700</td>\n",
       "      <td>2.091319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.023600</td>\n",
       "      <td>2.085798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.003500</td>\n",
       "      <td>2.084075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.039800</td>\n",
       "      <td>2.081419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.985400</td>\n",
       "      <td>2.078114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.040600</td>\n",
       "      <td>2.070205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>2.066294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.988700</td>\n",
       "      <td>2.060286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.998900</td>\n",
       "      <td>2.057756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.877800</td>\n",
       "      <td>2.058093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.932000</td>\n",
       "      <td>2.064125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.891700</td>\n",
       "      <td>2.062751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.930700</td>\n",
       "      <td>2.056555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.905400</td>\n",
       "      <td>2.054170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.930000</td>\n",
       "      <td>2.052633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.905700</td>\n",
       "      <td>2.052237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.879300</td>\n",
       "      <td>2.049771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.966600</td>\n",
       "      <td>2.045280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.868100</td>\n",
       "      <td>2.042735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.841100</td>\n",
       "      <td>2.053450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>2.053549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.910800</td>\n",
       "      <td>2.052312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.817000</td>\n",
       "      <td>2.056689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.747500</td>\n",
       "      <td>2.056017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.820900</td>\n",
       "      <td>2.055055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.841600</td>\n",
       "      <td>2.052969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.888500</td>\n",
       "      <td>2.048048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>2.045763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.878500</td>\n",
       "      <td>2.043653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.749400</td>\n",
       "      <td>2.062562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.668100</td>\n",
       "      <td>2.061251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.746000</td>\n",
       "      <td>2.070878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.747100</td>\n",
       "      <td>2.064491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.786200</td>\n",
       "      <td>2.066810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.779000</td>\n",
       "      <td>2.066617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.750900</td>\n",
       "      <td>2.056399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>2.064384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>2.058685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.808700</td>\n",
       "      <td>2.055054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1255, training_loss=1.941590461882937, metrics={'train_runtime': 31682.2547, 'train_samples_per_second': 0.158, 'train_steps_per_second': 0.04, 'total_flos': 4.777670765091226e+16, 'train_loss': 1.941590461882937, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_config, prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "#or\n",
    "#target_modules = ['q_proj','v_proj', 'k_proj', 'o_proj']\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "print(\"Extracting parameter efficient model ...\")\n",
    "s_time = time.time()\n",
    "refined_model = prepare_model_for_kbit_training(model)\n",
    "refined_model = get_peft_model(refined_model, peft_config)\n",
    "e_time = time.time()\n",
    "refined_model.print_trainable_parameters()\n",
    "print(\"PEFT loading time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "base_dir = \"llama\"\n",
    "\n",
    "'''per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\"'''\n",
    "\n",
    "print(\"Setting up Training arguments ...\")\n",
    "\n",
    "# Set training parameters\n",
    "'''training_params = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"tensorboard\"\n",
    ")'''\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    optim=\"adamw_hf\", #\"paged_adamw_32bit\",\n",
    "    save_steps=200, #change to 500\n",
    "    logging_steps=25, #change to 100\n",
    "    learning_rate=1e-4,\n",
    "    eval_steps=25, #change to 200\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=5, # remove \"#\"\n",
    "    #max_steps=10, #remove this\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "print(\"Setting up SFTTrainer ...\")\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "\n",
    "'''trainer = SFTTrainer(\n",
    "    refined_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=128,\n",
    "    args=training_args,\n",
    ")'''\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "'''trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    peft_config=peft_args,\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    #packing=False,\n",
    ")'''\n",
    "\n",
    "e_time = time.time()\n",
    "print(\"SFTTTrainer setting up time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "print(\"Start training ...\")\n",
    "trainer.train()\n",
    "\n",
    "#trainer.model.save_pretrained(new_model)\n",
    "# move this config to checkpoint folder for model reconstruction\n",
    "#refined_model.config.to_json_file(\"saved-config/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8beb65da-77cf-40f3-b8f8-e48acc48d203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 4000;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "log_dir = \"llama-qlora/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e144ac-e1b3-4cd0-ac5f-5ccc6d9436e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "I am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\n",
      "When I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\n",
      "uvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\n",
      "I don’t understand this error. Could someone please help me out?\n",
      "I am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\n",
      "Thanks!\n",
      "\n",
      "<s> ### Instruction:\n",
      "  Hello!\n",
      "I am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\n",
      "When I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\n",
      "uvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\n",
      "I don’t understand this error. Could someone please help me out?\n",
      "I am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\n",
      "Thanks!. \n",
      "  \n",
      "\n",
      "  ### Response: \n",
      "Hi @user\n",
      "@quote\n",
      "I could not resolve this issue yet and is still running.\n",
      "try  to rerun the job. If it fails you can give the bam and the sample.FASTA the job was running on a few days ago to me via email.\n",
      "Thanks!\n",
      "Lefteris J. Iliadis – Apr 12 2022 12:07 AM\n",
      "@quote\n",
      "when i try with the hg38 build , i can´t manage to run the deep variant\n",
      "try this genome build, is valid for the reference-based analysis.\n",
      "\n",
      "\n",
      "\n",
      "Galaxy Help (@link https://galaxyproject.org/support/#having-trouble\n",
      "\n",
      "Hi @user\n",
      "@quote\n",
      "I could not resolve this issue yet and is still running.\n",
      "try  to rerun the job. If it fails you can give the bam and the sample.FASTA the job was running on a few days ago to me via email.\n",
      "Thanks!\n",
      "Lefteris J. Iliadis – Apr 12 2022 12:07 AM\n",
      "@quote\n",
      "when i try with the hg38 build , i can´t manage to run the deep variant\n",
      "try this genome build, is valid for the reference-based analysis.\n",
      "\n",
      "\n",
      "\n",
      "Galaxy Help (@link https://galaxyproject.org/support/#having-trouble\n",
      "==============================\n",
      "Dear Sir,\n",
      "Kindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\n",
      "Is it ok??? or did I out something wrong\n",
      "Kindly help\n",
      "\n",
      "<s> ### Instruction:\n",
      "  Dear Sir,\n",
      "Kindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\n",
      "Is it ok??? or did I out something wrong\n",
      "Kindly help. \n",
      "  \n",
      "\n",
      "  ### Response: \n",
      "The run is most likely running very fast.\n",
      "\n",
      "\n",
      "\n",
      "Greetings, \n",
      "Steady state runs usually take around 15 minutes on the most powerful cluster at UMD, and often longer for larger genomes. There is some discussion about the cluster configuration here  (@link https://github.com/galaxyproject/csc-sloth/issues/558) and I think you might find it informative. The cluster can also be accessed anywhere in the globe on-demand, and I believe you can access UMD’s cluster there as well.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "\n",
      "\n",
      "Galaxy Team \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Related to this thread:  Tr\n",
      "\n",
      "The run is most likely running very fast.\n",
      "\n",
      "\n",
      "\n",
      "Greetings, \n",
      "Steady state runs usually take around 15 minutes on the most powerful cluster at UMD, and often longer for larger genomes. There is some discussion about the cluster configuration here  (@link https://github.com/galaxyproject/csc-sloth/issues/558) and I think you might find it informative. The cluster can also be accessed anywhere in the globe on-demand, and I believe you can access UMD’s cluster there as well.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "\n",
      "\n",
      "Galaxy Team \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Related to this thread:  Tr\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "'''user_template = \"\"\" Below is a question asked by a user: \\n\n",
    "### Instruction:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "system_template = \"\"\"### Response by the system to the above instruction: \\n\n",
    "{}\n",
    "\"\"\"'''\n",
    "\n",
    "'''\n",
    "user_template = \"\"\"### Instruction:{}\"\"\"\n",
    "\n",
    "system_template = \"\"\"### Response: {}\"\"\"\n",
    "'''\n",
    "\n",
    "# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\"### Instruction:\n",
    "  {}. \n",
    "  \\n\n",
    "  ### Response: \\n\"\"\".format(test)\n",
    "\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = refined_model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           #top_p=0.9,\n",
    "                           #temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response: \\n'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "I am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\n",
      "When I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\n",
      "uvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\n",
      "I don’t understand this error. Could someone please help me out?\n",
      "I am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\n",
      "Thanks!\n",
      "\n",
      "<s> ### Instruction: Hello!\n",
      "I am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\n",
      "When I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\n",
      "uvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\n",
      "I don’t understand this error. Could someone please help me out?\n",
      "I am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\n",
      "Thanks!.\n",
      "  ### Response: \n",
      "\n",
      "Thanks so much for the detailed error! This will help us get you up.\n",
      "\n",
      "The job was running for 2 hours on @link http://usegalaxy.org and 20 minutes on @link http://usegalaxy.eu. The error should end the job at either instance. I will leave and queue it back up at both places for a later investigation of the discrepancy.\n",
      "\n",
      "The tool itself runs, so this is not an immediate galaxy-api bug. You are likely encountering some sort of transient error at the local instance. I see a number of cluster-related messages on the local UI. One of those might be the root cause. Please retry and take note of any messages from the local UI\n",
      "\n",
      "\n",
      "Thanks so much for the detailed error! This will help us get you up.\n",
      "\n",
      "The job was running for 2 hours on @link http://usegalaxy.org and 20 minutes on @link http://usegalaxy.eu. The error should end the job at either instance. I will leave and queue it back up at both places for a later investigation of the discrepancy.\n",
      "\n",
      "The tool itself runs, so this is not an immediate galaxy-api bug. You are likely encountering some sort of transient error at the local instance. I see a number of cluster-related messages on the local UI. One of those might be the root cause. Please retry and take note of any messages from the local UI\n",
      "==============================\n",
      "Dear Sir,\n",
      "Kindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\n",
      "Is it ok??? or did I out something wrong\n",
      "Kindly help\n",
      "\n",
      "<s> ### Instruction: Dear Sir,\n",
      "Kindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\n",
      "Is it ok??? or did I out something wrong\n",
      "Kindly help.\n",
      "  ### Response: \n",
      "Welcome @user\n",
      "It is normal for a de novo assembly job to take longer to complete.\n",
      "Every so often the queue at @link http://usegalaxy.org will adjust and jobs will move up/down in the work queue. This may be due to various factors such as:\n",
      "\n",
      "More active users = jobs move into deeper/sparser node queues\n",
      "\n",
      "The server is busy processing prior jobs = jobs move into the queued state waiting to start with fewer node queues defined\n",
      "\n",
      "Network conditions changed, causing the submit fails/restart\n",
      "\n",
      "The job was rerun with changed parameters\n",
      "\n",
      "The original job failed for some reason and was rerun with the same or changed parameters\n",
      "\n",
      "\n",
      "All of these are possible for\n",
      "\n",
      "Welcome @user\n",
      "It is normal for a de novo assembly job to take longer to complete.\n",
      "Every so often the queue at @link http://usegalaxy.org will adjust and jobs will move up/down in the work queue. This may be due to various factors such as:\n",
      "\n",
      "More active users = jobs move into deeper/sparser node queues\n",
      "\n",
      "The server is busy processing prior jobs = jobs move into the queued state waiting to start with fewer node queues defined\n",
      "\n",
      "Network conditions changed, causing the submit fails/restart\n",
      "\n",
      "The job was rerun with changed parameters\n",
      "\n",
      "The original job failed for some reason and was rerun with the same or changed parameters\n",
      "\n",
      "\n",
      "All of these are possible for\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "### Original Llama2 model output\n",
    "\n",
    "# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\"### Instruction: {}.\n",
    "  ### Response: \\n\"\"\".format(test)\n",
    "\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           #top_p=0.9,\n",
    "                           #temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response: \\n'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from transformers import LlamaTokenizer, LlamaForCausalLM\\n\\nsaved_model_path = 'llama/checkpoint-2'\\nre_tokenizer = LlamaTokenizer.from_pretrained(saved_model_path)\\nre_model = LlamaForCausalLM.from_pretrained(saved_model_path, device_map='auto')\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload PEFT fined tuned model\n",
    "# https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo\n",
    "'''from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "saved_model_path = 'llama/checkpoint-2'\n",
    "re_tokenizer = LlamaTokenizer.from_pretrained(saved_model_path)\n",
    "re_model = LlamaForCausalLM.from_pretrained(saved_model_path, device_map='auto')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5180ec96-29f3-42f0-803e-bed74c873cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# test fine-tuned model\\n# Number 1000, 1001 from data.json\\n\\n#re_tokenizer.pad_token = \"[PAD]\"\\nre_tokenizer.add_special_tokens({\\'pad_token\\': \\'[PAD]\\'})\\nre_tokenizer.padding_side = \"left\"\\n\\ntest_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\\n\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\\n               ]\\n\\n#response_test_strings = [\\n#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \\n# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\\n\\npredictions = []\\n\\nfor test in test_strings:\\n  prompt = \"\"\" Below is a question asked by a user: \\n\\n  ### Instruction:\\n  {}. \\n  ### Response by the system to the above instruction:\"\"\".format(test)\\n\\n  input_ids = re_tokenizer.encode(prompt, return_tensors=\"pt\").to(\\'cuda\\')\\n\\n  outputs = re_model.generate(input_ids=input_ids, \\n                           max_new_tokens=156, \\n                           do_sample=True, \\n                           top_p=0.9,\\n                           temperature=0.9\\n                          )\\n  predictions.append(tokenizer.decode(outputs[0]))\\n    \\n\\ndef extract_response_text(input_string):\\n    start_marker = \\'Response by the system to the above instruction:\\'\\n    end_marker = \\'###\\'\\n    start_index = input_string.find(start_marker)\\n    if start_index == -1:\\n        return None\\n    \\n    start_index += len(start_marker)\\n    \\n    end_index = input_string.find(end_marker, start_index)\\n    if end_index == -1:\\n        return input_string[start_index:]\\n    \\n    return input_string[start_index:end_index].strip()\\n\\nfor i in range(len(test_strings)): \\n  pred = predictions[i]\\n  text = test_strings[i]\\n  print(text+\\'\\n\\')\\n  print(pred+\\'\\n\\')\\n  print(extract_response_text(pred))\\n  print(\"==============================\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "#re_tokenizer.pad_token = \"[PAD]\"\n",
    "re_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "re_tokenizer.padding_side = \"left\"\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\" Below is a question asked by a user: \\n\n",
    "  ### Instruction:\n",
    "  {}. \n",
    "  ### Response by the system to the above instruction:\"\"\".format(test)\n",
    "\n",
    "  input_ids = re_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = re_model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           top_p=0.9,\n",
    "                           temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response by the system to the above instruction:'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f9de-57e2-46fe-93e0-26c09a68027c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1874-733f-4cb1-819f-b9d765d7b8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab312b-de95-42cd-8150-ebd676dabcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
