{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8f95bd-7224-4a1c-b350-5fab83141e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b063814-eadf-4a17-9c83-ed2418defac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tokens\n",
       "0     Act like Bioinformatician who uses Galaxy plat...     462\n",
       "1     Act like Bioinformatician who uses Galaxy plat...     143\n",
       "2     Act like Bioinformatician who uses Galaxy plat...     302\n",
       "3     Act like Bioinformatician who uses Galaxy plat...     204\n",
       "4     Act like Bioinformatician who uses Galaxy plat...     445\n",
       "...                                                 ...     ...\n",
       "1122  Act like Bioinformatician who uses Galaxy plat...     232\n",
       "1123  Act like Bioinformatician who uses Galaxy plat...     187\n",
       "1124  Act like Bioinformatician who uses Galaxy plat...     262\n",
       "1125  Act like Bioinformatician who uses Galaxy plat...     383\n",
       "1126  Act like Bioinformatician who uses Galaxy plat...     234\n",
       "\n",
       "[1127 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_dataframe = pd.read_csv(\"../data/conversations-galaxy-q-a.csv\", sep=\"\\t\")\n",
    "# all-conv-galaxy-q-a.csv # conversations-galaxy-q-a.csv\n",
    "# conversations-galaxy-q-a.csv\n",
    "conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb32a8a-140d-4161-861d-3c144e46bb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1127"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c691a856-202c-4937-9ca2-5dc6088aec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "tr_index = 20\n",
    "final_index = len(conv_dataframe)\n",
    "tr_conv = conv_dataframe[:tr_index]\n",
    "eval_conv = conv_dataframe[tr_index + 1: final_index]\n",
    "dataset = Dataset.from_pandas(tr_conv).train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624b2cbc-6692-4fd6-8969-a5b8d5ffacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_conv.to_csv(\"../data/eval_dataset.csv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a358fd-e495-46ad-9b01-eec5fe4ee5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-08 15:18:15,711] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caad485f-1d7c-4519-8b26-356b7ba99b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:22<00:00, 11.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameter efficient model ...\n",
      "trainable params: 162,217,984 || all params: 6,900,633,600 || trainable%: 2.350769413405749\n",
      "PEFT loading time: 4.816115617752075 seconds\n",
      "Setting up Training arguments ...\n",
      "Setting up SFTTrainer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 559.27 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 383.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTTrainer setting up time: 0.18945598602294922 seconds\n",
      "Start training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/centos/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 01:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.518900</td>\n",
       "      <td>3.087666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=3.5188612937927246, metrics={'train_runtime': 94.4784, 'train_samples_per_second': 0.169, 'train_steps_per_second': 0.021, 'total_flos': 135272832565248.0, 'train_loss': 3.5188612937927246, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_config, prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "#or\n",
    "#target_modules = ['q_proj','v_proj', 'k_proj', 'o_proj']\n",
    "#target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "print(\"Extracting parameter efficient model ...\")\n",
    "s_time = time.time()\n",
    "refined_model = prepare_model_for_kbit_training(model)\n",
    "refined_model = get_peft_model(refined_model, peft_config)\n",
    "e_time = time.time()\n",
    "refined_model.print_trainable_parameters()\n",
    "print(\"PEFT loading time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "base_dir = \"llama-test-galaxy-conv-dec-8-1\"\n",
    "\n",
    "print(\"Setting up Training arguments ...\")\n",
    "\n",
    "#optimizer = AdamW(refined_model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_steps=2,\n",
    "    logging_steps=2,\n",
    "    learning_rate=learning_rate,\n",
    "    eval_steps=2,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=1, # remove \"#\"\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "print(\"Setting up SFTTrainer ...\")\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=refined_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=700,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "e_time = time.time()\n",
    "print(\"SFTTTrainer setting up time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "print(\"Start training ...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69142c5-522c-4097-9669-296bc31f09db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be91d9ef-10f2-49eb-9632-62a154989aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1106 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tokens\n",
       "0     Act like Bioinformatician who uses Galaxy plat...     301\n",
       "1     Act like Bioinformatician who uses Galaxy plat...     410\n",
       "2     Act like Bioinformatician who uses Galaxy plat...     185\n",
       "3     Act like Bioinformatician who uses Galaxy plat...     563\n",
       "4     Act like Bioinformatician who uses Galaxy plat...     173\n",
       "...                                                 ...     ...\n",
       "1101  Act like Bioinformatician who uses Galaxy plat...     232\n",
       "1102  Act like Bioinformatician who uses Galaxy plat...     187\n",
       "1103  Act like Bioinformatician who uses Galaxy plat...     262\n",
       "1104  Act like Bioinformatician who uses Galaxy plat...     383\n",
       "1105  Act like Bioinformatician who uses Galaxy plat...     234\n",
       "\n",
       "[1106 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "eval_conv = pd.read_csv(\"../data/eval_dataset.csv\", sep=\"\\t\")\n",
    "eval_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b8510d-7f36-4095-bcfd-978a164ad72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20231208-152029'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "filedt = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filedt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26875e21-fb00-46d5-a7dd-2523276596a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am trying to assemble a plant genome using ont data with flye. prior to launch the assembly with all my data (about 37 gb in fastqsanger.gz) i recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. i then launched with all the available sequences (37 gb in fastqsanger.gz) and it is now been running for 5 days. i am just wondering if i am using more ressources than allowed for this tool or if it is still running. what makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ? thank you very much. ben [/INST]\n",
      "Instruction: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am trying to assemble a plant genome using ont data with flye. prior to launch the assembly with all my data (about 37 gb in fastqsanger.gz) i recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. i then launched with all the available sequences (37 gb in fastqsanger.gz) and it is now been running for 5 days. i am just wondering if i am using more ressources than allowed for this tool or if it is still running. what makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ? thank you very much. ben\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "we updated the funannotate databases and provided more cores to this tool. you can see how many cores and memory every tools gets here: infrastructure-playbook/tool_destinations.yaml at master · usegalaxy-eu/infrastructure-playbook github (@link https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml#l196)\n",
      "\n",
      "encoding prompt number 1...\n",
      "generating response number 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "extracted_answers = []\n",
    "\n",
    "start_marker = 'Act like'#'[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"\"\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index:end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"Prompt: \\n\")\n",
    "    print(prompt)\n",
    "    print(\"Instruction: \\n\")\n",
    "    print(instruction)\n",
    "    print()\n",
    "    print(\"Ground truth answer: \\n\")\n",
    "    print(original_answer)\n",
    "    print()\n",
    "    print(\"encoding prompt number {}...\".format(ri+1))\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response number {} ...\".format(ri+1))\n",
    "    outputs = refined_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = tokenizer.decode(outputs[0])\n",
    "    extracted_pred = pred[pred.find(end_marker) + len(end_marker): len(pred)].strip()\n",
    "    predictions.append(pred)\n",
    "    extracted_answers.append(extracted_pred)\n",
    "    print(\"Generated answer: \\n\")\n",
    "    print(extracted_pred)\n",
    "    print()\n",
    "    print(\"====================\")\n",
    "    if ri == 5:\n",
    "        break\n",
    "\n",
    "output_file_name = \"generated_answers_peft_{}_{}\".format(base_dir, filedt)\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, extracted_answers, predictions), columns=[\"Instructions\", \"Ground truth answers\", \"Predicted answers\", \"Full generated answers\"])\n",
    "pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)\n",
    "\n",
    "e_time = time.time()\n",
    "\n",
    "print(\"Finished generation in {} seconds\".format(e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb65da-77cf-40f3-b8f8-e48acc48d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60865ccb-d84f-4121-8fa0-9dfd9a374a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90260d-0ac7-4083-a683-3d2a32baaf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdeaf58-5fe3-480f-a4d7-5690fa07a7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e144ac-e1b3-4cd0-ac5f-5ccc6d9436e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180ec96-29f3-42f0-803e-bed74c873cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f9de-57e2-46fe-93e0-26c09a68027c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1874-733f-4cb1-819f-b9d765d7b8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab312b-de95-42cd-8150-ebd676dabcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
