{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a1cb9-8f75-43d4-8b19-36c59e55c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-26 12:04:03,221] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215a1910-5135-4eba-91be-31d1c86e9e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.19s/it]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/peft/main/en/tutorial/peft_model_config\n",
    "\n",
    "import deepspeed\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "\n",
    "\n",
    "base_dir = \"llama-linear-layers-all-conv-Feb-19-24-1\"\n",
    "checkpoint_number = \"checkpoint-2500\"\n",
    "checkpoint_path = \"{}/{}\".format(base_dir, checkpoint_number)\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model, checkpoint_path)\n",
    "#peft_model = peft_model.merge_and_unload()\n",
    "restored_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1b2484-3cd7-43f3-9d31-a540e9ccae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes from 0.41.1 to 0.43.0\n",
    "# transformers from 4.36.2 to 4.37.0\n",
    "peft_model.save_pretrained(\"saved_model_hub/fine-tuned-gllm\") #saved_model_hub/fine-tuned-gllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17328d64-01ef-4370-917f-6386e16ecb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 649M/649M [00:25<00:00, 25.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/anuprulez/fine-tuned-gllm/commit/c5a3ec244a88e23f64089cd231fba454b6d3df4f', commit_message='Upload model', commit_description='', oid='c5a3ec244a88e23f64089cd231fba454b6d3df4f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.push_to_hub(\"anuprulez/fine-tuned-gllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45596c1-ca99-4184-a091-2d7d086e5cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddf4e7-2fda-41b4-b549-ccc76011530e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e52bc-6d37-465e-a322-ee8253a60c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb5206-cde3-4555-83f8-22720a725d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602d6aa-45d9-4636-8953-0e68e00a2320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980055f-3c52-4580-a853-9b2c8d49073b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78049e-d09d-4595-9c85-cf7c0ee6a122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e258237-2402-4f88-b757-8c827f50572f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1d973-4489-4dac-abc5-58d4dc854805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
