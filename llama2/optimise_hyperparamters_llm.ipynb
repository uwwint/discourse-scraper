{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3f88f8-a3f8-418f-9ba0-06afa22c2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/miniconda/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-11 12:04:37,775] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Size of Galaxy conversation data: 1122\n",
      "Size of Biostars conversation data: 3782\n",
      "Size of tr/te: 200/49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/miniconda/envs/finetune-gllm/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/hf_cache/'\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Load conversation dataset\n",
    "max_token_size = 700\n",
    "\n",
    "galaxy_conv_dataframe = pd.read_csv(\"../data/conversations-galaxy-q-a.csv\", sep=\"\\t\")\n",
    "galaxy_conv_dataframe = galaxy_conv_dataframe[galaxy_conv_dataframe[\"tokens\"] <= max_token_size]\n",
    "\n",
    "biostar_conv_dataframe = pd.read_csv(\"../data/conversations-biostars-q-a.csv\", sep=\"\\t\")\n",
    "biostar_conv_dataframe = biostar_conv_dataframe[biostar_conv_dataframe[\"tokens\"] <= max_token_size]\n",
    "\n",
    "print(\"Size of Galaxy conversation data: {}\".format(len(galaxy_conv_dataframe)))\n",
    "print(\"Size of Biostars conversation data: {}\".format(len(biostar_conv_dataframe)))\n",
    "\n",
    "# Split dataset into training and evaluation sets, but only for Galaxy conversations\n",
    "tr_index = 200\n",
    "final_index = 250 #len(galaxy_conv_dataframe)\n",
    "tr_conv = galaxy_conv_dataframe[:tr_index]\n",
    "eval_conv = galaxy_conv_dataframe[tr_index + 1: final_index]\n",
    "\n",
    "#biostar_conv_dataframe = biostar_conv_dataframe[:20]\n",
    "# combine tr_conv with biostars data for training\n",
    "#tr_conv = pd.concat([tr_conv, biostar_conv_dataframe], axis=0)\n",
    "\n",
    "print(\"Size of tr/te: {}/{}\".format(len(tr_conv), len(eval_conv)))\n",
    "dataset = Dataset.from_pandas(tr_conv).train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Save evaluation dataset to a CSV file\n",
    "eval_conv.to_csv(\"../data/eval_dataset.csv\", sep=\"\\t\", index=None)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c7ae4e-9d0f-41b8-8669-5090a1be3e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "compute_dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache=True, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9f0ecc-1b60-4ea1-b2dd-0bf417e91eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameter efficient model ...\n",
      "trainable params: 162,217,984 || all params: 6,900,633,600 || trainable%: 2.350769413405749\n",
      "PEFT loading time: 2.6068170070648193 seconds\n"
     ]
    }
   ],
   "source": [
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "#target_modules = ['q_proj','v_proj', 'k_proj', 'o_proj']\n",
    "#target_modules = [\"q_proj\",\"v_proj\"]\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(lora_alpha=32, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "                          target_modules=target_modules)\n",
    "\n",
    "print(\"Extracting parameter efficient model ...\")\n",
    "start_time = time.time()\n",
    "refined_model = get_peft_model(prepare_model_for_kbit_training(model), peft_config)\n",
    "end_time = time.time()\n",
    "refined_model.print_trainable_parameters()\n",
    "print(f\"PEFT loading time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99be616d-3287-4767-b827-6e9581472305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Training arguments ...\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"llama-linear-layers-all-conv-June-11-24\"\n",
    "\n",
    "print(\"Setting up Training arguments ...\")\n",
    "\n",
    "log_steps = 100\n",
    "save_steps = 100\n",
    "\n",
    "# Set up training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=6,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=log_steps,\n",
    "    eval_steps=log_steps,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9768c0-aaa9-4e07-83aa-bf42f780b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f793a-acd4-4ccb-8703-c61dbc142c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SFTTrainer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:00<00:00, 3389.94 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 2702.82 examples/s]\n",
      "/mnt/miniconda/envs/finetune-gllm/lib/python3.9/site-packages/transformers/trainer.py:370: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n",
      "[I 2024-06-11 13:39:07,658] A new study created in memory with name: no-name-90f7af65-1d7d-4750-950c-ce25a5f77761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTTrainer setting up time: 0.22990632057189941 seconds\n",
      "Start Hyperparameter search ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/miniconda/envs/finetune-gllm/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65/135 02:04 < 02:18, 0.51 it/s, Epoch 2.37/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/huggingface/trl/issues/953\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "print(\"Setting up SFTTrainer ...\")\n",
    "\n",
    "start_time = time.time()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache=True, device_map=\"auto\")\n",
    "def model_init_hs(trial):\n",
    "    #base_model_hs = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "    #peft_config_hs = LoraConfig(lora_alpha=32, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=target_modules)\n",
    "    return refined_model #get_peft_model(base_model_hs, peft_config_hs)\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=refined_model,\n",
    "    model_init=model_init_hs,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=max_token_size,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "'''trainer = Trainer(\n",
    "    model=None,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    #dataset_text_field=\"conversations\",\n",
    "    #max_seq_length=max_token_size,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    model_init=model_init_hs,\n",
    ")'''\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"SFTTTrainer setting up time: {end_time - start_time} seconds\")\n",
    "\n",
    "print(\"Start Hyperparameter search ...\")\n",
    "\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True),\n",
    "        \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.01, 0.5),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 1e-4, 1e-1, log=True),\n",
    "    }\n",
    "\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5\n",
    ")\n",
    "\n",
    "#trainer.train()\n",
    "#trainer.save_model()\n",
    "\n",
    "print(\"Finished Hyperparameter search ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48bffca-f646-46db-abc5-8be1f6583757",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e522bd-e0da-485e-9991-a1d252252a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = base_dir + \"/best_params.txt\"\n",
    "with open(save_path) as f:\n",
    "    f.write(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2156e2-ce04-4336-964a-f1f6619ade30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def optuna_hp_space(trial):\\n    return {\\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True),\\n        \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.01, 0.5),\\n        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 1e-4, 1e-1, log=True),\\n    }\\n    \\n\\nbest_trial = trainer.hyperparameter_search(\\n    direction=\"maximize\",\\n    backend=\"optuna\",\\n    hp_space=optuna_hp_space,\\n    n_trials=20,\\n    #compute_objective=compute_objective,\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "'''\n",
    "\n",
    "'''def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True),\n",
    "        \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.01, 0.5),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 1e-4, 1e-1, log=True),\n",
    "    }\n",
    "    \n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=20,\n",
    "    #compute_objective=compute_objective,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be69414-4d3c-450a-ae96-390806e53993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd31853-a188-451f-aa65-34a83112fce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ac580-6363-4b7e-bc9b-c04f0f77ebdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36b4bb-ba21-4ee0-960a-17d4828a8e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5acb4-a2eb-4f24-9881-78a8f718e979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
