{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea337bcc-077e-4f0c-833a-cab97da6a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "saved_path = \"saved-model/saved_model_20231201-145315\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "re_orig_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "re_orig_model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588e7a3-11e0-4076-92c1-3afdbb31af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_orig_model.load_state_dict(torch.load(saved_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05dbf2-567e-4584-957a-5d140eeb0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "\n",
    "start_marker = '<s>[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"</s>\"\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index + len(start_marker):end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"encoding prompt ...\")\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response ...\")\n",
    "    outputs = refined_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=156,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = tokenizer.decode(outputs[0])\n",
    "    predictions.append(pred)\n",
    "    break\n",
    "\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, predictions), columns=[\"instructions\", \"ground truth answers\", \"generated answers\"])\n",
    "pred_dataframe.to_csv(\"../data/generated_answers.csv\", sep=\"\\t\", index=None)\n",
    "\n",
    "print(\"Finished generation =======\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
