{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247c512b-f208-4b5f-b695-3b238dcdf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 14 14:23:46 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   62C    P0              30W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c691a856-202c-4937-9ca2-5dc6088aec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tensor_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         conversations  tensor_size\n",
       "0    Below is an instruction that describes a task....          103\n",
       "1    Below is an instruction that describes a task....          199\n",
       "2    Below is an instruction that describes a task....          156\n",
       "3    Below is an instruction that describes a task....          137\n",
       "4    Below is an instruction that describes a task....          189\n",
       "..                                                 ...          ...\n",
       "255  Below is an instruction that describes a task....          154\n",
       "256  Below is an instruction that describes a task....          163\n",
       "257  Below is an instruction that describes a task....           54\n",
       "258  Below is an instruction that describes a task....           82\n",
       "259  Below is an instruction that describes a task....           58\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conv_dataframe = pd.read_csv(\"../data/small-conversations.csv\", sep=\"\\t\")\n",
    "conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef11d0-36b7-456c-9661-1c3c2304f3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9bb338-3f61-4154-bfbe-c474a8713842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "#conv_dataframe = conv_dataframe[:900]\n",
    "dataset = Dataset.from_pandas(conv_dataframe).train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caad485f-1d7c-4519-8b26-356b7ba99b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 3200, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8640, out_features=3200, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3200, out_features=32000, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "from transformers import (LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig, AutoModelForCausalLM,\n",
    "    AutoTokenizer)\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    load_in_8bit=True,\n",
    "    device_map='auto', \n",
    "    # If passing a string for `device_map`, please choose 'auto', 'balanced', 'balanced_low_0' or 'sequential'.\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model_modules = str(model.modules)\n",
    "print(model_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984eda1-a0be-439c-98b5-54dc7a8164ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283c2dfd-17cd-4143-aff3-7c73ccaa637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LlamaTokenizer(name_or_path='openlm-research/open_llama_3b_v2', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),\n",
       " 32000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940169-d8db-4899-a767-8e0ffaa1015a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c2617-95d6-43e3-9da1-f8e00ae39094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe83d5c-896a-4f93-af09-6b0e4612358f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a259efd9-7d59-4a0c-97e8-9ce19c9fdda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gate_proj',\n",
       " 'k_proj',\n",
       " 'lm_head',\n",
       " 'o_proj',\n",
       " 'q_proj',\n",
       " 'v_proj',\n",
       " 'up_proj',\n",
       " 'down_proj']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "model_modules = str(model.modules)\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "names = []\n",
    "# Print the names of the Linear layers\n",
    "for name in linear_layer_names:\n",
    "    names.append(name)\n",
    "target_modules = list(set(names))\n",
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameter efficient model ...\n",
      "trainable params: 25,989,120 || all params: 3,452,462,720 || trainable%: 0.7527704745208661\n",
      "PEFT loading time: 0.9485962390899658 seconds\n",
      "Setting up Training arguments ...\n",
      "Setting up SFTTrainer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 208/208 [00:00<00:00, 747.54 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 494.57 examples/s]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTTrainer setting up time: 0.4101696014404297 seconds\n",
      "Start training ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 07:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.190400</td>\n",
       "      <td>3.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.101000</td>\n",
       "      <td>2.998047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training ...\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "#target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'] # all linear layers\n",
    "#or\n",
    "#target_modules = ['q_proj','v_proj', 'k_proj', 'o_proj'] # attention layers\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #r=16\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"Extracting parameter efficient model ...\")\n",
    "s_time = time.time()\n",
    "refined_model = get_peft_model(model, lora_config)\n",
    "e_time = time.time()\n",
    "refined_model.print_trainable_parameters()\n",
    "print(\"PEFT loading time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "base_dir = \"llama\"\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\" \n",
    "\n",
    "print(\"Setting up Training arguments ...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs = 5.0,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    weight_decay=0.001,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=False, #True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    max_steps=-1,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "print(\"Setting up SFTTrainer ...\")\n",
    "\n",
    "s_time = time.time()\n",
    "trainer = SFTTrainer(\n",
    "    refined_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"conversations\",\n",
    "    args=training_args,\n",
    "    max_seq_length=200,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "e_time = time.time()\n",
    "print(\"SFTTTrainer setting up time: {} seconds\".format(e_time - s_time))\n",
    "\n",
    "print(\"Start training ...\")\n",
    "trainer.train()\n",
    "print(\"Finished training ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ff20a5-48c2-442a-a4e1-350f6797e2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from tensorboard import notebook\\nlog_dir = \"llama/runs\"\\nnotebook.start(\"--logdir {} --port 4001\".format(log_dir))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from tensorboard import notebook\n",
    "log_dir = \"llama/runs\"\n",
    "notebook.start(\"--logdir {} --port 4001\".format(log_dir))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e144ac-e1b3-4cd0-ac5f-5ccc6d9436e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "#test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "#\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "#               ]\n",
    "# \n",
    "\n",
    "test_strings = [\n",
    "    \"{ “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  Is there a list of exit codes somewhere so I can determine what went wrong?\",\n",
    "    \"Hoping to revive this. Same problem here. Ran Trim Galore on approx 50fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\",  \n",
    "]\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "    #prompt = \"\"\"<s> [INST] {} [/INST]\"\"\".format(test)\n",
    "    prompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction:\n",
    "    \n",
    "    {}\n",
    "    \n",
    "    ### Response: \\n\"\"\".format(test)\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    outputs = refined_model.generate(input_ids=input_ids, max_new_tokens=256)\n",
    "    predictions.append(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203d6dd9-6df9-4d36-b42e-91dfdf9c0d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  Is there a list of exit codes somewhere so I can determine what went wrong?\n",
      "\n",
      "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "    \n",
      "    ### Instruction:\n",
      "    \n",
      "    { “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  Is there a list of exit codes somewhere so I can determine what went wrong?\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    { “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  No, there is not a list of exit codes.\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    { “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  No, there is not a list of exit codes.\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    { “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  No, there is not a list of exit codes.\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    { “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “\n",
      "\n",
      "{ “code_desc”: “”, “desc”: “Fatal error: Exit code 126 ()”, “error_level”: 3, “exit_code”: 126, “type”: “exit_code” }  No, there is not a list of exit codes.\n",
      "==============================\n",
      "Hoping to revive this. Same problem here. Ran Trim Galore on approx 50fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\n",
      "\n",
      "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "    \n",
      "    ### Instruction:\n",
      "    \n",
      "    Hoping to revive this. Same problem here. Ran Trim Galore on approx 50fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    I have the same problem. I have tried to run Trim Galore on 50 fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\n",
      "    \n",
      "    ### Response: \n",
      "    \n",
      "    I have the same problem. I have tried to run Trim Galore on 50 fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\n",
      "    \n",
      "    \n",
      "\n",
      "I have the same problem. I have tried to run Trim Galore on 50 fastq files. 44 of them worked fine (produced output files). 3 of the paired files did not. So Sample1 R1 Sample1 R2 Sample2 R1 Sample2 R2 etc…did not work. The error for these say: Could not detect version of Python used by Cutadapt from the first line of Cutadapt (but found this: >>> The files look like they have the same data in header as the files that worked. Please help!\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "def extract_response_text(input_string):\n",
    "    start_marker = '### Response: \\n'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "    \n",
    "for i in range(len(test_strings)): \n",
    "    pred = predictions[i]\n",
    "    text = test_strings[i]\n",
    "    print(text+'\\n')\n",
    "    print(pred+'\\n')\n",
    "    print(extract_response_text(pred))\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36f733-52bc-4e40-b3bc-e8ac7e02c9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaebdb-596d-4b60-ad7f-ab266db42ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload PEFT fined tuned model\n",
    "# https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo\n",
    "'''from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "saved_model_path = 'llama/checkpoint-2'\n",
    "re_tokenizer = LlamaTokenizer.from_pretrained(saved_model_path)\n",
    "re_model = LlamaForCausalLM.from_pretrained(saved_model_path, device_map='auto')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180ec96-29f3-42f0-803e-bed74c873cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "#re_tokenizer.pad_token = \"[PAD]\"\n",
    "re_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "re_tokenizer.padding_side = \"left\"\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\" Below is a question asked by a user: \\n\n",
    "  ### Instruction:\n",
    "  {}. \n",
    "  ### Response by the system to the above instruction:\"\"\".format(test)\n",
    "\n",
    "  input_ids = re_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = re_model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           top_p=0.9,\n",
    "                           temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response by the system to the above instruction:'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f9de-57e2-46fe-93e0-26c09a68027c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1874-733f-4cb1-819f-b9d765d7b8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab312b-de95-42cd-8150-ebd676dabcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
