{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8f95bd-7224-4a1c-b350-5fab83141e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b063814-eadf-4a17-9c83-ed2418defac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>Below is a question asked by a user: \\n\\n### ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations\n",
       "0      Below is a question asked by a user: \\n\\n### ...\n",
       "1      Below is a question asked by a user: \\n\\n### ...\n",
       "2      Below is a question asked by a user: \\n\\n### ...\n",
       "3      Below is a question asked by a user: \\n\\n### ...\n",
       "4      Below is a question asked by a user: \\n\\n### ...\n",
       "...                                                 ...\n",
       "1406   Below is a question asked by a user: \\n\\n### ...\n",
       "1407   Below is a question asked by a user: \\n\\n### ...\n",
       "1408   Below is a question asked by a user: \\n\\n### ...\n",
       "1409   Below is a question asked by a user: \\n\\n### ...\n",
       "1410   Below is a question asked by a user: \\n\\n### ...\n",
       "\n",
       "[1411 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../out/data.json\") as fout:\n",
    "    raw_data = json.load(fout)\n",
    "\n",
    "user_template = \"\"\" Below is a question asked by a user: \\n\n",
    "### Instruction:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "system_template = \"\"\"### Response by the system to the above instruction: \\n\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "agg_conversations = []\n",
    "for idx_thread, thread in enumerate(raw_data):\n",
    "    conversations = \"\"\n",
    "    for idx_post in range(len(thread)):\n",
    "        post = thread[idx_post]\n",
    "        if post[\"role\"] == \"user\":\n",
    "            conversations += user_template.format(post[\"text\"])\n",
    "            conversations += \"\\n\"\n",
    "        if post[\"role\"] == \"system\":\n",
    "            conversations += system_template.format(post[\"text\"])\n",
    "            conversations += \"\\n\"\n",
    "    agg_conversations.append(conversations)\n",
    "\n",
    "# create dataframe\n",
    "conv_dataframe = pd.DataFrame(agg_conversations, columns=[\"conversations\"])\n",
    "conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325eeaa-1fd9-443c-829b-1bc2427fd8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb4441-7bce-4a2d-b405-10efc98d2ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c50a8-74b8-41fc-ad6a-e8250653728c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c691a856-202c-4937-9ca2-5dc6088aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "conv_dataframe = conv_dataframe[:1300]\n",
    "dataset = Dataset.from_pandas(conv_dataframe).train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caad485f-1d7c-4519-8b26-356b7ba99b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 3200, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8640, out_features=3200, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3200, out_features=32000, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, load_in_8bit=True, device_map='auto',\n",
    ")\n",
    "\n",
    "model_modules = str(model.modules)\n",
    "print(model_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameter efficient model ...\n",
      "trainable params: 2,662,400 || all params: 3,429,136,000 || trainable%: 0.07764054852300988\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import os\n",
    "import sys\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "#target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "#or\n",
    "target_modules = ['q_proj','v_proj']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,#or r=16\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules = target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(\"Extracting parameter efficient model ...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "base_dir = \"llama\"\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs = 5.0,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=200,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=False, #True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    dataset_text_field=\"conversations\",\n",
    "    max_seq_length=256,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "'''print(\"Upcasting layer norms ...\")\n",
    "#Upcast layer norms to float 32 for stability\n",
    "for name, module in trainer.model.named_modules():\n",
    "  if \"norm\" in name:\n",
    "    module = module.to(torch.float32)'''\n",
    "\n",
    "print(\"Start training ...\")\n",
    "trainer.train()\n",
    "\n",
    "# move this config to checkpoint folder for model reconstruction\n",
    "model.config.to_json_file(\"saved-config/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d24d4-573c-490d-9d1f-793faa52e18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4fbf8-c43f-4a3e-b084-d034a8a2872f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e144ac-e1b3-4cd0-ac5f-5ccc6d9436e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''user_template = \"\"\" Below is a question asked by a user: \\n\n",
    "### Instruction:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "system_template = \"\"\"### Response by the system to the above instruction: \\n\n",
    "{}\n",
    "\"\"\"'''\n",
    "\n",
    "# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\" Below is a question asked by a user: \\n\n",
    "  ### Instruction:\n",
    "  {}. \n",
    "  ### Response by the system to the above instruction:\"\"\".format(test)\n",
    "\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           top_p=0.9,\n",
    "                           temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response by the system to the above instruction:'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at llama/checkpoint-2 were not used when initializing LlamaForCausalLM: ['base_model.model.model.layers.14.self_attn.o_proj.weight_format', 'base_model.model.model.layers.9.mlp.gate_proj.weight_format', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight_format', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight_format', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight_format', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight_format', 'base_model.model.model.layers.24.mlp.up_proj.weight_format', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.3.mlp.down_proj.weight_format', 'base_model.model.model.layers.10.mlp.up_proj.weight_format', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight_format', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight_format', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight_format', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight_format', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.6.self_attn.o_proj.weight_format', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight_format', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight_format', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight_format', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight_format', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight_format', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight_format', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight_format', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight_format', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.8.mlp.down_proj.weight_format', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight_format', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight_format', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight_format', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight_format', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.15.self_attn.o_proj.weight_format', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight_format', 'base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight_format', 'base_model.model.model.norm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.25.self_attn.k_proj.weight_format', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight_format', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight_format', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.23.self_attn.o_proj.weight_format', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.23.mlp.gate_proj.weight_format', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight_format', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight_format', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.11.self_attn.o_proj.weight_format', 'base_model.model.model.layers.3.self_attn.o_proj.weight_format', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight_format', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight_format', 'base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.6.self_attn.k_proj.weight_format', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.13.self_attn.k_proj.weight_format', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight_format', 'base_model.model.model.layers.3.mlp.gate_proj.weight_format', 'base_model.model.model.layers.24.mlp.gate_proj.weight_format', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.25.self_attn.o_proj.weight_format', 'base_model.model.model.layers.20.mlp.gate_proj.weight_format', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight_format', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight_format', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight_format', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.0.self_attn.k_proj.weight_format', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight_format', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight_format', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight_format', 'base_model.model.model.layers.10.mlp.gate_proj.weight_format', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight_format', 'base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.11.mlp.gate_proj.weight_format', 'base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight_format', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight_format', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight_format', 'base_model.model.model.layers.10.mlp.down_proj.weight_format', 'base_model.model.model.layers.19.mlp.up_proj.weight_format', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight_format', 'base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.15.self_attn.k_proj.weight_format', 'base_model.model.model.layers.12.self_attn.o_proj.weight_format', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight_format', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight_format', 'base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.2.self_attn.k_proj.weight_format', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight_format', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight_format', 'base_model.model.model.layers.14.mlp.up_proj.weight_format', 'base_model.model.model.layers.7.self_attn.k_proj.weight_format', 'base_model.model.model.layers.16.self_attn.o_proj.weight_format', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight_format', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight_format', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight_format', 'base_model.model.model.layers.17.mlp.gate_proj.weight_format', 'base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight_format', 'base_model.model.model.layers.18.mlp.gate_proj.weight_format', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight_format', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.17.self_attn.k_proj.weight_format', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight_format', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight_format', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight_format', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight_format', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.8.self_attn.o_proj.weight_format', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight_format', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight_format', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.20.self_attn.k_proj.weight_format', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight_format', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.13.self_attn.o_proj.weight_format', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight_format', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight_format', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight_format', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight_format', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight_format', 'base_model.model.model.layers.5.mlp.down_proj.weight_format', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight_format', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight_format', 'base_model.model.model.layers.7.mlp.gate_proj.weight_format', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight_format', 'base_model.model.model.layers.21.mlp.gate_proj.weight_format', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight_format', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.14.mlp.gate_proj.weight_format', 'base_model.model.model.layers.21.self_attn.o_proj.weight_format', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight_format', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight_format', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight_format', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight_format', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight_format', 'base_model.model.model.layers.9.mlp.down_proj.weight_format', 'base_model.model.model.layers.9.self_attn.o_proj.weight_format', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.12.mlp.gate_proj.weight_format', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight_format', 'base_model.model.model.layers.2.mlp.down_proj.weight_format', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight_format', 'base_model.model.model.layers.2.mlp.up_proj.weight_format', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight_format', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight_format', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight_format', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight_format', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight_format', 'base_model.model.lm_head.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight_format', 'base_model.model.model.layers.8.self_attn.k_proj.weight_format', 'base_model.model.model.layers.22.self_attn.k_proj.weight_format', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight_format', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight_format', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight_format', 'base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight_format', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight_format', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight_format', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at llama/checkpoint-2 and are newly initialized: ['layers.1.self_attn.v_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.9.self_attn.rotary_emb.inv_freq', 'layers.8.self_attn.q_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.23.self_attn.rotary_emb.inv_freq', 'layers.23.mlp.up_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.23.input_layernorm.weight', 'layers.5.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.16.mlp.up_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.6.self_attn.k_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.17.self_attn.rotary_emb.inv_freq', 'layers.21.post_attention_layernorm.weight', 'layers.3.self_attn.rotary_emb.inv_freq', 'layers.18.self_attn.q_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.8.mlp.gate_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.5.self_attn.rotary_emb.inv_freq', 'layers.17.input_layernorm.weight', 'layers.11.mlp.gate_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.25.input_layernorm.weight', 'layers.2.self_attn.v_proj.weight', 'layers.23.self_attn.k_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.15.self_attn.rotary_emb.inv_freq', 'layers.3.post_attention_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.11.input_layernorm.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.1.input_layernorm.weight', 'layers.16.mlp.gate_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.19.self_attn.rotary_emb.inv_freq', 'layers.4.self_attn.q_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.14.mlp.up_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.16.self_attn.k_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.22.self_attn.q_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.7.input_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.9.mlp.up_proj.weight', 'layers.6.self_attn.rotary_emb.inv_freq', 'layers.24.self_attn.rotary_emb.inv_freq', 'layers.13.mlp.up_proj.weight', 'layers.14.mlp.down_proj.weight', 'layers.16.post_attention_layernorm.weight', 'embed_tokens.weight', 'layers.3.self_attn.k_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.18.self_attn.rotary_emb.inv_freq', 'layers.24.self_attn.v_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.22.self_attn.k_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.5.self_attn.v_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.21.input_layernorm.weight', 'layers.13.self_attn.rotary_emb.inv_freq', 'layers.15.post_attention_layernorm.weight', 'layers.17.post_attention_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.4.input_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.15.mlp.gate_proj.weight', 'norm.weight', 'layers.15.self_attn.v_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.22.post_attention_layernorm.weight', 'layers.0.self_attn.rotary_emb.inv_freq', 'layers.18.self_attn.k_proj.weight', 'layers.25.self_attn.rotary_emb.inv_freq', 'layers.1.self_attn.q_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.23.mlp.down_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.7.self_attn.rotary_emb.inv_freq', 'layers.20.self_attn.q_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.20.self_attn.k_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.8.input_layernorm.weight', 'layers.13.self_attn.v_proj.weight', 'layers.20.self_attn.rotary_emb.inv_freq', 'layers.12.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.17.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.self_attn.rotary_emb.inv_freq', 'layers.15.input_layernorm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.3.input_layernorm.weight', 'layers.8.self_attn.v_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.8.self_attn.rotary_emb.inv_freq', 'lm_head.weight', 'layers.22.self_attn.rotary_emb.inv_freq', 'layers.10.self_attn.rotary_emb.inv_freq', 'layers.9.mlp.down_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.12.self_attn.rotary_emb.inv_freq', 'layers.6.self_attn.o_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.k_proj.weight', 'layers.0.input_layernorm.weight', 'layers.18.post_attention_layernorm.weight', 'layers.0.mlp.up_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.16.input_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.23.self_attn.v_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.20.input_layernorm.weight', 'layers.7.self_attn.v_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.16.self_attn.rotary_emb.inv_freq', 'layers.10.mlp.down_proj.weight', 'layers.14.self_attn.rotary_emb.inv_freq', 'layers.1.self_attn.rotary_emb.inv_freq', 'layers.18.input_layernorm.weight', 'layers.1.mlp.up_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.11.self_attn.rotary_emb.inv_freq', 'layers.3.self_attn.v_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.14.self_attn.o_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.10.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.4.self_attn.rotary_emb.inv_freq', 'layers.0.self_attn.q_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.9.input_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.2.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.q_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Reload PEFT fined tuned model\n",
    "# https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo\n",
    "'''from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "saved_model_path = 'llama/checkpoint-2'\n",
    "re_tokenizer = LlamaTokenizer.from_pretrained(saved_model_path)\n",
    "re_model = LlamaForCausalLM.from_pretrained(saved_model_path, device_map='auto')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5180ec96-29f3-42f0-803e-bed74c873cbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m   prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m Below is a question asked by a user: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  ### Instruction:\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m  ### Response by the system to the above instruction:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test)\n\u001b[1;32m     24\u001b[0m   input_ids \u001b[38;5;241m=\u001b[39m re_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mre_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   predictions\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_response_text\u001b[39m(input_string):\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1565\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1558\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1559\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1560\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:2648\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2648\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "\n",
    "'''# test fine-tuned model\n",
    "# Number 1000, 1001 from data.json\n",
    "\n",
    "#re_tokenizer.pad_token = \"[PAD]\"\n",
    "re_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "re_tokenizer.padding_side = \"left\"\n",
    "\n",
    "test_strings = [\"Hello!\\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error “Fatal error: Exit code 127 ()” comes up. Further, it says that the tool generated the following error: “line 9: run_deepvariant: command not found”.\\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - “GET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1” 200\\nI don’t understand this error. Could someone please help me out?\\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\\nThanks!\",\n",
    "\"Dear Sir,\\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\\nIs it ok??? or did I out something wrong\\nKindly help\"\n",
    "               ]\n",
    "\n",
    "#response_test_strings = [\n",
    "#\"Hello, I can’t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a “docker tool dependency”, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\\n  \\n      \\n\\n      training.galaxyproject.org\\n  \\n\\n  \\n    \\n\\nGalaxy Training: Tool Dependencies and Containers\\n\\n  Galaxy is an open-source project. Everyone can contribute...\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\\nThe requirement can be seen here:\\n  \\n\\n      github.com\\n  \\n\\n  \\n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\\n\\n\\n      <macros>\\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\\n    <token name=\"@SUFFIX_VERSION@\">0</token>\\n    <xml name=\"edam_ontology\">\\n        <edam_topics>                                                                                  \\n            <edam_topic>topic_0199</edam_topic>\\n        </edam_topics>\\n        <edam_operations>\\n            <edam_operation>operation_3227</edam_operation>\\n        </edam_operations>\\n    </xml>\\n    <xml name=\"requirements\">\\n        <requirements>\\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\\n        </requirements>\\n    </xml>\\n    <xml name=\"citations\">\\n        <citations>\\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\\n        </citations>\\n\\n\\n\\n\\n  This file has been truncated. show original\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n\", \n",
    "# \"Hello @Sachin_Srivastava\\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\\n20 GB of fastq data – uncompressed – creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you’ll need to do one or more of these:\\n\\nTry a rerun to eliminate cluster issues\\nMore QA/QC on the input reads (always recommended)\\nConsider downsampling the reads (tool: Seqtk)\\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\\n\\n\\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\\nYou didn’t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\\nThanks!\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for test in test_strings:\n",
    "  prompt = \"\"\" Below is a question asked by a user: \\n\n",
    "  ### Instruction:\n",
    "  {}. \n",
    "  ### Response by the system to the above instruction:\"\"\".format(test)\n",
    "\n",
    "  input_ids = re_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = re_model.generate(input_ids=input_ids, \n",
    "                           max_new_tokens=156, \n",
    "                           do_sample=True, \n",
    "                           top_p=0.9,\n",
    "                           temperature=0.9\n",
    "                          )\n",
    "  predictions.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "\n",
    "def extract_response_text(input_string):\n",
    "    start_marker = 'Response by the system to the above instruction:'\n",
    "    end_marker = '###'\n",
    "    start_index = input_string.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = input_string.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        return input_string[start_index:]\n",
    "    \n",
    "    return input_string[start_index:end_index].strip()\n",
    "\n",
    "for i in range(len(test_strings)): \n",
    "  pred = predictions[i]\n",
    "  text = test_strings[i]\n",
    "  print(text+'\\n')\n",
    "  print(pred+'\\n')\n",
    "  print(extract_response_text(pred))\n",
    "  print(\"==============================\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f9de-57e2-46fe-93e0-26c09a68027c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1874-733f-4cb1-819f-b9d765d7b8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab312b-de95-42cd-8150-ebd676dabcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
