{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d536c2-457e-417e-adea-e8a65b1e4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-26 16:53:34,389] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1b9396-7d79-4a79-9bfb-6157c88528c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.09s/it]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "\n",
    "base_dir = \"llama-linear-layers-all-conv-Feb-19-24-1\"\n",
    "#\"llama-linear-layers-all-conv-Dec-08-02\"\n",
    "checkpoint_number = \"checkpoint-2500\"\n",
    "checkpoint_path = \"{}/{}\".format(base_dir, checkpoint_number)\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model, checkpoint_path)\n",
    "#peft_model = peft_model.merge_and_unload()\n",
    "#restored_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef97636-d7e5-4dd4-9c93-3299a093839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12350251-0e1c-4da5-ade2-e436b4a5eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/mixtral-8x7b-for-web-qa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1eaf9-d717-494b-9899-d22bfc6614e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52dcce-bea8-495e-b37e-1d9beb6bc0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc65fb3-5750-4e02-9cf4-e6a34d4b64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "<<>> ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "HF_TOKEN = getpass(\"<<>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38b433a-308f-47a8-a5d5-8b43c41253d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import markdown\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009d0b91-b89d-4285-a222-5afb213e4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import haystack\n",
    "from haystack import Document, Pipeline\n",
    "#from haystack.nodes import PreProcessor\n",
    "#from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "#from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7b8f4d-fc3c-442e-86f7-0b9da3c8d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect from GTN\n",
    "\n",
    "docs = []\n",
    "directory_path = \"../../../gtn-data/\"\n",
    "\n",
    "def read_md_file_1(path):\n",
    "    with open(path) as f:\n",
    "        content = f.read()\n",
    "        return content\n",
    "\n",
    "def read_md_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "    return extract_plain_text_from_md(md_content)\n",
    "\n",
    "def extract_plain_text_from_md(md_content):\n",
    "    html_content = markdown.markdown(md_content)\n",
    "    plain_text = ''.join(BeautifulSoup(html_content, \"html.parser\").findAll(text=True))\n",
    "    return plain_text.strip()\n",
    "    \n",
    "#included_content = \"/topics/statistics/tutorials/intro_deep_learning/\"\n",
    "\n",
    "included_content = \"/topics/\"\n",
    "\n",
    "'''for root, dirs, files in os.walk(directory_path):\n",
    "    for filename in files:\n",
    "        if fnmatch.fnmatch(filename, '*.md'):\n",
    "            path = os.path.join(root, filename)\n",
    "            if included_content in path:\n",
    "                s_path = path.split(\"/\")[-3:]\n",
    "                tutorial_name = \"_\".join(s_path)\n",
    "                md_plain_text = read_md_file(path)\n",
    "                #pr_dict = {\"content\": md_plain_text, \"meta\": {\"name\": tutorial_name}}\n",
    "                doc = Document(content=md_plain_text)\n",
    "                #doc = Document.from_json(json.dumps(pr_dict))\n",
    "                docs.append(doc)'''\n",
    "\n",
    "\n",
    "# Collect from PRs\n",
    "import secrets\n",
    "\n",
    "def generate_hex_id(length):\n",
    "    # Generate a random byte string\n",
    "    random_bytes = secrets.token_bytes(length // 2)  # Length is halved because 1 byte = 2 hex characters\n",
    "\n",
    "    # Convert the byte string to a hexadecimal representation\n",
    "    hex_id = random_bytes.hex()\n",
    "\n",
    "    return hex_id\n",
    "\n",
    "# process PRs\n",
    "for json_file in glob.glob(\"../out/github_pr_page_*.json\"):\n",
    "    with open(json_file, \"r\") as fin:\n",
    "        doc_json = json.load(fin)\n",
    "        for pr in doc_json:\n",
    "            pr_text = pr[\"body\"]\n",
    "            if pr_text != None:\n",
    "                useful_text_limit = pr_text.find(\"## How to test the changes\")\n",
    "                if useful_text_limit > 0:\n",
    "                    pr_text = pr_text[:useful_text_limit].strip()\n",
    "                    #pr_dict = {\"content\": pr_text, \"meta\": {\"name\": pr[\"number\"]}}\n",
    "                    doc = Document(content=pr_text, id=generate_hex_id(10))\n",
    "                    #doc = Document.from_json(json.dumps(pr_dict))\n",
    "                    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3762657b-ac42-400c-8866-bfe027fb025c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3654"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0,\n",
    "    language=\"en\",\n",
    ")'''\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store, top_k=4)\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=retriever, name=\"retriever\")\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "#result = pipeline.run(data={\"retriever\": {\"query\": \"How to make Galaxy pull request?\"}})\n",
    "\n",
    "#print(result[\"retriever\"][\"documents\"])\n",
    "\n",
    "#preprocessed_docs = processor.process(docs)\n",
    "#document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "#document_store.write_documents(preprocessed_docs)\n",
    "#retriever = BM25Retriever(document_store, top_k=4)\n",
    "\n",
    "# a good Question Answering template, adapted for the instruction format\n",
    "# (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e7832a-ef23-4565-8913-6c099900a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "\n",
    "@component\n",
    "class FineTunedGenerator:\n",
    "  \"\"\"\n",
    "  A component generating query response by fine-tuned LLM\n",
    "  \"\"\"\n",
    "  @component.output_types(response=str)\n",
    "  def run(self, query:str):\n",
    "      print(\"Generating text...\")\n",
    "      restored_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      input_ids = restored_tokenizer.encode(query, return_tensors=\"pt\").to('cuda')\n",
    "      outputs = peft_model.generate(\n",
    "          input_ids=input_ids, \n",
    "          max_new_tokens=256,\n",
    "          do_sample=True,\n",
    "      )\n",
    "      print(\"Finished generation\")\n",
    "      return {\"response\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d05a335e-132f-4e66-b970-d510f7c7018e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "According to these documents:\n",
    "\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Answer the given question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "\n",
    "pipeline = Pipeline()\n",
    "#pipeline.add_component(\"fetcher\", fetcher)\n",
    "#pipeline.add_component(\"converter\", converter)\n",
    "#pipeline.add_component(\"splitter\", document_splitter)\n",
    "#pipeline.add_component(\"ranker\", similarity_ranker)\n",
    "pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "pipeline.add_component(name=\"llm\", instance= FineTunedGenerator())\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad82379a-72c8-4ca0-8e9a-bd884087c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Finished generation\n",
      "tensor([[    1, 29871,    13,  7504,  3278,   304,  1438, 10701, 29901,    13,\n",
      "            13,    13,    13, 22550,   278,  2183,  1139, 29901,  1128,   508,\n",
      "           306,  1065,   263,   364,  1056, 29899, 11762,  7418,   297,  5208,\n",
      "         26825, 29973,    13, 22550, 29901,   364,  1056, 29899, 11762,   338,\n",
      "           263,  1134,   310,   302,  3174,   848, 29889,   727,   526,  1784,\n",
      "         25410,  1244,   363,   302,  3174,  7418, 29889,    13,    13,    13,\n",
      "           991,   597, 23014, 26825,  4836, 29889,  3292, 29889,   601, 29914,\n",
      "         26495, 29899, 15388, 29914,  3332,  1199, 29914,  3286,   924,   290,\n",
      "          1199, 29914, 12631, 29879, 29914,   999, 29899,  6707, 29914, 12631,\n",
      "         29889,  1420,    13,    13,   991,   597, 23014, 26825,  4836, 29889,\n",
      "          3292, 29889,   601, 29914, 26495, 29899, 15388, 29914,  3332,  1199,\n",
      "         29914,  3286,   924,   290,  1199, 29914, 12631, 29879, 29914, 27539,\n",
      "           559, 29939, 29899, 15916, 29914, 12631, 29889,  1420,    13,    13,\n",
      "           392, 29892,    13,    13,   991,   597, 23014, 26825,  4836, 29889,\n",
      "          3292, 29889,   601, 29914, 26495, 29899, 15388, 29914,  3332,  1199,\n",
      "         29914,  3286,   924,   290,  1199, 29914, 12631, 29879, 29914,   999,\n",
      "         29899,  6707, 29914, 12631, 29889,  1420,    13,    13,    13,   392,\n",
      "         29892,    13,    13,   991,   597, 24840,   535,  2199,   272, 29889,\n",
      "           990, 29914,  8318, 29914,   311,   955, 29914,  5365,   542, 29914,\n",
      "         29894,   647, 21158, 29914, 10718,   689, 15123, 24209,  3298, 29914,\n",
      "          2611, 29914,  1514, 29914, 10718,   689, 15123, 24209,  3298, 29889,\n",
      "          1420,    13,    13, 29961, 29896,  5387,  2045,   597, 29875, 29889,\n",
      "          1429, 29889,  3320, 29889,   510, 29914, 29920, 29939, 29945, 29884,\n",
      "         29955, 29889,  2732,    13, 29961, 29914,  8477, 29962,   474,   626,\n",
      "           451,  1854,   825,   278,  2228,   338, 29892,   541,   278,  1556,\n",
      "          1857,  1873,   310,   278,  8492,   338, 29871, 29906, 29889, 29896,\n",
      "         29889, 29906, 29889,   366,   508,  1018,   411,   393, 29889,   732,\n",
      "          1396,   474,   626,   773,   670,   271, 29906,   304,  2910, 13623,\n",
      "           304,   263,  3407,  2531,   608, 29889,   769,   474,   671,  1347,\n",
      "         29873]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I run a rna-seq analysis in Galaxy?\"\n",
    "result = pipeline.run({\"prompt_builder\": {\"question\": question}})\n",
    "\n",
    "print(result['llm']['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2731a54f-203d-4fd9-acab-ebe48cea0ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "model_name_or_path must be either a string or a PromptModel object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m qa_template \u001b[38;5;241m=\u001b[39m PromptTemplate(prompt\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  Context: {join(documents)};\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m  Question: {query}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m  [/INST]\"\"\"\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#model_name = \"NousResearch/Llama-2-7b-chat-hf\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m prompt_node \u001b[38;5;241m=\u001b[39m \u001b[43mPromptNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_prompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_max_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/base.py:46\u001b[0m, in \u001b[0;36mexportable_to_yaml.<locals>.wrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_component_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Call the actual __init__ function with all the arguments\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/haystack/nodes/prompt/prompt_node.py:126\u001b[0m, in \u001b[0;36mPromptNode.__init__\u001b[0;34m(self, model_name_or_path, default_prompt_template, output_variable, max_length, api_key, timeout, use_auth_token, use_gpu, devices, stop_words, top_k, debug, model_kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_model \u001b[38;5;241m=\u001b[39m model_name_or_path\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name_or_path must be either a string or a PromptModel object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: model_name_or_path must be either a string or a PromptModel object"
     ]
    }
   ],
   "source": [
    "'''qa_template = PromptTemplate(prompt=\n",
    "  \"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\n",
    "  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\n",
    "  Context: {join(documents)};\n",
    "  Question: {query}\n",
    "  [/INST]\"\"\")\n",
    "\n",
    "#model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=peft_model,\n",
    "    api_key=HF_TOKEN,\n",
    "    default_prompt_template=qa_template,\n",
    "    max_length=5500,\n",
    "    model_kwargs={\"model_max_length\":8000}\n",
    ")'''\n",
    "## https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/mixtral-8x7b-for-web-qa.ipynb#scrollTo=lpzQD_AHC6fp\n",
    "# from haystack.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f6a4d-8b3c-44f5-8abe-e6f017f05d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])\n",
    "pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "from pprint import pprint\n",
    "print_answer = lambda out: pprint(out[\"results\"][0].strip())\n",
    "\n",
    "print_answer(rag_pipeline.run(query=\"I would suggest installing the refseq_masher package. I checked earlier, and found it in the toolshed. \\\n",
    "Please, this package will help a lot.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8781d2-6af9-4a44-9614-5180102bdfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         conversations  tokens\n",
       "0    Galaxy is a web server to process scientific d...     184\n",
       "1    Galaxy is a web server to process scientific d...     245\n",
       "2    Galaxy is a web server to process scientific d...     189\n",
       "3    Galaxy is a web server to process scientific d...     154\n",
       "4    Galaxy is a web server to process scientific d...     608\n",
       "..                                                 ...     ...\n",
       "116  Galaxy is a web server to process scientific d...     245\n",
       "117  Galaxy is a web server to process scientific d...     200\n",
       "118  Galaxy is a web server to process scientific d...     275\n",
       "119  Galaxy is a web server to process scientific d...     396\n",
       "120  Galaxy is a web server to process scientific d...     247\n",
       "\n",
       "[121 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "eval_conv = pd.read_csv(\"../data/eval_dataset.csv\", sep=\"\\t\")\n",
    "eval_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062548b9-2e57-4a96-a58a-a5e0bb8c8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] hi~ i see deseq2 version 2.11.40.7+galaxy1) added the factor\" option in output, rather than version 2.11.40.6, which is a big improvement for me. further more, i wonder could we have the option to use custom defined size factor in deseq2? because it is useful for spike-in samples. [/INST]\n",
      "Instruction: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] hi~ i see deseq2 version 2.11.40.7+galaxy1) added the factor\" option in output, rather than version 2.11.40.6, which is a big improvement for me. further more, i wonder could we have the option to use custom defined size factor in deseq2? because it is useful for spike-in samples.\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      ", i created an issue in order to request this functionality: deseq2: enable custom defined size factor (@link https://github.com/galaxyproject/tools-iuc/issues/4425). regards\n",
      "\n",
      "encoding prompt number 1...\n",
      "generating response number 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-gllm/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: \n",
      "\n",
      "great question! the new option to have a custom \"sizefactor\" in deseq2 is indeed a very useful improvement. while you are correct that version 2.11.40.6 does not have this feature, it is included in version 2.11.40.7 and higher.\n",
      "\n",
      "the \"sizefactor\" option is used to control the amount of dispersion in the design matrix for poisson glm, and can be useful for spike-in samples as you mentioned. previously, the only way to account for spike-in samples was by using the \"condition\" option in deseq2, which is not as flexible as \"sizefactor\" and may not always be appropriate for all experimental designs.\n",
      "\n",
      "to use the \"sizefactor\" option in deseq2, you can simply specify the size factor for each sample in the design matrix as follows:\n",
      "\n",
      "suppose you have a design matrix called \"design\" with columns for the samples (e.g. \"sample1\", \"sample2\", etc.) and the features of interest (e.g. \"expression1\", \"expression2\", etc.). then, to use the \"sizefactor\" option for a given sample, you can add\n",
      "\n",
      "====================\n",
      "Prompt: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i have a question related to this issue. i am using hisat2 but in the built-in reference genome there is no mm28 only mm9 and mm10. my question can i select mm10 during hisat2 and in the next stingtie i use the earlier version mm39 which i uploaded? [/INST]\n",
      "Instruction: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i have a question related to this issue. i am using hisat2 but in the built-in reference genome there is no mm28 only mm9 and mm10. my question can i select mm10 during hisat2 and in the next stingtie i use the earlier version mm39 which i uploaded?\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "@quote can i select mm10 during hisat2 and in the next stingtie i use the earlier version mm39 which i uploaded you will have technical and scientific problems if data based on different genome assemblies are used together in the same analysis. choose a matching reference genome and reference annotation at the start of an analysis and use those throughout. public servers host different genome indexes. http://usegalaxy.eu hosts mm39 for now. note: next time, please consider asking a new question for new problems. you can reference other topics by url for context.\n",
      "\n",
      "encoding prompt number 2...\n",
      "generating response number 2 ...\n",
      "Generated answer: \n",
      "\n",
      "hello! yes, you can select mm10 as the reference genome during hisat2 mapping and then use a later version like mm39 for further analysis. hisat2 allows you to specify a different reference genome for the mapping step than what it uses for other steps like spliced alignments and variant calling. so, after running hisat2 with mm10 as the reference genome, you can proceed to use a different genome version like mm39 for your downstream analysis without any issues. make sure the mm39 genome is properly indexed before using it for any analysis in galaxy. you can use tools like genome_index or faidx to create an index for the mm39 genome. thanks for using galaxy!\n",
      "\n",
      "====================\n",
      "Prompt: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] i locally installed the docker version with qiime2 http://quay.io/qiime2/q2galaxy)). when i use get data upload file rule based and i paste remote urls https://zenodo.org/api/xxx/xxx.fastqsanger.gz) i get this message: \"build rules for updating datasets - access to this address in not permitted by server configuration\". any suggestions? thanks [/INST]\n",
      "Instruction: \n",
      "\n",
      "Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\n",
      "\n",
      "[INST] i locally installed the docker version with qiime2 http://quay.io/qiime2/q2galaxy)). when i use get data upload file rule based and i paste remote urls https://zenodo.org/api/xxx/xxx.fastqsanger.gz) i get this message: \"build rules for updating datasets - access to this address in not permitted by server configuration\". any suggestions? thanks\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "please see: troubleshooting galaxy docker -- qiime2 (@link https://help.galaxyproject.org/t/troubleshooting-galaxy-docker-qiime2/9836)\n",
      "\n",
      "encoding prompt number 3...\n",
      "generating response number 3 ...\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "extracted_answers = []\n",
    "\n",
    "start_marker = 'Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.'#'[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"\"\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index:end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"Prompt: \\n\")\n",
    "    print(prompt)\n",
    "    print(\"Instruction: \\n\")\n",
    "    print(instruction)\n",
    "    print()\n",
    "    print(\"Ground truth answer: \\n\")\n",
    "    print(original_answer)\n",
    "    print()\n",
    "    print(\"encoding prompt number {}...\".format(ri+1))\n",
    "    input_ids = restored_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response number {} ...\".format(ri+1))\n",
    "    outputs = peft_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = restored_tokenizer.decode(outputs[0])\n",
    "    extracted_pred = pred[pred.find(end_marker) + len(end_marker): len(pred)].strip()\n",
    "    predictions.append(pred)\n",
    "    extracted_answers.append(extracted_pred)\n",
    "    print(\"Generated answer: \\n\")\n",
    "    print(extracted_pred)\n",
    "    print()\n",
    "    print(\"====================\")\n",
    "    if ri == 10:\n",
    "        break\n",
    "\n",
    "#\"{}/{}\".format(base_dir, checkpoint_number)\n",
    "#output_file_name = \"generated_answers_peft_{}_{}\".format(base_dir, checkpoint_number)\n",
    "\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, extracted_answers, predictions), columns=[\"Instructions\", \"Ground truth answers\", \"Predicted answers\", \"Full generated answers\"])\n",
    "\n",
    "#pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)\n",
    "\n",
    "e_time = time.time()\n",
    "\n",
    "print(\"Finished generation in {} seconds\".format(e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fc458-25ff-4e67-95a0-21efbdda5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deepspeed\n",
    "\n",
    "'''config = {\n",
    "    #\"kernel_inject\": True,\n",
    "    #\"tensor_parallel\": {\"tp_size\": 4},\n",
    "    \"dtype\": torch.float16 #\"fp16\",\n",
    "    #\"enable_cuda_graph\": False\n",
    "}\n",
    "\n",
    "ds_engine = deepspeed.init_inference(peft_model, config=config) #config=config\n",
    "deepspeed_peft_model = ds_engine.module'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a5754-9653-4f19-9780-baa28bc615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_file_name = \"generated_answers_peft_fine-tuned-feb19-0_llama-linear-layers-all-conv-Feb-07-24_checkpoint-2200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58316db2-aeac-4048-ad81-68dc0229acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33baff-5c87-447a-a07f-d48ee826825a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c34ad-2d84-48c3-8d0c-38b3c895fe95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
