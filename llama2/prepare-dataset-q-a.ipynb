{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc0c4e4-a007-4d83-b543-15d5b2d7f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-07 10:35:47,790] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "from transformers import (LlamaTokenizer, LlamaForCausalLM)\n",
    "\n",
    "model_path = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465acb07-da84-433b-a4fe-dbc24f30fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/centos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/centos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## Clean text\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import contractions\n",
    "import ftfy\n",
    "\n",
    "\n",
    "## Text preprocessing for fine tuning\n",
    "## https://www.linkedin.com/pulse/pre-processing-text-data-gpt-models-techniques-best-practices-tilix/\n",
    "## TODO: https://ftfy.readthedocs.io/en/latest/\n",
    "## TODO: Read this: https://arxiv.org/abs/2212.10496\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    clean_text = text.translate(translator)\n",
    "    return clean_text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def wikitext_detokenizer(string):\n",
    "    # https://github.com/kingoflolz/mesh-transformer-jax/blob/master/create_finetune_tfrecords.py\n",
    "    # contractions\n",
    "    string = string.replace(\"s '\", \"s'\")\n",
    "    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "    # number separators\n",
    "    string = string.replace(\" @-@ \", \"-\")\n",
    "    string = string.replace(\" @,@ \", \",\")\n",
    "    string = string.replace(\" @.@ \", \".\")\n",
    "    # punctuation\n",
    "    string = string.replace(\" : \", \": \")\n",
    "    string = string.replace(\" ; \", \"; \")\n",
    "    string = string.replace(\" . \", \". \")\n",
    "    string = string.replace(\" ! \", \"! \")\n",
    "    string = string.replace(\" ? \", \"? \")\n",
    "    string = string.replace(\" , \", \", \")\n",
    "    # double brackets\n",
    "    string = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "    string = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "    string = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "    string = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "    string = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "    # miscellaneous\n",
    "    string = string.replace(\"= = = =\", \"====\")\n",
    "    string = string.replace(\"= = =\", \"===\")\n",
    "    string = string.replace(\"= =\", \"==\")\n",
    "    string = string.replace(\" \" + chr(176) + \" \", chr(176))\n",
    "    string = string.replace(\" \\n\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\" N \", \" 1 \")\n",
    "    string = string.replace(\" 's\", \"'s\")\n",
    "\n",
    "    return string\n",
    "\n",
    "def clean_html_text(soup: BeautifulSoup):\n",
    "    # Process quote aside tags\n",
    "    rgx = \"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\"\n",
    "    cleanr = re.compile(rgx)\n",
    "    cleantext = re.sub(cleanr, '', str(soup))\n",
    "\n",
    "    cleantext = re.sub(r'Screen.+KB', '', cleantext)\n",
    "    cleantext = re.sub(r'[0-9].+KB', '', cleantext)\n",
    "    cleantext = re.sub(r'Kind.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Dear @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Hi @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Hello @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Cheers', '', cleantext)\n",
    "    cleantext = re.sub(r'Best .+[a-z0-9]', '', cleantext)\n",
    "    cleantext = wikitext_detokenizer(cleantext)\n",
    "    cleantext = lowercase_text(cleantext)\n",
    "    cleantext = ftfy.fix_text(cleantext)\n",
    "    tagged_sentence = nltk.tag.pos_tag(cleantext.split())\n",
    "    cleantext = [word for word, tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    cleantext = \" \".join(cleantext)\n",
    "    # fix contractions\n",
    "    cleantext = contractions.fix(cleantext)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5240d8e0-0b92-4d4e-b9f2-cb0944b49a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1032269/557722856.py:36: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  systemsoup = BeautifulSoup(system_post[\"text\"], 'html.parser')\n",
      "/tmp/ipykernel_1032269/557722856.py:35: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  usersoup = BeautifulSoup(user_post[\"text\"], 'html.parser')\n",
      "/tmp/ipykernel_1032269/557722856.py:36: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  systemsoup = BeautifulSoup(system_post[\"text\"], 'html.parser')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>1584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>1757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>Galaxy is a web server to process scientific d...</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tokens\n",
       "0     Galaxy is a web server to process scientific d...     311\n",
       "1     Galaxy is a web server to process scientific d...    1584\n",
       "2     Galaxy is a web server to process scientific d...    1452\n",
       "3     Galaxy is a web server to process scientific d...     429\n",
       "4     Galaxy is a web server to process scientific d...     131\n",
       "...                                                 ...     ...\n",
       "4798  Galaxy is a web server to process scientific d...    1757\n",
       "4799  Galaxy is a web server to process scientific d...     660\n",
       "4800  Galaxy is a web server to process scientific d...     193\n",
       "4801  Galaxy is a web server to process scientific d...     203\n",
       "4802  Galaxy is a web server to process scientific d...     379\n",
       "\n",
       "[4803 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "file_name = \"data_biostars_q_a.json\" #\"data_galaxy_q_a.json\"\n",
    "output_file_name = \"conversations-biostars-q-a.csv\" #\"conversations-galaxy-q-a.csv\"\n",
    "\n",
    "with open(\"../out/\" + file_name) as fout:\n",
    "    raw_data = json.load(fout)\n",
    "\n",
    "\"\"\"\n",
    "<s>[INST] <<SYS>>\\n \\n\n",
    "<</SYS>>\\n\\n {} [/INST] {} </s>\n",
    "<s>[INST] {user_message_2} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "#system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "system_message = \"Galaxy is a web server to process scientific datasets. Act like a Bioinformatician who uses the Galaxy platform for biological data analysis. Understand the following instructions and prepare a suitable response.\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "[INST] <<SYS>> \\n {} \\n <</SYS>> \\n\\n {} [/INST] {}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_no_sys = \"\"\"\n",
    "[INST] {} [/INST] {}\n",
    "\"\"\"\n",
    "\n",
    "agg_conversations = []\n",
    "size_conversations = []\n",
    "\n",
    "for idx_thread in range(len(raw_data)):\n",
    "    user_post = raw_data[idx_thread][0]\n",
    "    system_post = raw_data[idx_thread][1]\n",
    "    if user_post[\"role\"] == \"user\" and system_post[\"role\"] == \"system\":\n",
    "        \n",
    "        usersoup = BeautifulSoup(user_post[\"text\"], 'html.parser')\n",
    "        systemsoup = BeautifulSoup(system_post[\"text\"], 'html.parser')\n",
    "        user_cleantext = clean_html_text(usersoup)\n",
    "        system_cleantext = clean_html_text(systemsoup)\n",
    "        conversations = system_message + \"\\n\" + prompt_template_no_sys.format(user_cleantext, system_cleantext)\n",
    "        input_ids = tokenizer.encode(conversations, return_tensors=\"pt\")\n",
    "        size_conversations.append(len(input_ids[0]))\n",
    "        agg_conversations.append(conversations)\n",
    "\n",
    "# create dataframe\n",
    "test_conv_dataframe = pd.DataFrame(zip(agg_conversations, size_conversations), columns=[\"conversations\", \"tokens\"])\n",
    "test_conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b662912d-a8f1-429b-8850-69e97531cc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 548.2904434728295, 413.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = test_conv_dataframe[\"tokens\"]\n",
    "len(size), np.mean(size), np.median(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e70dd3-6409-46e8-8873-185206362bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_df = test_conv_dataframe[test_conv_dataframe[\"tokens\"] <= 700]\n",
    "#small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_dataframe.to_csv(\"../data/\" + output_file_name, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e351d-9b09-45b8-8184-4dec799a1a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
