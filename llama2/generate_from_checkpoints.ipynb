{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d536c2-457e-417e-adea-e8a65b1e4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-08 16:31:26,288] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TextIteratorStreamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1b9396-7d79-4a79-9bfb-6157c88528c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.69s/it]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:213: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_dir = \"llama-linear-layers-all-conv-Dec-08-02\"\n",
    "checkpoint_number = \"checkpoint-2\"\n",
    "checkpoint_path = \"{}/{}\".format(base_dir, checkpoint_number)\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model, checkpoint_path)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "restored_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8781d2-6af9-4a44-9614-5180102bdfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       conversations  tokens\n",
       "0  Act like Bioinformatician who uses Galaxy plat...     301\n",
       "1  Act like Bioinformatician who uses Galaxy plat...     410\n",
       "2  Act like Bioinformatician who uses Galaxy plat...     185\n",
       "3  Act like Bioinformatician who uses Galaxy plat...     563\n",
       "4  Act like Bioinformatician who uses Galaxy plat...     173\n",
       "5  Act like Bioinformatician who uses Galaxy plat...     191\n",
       "6  Act like Bioinformatician who uses Galaxy plat...     455\n",
       "7  Act like Bioinformatician who uses Galaxy plat...     565\n",
       "8  Act like Bioinformatician who uses Galaxy plat...     339"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "eval_conv = pd.read_csv(\"../data/eval_dataset.csv\", sep=\"\\t\")\n",
    "eval_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062548b9-2e57-4a96-a58a-a5e0bb8c8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am trying to assemble a plant genome using ont data with flye. prior to launch the assembly with all my data (about 37 gb in fastqsanger.gz) i recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. i then launched with all the available sequences (37 gb in fastqsanger.gz) and it is now been running for 5 days. i am just wondering if i am using more ressources than allowed for this tool or if it is still running. what makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ? thank you very much. ben [/INST]\n",
      "Instruction: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am trying to assemble a plant genome using ont data with flye. prior to launch the assembly with all my data (about 37 gb in fastqsanger.gz) i recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. i then launched with all the available sequences (37 gb in fastqsanger.gz) and it is now been running for 5 days. i am just wondering if i am using more ressources than allowed for this tool or if it is still running. what makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ? thank you very much. ben\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "we updated the funannotate databases and provided more cores to this tool. you can see how many cores and memory every tools gets here: infrastructure-playbook/tool_destinations.yaml at master · usegalaxy-eu/infrastructure-playbook github (@link https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml#l196)\n",
      "\n",
      "encoding prompt number 1...\n",
      "generating response number 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: \n",
      "\n",
      "Hello Ben,\n",
      "\n",
      "Thank you for reaching out! I'm happy to help you troubleshoot your issue with assembling a plant genome using the Galaxy platform and the Flye tool.\n",
      "\n",
      "Based on your message, it seems that you have been running the Flye assembly job with all 37 GB of your FASTQ data, and it has been running for 5 days without completing. This suggests that the job is indeed using more resources than expected, possibly due to the large amount of data you are trying to assemble.\n",
      "\n",
      "Quota usage of 0% could indicate a bug in the system, but it's also possible that the job is simply taking longer than expected to complete due to the large size of your data.\n",
      "\n",
      "To help you troubleshoot this issue, I have a few questions:\n",
      "\n",
      "1. Have you checked the Flye documentation to see if there are any recommendations for the minimum amount of memory and resources required for large genome assemblies?\n",
      "2. How many CPU cores does your machine have? Does the Flye job use multiple CPU cores, or does it run on a single core?\n",
      "3. Can you provide more details about your machine's specifications\n",
      "\n",
      "====================\n",
      "Prompt: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am developing galaxy tools, and constructed a workflow to chain them together. the problem i am encountering comes from one of the tools executing before a previous tool finishes to run. attached are two images. one with the workflow where circled in red (rp selenzyme) is the tool that executes too early (before rp thermodynamics finishes). the second image is the side panel showing that the tool rp selenzyme returns an error due to an empty input since it executes earlier than it should. (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2x/4/44108cc385bbaa96eca635a8894187be9e7b18cd.png) https://global.discourse-cdn.com/business7/uploads/galaxy/original/2x/3/351fc181ea5cfdc0ce242699ff4a54faa61f61d5.png) any help would be very welcomed. do not hesitate to ask me for any logs. i tried this on the galaxy 19.05 and 20.01 releases and the same error occurs. thank you, [/INST]\n",
      "Instruction: \n",
      "\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST] hello, i am developing galaxy tools, and constructed a workflow to chain them together. the problem i am encountering comes from one of the tools executing before a previous tool finishes to run. attached are two images. one with the workflow where circled in red (rp selenzyme) is the tool that executes too early (before rp thermodynamics finishes). the second image is the side panel showing that the tool rp selenzyme returns an error due to an empty input since it executes earlier than it should. (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2x/4/44108cc385bbaa96eca635a8894187be9e7b18cd.png) https://global.discourse-cdn.com/business7/uploads/galaxy/original/2x/3/351fc181ea5cfdc0ce242699ff4a54faa61f61d5.png) any help would be very welcomed. do not hesitate to ask me for any logs. i tried this on the galaxy 19.05 and 20.01 releases and the same error occurs. thank you,\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "so connecting a text parameter to a select is a new feature in 20.01, this will not work on 19.09. then there is the issue that i think there are 2 parameters called input in rpselenzyme. can you try changing the one taxonomy json input to a different name and rebuild and re-run the workflow ?\n",
      "\n",
      "encoding prompt number 2...\n",
      "generating response number 2 ...\n",
      "Generated answer: \n",
      "\n",
      "Thank you for reaching out for help! It's great to hear that you're developing new tools for Galaxy and chaining them together to create a workflow. I understand that you're encountering an issue with one of the tools executing too early, which is causing an error in the next tool.\n",
      "\n",
      "Based on the images you provided, it seems that the tool \"rp selenzyme\" is executing before the previous tool \"rp thermodynamics\" finishes. This is causing the \"rp selenzyme\" tool to return an error due to an empty input.\n",
      "\n",
      "To troubleshoot this issue, I would suggest the following:\n",
      "\n",
      "1. Check your workflow structure: Make sure that the tools are properly ordered in your workflow. You can do this by clicking on the \"Workflow Structure\" button in the Galaxy Main Menu and verifying that the tools are in the correct order.\n",
      "2. Inspect the Toolboxes: Check the toolboxes of the tools involved in the issue. Make sure that the toolboxes are properly configured and that the tools are not attempting to access the same file or data before it is available.\n",
      "3. Check the Tool Execution Order: Ver\n",
      "\n",
      "====================\n",
      "Finished generation in 902.8762576580048 seconds\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "extracted_answers = []\n",
    "\n",
    "start_marker = 'Act like'#'[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"\"\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index:end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"Prompt: \\n\")\n",
    "    print(prompt)\n",
    "    print(\"Instruction: \\n\")\n",
    "    print(instruction)\n",
    "    print()\n",
    "    print(\"Ground truth answer: \\n\")\n",
    "    print(original_answer)\n",
    "    print()\n",
    "    print(\"encoding prompt number {}...\".format(ri+1))\n",
    "    input_ids = restored_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response number {} ...\".format(ri+1))\n",
    "    outputs = peft_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = restored_tokenizer.decode(outputs[0])\n",
    "    extracted_pred = pred[pred.find(end_marker) + len(end_marker): len(pred)].strip()\n",
    "    predictions.append(pred)\n",
    "    extracted_answers.append(extracted_pred)\n",
    "    print(\"Generated answer: \\n\")\n",
    "    print(extracted_pred)\n",
    "    print()\n",
    "    print(\"====================\")\n",
    "    if ri == 1:\n",
    "        break\n",
    "\n",
    "#\"{}/{}\".format(base_dir, checkpoint_number)\n",
    "output_file_name = \"generated_answers_peft_{}_{}\".format(base_dir, checkpoint_number)\n",
    "\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, extracted_answers, predictions), columns=[\"Instructions\", \"Ground truth answers\", \"Predicted answers\", \"Full generated answers\"])\n",
    "\n",
    "pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)\n",
    "\n",
    "e_time = time.time()\n",
    "\n",
    "print(\"Finished generation in {} seconds\".format(e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fc458-25ff-4e67-95a0-21efbdda5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deepspeed\n",
    "\n",
    "'''config = {\n",
    "    #\"kernel_inject\": True,\n",
    "    #\"tensor_parallel\": {\"tp_size\": 4},\n",
    "    \"dtype\": torch.float16 #\"fp16\",\n",
    "    #\"enable_cuda_graph\": False\n",
    "}\n",
    "\n",
    "ds_engine = deepspeed.init_inference(peft_model, config=config) #config=config\n",
    "deepspeed_peft_model = ds_engine.module'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58316db2-aeac-4048-ad81-68dc0229acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33baff-5c87-447a-a07f-d48ee826825a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
