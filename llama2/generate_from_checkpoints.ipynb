{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d536c2-457e-417e-adea-e8a65b1e4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype #torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69febd92-896d-4f57-aa1c-730d40a6395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1b9396-7d79-4a79-9bfb-6157c88528c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_number = \"checkpoint-4\"\n",
    "checkpoint_path = \"llama/{}\".format(checkpoint_number)\n",
    "model_path = \"\"\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "#restored_model = AutoModel.from_pretrained(checkpoint_path, torch_dtype = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64092f1-f2fa-4173-89c7-415359fe5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13500d5f-bd89-4b0d-bb78-e04f78f1d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path) #checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062548b9-2e57-4a96-a58a-a5e0bb8c8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding prompt ...\n",
      "generating response ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generation in 71.81478762626648 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "eval_conv = pd.read_csv(\"../data/eval_dataset.csv\", sep=\"\\t\")\n",
    "\n",
    "predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "\n",
    "start_marker = '<s>[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"</s>\"\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index + len(start_marker):end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"encoding prompt ...\")\n",
    "    input_ids = restored_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response ...\")\n",
    "    outputs = peft_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=156,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = restored_tokenizer.decode(outputs[0])\n",
    "    predictions.append(pred)\n",
    "    break\n",
    "\n",
    "output_file_name = \"generated_answers_peft_{}\".format(checkpoint_number)\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, predictions), columns=[\"instructions\", \"ground truth answers\", \"generated answers\"])\n",
    "pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)\n",
    "\n",
    "e_time = time.time()\n",
    "\n",
    "print(\"Finished generation in {} seconds\".format(e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fc458-25ff-4e67-95a0-21efbdda5749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
