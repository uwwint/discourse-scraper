{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d536c2-457e-417e-adea-e8a65b1e4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 16:18:10,826] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69febd92-896d-4f57-aa1c-730d40a6395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1b9396-7d79-4a79-9bfb-6157c88528c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.01s/it]\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:213: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_dir = \"llama-linear-layers-all-conv-Nov-30-2\"\n",
    "checkpoint_number = \"checkpoint-700\"\n",
    "checkpoint_path = \"{}/{}\".format(base_dir, checkpoint_number)\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(original_model, checkpoint_path)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "'''peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")'''\n",
    "\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "#re_tokenizer = AutoTokenizer.from_pretrained(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b605852-5380-4482-a1a7-bb24a4739c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f70705d-3161-4c12-b281-0a7089303b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config = {\\n    #\"kernel_inject\": True,\\n    #\"tensor_parallel\": {\"tp_size\": 4},\\n    \"dtype\": torch.float16 #\"fp16\",\\n    #\"enable_cuda_graph\": False\\n}\\n\\nds_engine = deepspeed.init_inference(peft_model, config=config) #config=config\\ndeepspeed_peft_model = ds_engine.module'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''config = {\n",
    "    #\"kernel_inject\": True,\n",
    "    #\"tensor_parallel\": {\"tp_size\": 4},\n",
    "    \"dtype\": torch.float16 #\"fp16\",\n",
    "    #\"enable_cuda_graph\": False\n",
    "}\n",
    "\n",
    "ds_engine = deepspeed.init_inference(peft_model, config=config) #config=config\n",
    "deepspeed_peft_model = ds_engine.module'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64092f1-f2fa-4173-89c7-415359fe5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepspeed_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13500d5f-bd89-4b0d-bb78-e04f78f1d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path) #checkpoint_path #model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8781d2-6af9-4a44-9614-5180102bdfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tensor_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n[INST]\\ni am trying to use the infinium huma...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n[INST]\\ni wanted to run genes to transcript ...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n[INST]\\nhi dears, i processed my pseudomonas...</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n[INST]\\ndear all, i have cutadapt raw output...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n[INST]\\nhi all, i am currently analysing dat...</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>\\n[INST]\\nhello, . i am working with candida g...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>\\n[INST]\\ni have been trying to create a galax...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>\\n[INST]\\ni am trying to follow this transcrip...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>\\n[INST]\\nhello. two questions does anyone kno...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>\\n[INST]\\ngood morning! i hereby inform you th...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         conversations  tensor_size\n",
       "0    \\n[INST]\\ni am trying to use the infinium huma...          179\n",
       "1    \\n[INST]\\ni wanted to run genes to transcript ...          103\n",
       "2    \\n[INST]\\nhi dears, i processed my pseudomonas...          219\n",
       "3    \\n[INST]\\ndear all, i have cutadapt raw output...          167\n",
       "4    \\n[INST]\\nhi all, i am currently analysing dat...          572\n",
       "..                                                 ...          ...\n",
       "183  \\n[INST]\\nhello, . i am working with candida g...          192\n",
       "184  \\n[INST]\\ni have been trying to create a galax...          108\n",
       "185  \\n[INST]\\ni am trying to follow this transcrip...          179\n",
       "186  \\n[INST]\\nhello. two questions does anyone kno...          267\n",
       "187  \\n[INST]\\ngood morning! i hereby inform you th...          207\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "eval_conv = pd.read_csv(\"../data/eval_dataset.csv\", sep=\"\\t\")\n",
    "eval_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "062548b9-2e57-4a96-a58a-a5e0bb8c8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding prompt number 1...\n",
      "generating response number 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "i am trying to use the infinium human methylation beadchip workflow in galaxy with idat files from an illumina epic chip. it is based on minfi, which i usually run in rstudio but wanted to give it a try. it is ok with the idat files ant the phenotype table, but then i am asked for a gtf genome table (in the example wgencodehaibmethyl450gm12878sitesrep1). what should this genome table be for an epic chip?\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "welcome, @ettoremeccia instructions are in the tutorial associated with the workflow in this section: galaxy training network galaxy training: infinium human methylation beadchip dna methylation is an epigenetic mechanism used by higher..\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "The Infinium HumanMethylation Chip workflow in Galaxy uses the UCSC hg19 genome assembly as a reference, so you will need to create a custom genome table for your Epic chip data.\n",
      "\n",
      "The gtf genome table is a tab-delimited text file that contains information about the genomic location of the probes on the array. Each line in the file represents a probe, and the columns are as follows:\n",
      "\n",
      "* chrom: the chromosome number (e.g. 1, 2, etc.)\n",
      "* pos: the position of the probe on the chromosome (e.g. 1000000, 2000000, etc.)\n",
      "* name: the name of the probe (e.g. \"HM450_01\", \"HM450_02\", etc.)\n",
      "* strand: the strand orientation of the probe (e.g. \"+\" for forward, \"-\" for reverse)\n",
      "* len: the length of the probe (e.g. 100, 200, etc.)\n",
      "\n",
      "====================\n",
      "encoding prompt number 2...\n",
      "generating response number 2 ...\n",
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "i wanted to run genes to transcript map output on my trinity assembly data, but i am not able to find that tool. is there any alternative tool for this?\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "hi @rrr, this tool is not available currently in usegalaxy.org; i opened a pr in order to install it; meanwhile, you can run your analysis in usegalaxy.eu, since in this instance this tool is available: genes to transcripts. regard\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "Yes, there are several tools available that can be used to map RNA-seq read alignments to a genome assembly. Here are a few alternatives to Trinity that you can consider:\n",
      "\n",
      "1. STAR: STAR (Spliced Transcripts Alignment to a Reference) is a popular tool for mapping RNA-seq read alignments to a genome assembly. STAR can align reads to a reference genome and identify spliced transcripts, including alternative splicing isoforms.\n",
      "2. HISAT2: HISAT2 (Hierarchical Indexing for Spliced Transcript Alignment) is another widely used tool for RNA-seq read alignment. HISAT2 can align reads to a reference genome and identify spliced transcripts, including alternative splicing isoforms.\n",
      "3. Kallisto: Kallisto is a fast and sensitive tool for aligning RNA-seq read alignments to a reference genome. Kallisto can also identify spliced transcripts, including alternative splicing isoforms.\n",
      "4. Salmon: Salmon is a fast and sensitive tool for aligning RNA-seq read\n",
      "\n",
      "====================\n",
      "encoding prompt number 3...\n",
      "generating response number 3 ...\n",
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "hi dears, i processed my pseudomonas aeruginosa sequences with the reference sequence pseudomonas aeruginosa (str.pa7): 16720 on the map with bow tie for illumina tool. i need the ncbi accession number of the reference sequences pseudomonas aeruginosa (str.pa7): 16720 on the galaxy au platform. can you help me? also, pseudomonas aeruginosa reference sequence is not available in map with bwa-mem, can you add it?\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "this is where the index was sourced: index of the accession is nc_009656. inside the directory is the fasta: pseuaeru_pa7.fa add the fasta to a history (capture the url and paste into the upload tool) then use it as a custom reference genome with any tools that do not have it already indexed. how to use custom reference genomes\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "Hello! I'm happy to help you with your query.\n",
      "\n",
      "To obtain the NCBI accession number of the reference sequence Pseudomonas aeruginosa (str.pa7): 16720 on the Galaxy au platform, you can follow these steps:\n",
      "\n",
      "1. Launch the Galaxy au platform and log in to your account.\n",
      "2. Click on the \"Load Data\" button and select \"FASTA\" as the data format.\n",
      "3. Enter the following information in the \"Load data\" section:\n",
      "\t* Sequence name: Pseudomonas aeruginosa (str.pa7)\n",
      "\t* Accession number: N/A (since the reference sequence is not available on Galaxy au)\n",
      "\t* Format: Fasta\n",
      "\t* File: Enter the path to the Pseudomonas aeruginosa (str.pa7) reference sequence file that you downloaded from NCBI.\n",
      "4. Click on the \"Load\" button to load the reference sequence into Galaxy au.\n",
      "\n",
      "As for adding the reference sequence to the \"map with bow tie for Illumina\" tool in Galaxy au, I apologize, but the reference sequence for Pseudomonas aerugin\n",
      "\n",
      "====================\n",
      "encoding prompt number 4...\n",
      "generating response number 4 ...\n",
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "dear all, i have cutadapt raw output files and want to run cutadapt to trim nextera transposase sequences. trim galore cannot find any files neither my original fastq.gz files. could you please help me? thanks in advance, best bernhard\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      ", most likely your files have 'fastq.gz' datatype in galaxy, eg format: fastq.gz. cutadapt recognizes this datatype, while trim galore works with datatype like fastqsanger.gz. similar topic discussed on the forum, eg unicycler failed - \"reads unavailable\" -- inputs not recognized due to an incompatible datatype assignment - #3 by vanina.guernier , igo\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "It sounds like you are facing an issue with running Cutadapt on your raw FASTQ files. Cutadapt is a tool that can trim adapters from high-throughput sequencing reads, but it needs access to the raw reads in order to work.\n",
      "\n",
      "Here are some possible reasons why Cutadapt is unable to find your files:\n",
      "\n",
      "1. Incorrect file names: Make sure that the input file names are correct and match the format expected by Cutadapt. The default input format for Cutadapt is \"fastq_file.fq\", where \"fastq_file\" is the name of the input file.\n",
      "2. Files not in the same directory: Cutadapt assumes that the input files are located in the same directory as the tool. Make sure that your raw FASTQ files are in the same directory as the Cutadapt executable.\n",
      "3. Incorrect path: If you are running Cutadapt in a different directory than the one where your raw FASTQ files are located, you will need to specify the path to the input files using the -f or --input-file option. For example, you can run Cutadapt like\n",
      "\n",
      "====================\n",
      "encoding prompt number 5...\n",
      "generating response number 5 ...\n",
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "hi all, i am currently analysing data coming from rna sequence. my samples come from mice. i would like to use the goseq tool. the available genome is mm10. however the alignment was done with the previous version of the genome (grcm38.p6). i do not think that i can use the mm10 version for the goseq. do you know if i have any other possibility or i should use external tools for the go analysis david etc) ? thank you!\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "welcome, @des_b des_b: however the alignment was done with the previous version of the genome grcm38 and mm10 are the same genomic assembly. but the \"build\" can differ between data providers (chromosome attribute). then, the annotation used can also differ between sources transcript attributes). see mouse assemblies here for what is used in galaxy for grcm38/mm10 ucsc genome browser downloads if your data is based on a non-ucsc source for grcm38/mm10 you might be able to convert the chromosome labels using the tool replace column by values which are defined in a convert file. common convert file mappings can be obtained through the link in the down in the help section. mapping between annotation sources is a bit trickier since those are usually not 1-1 pairings. but, you can try using annotatemyids, before deciding to use an annotation that is directly supported by goseq instead (or, before bothering to create a custom go mapping input, see next). des_b: do you know if i have any other possibility or i should use external tools for the go analysis david etc) ? the galaxy wrapped goseq tool allows for custom go mappings. examples of the file formats are down in the tool form help. the other two tools are also wrapped in galaxy. if these tools are used in galaxy or directly, both will involve making sure that the data inputs and the reference files are based on the same genomic assembly build and a common annotation source. tl;dr the idea is to make sure all data is from the same assembly grcm38) and the build identifiers \"match up\" as a technical consideration, then the scientific algorithms are applied. computers are literal when processing important values. chr1 and chr1 and 1 all mean 1\" to a person, but are considered different by tools. same for feature annotations: genea and genea.2 and genea.20 are all different to tools but could all mean footprint a\" with no particular version to a person. resources 3: rna-seq genes to pathways data manipulation olympic\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "Hello! It's great that you are interested in using the goseq tool for your RNA-seq analysis. Yes, the goseq tool assumes that the reference genome used for alignment is the same as the one used for the annotation of the genes. Since you have data from mice and the available genome is mm10, you need to use the mm10 genome for the goseq analysis.\n",
      "\n",
      "Since you mentioned that the alignment was done with the previous version of the genome (grcm38.p6), you won't be able to use the goseq tool directly with your data. Instead, you can use the AnnoVar tool, which is part of the AnnoTraLator framework, to convert your alignment file (in BAM format) to the goseq format (in gseqformat.gff format).\n",
      "\n",
      "Here's how you can do it:\n",
      "\n",
      "1. Install AnnoTraLator: You can install AnnoTraLator using the following command: `sudo apt-get install annotator`.\n",
      "2. Convert your BAM alignment file to gseqformat.gff: `annotation-tool -b -g mm10 -o\n",
      "\n",
      "====================\n",
      "encoding prompt number 6...\n",
      "generating response number 6 ...\n",
      "Prompt: \n",
      "\n",
      "[INST]\n",
      "how to filter snp in file (a)as region in defined range in file (b) in usegalaxy? a: chr1 186 187 rs378896312 chr1 703 704 rs136740087 chr1 833 834 rs136710007 chr1 864 865 rs43096727 chr1 1610 1611 rs132965558 chr1 1611 1612 rs134909289 chr1 1628 1629 rs136933083 b: chr1 185 190 chr1 800 840 chr1 1600 1620 chr1 1625 1630 output: chr1 186 187 rs378896312 chr1 833 834 rs136710007 chr1 1610 1611 rs132965558 chr1 1611 1612 rs134909289 chr1 1628 1629 rs136933083\n",
      "\n",
      "[/INST]\n",
      "\n",
      "Ground truth answer: \n",
      "\n",
      "i hope i understand your question correctly. you want to have the snps in file a that are to 100% in the ranges listed in your file b, and thus generating your desired output? the tool you are searching for is bedtools intersect intervals. make sure to tick the option: write the original a entry once if any overlaps found in b. and under required overlap that you give minimum overlap required as a fraction of a the value 1.0. a detailed explaination of the tool is given in galaxy and here\n",
      "\n",
      "Generated answer: \n",
      "\n",
      "To filter SNPs in a range of genomic coordinates in Galaxy using the `usegalaxy` tool, you can use the `filter` function. Here is an example of how to use it:\n",
      "```\n",
      "# Import the required libraries\n",
      "from galaxy import usegalaxy\n",
      "from galaxy.tools import filter\n",
      "\n",
      "# Load the file (a)\n",
      "a = usegalaxy.get_data('a.bed')\n",
      "\n",
      "# Define the range of coordinates to filter\n",
      "start = 186\n",
      "end = 187\n",
      "\n",
      "# Define the range of coordinates to filter in file (b)\n",
      "b_start = 185\n",
      "b_end = 190\n",
      "\n",
      "# Filter the SNPs in file (a) that fall within the range defined in file (b)\n",
      "filter_a = filter(a, b_start, b_end)\n",
      "\n",
      "# Output the filtered SNPs in file (a)\n",
      "output_a = usegalaxy.get_data('filter_a.bed')\n",
      "```\n",
      "In this example, we are filtering the SNPs in file `a` that fall within the range defined in file `b`. The `filter\n",
      "\n",
      "====================\n",
      "Finished generation in 2142.6692197322845 seconds\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "original_instructions = []\n",
    "ground_truth_answer = []\n",
    "extracted_answers = []\n",
    "\n",
    "start_marker = '[INST]'\n",
    "end_marker = '[/INST]'\n",
    "end_tag = \"\"\n",
    "\n",
    "s_time = time.time()\n",
    "\n",
    "for ri, row in eval_conv.iterrows():\n",
    "    entire_conv = row[\"conversations\"]\n",
    "    start_index = entire_conv.find(start_marker)\n",
    "    end_index = entire_conv.find(end_marker)\n",
    "    instruction = entire_conv[start_index + len(start_marker):end_index].strip()\n",
    "    prompt = entire_conv[start_index:end_index + len(end_marker)].strip()\n",
    "    original_answer = entire_conv[end_index + len(end_marker): len(entire_conv) - len(end_tag) - 1].strip()\n",
    "    original_instructions.append(instruction)\n",
    "    ground_truth_answer.append(original_answer)\n",
    "    print(\"encoding prompt number {}...\".format(ri+1))\n",
    "    input_ids = restored_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print(\"generating response number {} ...\".format(ri+1))\n",
    "    outputs = peft_model.generate(input_ids=input_ids, \n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    pred = restored_tokenizer.decode(outputs[0])\n",
    "    extracted_pred = pred[pred.find(end_marker) + len(end_marker): len(pred)].strip()\n",
    "    predictions.append(pred)\n",
    "    extracted_answers.append(extracted_pred)\n",
    "    print(\"Prompt: \\n\")\n",
    "    print(prompt)\n",
    "    #print(\"Instruction: \\n\")\n",
    "    #print(instruction)\n",
    "    print()\n",
    "    print(\"Ground truth answer: \\n\")\n",
    "    print(original_answer)\n",
    "    print()\n",
    "    print(\"Generated answer: \\n\")\n",
    "    print(extracted_pred)\n",
    "    print()\n",
    "    print(\"====================\")\n",
    "    if ri == 5:\n",
    "        break\n",
    "\n",
    "#\"{}/{}\".format(base_dir, checkpoint_number)\n",
    "output_file_name = \"generated_answers_peft_{}_{}\".format(base_dir, checkpoint_number)\n",
    "\n",
    "pred_dataframe = pd.DataFrame(zip(original_instructions, ground_truth_answer, extracted_answers, predictions), columns=[\"Instructions\", \"Ground truth answers\", \"Predicted answers\", \"Full generated answers\"])\n",
    "\n",
    "pred_dataframe.to_csv(\"../data/{}.csv\".format(output_file_name), sep=\"\\t\", index=None)\n",
    "\n",
    "e_time = time.time()\n",
    "\n",
    "print(\"Finished generation in {} seconds\".format(e_time - s_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fc458-25ff-4e67-95a0-21efbdda5749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58316db2-aeac-4048-ad81-68dc0229acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
