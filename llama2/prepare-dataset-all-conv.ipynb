{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19a09af-cd3c-4114-8ad5-979d399cd0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 17:01:46,048] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "from transformers import (LlamaTokenizer, LlamaForCausalLM)\n",
    "\n",
    "model_path = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19e9519-7b25-41f8-8a50-7dc64c8bd4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/centos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/centos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e593754-a585-47c8-9065-be3a1a1ae745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67801/1064999737.py:152: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(post_curr[\"text\"], 'html.parser')\n",
      "/tmp/ipykernel_67801/1064999737.py:152: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(post_curr[\"text\"], 'html.parser')\n",
      "/scratch/users/anup/miniconda3/envs/finetune-dnabert2/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>\\n\\nBelow is an instruction that describes a t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations\n",
       "0     \\n\\nBelow is an instruction that describes a t...\n",
       "1     \\n\\nBelow is an instruction that describes a t...\n",
       "2     \\n\\nBelow is an instruction that describes a t...\n",
       "3     \\n\\nBelow is an instruction that describes a t...\n",
       "4     \\n\\nBelow is an instruction that describes a t...\n",
       "...                                                 ...\n",
       "1406  \\n\\nBelow is an instruction that describes a t...\n",
       "1407  \\n\\nBelow is an instruction that describes a t...\n",
       "1408  \\n\\nBelow is an instruction that describes a t...\n",
       "1409  \\n\\nBelow is an instruction that describes a t...\n",
       "1410  \\n\\nBelow is an instruction that describes a t...\n",
       "\n",
       "[1411 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import contractions\n",
    "import ftfy\n",
    "\n",
    "\n",
    "## Text preprocessing for fine tuning\n",
    "## https://www.linkedin.com/pulse/pre-processing-text-data-gpt-models-techniques-best-practices-tilix/\n",
    "## TODO: https://ftfy.readthedocs.io/en/latest/\n",
    "## TODO: Read this: https://arxiv.org/abs/2212.10496\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    clean_text = text.translate(translator)\n",
    "    return clean_text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def wikitext_detokenizer(string):\n",
    "    # https://github.com/kingoflolz/mesh-transformer-jax/blob/master/create_finetune_tfrecords.py\n",
    "    # contractions\n",
    "    string = string.replace(\"s '\", \"s'\")\n",
    "    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "    # number separators\n",
    "    string = string.replace(\" @-@ \", \"-\")\n",
    "    string = string.replace(\" @,@ \", \",\")\n",
    "    string = string.replace(\" @.@ \", \".\")\n",
    "    # punctuation\n",
    "    string = string.replace(\" : \", \": \")\n",
    "    string = string.replace(\" ; \", \"; \")\n",
    "    string = string.replace(\" . \", \". \")\n",
    "    string = string.replace(\" ! \", \"! \")\n",
    "    string = string.replace(\" ? \", \"? \")\n",
    "    string = string.replace(\" , \", \", \")\n",
    "    # double brackets\n",
    "    string = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "    string = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "    string = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "    string = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "    string = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "    # miscellaneous\n",
    "    string = string.replace(\"= = = =\", \"====\")\n",
    "    string = string.replace(\"= = =\", \"===\")\n",
    "    string = string.replace(\"= =\", \"==\")\n",
    "    string = string.replace(\" \" + chr(176) + \" \", chr(176))\n",
    "    string = string.replace(\" \\n\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\" N \", \" 1 \")\n",
    "    string = string.replace(\" 's\", \"'s\")\n",
    "\n",
    "    return string\n",
    "\n",
    "def clean_html_text(soup: BeautifulSoup):\n",
    "    # Process quote aside tags\n",
    "    for quote in soup.select('aside.quote'):\n",
    "        blockquote_content = quote.find('blockquote').get_text(strip=True)\n",
    "        quote.replace_with(f\"@quote\\n{blockquote_content}\")\n",
    "\n",
    "    # Remove all a.anchor elements\n",
    "    for anchor in soup.select('a.anchor'):\n",
    "        anchor.decompose()\n",
    "\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        text = a_tag.get_text()\n",
    "        href = a_tag.get('href', '')\n",
    "\n",
    "        if text in href:\n",
    "            a_tag.replace_with(f\"@link {href}\")\n",
    "        else:\n",
    "            a_tag.replace_with(f\"{text} (@link {href})\")\n",
    "\n",
    "    # Replace all img tags with their alt text\n",
    "    for img in soup.find_all('img'):\n",
    "        alt_text = img.get('alt', '')\n",
    "        img.replace_with(alt_text)\n",
    "\n",
    "    # Remove all svg elements\n",
    "    for svg in soup.find_all('svg'):\n",
    "        svg.decompose()\n",
    "\n",
    "    rgx = \"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\"\n",
    "    cleanr = re.compile(rgx)\n",
    "    cleantext = re.sub(cleanr, '', str(soup))\n",
    "\n",
    "    cleantext = re.sub(r'Screen.+KB', '', cleantext)\n",
    "    cleantext = re.sub(r'[0-9].+KB', '', cleantext)\n",
    "    cleantext = re.sub(r'Kind.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Dear @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Hi @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Hello @.+[a-z0-9]', '', cleantext)\n",
    "    cleantext = re.sub(r'Cheers', '', cleantext)\n",
    "    cleantext = re.sub(r'Best .+[a-z0-9]', '', cleantext)\n",
    "    cleantext = wikitext_detokenizer(cleantext)\n",
    "    cleantext = lowercase_text(cleantext)\n",
    "    cleantext = ftfy.fix_text(cleantext)\n",
    "\n",
    "    tagged_sentence = nltk.tag.pos_tag(cleantext.split())\n",
    "    cleantext = [word for word, tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    cleantext = \" \".join(cleantext)\n",
    "    # fix contractions\n",
    "    cleantext = contractions.fix(cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "with open(\"../out/data.json\") as fout:\n",
    "    raw_data = json.load(fout)\n",
    "\n",
    "prompt_template_no_sys = \"\"\"\n",
    "[INST] {} [/INST] {}\n",
    "\"\"\"\n",
    "\n",
    "user_template = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "[INST]\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "user_template_end_tag = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "[INST]\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "system_template = \"\"\"[/INST] {}\"\"\"\n",
    "\n",
    "end_template = \"\"\"{}\"\"\"\n",
    "\n",
    "agg_conversations = []\n",
    "for idx_thread, thread in enumerate(raw_data):\n",
    "    conversations = \"\"\n",
    "    prev_owner = \"\"\n",
    "    \n",
    "    for idx_post in range(len(thread)):\n",
    "        post_curr = thread[idx_post]\n",
    "        soup = BeautifulSoup(post_curr[\"text\"], 'html.parser')\n",
    "        cleantext = clean_html_text(soup)\n",
    "        \n",
    "        \n",
    "        if post_curr[\"role\"] == \"user\":\n",
    "            if prev_owner == \"user\":\n",
    "                conversations += \"\\n\" + cleantext\n",
    "            elif prev_owner == \"system\":\n",
    "                conversations += \"\\n\" + user_template_end_tag.format(cleantext)\n",
    "            else:\n",
    "                conversations += \"\\n\" + user_template.format(cleantext)\n",
    "\n",
    "        if post_curr[\"role\"] == \"system\":\n",
    "            if prev_owner == \"system\":\n",
    "                conversations += \"\\n\" + cleantext\n",
    "            elif prev_owner == \"user\":\n",
    "                conversations += \"\\n\" + system_template.format(cleantext)\n",
    "\n",
    "        prev_owner = post_curr[\"role\"]\n",
    "    agg_conversations.append(end_template.format(conversations))\n",
    "\n",
    "# create dataframe\n",
    "test_conv_dataframe = pd.DataFrame(agg_conversations, columns=[\"conversations\"])\n",
    "test_conv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9329897a-b60f-441a-810b-8c29c17afc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_dataframe.to_csv(\"../data/test-conversations-all-conv.csv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e351d-9b09-45b8-8184-4dec799a1a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c609e966-44c3-4f7b-8797-9d93eee1e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pre_instruction = \"Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\"\n",
    "\n",
    "def extract_ins_res_pairs(input_string):\n",
    "    pairs_ins_res = []\n",
    "    size_pairs = []\n",
    "    full_pat = '[INST]'\n",
    "    pattern = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\" #[INST]\n",
    "    matches = [m.start() for m in re.finditer(pattern, input_string)]\n",
    "    #print(matches)\n",
    "    for i in range(len(matches)):\n",
    "        if i < len(matches)-1:\n",
    "            f,n = matches[i], matches[i+1]\n",
    "            extracted_conv = input_string[f + len(pattern):n]\n",
    "        else:\n",
    "            extracted_conv = input_string[matches[i] + len(pattern):len(input_string)]\n",
    "        if \"[/INST]\" in extracted_conv:\n",
    "            input_ids = tokenizer.encode(extracted_conv, return_tensors=\"pt\")\n",
    "            extracted_conv = pre_instruction + \"\\n\" + extracted_conv\n",
    "            pairs_ins_res.append(extracted_conv)\n",
    "            size_pairs.append(len(input_ids[0]))\n",
    "    return pairs_ins_res, size_pairs\n",
    "\n",
    "ins_res_dataset = []\n",
    "prs_size_dataset = []\n",
    "for idx, item in test_conv_dataframe.iterrows():\n",
    "    prs, size_prs = extract_ins_res_pairs(item[\"conversations\"])\n",
    "    ins_res_dataset.extend(prs)\n",
    "    prs_size_dataset.extend(size_prs)\n",
    "individual_conversations = pd.DataFrame(zip(ins_res_dataset, prs_size_dataset), columns=[\"conversations\", \"tensor_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5180ec96-29f3-42f0-803e-bed74c873cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = individual_conversations[\"tensor_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb0f9de-57e2-46fe-93e0-26c09a68027c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 362.74459527400705, 235.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(size), np.mean(size), np.median(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ff1874-733f-4cb1-819f-b9d765d7b8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tensor_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tensor_size\n",
       "0     Act like Bioinformatician who uses Galaxy plat...          381\n",
       "1     Act like Bioinformatician who uses Galaxy plat...          429\n",
       "2     Act like Bioinformatician who uses Galaxy plat...          504\n",
       "3     Act like Bioinformatician who uses Galaxy plat...          570\n",
       "4     Act like Bioinformatician who uses Galaxy plat...          492\n",
       "...                                                 ...          ...\n",
       "1984  Act like Bioinformatician who uses Galaxy plat...          192\n",
       "1985  Act like Bioinformatician who uses Galaxy plat...          108\n",
       "1986  Act like Bioinformatician who uses Galaxy plat...          179\n",
       "1987  Act like Bioinformatician who uses Galaxy plat...          267\n",
       "1988  Act like Bioinformatician who uses Galaxy plat...          207\n",
       "\n",
       "[1989 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = individual_conversations\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48ab312b-de95-42cd-8150-ebd676dabcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.to_csv(\"../data/all-conv-galaxy-q-a.csv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d7c21b2-a21e-4ae5-bae5-541f798c52d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversations</th>\n",
       "      <th>tensor_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Act like Bioinformatician who uses Galaxy plat...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conversations  tensor_size\n",
       "0     Act like Bioinformatician who uses Galaxy plat...          381\n",
       "1     Act like Bioinformatician who uses Galaxy plat...          429\n",
       "2     Act like Bioinformatician who uses Galaxy plat...          504\n",
       "3     Act like Bioinformatician who uses Galaxy plat...          570\n",
       "4     Act like Bioinformatician who uses Galaxy plat...          492\n",
       "...                                                 ...          ...\n",
       "1984  Act like Bioinformatician who uses Galaxy plat...          192\n",
       "1985  Act like Bioinformatician who uses Galaxy plat...          108\n",
       "1986  Act like Bioinformatician who uses Galaxy plat...          179\n",
       "1987  Act like Bioinformatician who uses Galaxy plat...          267\n",
       "1988  Act like Bioinformatician who uses Galaxy plat...          207\n",
       "\n",
       "[1989 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small_df = small_df[small_df[\"tensor_size\"] <= 700]\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c4f3d84-ebbf-4da0-8d8d-2a19ad132d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.to_csv(\"../data/all-conv-galaxy-q-a.csv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed01b3aa-b450-403e-8d78-40681aac5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "hi, i have a very basic notebook running in our local galaxy portal with interactive tools. as you see, put() and get() functions are not recognized: the tool-wrapper is the following taken from some official galaxy jupyterlab/jupyternotebook tutorial page. what might i be missing to be able to have get() and put() working? tool wrapper: default empty notebook, reuse a previous one, or upload a new quay.io/bgruening/docker-jupyter-notebook:2021-03-05 8888 lab $__galaxy_url__ 8080 re export galaxy_working_dir=`pwd` mkdir mkdir re.sub('[^\\w\\-\\.]', str($input.element_identifier)) ln '$input' './jupyter/data/${cleaned_name}' && ## change into the directory where the notebooks are located cd export path=/home/jovyan/.local/bin:\\$path copy default notebook cp '$__tool_directory__/default_notebook.ipynb' jupyter trust jupyter lab cp '$jupyter_notebook' re.sub('[^\\w\\-\\.]', str($input.element_identifier)) cp '$mode.ipynb' jupyter trust jupyter nbconvert notebook --execute --allow-errors ./*.ipynb && jupyter lab #end if cp '$jupyter_notebook' #end if ]]> start with a fresh notebook load a previous notebook the jupyter notebook is an open-source web applicati...\n",
      "\n",
      "[/INST] cross-referenced post: galaxyproject/admins - gitter more feedback once this is looked into more (soon!).\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "any ideas yet what is going on with put and get not working? as i asked in the gitter post, are these python methods? do i maybe need to import some module?\n",
      "\n",
      "[/INST] maikenp: jupyter sorry for the delay. the application was in the middle of an update at the usegalaxy.* servers and i wanted to cross-test. the functions put and get are now working at those. i am not exactly sure what is needed for configuration at other places. the gtn tutorials that cover the usage are not quite current – the tools are now in the tool panel, not under the visualization masthead menu. it looks like you are getting some help at gitter – please see the latest message from last week: galaxyproject/admins - gitter\n",
      "there are lots of tutorials covering the usage of its, but none covering building its. get and put are some special magic in the jupyter notebook we build. we configure a special environment for the notebook which loads those commands: docker-jupyter-notebook/ipython-profile.py at a48b767cd7d030b743605bb7984eda5eae76eb69 · bgruening/docker-jupyter-notebook github so your tool looks fine, but it could be something about the specific container image you are using. in that container it should be loaded by default in the default user's home dir. i see that you are missing this statement: export home=/home/jovyan/ there is a strong change that could be causing it. generally looking at your xml it has diverged quite far from the in-galaxy jupyter notebook, you might try diffing those files and seeing if some of those changes you have made are relevant to the issues. if it is ok for everyone, maybe let us keep the conversation here, it is a bit easier to see what is going on than scrolling through hundreds of messages to tease apart the thread.\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "thanks, yes i much prefer disucssing here, the gitter is not ideal for discussions! let us see thanks, i will add the export. could you provide a link to the in-galaxy jupyter notebook? for our notebook docker image, we used as a basis: github - bgruening/docker-jupyter-notebook: jupyter running in a docker container. this image can be used to integrate jupyter into galaxy but added some more software.\n",
      "\n",
      "for our notebook docker image, we used as a basis: github - bgruening/docker-jupyter-notebook: jupyter running in a docker container. this image can be used to integrate jupyter into galaxy but added some more software. the xml copy-pasted in this post is the one from the installation found in - with one change - the ipython/lab which had to be changed to lab and some outdated serverapp replacement to notebookapp - see some earlier discussion on gitter. [root@galaxy-hepp interactive]# different interactivetool_jupyter_notebook.xml 1,4c1,5 quay.io/bgruening/docker-jupyter-notebook:2021-03-05 --- default empty notebook, reuse a previous one, or upload a new quay.io/bgruening/docker-jupyter-notebook:2021-03-05 8c9 ipython/lab lab 35c37 jupyter lab jupyter lab --serverapp.token='' 46c48 jupyter lab jupyter lab --serverapp.token=''\n",
      "answering myself: i can find the most recent jupyter_notebook here: galaxy/tools/interactive at dev galaxyproject/galaxy github\n",
      "update, using the newest toolwrapper () the put function is now recognized. still having some issues using it, will get back with details.\n",
      "would you like me to create a new help issue - since now put() is indeed recognized, but i get this:\n",
      "the key point being i think the use of the quay.io/bgruening/docker-jupyter-notebook:ie2 image.\n",
      "[/INST] ok! progress did you set galaxy_infrastructure_url, this can be important for the commands to connect to the galaxy server.\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "do not think so. where would i set that, and what should the value be?\n",
      "\n",
      "ah, configuration option, right, let us see…\n",
      "so, i set galaxy_infrastructure_url: http://localhost:8000 since i see in our nginx setup: [root@galaxy-hepp config]# cat /etc/nginx/conf.d/galaxy-gie-proxy.conf server listen on port 443 listen 443 ssl; various ssl settings) client_max_body_size 2g; proxy_intercept_errors on; fastcgi_buffers 16 16k; fastcgi_buffer_size 32k; #proxy all requests to the gie proxy application location { proxy_redirect off; proxy_http_version 1.1; proxy_set_header host $host; #proxy_set_header x-real-ip $remote_addr; proxy_set_header upgrade proxy_set_header connection proxy_pass http://localhost:8000;} } but in the galaxy.conf i see: [root@galaxy-hepp config]# cat server {listen 443 ssl; server_name galaxy-hepp.hpc.uio.no; .....(various settings) location { uwsgi_pass 127.0.0.1:8080; uwsgi_param uwsgi_scheme include uwsgi_params;} } which would be the correct to use? i read from the config doc pages that the default is http://localhost:8080 with the http://localhost:8000 the same error message occurs as the one in the screenshot above.\n",
      "@hxr do you have some advice on what would be the expected galaxy_infrastructure_url based on our nginx setup? in the tool wrapper for the interactive tool there is entry point port 8888 and galaxy_web_port 8080. should i be using any of these for galaxy_infrastructure_url? thanks! maiken\n",
      "[/INST] . infrastructure url should really be the address you access galaxy at, i see listen 443 in your nginx settings, so for example eu sets it to galaxy_infrastructure_url: 'https://usegalaxy.eu' then the container will use the external address to contact galaxy's api which is a lot more reliable than trying to figure out the internal networking path to reach galaxy (what it initially tries within the container)\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "thanks a lot for getting back to me, and yes, understand completely the delay. ok, i understand, thanks. will give that a try once we have our system up and going again (going through some reinstallations!)\n",
      "\n",
      "ok, unfortunately that did not help at all. exact same error now using galaxy_infrastructure_url: https://galaxy-hepp.hpc.uio.no what else could be wrong?\n",
      "could there be some selinux issues? firewall issues? docker argument issues? i have tried looking for hints in various log files, but not really finding anything of interest. so not at all sure where i should be looking. if it is relevant: the galaxy server sends the job via slurm to a compute node, and jupterlab runs inside the docker container on this compute node. also not that (not sure if it is a hint or not) that in the docker container i see: root@d8087e912320:/storage/galaxy/jobs_directory/001/1251/working/jupyter# curl https://galaxy-hepp.hpc.uio.no/ curl: ssl certificate problem: unable to get local issuer certificate more details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. to learn more about this situation and how to fix it, please visit the web page mentioned above.\n",
      "hi again, sorry to use the ping, but we would love to get the put() and get() working @hxr could the issue be related to some missing docker options? we have this currently in our job_conf.xml\n",
      "[/INST] the container has a debug option, i believe, i should have thought of that sooner. if you set that environment variable, then the get/put commands will print out additional logging information that might help you diagnose the issue. i think you will need to set that in the tool xml.\n",
      "try adding something like true\n",
      "xref galaxy_ie_helpers/galaxy_ie_helpers at master · bgruening/galaxy_ie_helpers · githubinit.py#l12\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "yes thanks, did that actually manually in the container. but where is actually this logged to? i.e. where should i be able to find the log file?\n",
      "\n",
      "[/INST] it should log inside the notebook, whenever you run the command. if not, you can try running the get command in a terminal inside jupyter\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "right, i see no logging output at all in the notebook, only the exception. in the running container i did: root@f0557899417c:/storage/galaxy/jobs_directory/001/1252/working/jupyter# echo true in addition i manually edited the init.py : #!/usr/bin/env python from bioblend.galaxy import objects from bioblend.galaxy import galaxyinstance from bioblend.galaxy.histories import historyclient from bioblend.galaxy.datasets import datasetclient import subprocess import argparse import re import os from string import template import logging debug os.environ.get('debug', 'true' debug: # logging.basicconfig(level=logging.debug) logging.basicconfig(level=logging.debug) log logging.getlogger() and i have added some more log.debug messages in the code. i see no trace of the logging output in the notebook though.\n",
      "\n",
      "this is how it looks, you see the printout of the code where i added log.debug message, but the logging message is not actually shown in the notebook:\n",
      "also, in a terminal inside jupyter i get this (put taken as bash command)\n",
      "[/INST] try put in the terminal\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "hi, i met an error when i used the trinity tool in galaxy website. first, i uploaded my paired-end rna-seq clean data (12 files in total, containing 3 control samples and 3 test samples, such as control1_1.fq.gz, control1-2.fq.gz, control2_1.fq.gz, control2_2.fq.gz, which had been quality-controlled and filtered by the fastp software.) from my computer. then, i found the trinity software by typing \"trinity\" in the tools bar in the upper left corner of the galaxy website homepage and used this tool. all other options are default, except for the option of 'paired end'. in the strand reads\", i selected all the six files(x represents control1, control2, control3, test1, test2, test3). in the \"right/reverse strand reads\", i selected all the six files(x represents control1, control2, control3, test1, test2, test3). afterwards, i clicked on the tools\" button to start the analysis. however, the results turns red. one of them show that \"trinity on data 47, data 45, and others: gene to transcripts map\", an error occurred with this dataset: 格式 tabular while the other show that \"trinity on data 47, data 45, and others: assembled transcripts\", an error occurred with this dataset:格式 fasta could you help me to solve this problem? thank you in advance.\n",
      "\n",
      "[/INST] try transcript assembly for a single sample. does it work? check the standard output and standard error log file by clicking at info icon of any output from the failed job. in the middle window find the the log files and expand the black boxes. do you see any meaningful messages at the end of the log files? , igor\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "dear igor: thank your kind help. i try to use the trinity tool on the galaxy website to assemble a paired-end single sample files, including forward and reverse data files) as you suggested, but an error still appeared. but i still could not find the reason which lead to my analysis failure. i uploaded the error log information error_report , could you help me to solve the problem? thanks in advance! , weidong lu\n",
      "\n",
      "[/INST] the screenshot you posted is from the bug icon, not the job information icon. the latter has many more details that help with solving problems, and is what was asking for to help more. how/where to find job logs: troubleshooting errors when you get a red dataset in your history, it means something went wrong. but how can you find out what it was? and how can you report errors? when something goes wrong in galaxy, there are a number of things you can do to find out what it was. error messages can help you figure out whether it was a problem with one of the settings of the tool, or with the input data, or maybe there is a bug in the tool itself and the problem should be reported. below are the steps you can follow to troubleshoot your galaxy errors. expand the red history dataset by clicking on it. sometimes you can already see an error message here view the error message by clicking on the bug icon check the logs. output and error logs (stderr) of the tool are available: expand the history item click on the details icon scroll down to the job information section to view the 2 logs: tool standard output tool standard error for more information about specific tool errors, please see the troubleshooting section what to do next please post back what is in the stderr logs. you could also copy/paste back that entire page, with each of the sections fully expanded. this will speed up getting help, since any of those details could be the clue to solving the input problem. next time, for the fastest help, go ahead and post back that entire job information page content when asking a question. do not worry about it being long. you will get faster and better help from many more people. if you decide not to post the full content for some reason, then feedback is mostly an educated guess or just more requests for that same information. when working in galaxy, most of the technical details are in that view, instead of having to list things out. meaning, most of these questions are answered in a single view: what information should i include when reporting a problem? writing bug reports is a good skill to have as bioinformaticians, and a key point is that you should include enough information from the first message to help the process of resolving your issue more efficient and a better experience for everyone. what to include which commands did you run, precisely, we want details. which flags did you set? which server(s) did you run those commands on? what account/username did you use? where did it go wrong? what were the stdout/stderr of the tool that failed? include the text. did you try any workarounds? what results did those produce? (if relevant) screenshot(s) that show exactly the problem, if it cannot be described in text. is there a details panel you could include too? if there are job ids, please include them as text so administrators do not have to manually transcribe the job id in your picture. it makes the process of answering reports' much smoother for us, as we will have to ask you these questions anyway. if you provide this information from the start, we can get straight to answering your question! please post back the content, and we can try to help solve the problem\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "dear jennaj thank you for your kind reply.to illustrate the issue, i will briefly describe the specific details of the entire process as follows: using illumina instrument and double ended sequencing method, we measured the transcription of fungal mycelium under abiotic stress. the raw data files were fq.gz format. we upload our data from my computer to the galaxy website. in the pop-up window for uploading files, we selected and the default \"auto-detect\" was changed to \"fq.gz\", while the other parameters were selected as default.the data upload process was successful, and the final status was displayed in green. then we use the fastp and trimmomatic tools separately to filter the raw data in order to obtain clean data (the quality control work using fastqc has been completed on the offline local computer).the data filtering work was also successful, and the output results of both software were displayed in green. afterwards, we use the output data (just using two files of a sample, including sample1_read1 output and sample1_read2 output) of the fastp tool as the input files for transcript assemble. the color of the result turned red. in addition, we use the output data (just using two files of a sample, including sample1_read1_paired and sample1_read2_paired files) of the trimmomatic tool as the input files for transcript assemble, and the color of the result also turned red. 1,when i click on the red result made by the fastp tool, the wrong information displayed as follows: fastp_wrong_infor cernvm-fs: loading fuse module… done cernvm-fs: mounted cvmfs on /scratch/03166/xcgalaxy/main/staging/50920122/.cvmfsexec/dist/cvmfs/data.galaxyproject.org cernvm-fs: loading fuse module… done cernvm-fs: mounted cvmfs on /scratch/03166/xcgalaxy/main/ 2,when i click on the red result made by the trimmomatic tool, the wrong information displayed as follows: trimmomatic_wrong_infor d ) | | obove garbled codes, we do not understand what they mean either)\n",
      "\n",
      "[/INST] jennaj: please post back the content, and we can try to help solve the problem jennaj: check the logs. output and error logs (stderr) of the tool are available: expand the history item click on the details icon scroll down to the job information section to view the 2 logs: tool standard output tool standard error please post that exact information back, or we cannot help more here. find those here, thanks! d\n",
      "\n",
      "\n",
      "-------------------\n",
      "Act like Bioinformatician who uses Galaxy platform for biological data analysis. Understand the following instruction and prepare a suitable response.\n",
      "\n",
      "[INST]\n",
      "ok, jennaj, i got it. the screenshots were attached below.\n",
      "\n",
      "\n",
      "then i expand the black box of the tool standard output for 41:\n",
      "continued…\n",
      "then, i expanded the black box of the tool standard error for 41. it just contained one line: 图片\n",
      "the wrong information of data 39 was different with the data 41. its tool standard output was attached below:\n",
      "continued…\n",
      "图片\n",
      "[/INST] couple comments on the procedure. you can do all data transformation with fastp, there is no need for trimmomatic. add filter on read length, say 30 nt or longer. note that some tools such as trimmomatic apply filters in the specified order, so filter by length must be the last option. can you share the history, so i can have a look? the procedure: history menu share or publish make history accessible copy the url and paste in in reply to this email. if you have many files in the history, copy relevant files into a new history and share it. , igor\n",
      "\n",
      "\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i, row in small_df.iterrows():\n",
    "    print(row[\"conversations\"])\n",
    "    print(\"-------------------\")\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96a8e7-09fa-424e-a168-f8d939f8d3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa68c52-a1c7-4c25-addb-9e64d10d4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3dc08706-39ca-419b-90e3-771631b0a325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219b6ac-2118-4e01-a05b-3b1e684bb817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc6247-7b56-4f36-a4c6-70113d952c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
