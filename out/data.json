[[{"role": "user", "text": "Hi, I have a very basic notebook running in our local galaxy portal with interactive tools.\nAs you see, put() and get() functions are not recognized:\n\nScreen Shot 2021-08-18 at 13.30.25859\u00d7789 63 KB\n\nThe tool-wrapper is the following taken from some official galaxy jupyterlab/jupyternotebook tutorial page.\nWhat might I be missing to be able to have get() and put() working?\nTool wrapper:\n<tool id=\"interactive_tool_jupyter_notebook\" tool_type=\"interactive\" name=\"General Jupyter Notebook\" version=\"0.1\">\n  <description>Default empty notebook, reuse a previous one, or upload a new</description>\n  <requirements>\n    <container type=\"docker\">quay.io/bgruening/docker-jupyter-notebook:2021-03-05</container>\n  </requirements>\n    <entry_points>\n        <entry_point name=\"Jupyter Interactive Tool\" requires_domain=\"True\">\n            <port>8888</port>\n            <url>lab</url>\n        </entry_point>\n    </entry_points>\n    <environment_variables>\n        <environment_variable name=\"HISTORY_ID\">$__history_id__</environment_variable>\n        <environment_variable name=\"REMOTE_HOST\">$__galaxy_url__</environment_variable>\n        <environment_variable name=\"GALAXY_WEB_PORT\">8080</environment_variable>\n        <environment_variable name=\"GALAXY_URL\">$__galaxy_url__</environment_variable>\n        <environment_variable name=\"API_KEY\" inject=\"api_key\" />\n    </environment_variables>\n    <command detect_errors=\"aggressive\"><![CDATA[\n        #import re\n        export GALAXY_WORKING_DIR=`pwd` &&\n        mkdir -p ./jupyter/outputs/ &&\n        mkdir -p ./jupyter/data &&\n\n        #set $cleaned_name = re.sub('[^\\w\\-\\.]', '_', str($input.element_identifier))\n        ln -sf '$input' './jupyter/data/${cleaned_name}' &&\n\n        ## change into the directory where the notebooks are located\n        cd ./jupyter/ &&\n        export PATH=/home/jovyan/.local/bin:\\$PATH &&\n\n        #if $mode.mode_select == 'scratch':\n            ## copy default notebook\n            cp '$__tool_directory__/default_notebook.ipynb' ./ipython_galaxy_notebook.ipynb &&\n            jupyter trust ./ipython_galaxy_notebook.ipynb &&\n            jupyter lab --allow-root --no-browser  --ServerApp.token=''  &&\n            cp ./ipython_galaxy_notebook.ipynb '$jupyter_notebook'\n\n        #else:\n            #set $cleaned_name = re.sub('[^\\w\\-\\.]', '_', str($input.element_identifier))\n            cp '$mode.ipynb' ./${cleaned_name}.ipynb &&\n            jupyter trust ./${cleaned_name}.ipynb &&\n\n            #if $mode.run_it:\n                jupyter nbconvert --to notebook --execute --output ./ipython_galaxy_notebook.ipynb --allow-errors  ./*.ipynb &&\n            #else:\n                jupyter lab --allow-root --no-browser  --ServerApp.token='' &&\n            #end if\n            cp ./ipython_galaxy_notebook.ipynb '$jupyter_notebook' \n\n        #end if\n    ]]>\n    </command>\n    <inputs>\n\n        <conditional name=\"mode\">\n            <param name=\"mode_select\" type=\"select\" label=\"Do you already have a notebook?\" help=\"If not, no problem we will provide you with a default one.\">\n                <option value=\"scratch\">Start with a fresh notebook</option>\n                <option value=\"previous\">Load a previous notebook</option>\n            </param>\n            <when value=\"scratch\"/>\n            <when value=\"previous\">\n                <param name=\"ipynb\" type=\"data\" format=\"ipynb\" label=\"IPython Notebook\"/>\n                <param name=\"run_it\" type=\"boolean\" truevalue=\"true\" falsevalue=\"false\" label=\"Execute notebook and return a new one.\"\n                    help=\"This option is useful in workflows when you just want to execute a notebook and not dive into the webfrontend.\"/>\n            </when>\n        </conditional>\n        <param name=\"input\" type=\"data\" optional=\"true\" label=\"Include data into the environment\"/>\n\n    </inputs>\n    <outputs>\n        <data name=\"jupyter_notebook\" format=\"ipynb\" label=\"Executed Notebook\"></data>\n    </outputs>\n    <tests>\n        <test expect_num_outputs=\"1\">\n            <param name=\"mode\" value=\"previous\" />\n            <param name=\"ipynb\" value=\"test.ipynb\" />\n            <param name=\"run_it\" value=\"true\" />\n            <output name=\"jupyter_notebook\" file=\"test.ipynb\" ftype=\"ipynb\"/>\n        </test>\n    </tests>\n    <help>\n    The Jupyter Notebook is an open-source web applicati...\n"}, {"role": "system", "text": "Cross-referenced post: galaxyproject/admins - Gitter\nMore feedback once this is looked into more (soon!)."}, {"role": "user", "text": "Any ideas yet what is going on with put and get not working? As I asked in the gitter post, are these python methods? Do I maybe need to import some module?"}, {"role": "system", "text": "\n\n\n maikenp:\n\njupyter\n\n\nHi @maikenp\nSorry for the delay. The application was in the middle of an update at the usegalaxy.* servers and I wanted to cross-test. The functions put and get are now working at those. I\u2019m not exactly sure what is needed for configuration at other places. The GTN tutorials that cover the usage are not quite current \u2013 the tools are now in the tool panel, not under the Visualization masthead menu.\nIt looks like you are getting some help at Gitter \u2013 please see the latest message from last week: galaxyproject/admins - Gitter"}, {"role": "system", "text": "There are lots of tutorials covering the usage of ITs, but none covering building ITs.\nget and put are some special magic in the jupyter notebook we build. We configure a special environment for the notebook which loads those commands: docker-jupyter-notebook/ipython-profile.py at a48b767cd7d030b743605bb7984eda5eae76eb69 \u00b7 bgruening/docker-jupyter-notebook \u00b7 GitHub\nSo your tool XML looks fine, but it could be something about the specific container image you\u2019re using. In that container it SHOULD be loaded by default in the default user\u2019s home dir.\nI see that you\u2019re missing this statement:\nexport HOME=/home/jovyan/ &&\n\nthere\u2019s a strong change that could be causing it. Generally looking at your XML it has diverged quite far from the in-galaxy jupyter notebook, you might try diffing those files and seeing if some of those changes you\u2019ve made are relevant to the issues.\nIf it\u2019s ok for everyone, maybe let\u2019s keep the conversation here, it\u2019s a bit easier to see what\u2019s going on than scrolling through hundreds of messages to tease apart the thread."}, {"role": "user", "text": "Thanks, yes I much prefer disucssing here, the gitter is not ideal for discussions!\nLet\u2019s see thanks, I will add the export.\nCould you provide a link to the in-galaxy jupyter notebook?\nFor our notebook docker image, we used as a basis:  GitHub - bgruening/docker-jupyter-notebook: Jupyter running in a docker container. This image can be used to integrate Jupyter into Galaxy but added some more software."}, {"role": "user", "text": "For our notebook docker image, we used as a basis:  GitHub - bgruening/docker-jupyter-notebook: Jupyter running in a docker container. This image can be used to integrate Jupyter into Galaxy but added some more software.\nThe xml copy-pasted in this post is the one from the installation found in /srv/galaxy/server/tools/interactive/interactivetool_jupyter_notebook.xml  - with one change - the  <url>ipython/lab</url> which had to be changed to <url>lab</url>  and some outdated ServerApp replacement to NotebookApp - see some earlier discussion on gitter.\n[root@galaxy-hepp interactive]# diff /srv/galaxy/server/tools/interactive/interactivetool_jupyter_notebook.xml.orig interactivetool_jupyter_notebook.xml\n1,4c1,5\n< <tool id=\"interactive_tool_jupyter_notebook\" tool_type=\"interactive\" name=\"Interactive Jupyter Notebook\" version=\"0.1\">\n<     <requirements>\n<         <container type=\"docker\">quay.io/bgruening/docker-jupyter-notebook:2021-03-05</container>\n<     </requirements>\n---\n> <tool id=\"interactive_tool_jupyter_notebook\" tool_type=\"interactive\" name=\"General Jupyter Notebook\" version=\"0.1\">\n>   <description>Default empty notebook, reuse a previous one, or upload a new</description>\n>   <requirements>\n>     <container type=\"docker\">quay.io/bgruening/docker-jupyter-notebook:2021-03-05</container>\n>   </requirements>\n8c9\n<             <url>ipython/lab</url>\n---\n>             <url>lab</url>\n35c37\n<             jupyter lab --allow-root --no-browser  &&\n---\n>             jupyter lab --allow-root --no-browser  --ServerApp.token=''  &&\n46c48\n<                 jupyter lab --allow-root --no-browser --NotebookApp.shutdown_button=True &&\n---\n>                 jupyter lab --allow-root --no-browser  --ServerApp.token='' &&\n"}, {"role": "user", "text": "Answering myself: I can find the most recent jupyter_notebook here: galaxy/tools/interactive at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub"}, {"role": "user", "text": "Update, using the newest toolwrapper ( ) the put function is now recognized. Still having some issues using it, will get back with details. \nScreen Shot 2021-09-14 at 11.51.451184\u00d7224 34.1 KB\n"}, {"role": "user", "text": "Would you like me to create a new help issue - since now put() is indeed recognized, but I get this:\n\nScreen Shot 2021-09-14 at 11.57.471259\u00d71167 142 KB\n"}, {"role": "user", "text": "The key point being I think the use of the quay.io/bgruening/docker-jupyter-notebook:ie2 image."}, {"role": "system", "text": "Ok! Progress  Did you set galaxy_infrastructure_url, this can be important for the commands to connect to the galaxy server."}, {"role": "user", "text": "Dont think so. Where would I set that, and what should the value be?"}, {"role": "user", "text": "Ah, configuration option, right, lets see\u2026"}, {"role": "user", "text": "So, I set\ngalaxy_infrastructure_url: http://localhost:8000\nsince I see in our nginx setup:\n[root@galaxy-hepp config]# cat /etc/nginx/conf.d/galaxy-gie-proxy.conf\nserver {\n    # Listen on port 443\n    listen       443 ssl;\n(.... various ssl settings)\n    client_max_body_size 2G;\n    proxy_intercept_errors on;\n\n    fastcgi_buffers 16 16k;\n    fastcgi_buffer_size 32k;\n\n    #Proxy all requests to the GIE Proxy application\n    location / {\n       proxy_redirect off;\n       proxy_http_version 1.1;\n       proxy_set_header Host $host;\n       #proxy_set_header X-Real-IP $remote_addr;\n       proxy_set_header Upgrade $http_upgrade;\n       proxy_set_header Connection \"upgrade\";\n       proxy_pass http://localhost:8000;\n    }\n}\n\nBut in the galaxy.conf I see:\n[root@galaxy-hepp config]# cat /etc/nginx/conf.d/galaxy.conf \nserver {\n  listen 443 ssl;\n  server_name  galaxy-hepp.hpc.uio.no;\n  \n.....(various settings)\n\n  location / {\n    uwsgi_pass 127.0.0.1:8080;\n    uwsgi_param UWSGI_SCHEME $scheme;\n    include uwsgi_params;\n  }\n}\n\n\nWhich would be the correct to use?\nI read from the config doc pages that the default is http://localhost:8080\nWith the http://localhost:8000 the same error message occurs as the one in the screenshot above."}, {"role": "user", "text": "@hxr do you have some advice on what would be the expected galaxy_infrastructure_url based on our nginx setup?\nIn the tool wrapper for the interactive tool there is \nentry point port 8888 and galaxy_web_port 8080. Should I be using any of these for galaxy_infrastructure_url?\nThanks!\nMaiken"}, {"role": "system", "text": "Hi @maikenp sorry for the delay, too many things going on.\nInfrastructure url should really be the address you access galaxy at, I see listen 443 in your nginx settings, so for example EU sets it to\ngalaxy_infrastructure_url: 'https://usegalaxy.eu'\n\nThen the container will use the external address to contact galaxy\u2019s API which is a lot more reliable than trying to figure out the internal networking path to reach galaxy (what it initially tries within the container)"}, {"role": "user", "text": "Thanks a lot for getting back to me, and yes, understand completely the delay.\nOk, I understand, thanks. Will give that a try once we have our system up and going again (going through some reinstallations!)"}, {"role": "user", "text": "Ok, unfortunately that did not help at all. Exact same error now using\ngalaxy_infrastructure_url: https://galaxy-hepp.hpc.uio.no\n\nScreen Shot 2021-09-16 at 13.38.351143\u00d7880 121 KB\n\nWhat else could be wrong?"}, {"role": "user", "text": "Could there be some\n\nselinux issues?\nfirewall issues?\ndocker argument issues?\n\nI have tried looking for hints in various log files, but not really finding anything of interest. So not at all sure where I should be looking.\nIf it is relevant: the galaxy server sends the job via slurm to a compute node, and jupterlab runs inside the docker container on this compute node.\nAlso not that (not sure if it is a hint or not) that in the docker container I see:\nroot@d8087e912320:/storage/galaxy/jobs_directory/001/1251/working/jupyter# curl https://galaxy-hepp.hpc.uio.no/\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: https://curl.se/docs/sslcerts.html\n\ncurl failed to verify the legitimacy of the server and therefore could not\nestablish a secure connection to it. To learn more about this situation and\nhow to fix it, please visit the web page mentioned above.\n"}, {"role": "user", "text": "Hi again, sorry to use the ping, but we would love to get the put() and get() working @hxr\nCould the issue be related to some missing docker options? We have this currently in our job_conf.xml\n\nScreen Shot 2021-09-17 at 11.27.34907\u00d7183 26.5 KB\n"}, {"role": "system", "text": "The container has a DEBUG option, I believe, I should have thought of that sooner. If you set that environment variable, then the get/put commands will print out additional logging information that might help you diagnose the issue.\nI think you\u2019ll need to set that in the tool xml."}, {"role": "system", "text": "Try adding something like <environment_variable name=\"DEBUG\">True</environment_variable>"}, {"role": "system", "text": "xref galaxy_ie_helpers/galaxy_ie_helpers at master \u00b7 bgruening/galaxy_ie_helpers \u00b7 GitHubinit.py#L12"}, {"role": "user", "text": "Yes thanks, did that actually manually in the container. But where is actually this logged to? I.e. where should I be able to find the log file?"}, {"role": "system", "text": "it should log inside the notebook, whenever you run the command. If not, you can try running the get command in a terminal inside jupyter"}, {"role": "user", "text": "Right, I see no logging output at all in the notebook, only the exception.\nIn the running container I did:\nroot@f0557899417c:/storage/galaxy/jobs_directory/001/1252/working/jupyter# echo $DEBUG\ntrue\n\nIn addition I manually edited the init.py :\n#!/usr/bin/env python                                                                                                                                                                                              \nfrom bioblend.galaxy import objects\nfrom bioblend.galaxy import GalaxyInstance\nfrom bioblend.galaxy.histories import HistoryClient\nfrom bioblend.galaxy.datasets import DatasetClient\nimport subprocess\nimport argparse\nimport re\nimport os\nfrom string import Template\nimport logging\nDEBUG = os.environ.get('DEBUG', \"False\").lower() == 'true'\n#if DEBUG:                                                                                                                                                                                                         \n#    logging.basicConfig(level=logging.DEBUG)                                                                                                                                                                      \n#logging.getLogger(\"bioblend\").setLevel(logging.CRITICAL)                                                                                                                                                          \nlogging.basicConfig(level=logging.DEBUG)\nlog = logging.getLogger()\n\nAnd I have added some more log.debug messages in the code. I see no trace of the logging output in the notebook though."}, {"role": "user", "text": "This is how it looks, you see the printout of the code where I added log.debug message, but the logging message is not actually shown in the notebook:\n\nScreen Shot 2021-09-17 at 12.22.461834\u00d7983 147 KB\n"}, {"role": "user", "text": "Also, in a terminal inside jupyter I get this (put taken as bash command)\n\nScreen Shot 2021-09-17 at 12.26.451222\u00d7406 43.9 KB\n"}, {"role": "system", "text": "try put -h in the terminal"}, {"role": "user", "text": "yes, sorry, realized. We get debug output now in the terminal, thanks!"}, {"role": "user", "text": "So this is the reason:\nroot@f0557899417c:/storage/galaxy/jobs_directory/001/1252/working/jupyter# put --history-id 63cd3858d057a6d1  -p /storage/galaxy/jobs_directory/001/1252/working/jupyter/testfile.txt \nDEBUG:root:here I am\nDEBUG:root:Host IP determined to be b'172.17.0.1\\n'\nDEBUG:root:TestURL url=https://galaxy-hepp.hpc.uio.no obj=True\nDEBUG:bioblend:GET - attempts left: 1; retry delay: 10\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): galaxy-hepp.hpc.uio.no:443\nERROR:bioblend:HTTPSConnectionPool(host='galaxy-hepp.hpc.uio.no', port=443): Max retries exceeded with url: /api/histories/63cd3858d057a6d1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1125)'))), 0 attempts left\nDEBUG:root:TestURL url=https://galaxy-hepp.hpc.uio.no state=failure\nDEBUG:root:TestURL url=http://b'172.17.0.1':8080 obj=True\nDEBUG:bioblend:GET - attempts left: 1; retry delay: 10\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): b'172.17.0.1':8080\nERROR:bioblend:HTTPConnectionPool(host=\"b'172.17.0.1'\", port=8080): Max retries exceeded with url: /api/histories/63cd3858d057a6d1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f90c336bbe0>: Failed to establish a new connection: [Errno -2] Name or service not known')), 0 attempts left\nDEBUG:root:TestURL url=http://b'172.17.0.1':8080 state=failure\nDEBUG:root:do I see this?\nTraceback (most recent call last):\n  File \"/opt/conda/bin/put\", line 17, in <module>\n    put( args.filepath, file_type=args.filetype, history_id=args.history_id )\n  File \"/opt/conda/lib/python3.8/site-packages/galaxy_ie_helpers/__init__.py\", line 115, in put\n    gi = get_galaxy_connection(history_id=history_id)\n  File \"/opt/conda/lib/python3.8/site-packages/galaxy_ie_helpers/__init__.py\", line 101, in get_galaxy_connection\n    raise Exception(msg)\nException: Could not connect to a galaxy instance. Please contact your SysAdmin for help with this error\n\nSo then to figure out how to fix that."}, {"role": "user", "text": "Could it be that the CA we use:\nissuer=C = NL, O = GEANT Vereniging, CN = GEANT OV RSA CA 4\nneeds to be included in  /etc/ssl/certs in the container?\nI do not see it there."}, {"role": "user", "text": "We have solved the put() error - the problem was due to the lack of certificate cocatination on our galaxy-hepp.uio.no server, so the CA certificate was not available for requests.\nNow in place and put() does not give that error anymore.\n(Another error is being dealt with related to permissions in /storage/galaxy/tmp - but that is another issue)."}], [{"role": "user", "text": "Hi, I met an error when I used the Trinity tool in Galaxy website.\nFirst, I uploaded my paired-end RNA-seq clean data (12 files in total, containing 3 control samples and 3 test samples, such as control1_1.fq.gz, control1-2.fq.gz, control2_1.fq.gz, control2_2.fq.gz, \u2026, which had been quality-controlled and filtered by the fastp software.) from my computer. Then, I found the Trinity software by typing \u201ctrinity\u201d in the tools bar in the upper left corner of the galaxy website homepage and used this tool. All other options are default, except for the option of \u2018paired end\u2019. In the \u201cLeft/Forward strand reads\u201d, I selected all the six X_1.fa.gz files( x represents control1, control2, control3, test1, test2, test3). In the \u201cRight/Reverse strand reads\u201d, I selected all the six X_2.fa.gz files( x represents control1, control2, control3, test1, test2, test3). Afterwards, I clicked on the \u201crun tools\u201d button to start the analysis. However, the results turns red. One of them show that \u201cTrinity on data 47, data 45, and others: Gene to transcripts map\u201d, An error occurred with this dataset: \u683c\u5f0f tabular \u6570\u636e\u5e93 [?]., while the other show that \u201cTrinity on data 47, data 45, and others: Assembled Transcripts\u201d, An error occurred with this dataset:\u683c\u5f0f fasta \u6570\u636e\u5e93 [?]\nCould you help me to solve this problem? Thank you in advance."}, {"role": "system", "text": "Hi @luweidong\nTry transcript assembly for a single sample. Does it work?\nCheck the standard output and standard error log file by clicking at  info icon of any output from the failed job. In the middle window find the the log files and expand the black boxes. Do you see any meaningful messages at the end of the log files?\nKind regards,\nIgor"}, {"role": "user", "text": "Dear Igor:\nThank your kind help. I try to use the Trinity tool on the Galaxy website to assemble a paired-end single sample (two files, including forward and reverse data files) as you suggested, but an error still appeared. But I still could not find the reason which lead to my analysis failure. I uploaded the error log information\n\nerror_report773\u00d7714 96.5 KB\n, could you help me to solve the problem? Thanks in advance!\nBest wishes,\nWeidong Lu"}, {"role": "system", "text": "Hi @luweidong\nThe screenshot you posted is from the bug icon, not the job information icon. The latter has many more details that help with solving problems, and is what @igor was asking for to help more.\nHow/where to find job logs: Troubleshooting errors\n\nWhen you get a red dataset in your history, it means something went wrong. But how can you find out what it was? And how can you report errors?\nWhen something goes wrong in Galaxy, there are a number of things you can do to find out what it was. Error messages can help you figure out whether it was a problem with one of the settings of the tool, or with the input data, or maybe there is a bug in the tool itself and the problem should be reported. Below are the steps you can follow to troubleshoot your Galaxy errors.\n\n\nExpand the red history dataset by clicking on it.\n\n\nSometimes you can already see an error message here\n\n\n\nView the error message by clicking on the bug icon\n\n\nCheck the logs. Output (stdout) and error logs (stderr) of the tool are available:\n\n\nExpand the history item\nClick on the details icon\nScroll down to the Job Information section to view the 2 logs:\n\nTool Standard Output\nTool Standard Error\n\n\nFor more information about specific tool errors, please see the Troubleshooting section\n\n\n\nWhat to do next\n\nPlease post back what is in the stderr logs.\nYou could also copy/paste back that entire page, with each of the sections fully expanded. This will speed up getting help, since any of those details could be the clue to solving the input problem.\n\nNext time, for the fastest help, go ahead and post back that entire job information page content when asking a question. Don\u2019t worry about it being long. You\u2019ll get faster and better help from many more people. If you decide not to post the full content for some reason, then feedback is mostly an educated guess or just more requests for that same information.\nWhen working in Galaxy, most of the technical details are in that view, instead of having to list things out. Meaning, most of these questions are answered in a single view: What information should I include when reporting a problem?\n\nWriting bug reports is a good skill to have as bioinformaticians, and a key point is that you should include enough information from the first message to help the process of resolving your issue more efficient and a better experience for everyone.\nWhat to include\n\nWhich commands did you run, precisely, we want details. Which flags did you set?\nWhich server(s) did you run those commands on?\nWhat account/username did you use?\nWhere did it go wrong?\nWhat were the stdout/stderr of the tool that failed? Include the text.\nDid you try any workarounds? What results did those produce?\n(If relevant) screenshot(s) that show exactly the problem, if it cannot be described in text. Is there a details panel you could include too?\nIf there are job IDs, please include them as text so administrators don\u2019t have to manually transcribe the job ID in your picture.\n\nIt makes the process of answering \u2018bug reports\u2019 much smoother for us, as we will have to ask you these questions anyway. If you provide this information from the start, we can get straight to answering your question!\n\nPlease post back the content, and we can try to help solve the problem "}, {"role": "user", "text": "Dear jennaj\nThank you for your kind reply.To illustrate the issue, I will briefly describe the specific details of the entire process as follows:\nUsing illumina instrument and double ended sequencing method, we measured the transcription of fungal mycelium under abiotic stress. The raw data files were fq.gz format. We upload our data from my computer to the Galaxy website. In the pop-up window for uploading files, we selected \u201cregular\u201d and the default \u201cAuto-detect\u201d was changed to \u201cfq.gz\u201d, while the other parameters were selected as default.The data upload process was successful, and the final status was displayed in green. Then we use the fastp and trimmomatic tools separately to filter the raw data in order to obtain clean data (the quality control work using Fastqc has been completed on the offline local computer).The data filtering work was also successful, and the output results of both software were displayed in green. Afterwards, we use the output data (just using two files of a sample, including sample1_read1 output and sample1_read2 output) of the fastp tool as the input files for transcript assemble. The color of the result turned red. In addition, we use the output data (just using two files of a sample, including sample1_read1_paired and sample1_read2_paired files) of the trimmomatic tool as the input files for transcript assemble, and the color of the result also turned red.\n1,when I click on the red result made by the fastp tool, the wrong information displayed as follows:\n\nfastp_wrong_infor792\u00d7521 45 KB\n\nCernVM-FS: loading Fuse module\u2026 done\nCernVM-FS: mounted cvmfs on /scratch/03166/xcgalaxy/main/staging/50920122/.cvmfsexec/dist/cvmfs/data.galaxyproject.org\nCernVM-FS: loading Fuse module\u2026 done\nCernVM-FS: mounted cvmfs on /scratch/03166/xcgalaxy/main/\n2,when I click on the red result made by the trimmomatic tool, the wrong information displayed as follows:\n\ntrimmomatic_wrong_infor802\u00d7480 25.4 KB\n\n\n|      ||    \\ |    ||    \\ |    ||      ||  |  |\n|      ||  D  ) |  | |  _  | |  | |      ||  |  |\n|_|  |_||    /  |  | |  |  | |  | |_|  |_||  ~  |\n  |  |  |    \\  |  | |  |  | |  |   |  |\n\n(The obove garbled codes, we don\u2019t understand what they mean either)"}, {"role": "system", "text": "\n\n\n jennaj:\n\nPlease post back the content, and we can try to help solve the problem \n\n\n\n\n\n jennaj:\n\n\n\nCheck the logs. Output (stdout) and error logs (stderr) of the tool are available:\n\n\nExpand the history item\nClick on the details icon\nScroll down to the Job Information section to view the 2 logs:\n\nTool Standard Output\nTool Standard Error\n\n\n\n\n\nPlease post that exact information back, or we cannot help more here.\nFind those here, thanks!\n\nd26b531072117e62af41ecb297e8b0d18aad435e802\u00d7480 28.8 KB\n"}, {"role": "user", "text": "OK, jennaj, I got it. The screenshots were attached below.\n\n11192\u00d7752 95.4 KB\n\n\n21201\u00d7705 87.6 KB\n\n\n31193\u00d7758 90.8 KB\n"}, {"role": "user", "text": "\n41052\u00d7757 71.4 KB\n\n\n51041\u00d7755 61.9 KB\n\n\n61049\u00d7758 63.9 KB\n"}, {"role": "user", "text": "Then I expand the black box of the Tool Standard Output for 41:\n\n10677\u00d7672 45.8 KB\n\n\n11682\u00d7703 29.2 KB\n\n\n12686\u00d7682 102 KB\n\n\n13679\u00d7705 115 KB\n"}, {"role": "user", "text": "continued\u2026\n\n14676\u00d7636 104 KB\n\n\n15677\u00d7698 99.6 KB\n\n\n161014\u00d7753 129 KB\n"}, {"role": "user", "text": "Then, I expanded the black box of the Tool Standard Error for 41. It just contained one line:\n\n\u56fe\u72471008\u00d783 5.8 KB\n"}, {"role": "user", "text": "The wrong information of data 39 was different with the data 41. Its tool standard output was attached below:\n\n20506\u00d7707 40.9 KB\n\n\n21504\u00d7696 49 KB\n\n\n22506\u00d7712 61.6 KB\n"}, {"role": "user", "text": "continued\u2026\n\n23506\u00d7681 63.7 KB\n\n\n24500\u00d7670 54.9 KB\n\n\n25782\u00d7758 96.9 KB\n"}, {"role": "user", "text": "\n\u56fe\u7247778\u00d7264 22.4 KB\n"}, {"role": "system", "text": "Hi @luweidong\ncouple comments on the procedure. You can do all data transformation with fastp, there is no need for Trimmomatic. Add filter on read length, say 30 nt or longer. Note that some tools such as Trimmomatic apply filters in the specified order, so filter by length must be the last option.\nCan you share the history, so I can have a look? The procedure: History menu > Share or publish > Make history accessible > copy the URL and paste in in reply to this email. If you have many files in the history, copy relevant files into a new history and share it.\nKind regards,\nIgor"}, {"role": "user", "text": "The output result of fastp software is used as an input file for Trinity software, but the result is still incorrect and displayed in red. The corresponding error message is as follows:\n\n301066\u00d7718 87.3 KB\n\n\n31772\u00d7714 57.1 KB\n\n\n32525\u00d7713 41.2 KB\n\n\n33504\u00d7689 24.8 KB\n\n\n34504\u00d7705 60.7 KB\n"}, {"role": "user", "text": "\n35511\u00d7707 65 KB\n\n\n36507\u00d7689 53.5 KB\n\n\n37506\u00d7695 17.9 KB\n\n\n38504\u00d7707 19 KB\n"}, {"role": "user", "text": "\n39507\u00d7711 26.2 KB\n\n\n40505\u00d7708 25.1 KB\n\n\n41771\u00d7657 64.1 KB\n"}, {"role": "system", "text": "Hi @luweidong\nthe links are for datasets. With these links I can download the data, but I need access to the history. You can delete the message with the links. Please try the following: History menu in the top right corner of the history panel > Share or Publish > Make history accessible > Copy and paste the URL here.\nThank you!\nKind regards,\nIgor"}, {"role": "user", "text": "Dear igor, this is the URL of history:\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBest wishes,\nWeidong Lu"}, {"role": "system", "text": "@luweidong\nReview these items:\n\nThe FastQC report shows that your reads need 5\u2019 trimming\nThe logs from Trinity are still showing that reads <25 bases also remain\nYou haven\u2019t used fastp yet as @igor recommended. Trimmomatic can do this too. Both can address items 1 and 2. I think we have already shared the QA tutorial, but here is the link again: Quality Control\n\n\nrnaSPAdes was successful, and is a better tool choice overall, but remember: not failing only means that nothing technically went wrong and doesn\u2019t indicate a quality scientific result.\nStart with the QA steps first. Leave the FastQC result from before and after fastp undeleted and in that same history.\n"}, {"role": "system", "text": "Hi @luweidong\nI got the history. Thank you! You can delete the link or keep it for some time, it is up to you.\nI could not spot any any obvious issue. Nucleotide composition changes along the read length. Phred quality scores use just several values. I submitted couple test jobs. The jobs will take some time.\n@jennaj I am not familiar with setup of the main server. What does this error means?\nocean/projects/mcb140028p/xcgalaxy/main/staging/50953944/.cvmfsexec/mountrepo: line 70: cd: /ocean/projects/mcb140028p/xcgalaxy/main/staging/50953944/.cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch/etc/cvmfs: No such file or directory\nThank you!\nIgor"}, {"role": "system", "text": "Hi @igor\nIf a tool does not produce any output at all, that error can result from this specific tool.\nI think the reads need to be 5\u2019 trimmed, then length filtered. Trimmomatic with the right settings will do that, or fastp \nIf just getting any result is the goal, that is already achieved with the other tool. No idea how downstream tools will work on that output though. At ORG, rnaSPAdes is more robust and will \u201cdo something\u201d with reads that fail Trinity. Eg: with insufficient QA. Specifically, a lot of very short reads clogs up the Trimmomatic assembly process and the tool dies due to limited runtime memory available. We won\u2019t be allocating more, or at least not now."}, {"role": "user", "text": "Dear @jennaj,\nBefore I sent my former history, I had already used the fastp tool to filter the data. In that case, when the output files of Fastp were used as the input files for trinity analysis, the results still display in red.\nIn order to simplify and fully display the entire process, I created a new history and uploaded two original sequencing data (forward and reverse reads) of a sample, used the fastqc tool to control the quality of the data before and after using fastp, and leaved these results in the history. According to the fastqc report and your suggestion, 5 'trim was performed with the fastp tool, and the first 10 bases of each read were deleted. I believe that the cleaned data should meet the analysis requirements of the Trinity tool. As a control, the rnaSPAdes tool was used to assemble transcript reads as well. Unfortunately, the result of Trinity analysis showed red color, while the rnaSPAdes result was blank (0 byte). I uploads the history of this analysis, as shown below:\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBest wishes,\nWeidong Lu"}, {"role": "system", "text": "Hi @jennaj\nI also have impression that rnaSPAdes is more forgiving on reads compared to Trinity.\nI have access only to Trinity job submitted on untrimmed reads. The job was submitted with digital normalization. This should reduce amount of data used for assembly, but I cannot check requested memory on ORG.\nBiased nucleotide composition at 5\" ends is common for illumina RNA-Seq data. I don\u2019t know how much problem it causes.\ni."}, {"role": "user", "text": "Dear @igor @jennaj ,\nI have an intuition that the reason why Trinity analysis results always report errors is that the Galaxy platform allocates insufficient memory for each file. Trinity needs huge memory compared with other tools. This understanding comes from a literature titled \u201cDe novo transcriptome assembly: A comprehensive cross-species comparison of short-read RNA-Seq assemblers. GigaScience, 2019,8:1\u201316\u201d.\n  \n      \n\n      OUP Academic\n  \n\n  \n    \n\nDe novo transcriptome assembly: A comprehensive cross-species comparison of...\n\n  AbstractBackground. In recent years, massively parallel complementary DNA sequencing (RNA sequencing [RNA-Seq]) has emerged as a fast, cost-effective, and robus\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nTable 3 of this article was screenshot as follows:\n\n\u56fe\u7247673\u00d7709 49.8 KB\n"}, {"role": "user", "text": "A very interesting thing. As shown in the history (16), the results of rnaSPAdes analysis had no sequence if each reads were 5\u2019 trimmed for 10 bp with fastp while other paramters remains unchanged, but contained 25071 assemble sequences (21) if using the default parameters of the fastp.\n\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "Hi @luweidong\nit can be memory, but I don\u2019t have admin access to ORG and cannot check it.\nOld version of Trinity works on the untrimmed reads. You are after datasets #14 and #15 in this history:\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nYou can import the history. I will delete it at some point.\nKind regards,\nIgor"}, {"role": "user", "text": "Dear @igor ,\nThank you very much. I saw the datasets #14 and #15 in the history. How can I choose the older version of the trinity software in the Galaxy platform?\nI am currently dividing both files of a sample into four small files to reduce the memory requirement for calculations and hope this works."}, {"role": "system", "text": "Hi @luweidong\nduring job setup click at three blocks (versions) icon at the top right corner of the middle window and select any available version from the pull-down menu.\nYou can use assembled transcripts and transcript to genes map from history I shared.\nYou can copy datasets from one history to another using See histories side by side (in history menu)\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI\u2019m attempting to run HISAT2 on paired RNAseq data. I have run it successfully previously on the main server using the mm10 built-in reference genome, however, I am now using a local server and the built-in reference genomes have apparently not been included in the set-up. I\u2019m hoping to get some assistance on how to obtain the right reference genome file for mm10 installed, or even better how to update the local server so the same built-in reference genome will be available as in the main server?\nThank you,\nAsta"}, {"role": "system", "text": "Hi,\nThe mm10 reference genome/build is sourced from UCSC.\nYou\u2019ll need to index the genome on your server with Data Manager tools.\nInstall the genome with Data Managers (sourced from the ToolShed, all are in a separate category there). Install DMs like any other tool using the Admin functions.\nYou\u2019ll need these DMs at a minimum. Execute them in this order first:\n\n\nFasta fetcher \u2013 has an option to pick UCSC as the data source.\nSAM indexer\nPicard indexer\n2bit (twoBit) indexer\n\nThen get the DMs that create indexes for the tools you want to use. Run these after the others have above have completed for the best results.\nThe above was copied over and slightly updated from this prior Galaxy Biostars Q&A. It has more details (worth reviewing):\nhttps://biostar.usegalaxy.org/p/19371/\nA search here with the keyword string \u201cbiostars data manager fetch sam picard\u201d will find more prior Q&A that covers many different use cases: https://galaxyproject.org/search/\n\n\nMore details\n\nBe very careful about not running an indexing tool twice \u2013 duplicated lines will cause problems and are a hassle to correct.\nIf you use a \u201cdbkey\u201d that is already available, you must make sure that your genome is an exact match for that content. Genomes that are already indexed on one of the public Galaxy servers (and associated with an existing dbkey) can be found here:  http://datacache.galaxyproject.org/\n\nData Libraries are great for storing datasets commonly used (copies do not consume any disk space), and you  could  put in a fasta for a genome in one to use it with tools as a Custom Genome (no precomputed tool  indexes  can be used from there or a history, so don\u2019t bother to load them).\nCustom genome help is in this FAQ and a related FAQ. If you are indexing a genome from an uploaded fasta file for some reason (not available directly from a known source), the same fasta formatting rules apply:\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\n\n\n\nBut IF you are admin of a server  it would be better to install the genome directly and index it for tools on the server. Jobs will use fewer resources and run faster this way, plus be much less likely to fail for memory reasons.\nData Manager tools are installed/used under the \u201cAdmin\u201d masthead, top section. The \u201cloc\u201d files/table content is there as well.\nThe jobs that are run by Data Manager tools will be sent to your active history. It is strongly recommended to set up a distinct history for each genome, or batch of genomes, and date it in the name, to better keep track of what was done over time.\n\n"}], [{"role": "user", "text": "submitting a job to a SGE 8.1.9 Cluster  from an application called galaxy logged the following error:\ngalaxy.jobs.runners.drmaa WARNING 2019-02-13 13:20:43,111 (427) drmaa.Session.runJob() failed, will retry: code 17: MUNGE authentication failed: Invalid credential format\nI have verified UIDs and GIDs across host and cluster are alike as well as verified perms for munge dirs and files match install docs.  Also verfied munge.key matched across cluster and host.\nUsed this python script outside of galaxy to submit job to cluster with success:\nimport drmaa\nfrom multiprocessing.pool import ThreadPool\nimport tempfile\nimport os\nimport stat\n\n\nsession = drmaa.Session()\nsession.initialize()\ndef main():\n    smt = \"ls . > test.out\"\n    script_file = tempfile.NamedTemporaryFile(mode=\"w\", dir=os.getcwd(), delete=False)\n    script_file.write(smt)\n    script_file.close()\n    print \"Job is in file %s\" % script_file.name\n    os.chmod(script_file.name, stat.S_IRWXG | stat.S_IRWXU)\n    jt = session.createJobTemplate()\n    print \"jt created\"\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    print \"environment set\"\n    jt.remoteCommand = os.path.join(os.getcwd(),script_file.name)\n    print \"remote command set\"\n    jobid = session.runJob(jt)\n    print \"Job submitted with id: %s, waiting ...\" % jobid\n    retval = session.wait(jobid, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n\nif __name__=='__main__':\n    main()\n\nWHEN I try this same script with Python Multithreading, I get error Script and error are:\nimport drmaa\nfrom multiprocessing.pool import ThreadPool\nimport tempfile\nimport os\nimport stat\n\npool = ThreadPool(1)\n\nsession = drmaa.Session()\nsession.initialize()\n\ndef pTask(n):\n    smt = \"ls . > test.out\"\n    script_file = tempfile.NamedTemporaryFile(mode=\"w\", dir=os.getcwd(), delete=False)\n    script_file.write(smt)\n    script_file.close()\n    print \"Job is in file %s\" % script_file.name\n    os.chmod(script_file.name, stat.S_IRWXG | stat.S_IRWXU)\n    jt = session.createJobTemplate()\n    print \"jt created\"\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    print \"environment set\"\n    jt.remoteCommand = os.path.join(os.getcwd(),script_file.name)\n    print \"remote command set\"\n    jobid = session.runJob(jt)\n    print \"Job submitted with id: %s, waiting ...\" % jobid\n    retval = session.wait(jobid, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n\npool.map(pTask, (1,))\nResult is:\nJob is in file /home/svc-clingalprod/tmpu3A6Rk\njt created\nenvironment set\nerror: getting configuration: MUNGE authentication failed: Invalid credential format\nremote command set\nTraceback (most recent call last):\n  File \"remote_mthread.py\", line 29, in <module>\n    pool.map(pTask, (1,))\n  File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 250, in map\n    return self.map_async(func, iterable, chunksize).get()\n  File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 554, in get\n    raise self._value\ndrmaa.errors.DeniedByDrmException: code 17: MUNGE authentication failed: Invalid credential format\n\nWhere do I go from here in isolating the cause of the Invalid Credential format error?"}, {"role": "system", "text": "So this is interesting and going a bit further than https://github.com/pygridtools/drmaa-python/issues/44, as this is failing also with just a single thread. Maybe we can come up with a patch for drmaa-python, which would probably be the best solution. Can you check that this works with a ProcessPool instead of a ThreadPool ?"}, {"role": "system", "text": "Also, does it work if you initialize the session within the thread:\nimport drmaa\nfrom multiprocessing.pool import ThreadPool\nimport tempfile\nimport os\nimport stat\n\npool = ThreadPool(1)\n\ndef pTask(n):\n    session = drmaa.Session()\n    session.initialize()\n    smt = \"ls . > test.out\"\n    script_file = tempfile.NamedTemporaryFile(mode=\"w\", dir=os.getcwd(), delete=False)\n    script_file.write(smt)\n    script_file.close()\n    print \"Job is in file %s\" % script_file.name\n    os.chmod(script_file.name, stat.S_IRWXG | stat.S_IRWXU)\n    jt = session.createJobTemplate()\n    print \"jt created\"\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    print \"environment set\"\n    jt.remoteCommand = os.path.join(os.getcwd(),script_file.name)\n    print \"remote command set\"\n    jobid = session.runJob(jt)\n    print \"Job submitted with id: %s, waiting ...\" % jobid\n    retval = session.wait(jobid, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n\npool.map(pTask, (1,))\n"}, {"role": "user", "text": "That will take me a little while to work through.  Parallel processes and threads are new to me.  I will let you know.  In the meantime is Pulsar a viable option for my situation?  I need to provide some options for solutions."}, {"role": "system", "text": "Yes, but that\u2019s more complicated than necessary. If you can use qsub on the host where Galaxy runs your best bet is probably to use the command line interface runner with the PBS backend. I\u2019m using this in production (though ssh), this should be a viable option."}], [{"role": "user", "text": "I need a tool which can change the transcript id to ensemble or entraz ID ? i need it for goseq analysis, i think the problem in that tool is from the IDs.\nthanks"}, {"role": "system", "text": "Hi @amir\nThis tool will convert IDs. You may need to make a \u201cconvert\u201d file yourself from other public resources.\n\n\nReplace column  by values which are defined in a convert file (Galaxy Version 0.2)\n\nThanks!"}, {"role": "user", "text": "@jennaj Hi\nIf i use this tool, you sure the counts doesnt change ? because your saying i need to find the id list from another ID list, So this tool can identify which is which in my ID list /?\nAnd BTW\u2026 i need this for Goseq analysis, why the goseq does not recognize the transcript id ? whats wrong about that tool? I did tried some many times but it never worked"}, {"role": "system", "text": "That Replace tool will transform IDs if you need to. It accepts any two column tabular mapping file. The ID it changes has to be in the first column of your input dataset. There are other ways to do that of course \u2013 many tools will match up data based on a common value, that one just happens to do the replace all in one step. Values in other fields are not modified.\nThe goseq tool does not look up transcriptIDs, but geneIDs. Maybe that is where you are confused? A transcript-to-gene mapping input is required.\nOther troubleshooting help that has resolved usage issues in the past for goseq:\n\nThere should be no versions on transcript or gene names. If present, remove those (the . dot and any number after).\n\nA correctly formatted transcript-to-gene reference is needed for the goseq tool. The IDs from all inputs need to match. This input can be a gtf dataset (with no header lines) or a two column tabular dataset. A gff3 annotation will not work. Sorry, wrong advice, this particular tool does not need a mapping file of this type, all inputs are already summarized by geneID (by the tool Featurecounts or however you decide to do the counting).\nThen confirm that the gene identifier type is match for the chosen \u201cSelect Gene ID format\u201d. You could just do a web search with your ID to find out what type it is.\nOnly four genome builds are supported as built-in indexes. Or you can supply one yourself.\nThe format for all the inputs is described in the help section with examples. If any are malformed, the tool will fail.\n\nGeneral FAQ for DE tools. The formatting rules apply not only to DE analysis (are general best practices, summarized, that link to the other FAQs) and covers some of the topics we have discussed when you had trouble with other tools. May help now: Extended Help for Differential Expression Analysis Tools\nIf you are still stuck after this, are you really working at usegalaxy.org? You could send in a bug report if so. Include a link to this Galaxy help post in the comments and send a reply here, so I know when to look for it. I might recognize what is going wrong. Make sure the error from goseq and all input dataset are left undeleted.\nI see several posts from you about this and some are labeled as usegalaxy.eu and some are for usegalaxy.org. Including this one, which does contain a history link: Problem in Goseq tool. Is that still current?"}, {"role": "system", "text": "Ok, I see the problems.\n\nThe IDs are transcripts, not genes. Only geneIDs work with this tool. A search for one of those IDs points to here at the Ensembl web site.\nThe IDs have a . plus version. Those need to be removed once you count up by gene, if still present.\nOne of your inputs has a header line. That should be removed, again, once you have counts by gene, not transcript.\nThe IDs are definitely Ensembl. And are human (GRCh38).\nThe \u201cGene Categories\u201d input was selected from the history, and was is a fasta dataset, not a tabular \u201cgene-to-go\u201d mapping file. For GRCh38 you don\u2019t need to supply your own mapping \u2013 the genome is supported. Instead set the form to use a built-in index and choose \u201cHuman (hg38)\u201d.\n\nThe proper inputs/formats are all listed out on tool form with examples."}, {"role": "user", "text": "I did these :\n\nexcluding the \u201c.\u201d from my transcript ID(A)\n2.get annotate my transcript ID to get Gene symbol with \u201cannotatemyID\u201d(B)\n3: join the two dataset( A and B results) with \u201cPaste\u201d\n4: Delete the other column with \u201cCut\u201d\n5:get the results into goseq.\nbut still i get this error (look at screen shot)\n\nUntitled1366\u00d7768 144 KB\n\n\n\nWhats the problem"}, {"role": "system", "text": "@amir\nYour original counts were \u201cby transcript\u201d and you swapped those out for gene symbols. Multiple transcripts can be associated with any single gene.\nDuplicated gene names won\u2019t work with goseq. This tool expects the input counts to be \u201cby gene\u201d. You probably need to back up and redo the counting steps by gene, not transcript. Just adding up the counts for each transcript belonging to the same gene will create scientific content problems (overcounting)."}, {"role": "user", "text": "So how do i exactly redo the counting steps,  which the Id, be gene instead of  trascript ?"}, {"role": "system", "text": "The counts are \u201cby gene\u201d when using Featurecounts with the default settings. And since your data is human, you could use the indexed hg38 genome and built-in annotation.\n\u201cTranscriptomics\u201d tutorials here go over the details, but I think you have seen those? Maybe review again and compare to your steps?\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training\n\nCollection of tutorials developed and maintained by the w...\n\n\n  \n  \n    \n    \n  \n  \n\n"}, {"role": "user", "text": "I did it with the default setting in featureCount. the Gene id looks like this.\n\n\n\n\nGeneid\nLength\n\n\n\n\n100287102\n1652\n\n\n653635\n1769\n\n\n102466751\n68\n\n\n100302278\n138\n\n\n645520\n1130\n\n\n79501\n918\n\n\n729737\n5474\n\n\n102725121\n1173\n\n\n\nits not what ive been thinking"}, {"role": "user", "text": "Hi @jennaj\nI used the default mode as you said and my geneID is like this : |Geneid|Length|\n| \u2014 | \u2014 |\n|100287102|1652|\n|653635|1769|\n|102466751|68|\n|100302278|138|\n|645520|1130|\n|79501|918|\n|729737|5474|\n|102725121|1173|\nare you sure this is working ? Goseq need ensembel ids and those are not Ensenbel IDs"}, {"role": "system", "text": "Right, these are Entrez IDs (expected).\nMap these to Ensembl with the tool:\nannotateMyIDs  annotate a generic set of identifiers (Galaxy Version 3.7.0+galaxy1)\nAfter, rearrange the columns so that the Ensembl name replaces the Entrez name, and the file is back in the original format, with no extra columns of data.\nThere are two different but similar \u201ccut\u201d tools. One will rearrange and eliminate columns of data (what you need) \u2013 the other only eliminates columns (preserves the original order of columns, not what you need). The tool version may change over time, but you can tell the difference by the name, or by reviewing the tool forms. The second below clearly states that it does not rearrange columns with a pointer to the first tool.\n\n\nCut  columns from a table (Galaxy Version 1.0.2) << Use this one\n\nCut  columns from a table (cut) (Galaxy Version 1.1.0)\n\nImportant: Swap the Entrez gene identifiers to be Ensembl gene identifiers for all of your inputs to Goseq. All inputs must match up."}, {"role": "user", "text": "Ok. it works. But about that Cut(table) you said, what should i put in field box to replace the Ensenbl ID and the counts back in the table, like the original format ?  i think if i want to put the counts beside the Ensembel IDs, should i not using the tools like Join two dataset ?"}, {"role": "user", "text": "@jennaj Hello\nIm still waiting for your respond. I know its close to Christmas and you are very busy .\\\nP.S Wish you the Best in 2020"}, {"role": "system", "text": "Yes, you might need to join datasets based on common keys, cut/rearrange columns, plus add back in prior headers or create new ones.\nI haven\u2019t been giving the full list of text manipulation tools, with exact steps, since you have been using Galaxy for a while now. Instead, more of an overview and pointing out the tool(s) that solve a particular problem you might not know about (ex: ID conversions) or that are more likely to be tricky to use (example: how/why to pick the version of the cut tools that will do what you need).\nUse your best judgment for what intermediates tools are needed for your data manipulations. There are usually many ways to do any particular data manipulation to meet the end goal of proper format/content for inputs.\nExample: use Text Manipulation and related tools (that do one thing each) in combination and/or use tools that allow for custom programming, if you know how to use them or are willing to learn (sed, awk, replace). Almost all of these data manipulation tools are based on basic line-command functions, or mimic them, on purpose, to give as much flexibility as possible. The tool name in Galaxy is often the same name as the line-command utility. Nearly anything that can be done line-command can be done in Galaxy.\nOne way isn\u2019t \u201cbetter\u201d than others as long as you end up with the final correct result at the end. Then put those tools/steps into mini-workflows for re-use. Workflows have an option to display directly in your tool history so they can be used like a \u201ccustom tool\u201d (can hide intermediate steps/datasets, or remove them when not needed, or rename/label the final output to be informative). Common manipulation steps, grouped for a specific goal and put into a workflow, can be used directly like any other tool, and/or added as a sub-workflow in a larger workflow \u2013 I and many others do both\u2026  it depends on the overall manipulation and how often I think I\u2019ll want to do it again.\nThe primary advantages of using Galaxy is that all your work is recorded for reproducibility, can be put into workflows, all is easily shared with others (in context), plus all usage is GUI based. When using a public server, then that compute resources on that server is used, with no need to provide that from your own compute resources/system and no need to install/configure tools at the technical level. Even super-users that know how to do analysis line-command choose to use Galaxy for these reasons. Keeping track of analysis pipelines \u2013 what was done, exactly \u2013 is a big headache when working line-command. Line-command work is also challenging to share with others in a clear reproducible way (for collaboration, publication, or however you decide to share/publish). In short, Galaxy makes it easy to track/share 1) input/result data (histories) and 2) methods/tools/parameters (workflows) \u2013 for your own personal use or otherwise."}, {"role": "system", "text": "There\u2019s also a one-step tool that can do this one task pretty well:\nhttps://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/annotatemyids/annotatemyids/3.7.0+galaxy1\nor https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/annotatemyids/annotatemyids/3.7.0+galaxy1\ndepending on which server you\u2019re using."}, {"role": "system", "text": "Right, that is what he is using after Featurecounts. But needs to reformat the results so can be used as inputs for goseq (counts and length data). Wants data using with Ensembl gene IDs instead of Entrez.\nI don\u2019t think annotatemyIds will reformat/create outputs directly for goseq \u2013 or am I missing something? Totally possible "}, {"role": "system", "text": "Goseq works with a list of differentially expressed genes, so that wouldn\u2019t be right. There\u2019s a full tutorial here that uses goseq: https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html"}, {"role": "system", "text": "Agree   Featurecounts is one step (counts) \u2013 the DE steps still need to be done for the true/false expression determination.\nI think @amir is already following that tutorial\u2019s workflow, just had some problems with getting the inputs right \u2013 matching annotation + ref genome. Was counting by transcript originally and had other input issues (fasta was input instead of tabular length). And wanted to use Ensembl IDs. But I\u2019ll let him follow up with actual goals, may have misunderstood."}, {"role": "user", "text": "So i done these steps \n1:  getting annotatation with  AnnotatedmyID, by the way, i Hit the box which was for deleting duplication, for my next step\n2: join both data(Change case+annotatedmyID) with Paste Tool\n3: Cut the colmunms needed for the goseq\n\u2026 Unfortunatly\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: row.names<- -> row.names<-.data.frame -> .rowNamesDF<-\nWarning message:\nnon-unique values when setting \u2018row.names\u2019: \u2018ENSG00000004866\u2019, \u2018ENSG00000011454\u2019, \u2018ENSG00000049449\u2019, \u2018ENSG00000076928\u2019, \u2018ENSG00000086205\u2019, \u2018ENSG00000104064\u2019, \u2018ENSG00000111850\u2019, \u2018ENSG00000112096\u2019, \u2018ENSG00000115221\u2019, \u2018ENSG00000124343\u2019, \u2018ENSG00000127603\u2019, \u2018ENSG00000129965\u2019, \u2018ENSG00000130035\u2019, \u2018ENSG00000137871\u2019, \u2018ENSG00000143226\u2019, \u2018ENSG00000156273\u2019, \u2018ENSG00000157326\u2019, \u2018ENSG00000159216\u2019, \u2018ENSG00000163444\u2019, \u2018ENSG00000163633\u2019, \u2018ENSG00000163945\u2019, \u2018ENSG00000169621\u2019, \u2018ENSG00000172366\u2019, \u2018ENSG00000178104\u2019, \u2018ENSG00000182230\u2019, \u2018ENSG00000182648\u2019, \u2018ENSG00000183292\u2019, \u2018ENSG00000187514\u2019, \u2018ENSG00000187951\u2019, \u2018ENSG00000188681\u2019, \u2018ENSG00000189064\u2019, \u2018ENSG00000204314\u2019, \u2018ENSG00000213077\u2019, \u2018ENSG00000215269\u2019, \u2018ENSG00000215483\u2019, \u2018ENSG00000223802\u2019, \u2018ENSG00000225830\u2019, \u2018ENSG00000230000\u2019, \u2018ENSG00000236362\u2019, \u2018ENSG00000239533\u2019, \u2018ENSG00000254911\u2019, \u2018ENSG000002 [\u2026 truncated]\nthis came up  and this time i really have no idea"}, {"role": "user", "text": "@jennaj I solved the problem ive mentioned in the last reply, But still have this  (shared in this link)\nhttps://usegalaxy.eu/u/amir/h/memar\nError in .rowNamesDF<-(x, value = value) :\nmissing values in \u2018row.names\u2019 are not allowed\nCalls: row.names<- -> row.names<-.data.frame -> .rowNamesDF<-"}, {"role": "system", "text": "The mapping may not be 1-1.\nI would suggest using the Entrez IDs directly from Featurecounts instead to ensure distinct geneIDs. Then, at the end, you can convert those IDs to Ensembl for other purposes you may have.\nFollow the tutorial, it will help to avoid problems like this."}], [{"role": "user", "text": "hi\ni am working with galaxy for analysis of metagenomics data. unfortunately i could not use the finch visualization.could you help me ,why pinch dont work for me\nthank you"}, {"role": "system", "text": "Dear @rez,\nCan you please open up a new question, stating your error of the tool and describing more what you have tried to analyse. This post is not related to your question.\nThank you very much and have a nice day!\nFlorian"}, {"role": "system", "text": "@rez Please post your reply to @Flow on this new topic."}, {"role": "user", "text": "Hi\nI am analysis my metagenomics data by galaxy(https://metagenomics.usegalaxy.eu/) , everyathing is ok but when I want visualize data by fhinch as below\n\n\nI could not make biom and system shows an answer like this:\nI would appreciate if you help me to solve this problem."}, {"role": "system", "text": "Dear @rez,\nCan you try to re-run the tool my assumption is that there was a hickup with Galaxy because of heavy traffic. However, if the error still comes up, then my next assumption is, that your data is either corrupted, empty, badly named or in the wrong format. So check first these things.\nIf you still struggle then you can share the history with me or here in the post (see Issue Reporting).\nHave a good day and best wishes,\nFlorian"}, {"role": "user", "text": "my history=reza nahai"}, {"role": "user", "text": "Dear flow\ni would appreciate if you tell me what is the means of more in bellow chart?\n\nimage781\u00d7488 153 KB\n"}, {"role": "user", "text": "i think there is not either corrupted, empty, badly named or in the wrong format because I still have the same problem when I use my own Galaxy data below.\n\nimage702\u00d7433 27.6 KB\n"}, {"role": "system", "text": "Dear @rez,\nI deleted one of the previous post for you for your own safety. Please look at the link I have provided and also this one Galaxy 101. To know how to share a history.\nThe more means that there are more taxa, but because of the current zoom level you cannot see it. It should zoom in, if you double click on the region.\nCheers,\nFlorian"}, {"role": "user", "text": "Thank you for answering me patiently.\nI used Galaxy data to analyze the data\n\nimage702\u00d7433 27.6 KB\n\nbut I still can\u2019t create make biome and I can\u2019t see the results in Finch."}, {"role": "user", "text": "only answer is\n\nThis is while I am using the Galaxy data as shown in the figure below\n\nimage702\u00d7433 27.6 KB\n"}, {"role": "system", "text": "Hi! Could you check for me the sample names in the collection you made right after the upload step? Errors can happen in make.biom step if the sample names in your history don\u2019t match those in the metadata file. Your collection should look something like this:\n\nimage308\u00d7586 19.1 KB\n"}, {"role": "system", "text": "Oh, actually your error message looks a bit different. Could you share your history with me (via link)? Here is instructions how you can do that: Sharing your History\nThen I can see if I can see what is going wrong for you."}, {"role": "user", "text": "https://metagenomics.usegalaxy.eu/u/665577/h/reza-nahai"}, {"role": "user", "text": "on the hand i want know about percentages of bacteria in samples but i dont know\n> Hands-on:\nMetaPhlAn2\nplease send me hands on of MetaPhlAn2 to get percentages of bacteria in samples of metagenome data"}, {"role": "system", "text": "Thanks for sharing your history. I see that you are using your own datasets here, not the tutorial data. This means you will also have to create your own metadata file (e.g. if you have groups). You can also create the biom file without such a metadata file as a start. I was able to create this biom file and view it in Phinch.\nAlso to answer your earlier questions about the \"16 more\u2026 \" in the Krona plot: this means there are a lot more taxons at that level that could not be displayed in this view, but you can click on taxons to \u201czoom in\u201d in Krona to see what these additional entries are. For example:\n\nimage1060\u00d7906 148 KB\n"}], [{"role": "user", "text": "hi all, I\u2019m trying to align some fasta using bowtie2 but I get the error:\ntool error\nAn error occurred with this dataset:\nUnable to finish job\nany help is appreciated!"}, {"role": "system", "text": "Hi @nibhelim\nThat\u2019s not enough information to tell what is going wrong, but most errors are due to input format problems. So I would suggest checking those first to see if it clears up the problem.\nFAQ: * My job ended with an error. What can I do?\nYou might also just want to rerun first. At least one rerun is recommended for failures (to eliminate transient cluster issues). Plus, we just updated the Galaxy Main https://usegalaxy.org server and that has had a few problems, mostly resolved now, and your prior job might have been impacted.\nLet us know how that goes and we can follow up from there if needed.\nThanks!"}, {"role": "user", "text": "hi, thanks for the help @jennaj! the thing is that there are no indications on the nature of the error, just unable to finish the job and the error detail is empty. I tried to rerun the analysis but still the same error.\nany tricks?"}, {"role": "system", "text": "I\u2019m getting the exact same error when aligning with both STAR and HISAT2. Any help or updates on what\u2019s happening would be appreciated!"}, {"role": "system", "text": "@nibhelim & @Priscilla607\nYou are both working at https://usegalaxy.org, correct?\nPlease send in a bug report from the problematic mapping jobs (just one each is enough). Include a link to this Galaxy Help post and your \u201c@\u201d here in the bug report comments, so we can track everything together.\nThere may be some type of cluster issue \u2013 this will help to clarify if that is the problem, or if there is some other server-side issue. If the problem is data/inputs/parameters, we can work on that directly via email privately.\nThanks!"}, {"role": "system", "text": "@nibhelim & @Priscilla607\nUpdate: We\u2019re having a few problems with bug report submissions. That should be resolved soon (possibly later tonight/early tomorrow). It wouldn\u2019t be easily noticeable as a problem from your side \u2013 so I wanted to let you know in case you sent something in and were not sure about the status. Right now, we don\u2019t have a bug report from either of you to work with and there isn\u2019t a way to get it sent in, yet.\nI\u2019ll post back once this functionality is intact again. Then, please resend whatever error you sent in before. Please be sure to include the originally requested information in the comments so I can find those.\nWe just updated the server and if there is some reproducible server issue with mapping jobs, that is definitely a priority concern, and you both would be helping us out by sharing examples. My guess is that the problem is isolated to a certain set of conditions, meaning == not all mapping jobs, but your bug reports will help to determine what is going wrong along with other tests I\u2019ll do.\nAnd if it turns out to be data/inputs, can help with that, too. Either way, we\u2019ll get this resolved.\nThanks! "}, {"role": "system", "text": "Hi,\nI\u2019ve been getting the same error message with Bowtie2 and BWA-MEM since yesterday. I ran mapping stats with Bowtie2 and I am getting a high alignment percentage for most of my fastqs (>90%) and a .bam file is created but it says it is unable to finish the job and I noticed it cannot create a .bai file.\nAny help would be much appreciated! Please let me know if I can share my history with you somehow.\nThank you!"}, {"role": "system", "text": "I am getting the same EXACT issuues as @Athina has mentioned. My alignment percentage is like ~98% but I get the same error message. I\u2019ve tried using different parameters for both BWA and Bowtie for days and still get the same error."}, {"role": "system", "text": "@Athina @Jamal_Williams @Priscilla607 @nibhelim\nThanks all for writing in about the problem. Direct bug reporting from the web interface at https://usegalaxy.org is still problematic. More updates soon about that.\nLet\u2019s try another way. Send a direct message to me (@jennaj) with:\n\nA share link to the history that contains the problematic BAM result(s). Be sure to check the box for sharing \u201cobjects\u201d in the history. Unshare/reshare the link if you are not sure if this box was checked. The function for \u201cShare or Publish\u201d is listed in the history menu.\nDataset number(s) for the BAM results\nPlease do not delete the BAM results or any of the inputs used to create it from your history\n\nDirect messages (sent, received, etc) can be found here in the Galaxy Help web interface:\n\nAs a short-cut to send a new direct message, click on the \u201c@\u201d for the person you want to send to. An \u201cenvelope\u201d icon will be in the pop-up. Clicking on the envelope will initiate a direct message.\nThere is likely either a server-side issue, the BAMs are too large to index, or some combination of the two. A few examples will help to clarify. We just updated the server \u2013 fixing any issues is a priority."}, {"role": "system", "text": "@jennaj Thank you for you reply. I now noticed that every time I logout and in again, the first run of Bowtie2 works fine, then the second crushes. For the same dataset. I am now trying to share everything with you as you suggested above."}, {"role": "system", "text": "If this problem gets resolved can you let me know here, I got frustrated and deleted everything before I read @jennaj last post. Thanks!"}, {"role": "system", "text": "@Jamal_Williams Definitely will post back the results, however it turns out. But need an example first, as I haven\u2019t been able to reproduce it on my own.\n@Athina Interesting pattern, I didn\u2019t try that. Also, I don\u2019t see a direct message from you yet but will keep eye out for it "}, {"role": "system", "text": "@jennaj This is going to sound a bit ridiculous I am sure but I am unable to find the sharing \u201cobjects\u201d box and how to actually create a link for you. Could you please help me with that too? Thanks!"}, {"role": "system", "text": "@Jamal_Williams By the way I just wanted to say that Bowtie2 has now started working fine for me. Just this morning I checked again. Five consecutive runs of Bowtie2 and it is good. Not sure what changed."}, {"role": "system", "text": "@Athina Thanks for letting me know! I will try again today."}, {"role": "system", "text": "@Jamal_Williams Sounds like the problem resolved itself. Good! We updated the server last week and had a bit of trouble initially but most of that has been sorted out. Reporting bugs directly via the web interface is still problematic but is on our priority to-do list.\n@Athina For future reference, to share a history, look under the History menu (gear icon) for the option \u201cShare or Publish\u201d. That option is also available for Workflows (in the pull-down menu, per workflow).\nThe interface has changed a bit since this FAQ and the video were created but it has the basics correct. It is on my list to update that \u2013 am waiting for the final round of UI changes to stabilize first.\nhttps://galaxyproject.org/support/\n\nSharing and Publishing your work\n\nWhat is new is the presence of an option when sharing histories to check whether to \u201cshare the objects\u201d included in history or not (meaning: grant access to the datasets themselves so they can be reviewed/downloaded). In most cases, including for troubleshooting reasons and in publications/blogs/etc, you\u2019ll want to check that box \u2013 unless you really just want to just share the analysis but not the actual data for some reason.\nScreenshot: Objects set to be shared before clicking on the button to generate the history share link. If you can\u2019t remember if you checked the box or not, just unshare, check the settings, adjust as wanted, and regenerate the link.\n"}, {"role": "system", "text": "Update: Ticket for the issue is now here https://github.com/galaxyproject/usegalaxy-playbook/issues/257\nIn short, until resolved (when the ticket closes out), set the destination cluster on the tool form as described in the ticket and the linked workaround Galaxy Help post: HISAT2 Not working\nI never received an example Bowtie2 error but was able to reproduce the issue in directed testing. The last round of the testing is in a shared history link in the Github ticket if interested.\nMapping jobs will route to available cluster resources. My guess is that initially for both of you that the jobs were routed to one cluster, then later on the other. One works, the other does not at present. This will be a priority correction."}, {"role": "system", "text": "@jennaj Thanks I just read your last post. prior to this I just ran Bowtie2 again today and I have the same issues. This is the link to my current example history:\nhttps://usegalaxy.org/u/asg/h/24092019col7x3gaetano"}, {"role": "system", "text": "@jennaj Hi, I used the solution you suggested and first time it worked fine, second time I got this error:\n\u201cFailed to write job script \u2018/galaxy-repl/main/jobdir/025/126/25126033/tool_script.sh\u2019, could not verify job script integrity\u201d"}, {"role": "system", "text": "@Athina The second run is a transient cluster error. Rerun that one.\n\n\n\n Athina:\n\nsecond time I got this error:\n\u201cFailed to write job script \u2018/galaxy-repl/main/jobdir/025/126/25126033/tool_script.sh\u2019, could not verify job script integrity\u201d\n\n\nGlad other(s) worked!"}, {"role": "user", "text": "@jennaj  still having issue running bowtie on galaxy.org.\nhere my history:\nhttps://usegalaxy.org/u/nibhelim/h/unnamed-history\nstill there is no code or reason in the error page.\nalso today is running just one jub at the time and is super slow\nthanks for the help!"}, {"role": "user", "text": "the second run gave this error istead:\n(33, \u201cHTTP server doesn\u2019t seem to support byte ranges. Cannot resume.\u201d)"}, {"role": "system", "text": "@nibhelim It looks like your jobs are running Ok now, the same as @Athina\u2019s are.\nThere were a few spikes of a job handler problems, Sunday and early Monday, now resolved. Jobs are running again and are executing in the order queued (by everyone).\nApologies for the overlap of issues!"}, {"role": "user", "text": "@jennaj today I have the same problem with job not running with the same unexpected error. never had a single one of these issues before the update. thanks for the help"}, {"role": "system", "text": "@nibhelim There were some server issues at https://usegalaxy.org in the last few days.\nTry a rerun now and be sure to specifically target the \u201cGalaxy cluster\u201d in the \u201cJob Resources\u201d tool form option. The Jetstream cluster is still problematic for mapping jobs, and jobs could be sent there if default form settings are used (confirmed again earlier today in this test history https://usegalaxy.org/u/jen/h/test-mappers-and-clusters)."}, {"role": "system", "text": "Update: Fixed\n@nibhelim @Jamal_Williams @Athina @Priscilla607 and any others that ran into this issue, with any mapping tool over the last few weeks\n\n\n\nTophat not working???\n\n\nNote to all end users running mapping: Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.\nTicket: https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708\n\n"}], [{"role": "user", "text": "I want to do Augustus training, but the rat model is not available on the list. Is there any other ways to do it?\nAnd when I look at the Maker training set, I see that I have to use the ESTs and protein evidence databases. Where to find those databases? I have downloaded the rat (rn7.2) assembly dataset. Is it correct that I use the rna.fna file from the rn7.2 assembly as the ESTs or assembled cDNA and use the protein.fna from the rn7.2 assembly as the protein evidence? It seemed not working in this way, but what else I should use then?"}, {"role": "system", "text": "Hi @daikez,\nregarding your first question, you can generate an Augustus model by using  the Augustus training tool.\nRespecting the following questions, if you pretend to re-annotate the rn7.2 assembly, then it is not a good idea to use the protein.fna and rna.fna datafiles from the rn7.2 assembly because that information has already been used for performing the annotation, then it won\u2019t provide any additional information.\nYou can get an updated EST dataset from Genbank. I recorded a short video about how to get the sequences.\n\n  \n    \n  \n\n\nOn the other hand, the protein sequences can be obtained from UniProt.\n\n  \n    \n  \n\n\nLet me know if you need additional information.\nRegards."}], [{"role": "user", "text": "Good day,\nAre there ARCTIC v4 amplicon info and v4 bed files to use for SARS-CoV-2 analysis. We recently started using the ARCTIC V4 primers. I had to try running the sequence data using the V3 primer set and bed on the SARS-CoV-2 workflow but it doesn\u2019t generate any info."}, {"role": "system", "text": "This maybe?\n  \n      \n\n      github.com\n  \n\n  \n    artic-ncov2019/primer_schemes/nCoV-2019 at master \u00b7 artic-network/artic-ncov2019\n\n  master/primer_schemes/nCoV-2019\n\n  ARTIC nanopore protocol for nCoV2019 novel coronavirus\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "Hi @Kathleen_Subramoney,\nwe\u2019ve updated our shared covid-19 resources history on usegalaxy.eu a while ago to include more primer schemes. Galaxy | Europe | Published History | COVID-19 resources has ARTIC v4 primers (and amplicon info) and others since then.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hello,\nI am a Galaxy user and I wish to download the FASTQC report generated as output on galaxy server, but I am unable to do so. I have run \u201cFASTQC read quality report\u201d to check the quality of my fastq reads, and the program ran successfully giving a HTML file as output. But I am not able to download/view the HTML file.\nIs this a bug of galaxy or is there anything I can do to download the file?\nKindly advise\n\u2013Lakshmi"}, {"role": "system", "text": "Hi Lakshmi\nWhen you click on the download button, what kind of file do you get? It should be a zipped file called something like: FastQC_on_data_X__Webpage.zip\nYou can unzip this file (this depends on the system software your local computer runs), and then you should get a directory with a fill called \u2018FastQC_on_data_1__Webpage_html.html\u2019. You can open this file in a web browser\nHope this helps\nRegards, Hans-Rudolf"}, {"role": "user", "text": "Thanks for prompt response!\nBut I do not get any tab when click on the download option. It redirects me to a new webpage which is basically empty. It neither display the results nor lets me download the file.\nAny advice on how I could download it?"}, {"role": "system", "text": "does this happen to for all downloads from the Galaxy history, or only for \u2018html files\u2019 ?\nhave you tried using a different borwser, i.e chrome?"}, {"role": "user", "text": "Yes, I\u2019ve tried different browsers, facing the same issue. It now shows an error http error 500.\nIt happens only with the HTML files on galaxy, I am able to download other fastq/bam files."}, {"role": "system", "text": "I\u2019m having a similar issue.  I can open and view HTML files in Galaxy but am also unable to download them.  Just two days ago I was able to download many HTML files, so it might be a bug?\nIn Chrome, the error message is:\nThis page isn\u2019t working\nusegalaxy.org  is currently unable to handle this request.\nHTTP ERROR 500"}, {"role": "user", "text": "Yes! It shows exactly the same error for me too! I tried on chrome, firefox, edge but it\u2019s the same issue. Hope we get a fix soon! I\u2019m on a tight timeline to get this data."}, {"role": "system", "text": "A little relieved to hear I\u2019m not the only one! My other MultiQC jobs failed as well, maybe it has to do with the MultiQC tool? Either way I also hope it can be resolved ASAP!\nIn the meantime, maybe you can try saving plots from the MultiQC webpage? There\u2019s an \u201cexport\u201d button which allows you to download the plots and/or the data."}, {"role": "user", "text": "Oh I see. I will try that. Thanks!"}, {"role": "system", "text": "or use: usegalaxy.eu it still works there"}, {"role": "system", "text": "Update: maintenance completed.\nThis notice applies to everyone using the server.\nThe UseGalaxy.org server is undergoing scheduled maintenance. Follow here for current status and updates: https://status.galaxyproject.org/\nThe estimated time window for the maintenance is:\nTuesday October 13 8:00 AM to 5:00 PM CDT\nUpdate: maintenance has been extended until October 14, 10 AM CDT\nUpdate: maintenance has been extended through October 14. No set end time.\nOnce that is completed and you can access the server again, please try any problematic actions again.\nThanking everyone for their patience, Galaxy team"}], [{"role": "user", "text": "Hi, I have started my own galaxy. I have registered my email id and also changed in galaxy.yml file. But I am not getting admin menu in the menu bar. Kindly help me to fix this. Thanks in advance."}, {"role": "system", "text": "Did you restart your instance? Galaxy only re-reads config on restart."}, {"role": "user", "text": "Thank you for your reply.\nI have restarted many times but I am not getting admin access."}, {"role": "system", "text": "\n\n\n preethib91:\n\nmy own galaxy. I have registered my email id and also changed in galaxy.yml file. But I am not getting admin menu in the menu bar. Kindly help me to fix this.\n\n\nThat means you have a typo somewhere, or your Galaxy is not loading the correct config file. Can you please post the exact entry in the galaxy,yml file and confirm that this is the file your Galaxy uses? (should be in the startup log)"}, {"role": "user", "text": "The common startup.sh is given below.\n#!/bin/sh\nset -e\n\n# The caller may do this as well, but since common_startup.sh can be called independently, we need to do it here\n. ./scripts/common_startup_functions.sh\n\nSET_VENV=1\nfor arg in \"$@\"; do\n    if [ \"$arg\" = \"--skip-venv\" ]; then\n        SET_VENV=0\n        skip_venv=1  # for common startup functions\n    fi\ndone\n\nDEV_WHEELS=0\nFETCH_WHEELS=1\nCREATE_VENV=1\nREPLACE_PIP=$SET_VENV\nCOPY_SAMPLE_FILES=1\nSKIP_CLIENT_BUILD=${GALAXY_SKIP_CLIENT_BUILD:-0}\n\nfor arg in \"$@\"; do\n    [ \"$arg\" = \"--skip-eggs\" ] && FETCH_WHEELS=0\n    [ \"$arg\" = \"--skip-wheels\" ] && FETCH_WHEELS=0\n    [ \"$arg\" = \"--dev-wheels\" ] && DEV_WHEELS=1\n    [ \"$arg\" = \"--no-create-venv\" ] && CREATE_VENV=0\n    [ \"$arg\" = \"--no-replace-pip\" ] && REPLACE_PIP=0\n    [ \"$arg\" = \"--replace-pip\" ] && REPLACE_PIP=1\n    [ \"$arg\" = \"--stop-daemon\" ] && FETCH_WHEELS=0\n    [ \"$arg\" = \"--skip-samples\" ] && COPY_SAMPLE_FILES=0\n    [ \"$arg\" = \"--skip-client-build\" ] && SKIP_CLIENT_BUILD=1\ndone\n\nSAMPLES=\"\n    config/migrated_tools_conf.xml.sample\n    config/shed_tool_conf.xml.sample\n    config/shed_tool_data_table_conf.xml.sample\n    config/shed_data_manager_conf.xml.sample\n    lib/tool_shed/scripts/bootstrap_tool_shed/user_info.xml.sample\n    tool-data/shared/ucsc/builds.txt.sample\n    tool-data/shared/ucsc/manual_builds.txt.sample\n    static/welcome.html.sample\n\"\n\nRMFILES=\"\n    lib/pkg_resources.pyc\n\"\n\n# return true if $1 is in $2 else false\nin_dir() {\n    case $1 in\n        $2*)\n            return 0\n            ;;\n        ''|*)\n            return 1\n            ;;\n    esac\n}\n\n# return true if $1 is in $VIRTUAL_ENV else false\nin_venv() {\n    in_dir \"$1\" \"$VIRTUAL_ENV\"\n}\n\n# return true if $1 is in $CONDA_PREFIX else false\nin_conda_env() {\n    in_dir \"$1\" \"$CONDA_PREFIX\"\n}\n\nif [ $COPY_SAMPLE_FILES -eq 1 ]; then\n    # Create any missing config/location files\n    for sample in $SAMPLES; do\n        file=${sample%.sample}\n        if [ ! -f \"$file\" -a -f \"$sample\" ]; then\n            echo \"Initializing $file from $(basename \"$sample\")\"\n            cp \"$sample\" \"$file\"\n        fi\n    done\nfi\n\n# remove problematic cached files\nfor rmfile in $RMFILES; do\n    [ -f \"$rmfile\" ] && rm -f \"$rmfile\"\ndone\n\n# Determine branch (if using git)\nif command -v git >/dev/null && [ -d .git ]; then\n    GIT_BRANCH=$(git rev-parse --abbrev-ref HEAD)\n    case $GIT_BRANCH in\n        release_*|master)\n            # All branches should now build the client as necessary, turn this\n            # back into an if?  Or is there more we need to do here?\n            ;;\n        *)\n            DEV_WHEELS=1\n            # SKIP_CLIENT_BUILD will default to false, but can be overridden by the command line argument\n            ;;\n    esac\nelse\n    GIT_BRANCH=0\n    DEV_WHEELS=1\nfi\n\n: ${GALAXY_VIRTUAL_ENV:=.venv}\n# GALAXY_CONDA_ENV is not set here because we don't want to execute the Galaxy version check if we don't need to\n\nif [ $SET_VENV -eq 1 -a $CREATE_VENV -eq 1 ]; then\n    if [ ! -d \"$GALAXY_VIRTUAL_ENV\" ]\n    then\n        # Locate `conda` and set $CONDA_EXE (if needed). If `python` is Conda Python and $GALAXY_VIRTUAL_ENV does not\n        # exist, virtualenv will not be used. setup_python calls this as well but in this case we need it done\n        # beforehand.\n        set_conda_exe\n        if [ -n \"$CONDA_EXE\" ]; then\n            echo \"Found Conda, virtualenv will not be used.\"\n            echo \"To use a virtualenv instead, create one with a non-Conda Python 2.7 at $GALAXY_VIRTUAL_ENV\"\n            : ${GALAXY_CONDA_ENV:=\"_galaxy_$(get_galaxy_major_version)\"}\n            if [ \"$CONDA_DEFAULT_ENV\" != \"$GALAXY_CONDA_ENV\" ]; then\n                if ! check_conda_env \"$GALAXY_CONDA_ENV\"; then\n                    echo \"Creating Conda environment for Galaxy: $GALAXY_CONDA_ENV\"\n                    echo \"To avoid this, use the --no-create-venv flag or set \\$GALAXY_CONDA_ENV to an\"\n                    echo \"existing environment before starting Galaxy.\"\n                    $CONDA_EXE create --yes --name \"$GALAXY_CONDA_ENV\" 'python=2.7' 'pip>=9'\n                    unset __CONDA_INFO\n                fi\n            fi\n        else\n            # If .venv does not exist, and there is no conda available, attempt to create it.\n            # Ensure Python is a supported version before creating .venv\n            echo \"Creating Python virtual environment for Galaxy: $GALAXY_VIRTUAL_ENV\"\n            echo \"To avoid this, use the --no-create-venv flag or set \\$GALAXY_VIRTUAL_ENV to an\"\n            echo \"existing environment before starting Galaxy.\"\n            python ./scripts/check_python.py || exit 1\n            if command -v virtualenv >/dev/null; then\n                virtualenv -p \"$(command -v python)\" \"$GALAXY_VIRTUAL_ENV\"\n            else\n                vvers=13.1.2\n                vurl=\"https://pypi.python.org/packages/source/v/virtualenv/virtualenv-${vvers}.tar.gz\"\n                vsha=\"aabc8ef18cddbd8a2a9c7f92bc43e2fea54b1147330d65db920ef3ce9812e3dc\"\n                vtmp=$(mktemp -d -t galaxy-virtualenv-XXXXXX)\n                vsrc=\"$vtmp/$(basename $vurl)\"\n                # SSL certificates are not checked to prevent problems with messed\n                # up client cert environments. We verify the download using a known\n                # good sha256 sum instead.\n                echo \"Fetching $vurl\"\n                if command -v curl >/dev/null; then\n                    curl --insecure -L -o \"$vsrc\" \"$vurl\"\n                elif command -v wget >/dev/null; then\n                    wget --no-check-certificate -O \"$vsrc\" \"$vurl\"\n                else\n                    python -c \"import urllib; urllib.urlretrieve('$vurl', '$vsrc')\"\n                fi\n                echo \"Verifying $vsrc checksum is $vsha\"\n                python -c \"import hashlib; assert hashlib.sha256(open('$vsrc', 'rb').read()).hexdigest() == '$vsha', '$vsrc: invalid checksum'\"\n                tar zxf \"$vsrc\" -C \"$vtmp\"\n                python \"$vtmp/virtualenv-$vvers/virtualenv.py\" \"$GALAXY_VIRTUAL_ENV\"\n                rm -rf \"$vtmp\"\n            fi\n        fi\n    fi\nfi\n\n# activate virtualenv or conda env, sets $GALAXY_VIRTUAL_ENV and $GALAXY_CONDA_ENV\nsetup_python\n\nif [ $SET_VENV -eq 1 -a -z \"$VIRTUAL_ENV\" -a -z \"$CONDA_DEFAULT_ENV\" ]; then\n    echo \"ERROR: A virtualenv cannot be found. Please create a virtualenv in $GALAXY_VIRTUAL_ENV, or activate one.\"\n    exit 1\nfi\n\n# this shouldn't happen, but check just in case\nif [ -z \"$VIRTUAL_ENV\" ] && [ \"$CONDA_DEFAULT_ENV\" = \"base\" -o \"$CONDA_DEFAULT_ENV\" = \"root\" ]; then\n    echo \"ERROR: Conda is in 'base' environment, refusing to continue\"\n    exit 1\nfi\n\n: ${GALAXY_WHEELS_INDEX_URL:=\"https://wheels.galaxyproject.org/simple\"}\n: ${PYPI_INDEX_URL:=\"https://pypi.python.org/simple\"}\n: ${GALAXY_DEV_REQUIREMENTS:=\"./lib/galaxy/dependencies/dev-requirements.txt\"}\nif [ $REPLACE_PIP -eq 1 ]; then\n    pip install 'pip>=8.1'\nfi\n\nrequirement_args=\"-r requirements.txt\"\nif [ $DEV_WHEELS -eq 1 ]; then\n    requirement_args=\"$requirement_args -r ${GALAXY_DEV_REQUIREMENTS}\"\nfi\n\nif [ $FETCH_WHEELS -eq 1 ]; then\n    pip install $requirement_args --index-url \"${GALAXY_WHEELS_INDEX_URL}\" --extra-index-url \"${PYPI_INDEX_URL}\"\n    GALAXY_CONDITIONAL_DEPENDENCIES=$(PYTHONPATH=lib python -c \"from __future__ import print_function; import galaxy.dependencies; print('\\n'.join(galaxy.dependencies.optional('$GALAXY_CONFIG_FILE')))\")\n    if [ -n \"$GALAXY_CONDITIONAL_DEPENDENCIES\" ]; then\n        if pip list --format=columns | grep \"psycopg2[\\(\\ ]*2.7.3\" > /dev/null; then\n            echo \"An older version of psycopg2 (non-binary, version 2.7.3) has been detected.  Galaxy now uses psycopg2-binary, which will be installed after removing psycopg2.\"\n            pip uninstall -y psycopg2 psycopg2-binary\n        fi\n        echo \"$GALAXY_CONDITIONAL_DEPENDENCIES\" | pip install -r /dev/stdin --index-url \"${GALAXY_WHEELS_INDEX_URL}\" --extra-index-url \"${PYPI_INDEX_URL}\"\n    fi\nfi\n\n# Check client build state.\nif [ $SKIP_CLIENT_BUILD -eq 0 ]; then\n    if [ -f static/client_build_hash.txt ]; then\n        # If git is not used and static/client_build_hash.txt is present, next\n        # client rebuilds must be done manually by the admin\n        if [ \"$GIT_BRANCH\" = \"0\" ]; then\n            SKIP_CLIENT_BUILD=1\n        else\n            # Check if anything has changed in client/ since the last build\n            if git diff --quiet $(cat static/client_build_hash.txt) -- client/; then\n                SKIP_CLIENT_BUILD=1\n            else\n                echo \"The Galaxy client is out of date and will be built now.\"\n            fi\n        fi\n    else\n        echo \"The Galaxy client has not yet been built and will be built now.\"\n    fi\nfi\n\n# Build client if necessary.\nif [ $SKIP_CLIENT_BUILD -eq 0 ]; then\n    # Ensure dependencies are installed\n    if [ -n \"$VIRTUAL_ENV\" ]; then\n        if ! in_venv \"$(command -v node)\"; then\n            echo \"Installing node into $VIRTUAL_ENV with nodeenv.\"\n            nodeenv -n 9.11.1 -p\n        fi\n        if ! in_venv \"$(command -v yarn)\"; then\n            echo \"Installing yarn into $VIRTUAL_ENV with npm.\"\n            npm install --global yarn\n        fi\n    elif [ -n \"$CONDA_DEFAULT_ENV\" -a -n \"$CONDA_EXE\" ]; then\n        if ! in_conda_env \"$(command -v yarn)\"; then\n            echo \"Installing yarn into '$CONDA_DEFAULT_ENV' Conda environment with conda.\"\n            $CONDA_EXE install --yes --override-channels --channel conda-forge --channel defaults --name $CONDA_DEFAULT_ENV yarn\n        fi\n    else\n        echo \"WARNING: Galaxy client build needed but there is no virtualenv enabled. Build may fail.\"\n    fi\n\n    # Build client\n    cd client\n    if yarn install --network-timeout 120000 --check-files; then\n        if ! yarn run build-production-maps; then\n            echo \"ERROR: Galaxy client build failed. See ./client/README.md for more information, including how to get help.\"\n            exit 1\n        fi\n    else\n        echo \"ERROR: Galaxy client dependency installation failed. See ./client/README.md for more information, including how to get help.\"\n        exit 1\n    fi\n    cd -\nelse\n    echo \"Regenerating static plugin directories.\"\n    python ./scripts/plugin_staging.py\nfi\n\n"}, {"role": "user", "text": "The Place were the changes made in the galaxy.yml file given below.\n\n  # Administrative users - set this to a comma-separated list of valid\n  # Galaxy users . These users will have access to the\n  # Admin section of the server, and will have access to create users,\n  # groups, roles, libraries, and more.  For more information, see:\n  #   # this should be a comma-separated list of valid Galaxy users\n  admin_users: '<redacted>@gmail.com'\n\n"}, {"role": "system", "text": "\n\n\n preethib91:\n\nadmin_users: \u2018<redacted>@gmail.com\u2019\n\n\nRemove the single quotes and restart \u2013 that should solve the problem.\nExample: https://galaxyproject.org/admin/get-galaxy/#become-an-admin"}], [{"role": "user", "text": "Dear All\nI am getting an unexpected during pear end read merger fastq error showing:\nAn error occurred with this dataset:\nformat fastqsanger database.\nHowever two samples are merged successfully.\nKindly suggest what to do.\nRegards"}, {"role": "system", "text": "Welcome, @mallamuneer123\n\n\n\n mallamuneer123:\n\nAn error occurred with this dataset:\nformat fastqsanger database.\nHowever two samples are merged successfully.\n\n\nThere are several fastq \u201cmerge\u201d tools. The message about the tool erroring and being successful at the same time is confusing for me to understand. Please try at least one rerun. If that fails, back up and do some QA on the reads.\nIf any tool is failing with paired-end fastq inputs, checking the format is a good place to start. Maybe the data is truncated or malformed in some way. That could be from a partial data upload to Galaxy, or some problem introduced before the data was in Galaxy.\nA tool that will verify basic formatting and intact paired-end reads is: Fastq info\nThese tutorials can help with understanding fastq data (format/content/variants) along with example QA steps/tools. Also see domain/topic tutorials at that same site for protocol-specific QA steps.\n\nNGS data logistics\nQuality Control\nMapping\n\nPlease run the QA steps in that same history as your error, and if you cannot solve the problem, we can try to help here. FAQ about where to find error logs and how to generate a shared history link for review \u2192 Troubleshooting errors"}, {"role": "user", "text": "Hi again\nAfter your suggestion I rerun the merge tool and got the same error.\nthen I checked the format and paired-ends reads by using Fastq info tools.\nthe results I got are as:\nFor read R1:\nFastq.info fasta  = 12,715,268 sequences\nFastq.info qual = 25,430,536 lines\nfastq scrap =  0 bytes\nfasta scrap = 0 bytes\nqual scrap = 0 lines\nand For Read R2:\nFastq.info fasta  = 12,363,913 sequences\nFastq.info qual = 24,727,826 lines\nfastq scrap =  0 bytes\nfasta scrap = 0 bytes\nqual scrap = 0 lines\nSo this is the result, what to do next.\nplease suggest.\nAnd my aim is to evaluate the microbial community structure and function, antibiotic resistance, and virulence determinants.\nPlease suggest\nRegards"}, {"role": "system", "text": "\nAre these the original \u201craw\u201d fastq files?\nWhich tool are you exactly using to merge?\nIf it are not the original fastq files which step do you do before merging?\nDoes the merging tool end in a red or green color?\nIf red, did you clicked the \u201cbug\u201d icon to see the error?\n\nI notice that your fastqc files do not have the same amount of reads. Mostly it has the same amount of reads unless you do something with it."}, {"role": "user", "text": "Hi Again\nYes, these are the original \u201craw\u201d fastq files.\nI used \u201cPaired-End read merger\u201d\nThe merging tool showed a red color\nYes, I clicked the \u201cbug\u201d icon and here is the error\nFatal error: Exit code 134 ()\n1 Problem, number of reads does not match! n1 = 0 n2 = 84196\n/mnt/pulsar/files/staging/6295728/command.sh: line 119: 2337290 Aborted                 (core dumped) pear -f \u201c/mnt/pulsar/files/staging/6295728/inputs/dataset_13793713.dat\u201d -r \u201c/mnt/pulsar/files/staging/6295728/inputs/dataset_13781585.dat\u201d --phred-base 33 --output pear --p-value 0.01 --min-overlap 10 --min-asm-length 50 --min-trim-length 1 --quality-theshold 0 --max-uncalled-base 1.0 --test-method 1 --empirical-freqs -j \u201c${GALAXY_SLOTS:-8}\u201d --score-method 2 --cap 40\nKindly suggest\nThanks in advance"}, {"role": "system", "text": "So the problem is clear now.\n\nProblem, number of reads does not match!\n\nThis is also confirmed by your previous post:\n\nFor read R1:\nFastq.info fasta = 12,715,268 sequences\nFastq.info qual = 25,430,536 lines\nfastq scrap = 0 bytes\nfasta scrap = 0 bytes\nqual scrap = 0 lines\nand For Read R2:\nFastq.info fasta = 12,363,913 sequences\n\nThe solution would be to give fastq files as input with the same amount of (matching) reads. Where did you got the files from? You could try to re-download and re-upload them."}, {"role": "user", "text": "Thank you, gbbio\nI have uploaded the file again\nNow the file size is:\n\nR1 = 4.1GB and R2 = 4.0 GB\nR1 = 4.3 GB and R2 = 4.2 GB\nR1 = 3.9 GB and R2 = 3.9 GB\nSo, looking at the same\nMoreover, Please suggest after merging the pair end what tool to use for taxonomic and functional analysis, ARG and pathogenic determinants.\nThanks and Regards\n"}, {"role": "user", "text": "Hello again\nWhile I uploaded the data files\nRan the Paired-End read merger and got the same error.\nPlease suggest\nI am stuck badly.\nthanks and regards"}, {"role": "system", "text": "Also, try this. Reloading the data was just one guess about why the reads are not all paired.\n\n\n\n gbbio:\n\nThe solution would be to give fastq files as input with the same amount of (matching) reads.\n\n\nThere are a few ways to filter out any forward or reverse reads that are missing a mate. After you are done, the Fastq info tool should report the same number of reads in the forward and reverse files. You won\u2019t be able to do more until that is resolved.\n\n\nRun the data through a QA tool that sorts the output into intact pairs and singletons. Example: Trimmomatic. See the tool form for how to use it, and our QA tutorials for usage examples.\n\n\nUse tools that compare the read identifiers between files, and only retain output where both files contain the base read name. You might need to convert fastq \u2192 tabular, do tabular manipulations, then covert tabular \u2192 fastq at the end. See the tutorial below for the choices with examples.\n\n\nUse tools from the Seqtk tool group. These do all sorts of logical data manipulations. Converting to an interleaved format then back to individual forward + reverse format can get rid of stray unpaired singleton reads. If you do not understand what these terms mean, see the first QA tutorial I linked before. You can compare data visually to the examples to learn the concepts.\n\n\nReferences:\n\nText manipulation tools: Data Manipulation Olympics\n\nQA tools \u2013 some basic checks to make sure data has valid format and content are always recommend, even if you don\u2019t plan on doing any trimming.\n\n\n\n\n jennaj:\n\nThese tutorials can help with understanding fastq data (format/content/variants) along with example QA steps/tools. Also see domain/topic tutorials at that same site for protocol-specific QA steps.\n\nNGS data logistics\nQuality Control\nMapping\n\n\n"}], [{"role": "user", "text": "Hello,\nI am using the RNA-seq dataset (triplicates) to analyze the differentially expressed genes in bacterial systems . I could analyze until featurecounts stage without any problem. But when I used Deseq2 tool to upload the featurecounts results, I couldn\u2019t find the featurecounts on the collection file for uploading. It is just showing the single replicate counts and not the collection file that has all three replicates. It would be great if soemone could help me with this.\nThank you\nJawahar"}, {"role": "system", "text": "Hi Jawahar,\ndo you switch to collection input mode during setup of DESeq2 job? By default DESeq2 shows individual files.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nAs I mentioned in the previous email, the data doesn\u2019t have replicates.\nThank you\nJawahar"}, {"role": "system", "text": "Hi Jawahar,\ncan you please share the history with me, so I can have a look. You can share history in the history menu and paste the link in reply. A brief description will help, something like: input files or collection.\nI am confused: in the original post you mention a collection with three replicates, and now you say data has no replicates.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nThank you very much for the reply and sorry about the confusion.\n1. Replications: Actually, I am working on multiple publicly available RNA datasets from several sulfate-reducing bacteria for machine learning studies. I can grasp your clarification about replications and this problem is resolved. (Question resolved)\n2. Hisat2 error:: I am working on a separate RNA seq-dataset from our wet-lab experiment with triplicates. Please find the link below for the history named \u201cRNA seq Gr Day 60\u201d. Here we have RNA seq data of Desulfovibrio alaskensis bacteria exposed to five different experimental conditions. Below are the GALAXY pipeline along with their file numbers. Also, as I mentioned in my previous email, I see that I have an issue with the HISAT2 file. Among the columns in the HISAT2 result, both MPOS and ISIZEcolumns had only zeros for all the genes. When I used the same GALAXY pipeline last month I had several numbers in these columns where I ended up with good results. I am skeptical that may HISAT2 BAM file has some issues and it is reflected in DESEQ2 error. (Question yet to be resolved)\nFiles 7, 14, 30, 37 and 44 - Raw reads made into a collection of triplicates\n143 - Trimmomatic od data collection 7\n150 - Trimmomatic od data collection 14\n157 - Trimmomatic od data collection 30\n164 - Trimmomatic od data collection 37\n180 - Trimmomatic od data collection 44 (error)\n187 - Reference genome (.fa)\n188 - Reference genome (.gff3)\n189 - HISAT2 of collection 143\n196 - HISAT2 of collection 150\n203 - HISAT2 of collection 157\n210 - HISAT2 of collection 164\n217/218 - Featurecounts for collection 189\n231/232 - Featurecounts for collection 196\n245/246 - Featurecounts for collection 203\n259/260 - Featurecounts for collection 210\n273/274 - Deseq 2 on paired-end reads of data 219 (as the collection was showing up while uploading data). Please find attached figures of screenshots.\n3. Deseq2 error?: Later I released that I may be running paired-end instead of collections as the collection was not showing up when I upload for Deseq2. Please find attached screenshots of the mouse pointer at Collection and Multiple datasets. (Question yet to be resolved)\nI believe problems 2 and 3 might be related. It would be great if you could clarify this.\n4. QC and Trimmomatic error for a replicate among triplicates: QC and Trimmomatic showed error for one replicate (the third one) among the triplicate. The raw data were in file 44, the third replicate. Files 129, 130 (QC) and 180 (trimmomatic) were the error files and I didn\u2019t process this sample to the next stage. (Question yet to be resolved)\nhttps://usegalaxy.org/history/view_multiple\nThank you very much and please let me know for more information. My research was stuck because of this issue for the past two weeks. It would be great if you could help me resolve this.\nJawahar\n\nCollection pointer.jpg2398\u00d71727 451 KB\n\n\nMultiple dataset pointer.jpg2087\u00d71692 461 KB\n"}, {"role": "user", "text": "Hello Igor,\nAlso, I could see the versions (attached) in my GALAXY. Could you please mention which version is before v1.22?\nThank you\nJawahar\n"}, {"role": "system", "text": "Hi Jawahar,\nDESeq2 version: you can find this information in the Galaxy tool shed. Switch to any version of DESeq2 in Galaxy > During job setup step click on Option (small triangle next to Versions icon in the top right corner of the middle window) > find requirements for this version.\nThe link you posted is a generic link to View all histories. I can see all my histories on the server.\nZeros in MPOS and ISIZE columns: by any chance, do you use single end data?\n#3 - sorry, I don\u2019t understand the problem.\nQC error: do you mean FastQC error? What kind of error message you got? Does it mention truncated file (premature EoF)? Check the fastq file(s) for integrity. If the files are plain text fastq, select the last four lines and check the last read(s) for integrity. If the reads are GZipped FASTQ, changed it to plain text FASTQ in Edit attributes (pencil icon) > Convert > select uncompress option. If it fails, the file is truncated.\nHope this helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nSorry about the link. The original link is here (https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.7+galaxy1). I hope now you can access all the data in the history. Here, I am unable to upload the Featurecounts dataset collection for Desq2 analysis as it is showing up in the folders? Could you please help me with the HISAT2 problem and the above Deseq2 problem?\nAlso, regarding the FASTQC error, it is telling Fatal error: Exit code 1 (FastQC returned non zero exit code)\nPlease suggest your thoughts.\nThank you very much\nJawahar\n\nCollection pointer.jpg2398\u00d71727 451 KB\n\n\nMultiple dataset pointer.jpg2087\u00d71692 461 KB\n"}, {"role": "system", "text": "Hi Jawahar,\nthe link you posted is for a job setup, not the history. You can share history in the history menu (cog wheel icon at the top of the history panel) and paste the link in reply.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nSorry again. Please find below the history link. Thank you\nhttps://usegalaxy.org/u/jawahar/h/copy-of-rna-seq-gr-day-60\nJawahar"}, {"role": "system", "text": "Hi Jawahar,\nthank you for the shared history.\nFastQC error on collection (dataset #129). Click on collection, click on the 3d sample, the last on, click on error icon of the failed job on reverse reads. The error log has the following:\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry.  Your file is probably truncated\nFailed DESeq2 (dataset #273). I am sorry for repeating the answer but DESeq2 does not support analysis without replicates since  v1.22. This job was submitted using the latest DESeq2 version and no replicates.\nNo MPOS   and ISIZE. You submitted these jobs with single end reads, two jobs per sample. You have two alignments for every sample, one for forward and one for reverse reads. During HiSAT2 job setup select paired-end option.\nHope this helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nThank you very much for your detailed replies. I see that for Deseq2 file upload, Galaxy 2.1.8.3 was the oldest version. Please suggest would it work for performing Deseq2 without replicate data.\nThank you very much for your great help.\nregards\nJawahar"}, {"role": "system", "text": "It should. You can check the DESeq2 version in the tool shed through Options (the icon next to Tool Version icon) > See in Tool Shed. For example, Galaxy version deseq2/2.11.39 uses bioconductor-deseq2 1.14.1, so the early versions all use even earlier version of the software.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nThank you very much for the help. I was able to fix the HISAT2 errors and reached the Deseq2. The tools are working well for replicates. But for the sample without replication, I ended up with an error in Deseq2. Although it turned green, the Deseq2 result file is empty (data 71 and 72). I used Galaxy Version 2.1.8.3. Below is the link for the data. It would be great if you could help me fix this.\nhttps://usegalaxy.eu/u/jawahar/h/dvh-cuo\nThank you very much for your great help.\nJawahar"}, {"role": "system", "text": "Hi Jawahar,\nthe standard error file has the following:\n/usr/local/tools/R/3.2.1/iuc/package_r_3_2_1/e46a7803f17b/lib/R/bin/exec/R: error while loading shared libraries: libreadline.so.6: cannot open shared object file: No such file or directory\nIt is an old version, and it might be non-functional. Try other versions, for example 2.11.39. Some of these tools fails when a header line present in count tables. If it fails with error pointing to non-numeric value, remove the first line from count files.\nKind regards,\nIgor"}], [{"role": "user", "text": "I wanted to move/copy my dataset collection (fastqsanger.gz files, >2GB each) from usegalaxyorg to usegalaxyeu as on the latter server there is a tool I am going to use. I tried to do as it was written in Question 2, Help 2. But as I copy-paste the URL to the dataset (from the \u2018Copy link\u2019 - chains icon) to the upload data tool it shows very small size of the data (few bytes) and the transfer ends with an error ('The uploaded file contains invalid HTML content\").\nThe history is accessible (Share and Publish).\nI have a collection (a list of pairs with 21 items) of total 42 fastqsanger.gz files.\nCan you help me what am I doing wrong?\nAccording to old posts it should work.\nBest regards,\nMarcin"}, {"role": "system", "text": "I am not sure this actually works with datasets in a collection (I just tested it and got the same error). However there should be a possible workaround \u2013 every dataset in a collection typically has a corresponding \u201chidden\u201d dataset in the same history. Can you try using those?"}, {"role": "user", "text": "Hi! I have no idea where to look for the hidden dataset. My collection looks\nlike a single object (no icons allowing to do anything). It could be expanded\ninto unnumbered pairs of datasets with names composed from matching parts\nof forward and reverse files with extension .fq.\nthen I can expand a single dataset:\n\nZrzut ekranu 2021-10-18 o 16.50.49544\u00d7810 40.5 KB\n\nNeither link taken from chains icon nor diskette icon allows to successfully download a file to usegalaxyeu"}, {"role": "system", "text": "Hidden datasets would be at the history level. In the top part of the History there would be \u201cshow hidden\u201d link. Try clicking that."}, {"role": "user", "text": "Hi! Neither the link that is copied to clipboard by clicking on \u2018chain\u2019 icon nor the one that initiates download (by clicking on \u2018diskette\u2019 icon) work.\n\nZrzut ekranu 2021-10-18 o 19.51.29544\u00d7528 33.8 KB\n"}, {"role": "system", "text": "What does the link that you get by clicking on the chain look like?"}, {"role": "system", "text": "Hi @marcinschmidt ,\nThe download function appears to be fine as of now. Instructions for dataset collections are in this FAQ: Downloading Data - Galaxy Community Hub\nBe aware that at some Galaxy servers the history must be first set to a shared state (by URL) before a URL will retrieve data due to a larger server-wide default preference. Example: \u201cfrom\u201d usegalaxy.eu \u201cto\u201d usegalaxy.org would require that a history is in a shared state, but not the reverse. All preferences can be adjusted by you for default permissions.\nThanks for following up!"}, {"role": "user", "text": "Hi! There is a post from \u2018jennaj\u2019 so it must be broken but the link looks like this:\nhttps:/\n/usegalaxy.org/datasets/bbd44e69cb8906b5fa7d3cf2966452cb/display?to_ext=fastqsanger.gz\n(I split the address in the beginning)"}, {"role": "system", "text": "I think you have those datasets private to your account, so another Galaxy cannot fetch it. If I try to open your link I cannot access it either. The other Galaxy cannot log in as you, the datasets need to have permissions open in order for this transfer to work. Check out the advice above from Jenn regarding the \u201cshared state\u201d of history."}, {"role": "user", "text": "I have the \u2018Make History accessible\u2019 on in the \u2019 Share or Publish History\u2019"}, {"role": "system", "text": "What do the permissions look like when you click on the pencil icon on dataset?\nGalaxy_\ud83d\udd0a816\u00d7358 48.4 KB\n"}, {"role": "user", "text": "Looks the same\n\nZrzut ekranu 2021-10-19 o 10.27.352076\u00d7870 126 KB\n"}, {"role": "user", "text": "Hi @marten ! I tried to make the dataset accessible for you but entering your email I had \u2018No matches found\u2019 ?!"}, {"role": "system", "text": "The fact you have your account in the \u201caccess\u201d field makes it inaccessible for everyone else. Remove it with the X and save. Then the link for the import to the other Galaxy will work."}], [{"role": "user", "text": "Could anyone advise on average runtime for the MiGMAP tool to map BCR sequences from single cell data? I started a MiGMAP run with a collection of 37 paired reads from a single cell sequencing experiment (SmartSeq, Nextera, HiSeq4000) 3.5 weeks ago and it is still showing in orange and as \u2018currently running\u2019. Does this tool just take a long time or is it likely there is an error?\nAnd are there any alternative tools in Galaxy to extract BCR variable region sequences from single cell data?"}, {"role": "system", "text": "Hi @carolyn_nielsen\nWould you confirm that you are working at usegalaxy.eu. If somewhere else, please note the server URL. Thanks!\ncc @gallardoalba"}, {"role": "user", "text": "Working at humancellatlas.usegalaxy.eu. Thanks."}, {"role": "system", "text": "Hi @carolyn_nielsen,\nwe have increased the number of cores assigned to this tool. Please repeat the analysis in a few hours.\nRegards"}, {"role": "user", "text": "Great thank you will try that."}, {"role": "system", "text": "@carolyn_nielsen the job is still running. But slowly. I see logging message in the last minutes like\n\n[Mon May 17 12:47:55 CEST 2021 MIGMAP +4159m15s] Loaded 1690 reads. Processed 871 reads, of them 85.76% mapped. Among mapped reads 97.45% passed quality filter and 0.0% passed all filters.\n\nIts doing something, but no clue how long this will take.\nWe have granted more compute resources to this tool (now 12 CPUs), so you could try to restart one job and see if its faster.\nHope that heps,\nBjoern"}, {"role": "user", "text": "Thanks Bjoern - I restarted on Tuesday so hopefully will be a bit faster."}, {"role": "system", "text": "Lets see. The tools sees to be mostly sleeping. Not much we can do here, its seems to be inefficient "}, {"role": "user", "text": "Still running! Are there any plans to make any other BCR tools available on Galaxy? e.g. BALDR or BraCeR?\n\n  \n      \n\n      github.com\n  \n\n  \n    \n\nBosingerLab/BALDR\n\n\n  Contribute to BosingerLab/BALDR development by creating an account on GitHub.\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      github.com\n  \n\n  \n    \n\nTeichlab/bracer\n\n\n  BraCeR - reconstruction of B cell receptor sequences from single-cell RNAseq data\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "@carolyn_nielsen I have no clue. This tool seems to be super slow. The multiple cores are not to be utilized much \nDo you have recommendations about BALDR and Bracer? wth which one we should start if we would have time?"}, {"role": "system", "text": "@carolyn_nielsen there is really something wrong. Can you please subsample your input file to a tiny tiny file or if you know some small files where you know those are working us them to see if you get some results. I suspect there is something wrong with your input data.\nI also see this in the logs, for weeks \u2026 \n[Thu Jun 10 09:16:06 CEST 2021 MIGMAP +19065m32s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:16:16 CEST 2021 MIGMAP +19065m42s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:16:26 CEST 2021 MIGMAP +19065m52s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:16:36 CEST 2021 MIGMAP +19066m02s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:16:46 CEST 2021 MIGMAP +19066m12s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:16:56 CEST 2021 MIGMAP +19066m22s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:06 CEST 2021 MIGMAP +19066m32s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:16 CEST 2021 MIGMAP +19066m42s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:26 CEST 2021 MIGMAP +19066m52s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:36 CEST 2021 MIGMAP +19067m03s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:46 CEST 2021 MIGMAP +19067m13s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:17:56 CEST 2021 MIGMAP +19067m23s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:06 CEST 2021 MIGMAP +19067m33s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:16 CEST 2021 MIGMAP +19067m43s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:26 CEST 2021 MIGMAP +19067m53s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:36 CEST 2021 MIGMAP +19068m03s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:46 CEST 2021 MIGMAP +19068m13s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:18:56 CEST 2021 MIGMAP +19068m23s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:06 CEST 2021 MIGMAP +19068m33s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:16 CEST 2021 MIGMAP +19068m43s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:26 CEST 2021 MIGMAP +19068m53s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:36 CEST 2021 MIGMAP +19069m03s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:46 CEST 2021 MIGMAP +19069m13s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:19:56 CEST 2021 MIGMAP +19069m23s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:06 CEST 2021 MIGMAP +19069m33s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:16 CEST 2021 MIGMAP +19069m43s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:26 CEST 2021 MIGMAP +19069m53s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:36 CEST 2021 MIGMAP +19070m03s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:46 CEST 2021 MIGMAP +19070m13s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:20:56 CEST 2021 MIGMAP +19070m23s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n[Thu Jun 10 09:21:06 CEST 2021 MIGMAP +19070m33s] Loaded 15625 reads. Processed 6027 reads, of them 89.28% mapped. Among mapped reads 97.9% passed quality filter and 0.0% passed all filters.\n"}, {"role": "user", "text": "Thanks for this! I am just confirming with collaborator which is recommends. Believe there is also another option called MiXCR."}, {"role": "user", "text": "Ok yes don\u2019t think MiGMAP will work- just returned errors on the original submitted samples:\n\nimage1151\u00d7455 69 KB\n\nWill get back to re alternative tool."}, {"role": "user", "text": "HI @bjoern.gruening - would be great if a MixCR wrapper could be added to Galaxy: MiXCR: a universal tool for fast and accurate analysis of T- and B- cell receptor repertoire sequencing data \u2014 mixcr documentation\nLet me know if you think this might be feasible!"}, {"role": "user", "text": "Hi @bjoern.gruening any thoughts on this? Installation \u2014 mixcr documentation"}, {"role": "system", "text": "Hi @carolyn_nielsen,\nI created an issue about it: Request for tool MiXCR.\nRegards"}, {"role": "system", "text": "Hi @carolyn_nielsen,\nthis tool is already in Galaxy: MiXCR.\nRegards"}], [{"role": "user", "text": "Hi everyone, I am still having a problem creating a QIIME2 artefact in galaxy. I have my paired end sequences and i have my metadata loaded up. All i need to do is combine them in QIIME2. The tutorial on this page Importing demultiplexed sequence data \u2014 QIIME 2 Cancer Microbiome Intervention Tutorial is pretty straight forward but it only teaches you to copy and paste the data from the website and not how to really create your own. Can anyone point me in the right direction or know who I can ask?\nimage1637\u00d7665 47 KB\nKind regards"}, {"role": "system", "text": "Hi @Martyn\nI\u2019m not sure exactly how to do this either. Suggestions were in the original post\u2026\nWere you able to reach the tool developer? Or get help at their forum? They are currently working to update this specific import tool last time we had an update here. Details for this also in the original post.\nXref: Import qiime2 artefacts? - #2 by jennaj\nWe might be able to help more here but would involve testing out options just like you would be (these tools are a bit special, with no test data/unit tests in the Galaxy wrapper yet). Since potential inputs can vary significantly, a real example is needed.\nYou could create a history with examples of what you have tried so far (with datasets undeleted) then we use that for more tests. No other data please \u2013 just the starting data and your tests isolated.\n\nStart over in a new empty history\nClearly label each test (using tags please \u2013 leaving the original dataset names intact). If you give each of the starting datasets a #tag, that will propagate to the outputs. I also tend to copy/paste the tool version into tags on outputs when testing across versions.\nUse the smallest test data possible. Just enough to be representative.\nA history with just the inputs will not be enough. We need your examples of failures.\nPosting that back here is just one small slice of potential help.\nSharing that at the QIIME forum with context (cross-link to this post) would probably get more help. Some Q&A there already has similar advice so check those methods first if you haven\u2019t already.\n"}, {"role": "user", "text": "\n\n\n jennaj:\n\nlabel each test (using tags please \u2013 leaving the original dataset names intact). If you give\n\n\nHi Jennaj,  I have asked on QIIME forum and I have reached the Goeks lab and they just tell me to ask on here. I have used QIIME2 in the past and I know the metadata needs to match the sample. The tutorial really isn\u2019t that helpful because it only tells you how to copy and paste some data that is already on the website.\nKind regards"}, {"role": "system", "text": "Ah, Ok.\nThen let\u2019s proceed with the tests. Please set up the history and your test runs and I can work with you to choose the correct way to get the data in. But I\u2019ll need something to start with first. This tool is newer to me but can probably diagnose technical issues that may come up versus data/usage, and sort out something to get it to work (assuming your data is supported). Should learn to do this anyway so would appreciate your help \nIf this seems a bit overwhelming, that\u2019s Ok. Open up the different options to see what is required. The naming of the sequences is important, but the form has guidance and will adjust those for you in some cases. Give that a try please."}, {"role": "user", "text": "Thank you Jennaj, this is where I am at now. Below is my QIIME2 metadata which i uploaded successfully.\nimage1513\u00d7923 59.3 KB\nand here is my collection of fastq files which have been paired\nimage1581\u00d7735 37.9 KB\nNow I need to create a QIIME2 artefact and following the tutorial from the website\nhttps://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/020-tutorial-upstream/030-importing.html\nSo I do exactly that\nimage1321\u00d7889 40.2 KB\nAnd here is the error report in Galaxy.\nimage1309\u00d7877 40.7 KB\nI do not know why it is telling me the file has been deleted when it is clearly in my directory. If you have any suggestions let me know \nKind regards"}, {"role": "system", "text": "Hi @Martyn\nThat looks like a tool problem on the server side.\nping @sargentl \u2013 do you recognize this error?"}, {"role": "system", "text": "hello,\nhaving had a look at the situation, i am still not entirely sure what\u2019s happening; do you think you could provide steps to reproduce this so i can analyze the results as they happen? thanks!"}, {"role": "user", "text": "Hi, thank you for getting back to me.\nI copied and pasted my meta table into the upload function and set it as a qiime tabular. I then used the meta tabulate function to create the qimme qzv file.\nI uploaded my fastq.gz files in the upload function by drag and dropping them from my computer and paired them together using the build dataset pair.\nAfter that I tried to use the qiime import tools function to create the qiime artefact and I get the error which now says this:\nimage982\u00d7308 11.9 KB\nIf you want, I can pm you my login details so you can see.\nBest wishes"}, {"role": "system", "text": "It seems like this is a different problem from the one before \u2013 one is the apparent premature deletion of datasets, the other is an input issue for a qiime2 tool.\nFor the first issue, if you can point me in the direction of inputs i can use (or a tutorial, etc) will try to reproduce; as the admin I can watch the process closely. I would have yanked the data from your history directly but you practice good data hygiene and have already purged them.\nRegarding the error you just posted, it seems like your input (a collection of pairs, e.g. a list of lists) is not playing nicely with your SampleData[PairedEndSequencesWithQuality] selection; judging by the selection name you\u2019d think it would want exactly that, but it is complaining about being given a list (presumably, the forward/reverse pair sublist).\nI noticed if you select \u2018Associate Individual Files\u2019 instead of \u2018Use collection to import\u2019, it seems to assume single files (it certainly doesn\u2019t explicitly support selecting forward and reverse sequences). Perhaps it wants a naming convention for forward/reverse instead of a list? I am going to level with you, this is outside my wheelhouse as a Galaxy admin and just guesswork at this point. Not to bounce you around to another platform, but this is almost certainly a qiime2-specific issue; something about these initial conditions is causing the tool to expect single files and be given multiple. I had a look at the data types but didn\u2019t gain much insight; this particular issue is probably best left to the qiime2 devs as they will understand these types more intimately. If it is indeed a Galaxy bug, I will need their response to know what to look for anyway."}, {"role": "user", "text": "Hi, Thanks again for getting back to me \nHere is a tutorial that I am trying to work from Importing demultiplexed sequence data \u2014 QIIME 2 Cancer Microbiome Intervention Tutorial\nI can\u2019t find where to upload as \u2019 Associate Individual Files\u2019 Is it somewhere in the upload function?\nBest wishes"}, {"role": "system", "text": "Hello,\nThanks for the tutorial, I\u2019ll have a look. Re: associate individual files: its in in the tool itself, under \u2018Import sequences \u2192 Select a mechanism\u2019:\nimage1632\u00d7788 67.4 KB"}, {"role": "user", "text": "Hi Sargentl, Thanks for your help. I have found this but I am not sure how to do it. It says to add element but what is that? Do I type the names of individual files?\nimage1590\u00d7687 37.1 KB\nSorry for all the questions but it is quite confusing to work out why it isn\u2019t working."}, {"role": "system", "text": "Questions are always welcome, that\u2019s what this site is for!\nWe are well out of my wheelhouse here as a server admin, but it looks like the text entry is just to specify what name you want to give the data on import; the \u2018data\u2019 selection below lets you pick inputs.\nMy only remaining suggestions are:\n\ncheck your inputs; do you definitely have data of the type \u2018paired end sequences with quality\u2019 in the \u2018casava one eight laneless per sample directory\u2019 format? (not that i know much about what any of that means)\nmaybe try to ask this in the qiime2 help forum, as this is a tool-specifics question beyond the purview of this simple server administrator\n\nhappy to try to answer any more questions you have, though the qiime2 specifics will be largely educated guesswork."}, {"role": "user", "text": "Thank you. I did already use the original QIIME2 to analyse my data but I want to start using Galaxy so I am trying to figure it out. I analysed my data as Paired End Manifest Phred33 with metadata but I have no idea how to do that in Galaxy because I am following the tutorial which shows casava one eight laneless per sample directory.\nBest wishes"}, {"role": "system", "text": "I ran through the tutorial you provided on the cancer server and was able to complete the qiime2 tools import step:\n\nGalaxy | Accessible History | qiime2 test\n\n\u2026whereas i was not able to do so with the inputs you generated. it seems telling that my data_to_import:sequences is a list with 82 items, whereas yours is a list of pairs. perhaps you need to revisit the logic in your rules-based uploading; what does your pasted table look like for the rules based uploader, and what column definitions did you use?"}, {"role": "user", "text": "Hi Sergentl, Thank you for spending time on that. I actually didn\u2019t use rule based uploading. The tutorial provides sequences which are copied and pasted from the site but when I did mine I just uploaded them normally from my computer and then just paired them together by using the pair dataset function and that is my list of sequences.\nBest wishes"}, {"role": "user", "text": "Hi sargentl, I changed mine from a list of pairs to a list just like in the tutorial and it still didn\u2019t work. I have no idea what else to try now.\nKind regards"}, {"role": "user", "text": "Hi Sargentl, I managed to sort it. It was such a simple thing i had not thought about and that is the names of my files had to have been in Casava format and they wasn\u2019t.\nThank you for spending time to help me with this.\nKind regards"}], [{"role": "user", "text": "Hi,\nHope I can find help here. I have a Galaxy server that was purchased a number of years ago from Bioteam (Slipstream Appliance). They administered it up to version 16.07 but then discontinued their support. I need to update to the latest Galaxy version using their instructions supplemented with instructions here - Galaxy Community Hub . I am at a beginner level of dealing with Linux servers (why I purchased a supported setup) so forgive my ignorance.\nSetup:\nBioteam Appliance (Galaxy preloaded Linux server)\nUbuntu 14.04.6 LTS (Trusty Tahr)\nPostgreSQL 9.3 database server\nGalaxy 16.07\nCurrent approach to upgrade:\n\nLogin, become root, and take a ZFS snapshot to enable rollback if problem.\n\nsudo \u2013I\ncd /slipstream\nsupervisorctl stop all && service postgresql stop\nzfs snapshot slipstream@pre_upgrade_from_galaxy16.07_2021-11-13\nservice postgresql start && supervisorctl start all\n\nFetch new version using git, update virtualenv, and migrate DB\n\ncd /slipstream/galaxy/production/galaxy-dist\ngit fetch origin && git checkout release_17.01 && git pull --ff-only origin release_17.01\n./scripts/common_startup.sh\nreboot\nsh manage_db.sh upgrade\ngit status\nsh ./scripts/migrate_tools/0011_tools.sh install_dependencies\nservice postgresql start && supervisorctl start all\n\nThe screen output I get with the last command is:\n\n\nStarting PostgreSQL 9.3 database server [ OK ]\n\nhandler1: ERROR (abnormal termination)\nweb0: ERROR (abnormal termination)\nweb1: ERROR (abnormal termination)\nweb5: ERROR (abnormal termination)\nweb4: ERROR (abnormal termination)\nhandler0: ERROR (abnormal termination)\nweb2: ERROR (abnormal termination)\nweb3: ERROR (abnormal termination)\nThere are other screen outputs from the prior commands that I could also share. Or can look for specific issues.\nI looked at the log files \u2013 comparing log files from startup before upgrade and after upgrade. One thing I noticed is that before upgrade it loads up to database version 135 (and indicates that the version is 135). But after the upgrade it only loads up to version 133 \u2013 and then checks for version and says that the database version is 135. Could this by related to why it won\u2019t start??\nAny suggestions would be greatly appreciated.\nThanks\nMichael"}, {"role": "system", "text": "First guess would be to make sure that your database (postgresql) is running when you execute sh manage_db.sh upgrade. Can you verify that, and provide the output?"}, {"role": "user", "text": "Sure!\nroot@wsu-appliance:/slipstream/home/michael# service postgresql start\n\nStarting PostgreSQL 9.3 database server                               [ OK ]\nroot@wsu-appliance:/slipstream/home/michael# sh manage_db.sh upgrade\nsh: 0: Can\u2019t open manage_db.sh\nroot@wsu-appliance:/slipstream/home/michael# cd /slipstream/galaxy/production/ga                                                      laxy-dist\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# service postgresql                                                       start\nStarting PostgreSQL 9.3 database server                               [ OK ]\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# sh manage_db.sh up                                                      grade\nActivating virtualenv at .venv\nTraceback (most recent call last):\nFile \u201c./scripts/manage_db.py\u201d, line 23, in \ninvoke_migrate_main()\nFile \u201c./scripts/manage_db.py\u201d, line 19, in invoke_migrate_main\nmain( repository=repo, url=db_url )\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/shell.py\u201d, line 209, in main\nret = command_func(**kwargs)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/api.py\u201d, line 186, in upgrade\nreturn _migrate(url, repository, version, upgrade=True, err=err, **opts)\nFile \u201c\u201d, line 2, in _migrate\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/util/init.py\u201d, line 160, in with_engine\nreturn f(*a, **kw)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/api.py\u201d, line 342, in _migrate\nschema = ControlledSchema(engine, repository)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/schema.py\u201d, line 33, in init\nself.load()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/schema.py\u201d, line 54, in load\nexceptions.DatabaseNotControlledError(str(exc)), tb)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/migrate/versioning/schema.py\u201d, line 45, in load\nself.table = Table(tname, self.meta, autoload=True)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/sql/schema.py\u201d, line 440, in new\nmetadata._remove_table(name, schema)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/util/langhelpers.py\u201d, line 60, in exit\ncompat.reraise(exc_type, exc_value, exc_tb)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/sql/schema.py\u201d, line 435, in new\ntable._init(name, metadata, *args, **kw)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/sql/schema.py\u201d, line 510, in _init\nself.autoload(metadata, autoload_with, include_columns)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/sql/schema.py\u201d, line 534, in autoload\nself, include_columns, exclude_columns\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/engine/base.py\u201d, line 1972, in run_callable\nreturn conn.run_callable(callable, *args, **kwargs)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/engine/base.py\u201d, line 1477, in run_callable\nreturn callable(self, *args, **kwargs)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/engine/default.py\u201d, line 364, in reflecttable\nreturn insp.reflecttable(table, include_columns, exclude_columns)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                      -packages/sqlalchemy/engine/reflection.py\u201d, line 571, in reflecttable\nraise exc.NoSuchTableError(table.name)\nmigrate.exceptions.DatabaseNotControlledError: migrate_version\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist#\n"}, {"role": "system", "text": "Can you try to explicitly set the galaxy config file when trying the update? Your version may still be using an ini file, so something like this:\nsh manage_db.sh upgrade -c /path/to/galaxy.ini\n"}, {"role": "user", "text": "\n\n\n blankenberg:\n\nsh manage_db.sh upgrade -c /path/to/galaxy.ini\n\n\nSure.  the *.ini is actually still universe_wsgi.ini\nHere is the output from this:\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# sh manage_db.sh upgrade -c /slipstream/galaxy/production/config/universe_wsgi.ini\nActivating virtualenv at .venv\nTraceback (most recent call last):\nFile \u201c./scripts/manage_db.py\u201d, line 23, in \ninvoke_migrate_main()\nFile \u201c./scripts/manage_db.py\u201d, line 19, in invoke_migrate_main\nmain( repository=repo, url=db_url )\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/shell.py\u201d, line 209, in main\nret = command_func(**kwargs)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/api.py\u201d, line 186, in upgrade\nreturn _migrate(url, repository, version, upgrade=True, err=err, **opts)\nFile \u201c\u201d, line 2, in _migrate\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/util/init.py\u201d, line 160, in with_engine\nreturn f(*a, **kw)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/api.py\u201d, line 342, in _migrate\nschema = ControlledSchema(engine, repository)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/schema.py\u201d, line 33, in init\nself.load()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/schema.py\u201d, line 54, in load\nexceptions.DatabaseNotControlledError(str(exc)), tb)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/migrate/versioning/schema.py\u201d, line 45, in load\nself.table = Table(tname, self.meta, autoload=True)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py\u201d, line 440, in new\nmetadata._remove_table(name, schema)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\u201d, line 60, in exit\ncompat.reraise(exc_type, exc_value, exc_tb)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py\u201d, line 435, in new\ntable._init(name, metadata, *args, **kw)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py\u201d, line 510, in _init\nself._autoload(metadata, autoload_with, include_columns)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py\u201d, line 534, in _autoload\nself, include_columns, exclude_columns\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1971, in run_callable\nwith self.contextual_connect() as conn:\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\u201d, line 2039, in contextual_connect\nself._wrap_pool_connect(self.pool.connect, None),\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\u201d, line 2078, in _wrap_pool_connect\ne, dialect, self)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1405, in _handle_dbapi_exception_noconnection\nexc_info\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py\u201d, line 202, in raise_from_cause\nreraise(type(exception), exception, tb=exc_tb, cause=cause)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\u201d, line 2074, in _wrap_pool_connect\nreturn fn()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 376, in connect\nreturn _ConnectionFairy._checkout(self)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 713, in _checkout\nfairy = _ConnectionRecord.checkout(pool)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 480, in checkout\nrec = pool._do_get()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 1060, in _do_get\nself._dec_overflow()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\u201d, line 60, in exit\ncompat.reraise(exc_type, exc_value, exc_tb)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 1057, in _do_get\nreturn self._create_connection()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 323, in _create_connection\nreturn _ConnectionRecord(self)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 449, in init\nself.connection = self.__connect()\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/pool.py\u201d, line 607, in __connect\nconnection = self.__pool._invoke_creator(self)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\u201d, line 97, in connect\nreturn dialect.connect(*cargs, **cparams)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\u201d, line 385, in connect\nreturn self.dbapi.connect(*cargs, **cparams)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/psycopg2/init.py\u201d, line 164, in connect\nconn = _connect(dsn, connection_factory=connection_factory, async=async)\nmigrate.exceptions.DatabaseNotControlledError: (psycopg2.OperationalError) FATAL:  role \u201croot\u201d does not exist"}, {"role": "system", "text": "\nmigrate.exceptions.DatabaseNotControlledError: (psycopg2.OperationalError) FATAL: role \u201croot\u201d does\n\nTry executing when logged in as the Galaxy user?"}, {"role": "user", "text": "I tried logging in as \u201cGalaxy\u201d and as my user name - \u201cMichael\u201d  :\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# su galaxy\ngalaxy@wsu-appliance:~/production/galaxy-dist$ sh manage_db.sh upgrade -c /slipstream/galaxy/production/config/universe_wsgi.ini\nActivating virtualenv at .venv\ngalaxy@wsu-appliance:~/production/galaxy-dist$ su michael\nPassword:\nmichael@wsu-appliance:/slipstream/galaxy/production/galaxy-dist$ sh manage_db.sh upgrade -c /slipstream/galaxy/production/config/universe_wsgi.ini\nActivating virtualenv at .venv\nTraceback (most recent call last):\nFile \u201c./scripts/manage_db.py\u201d, line 23, in \ninvoke_migrate_main()\nFile \u201c./scripts/manage_db.py\u201d, line 15, in invoke_migrate_main\nconfig = get_config( sys.argv )\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/model/orm/scripts.py\u201d, line 93, in get_config\nproperties = load_app_properties( ini_file=config_file, config_prefix=config_override )\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/properties.py\u201d, line 49, in load_app_properties\nwith open(ini_file) as f:\nIOError: [Errno 13] Permission denied: \u2018/slipstream/galaxy/production/config/universe_wsgi.ini\u2019\nmichael@wsu-appliance:/slipstream/galaxy/production/galaxy-dist$"}, {"role": "user", "text": "These are the other users:\nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\nlibuuid:x:100:101::/var/lib/libuuid:/bin/sh\nsyslog:x:101:103::/home/syslog:/bin/false\nmessagebus:x:102:105::/var/run/dbus:/bin/false\nsshd:x:103:65534::/var/run/sshd:/usr/sbin/nologin\npostgres:x:104:110:PostgreSQL administrator,:/var/lib/postgresql:/bin/bash\nslipstream:x:1005:1005:SlipStream-Management-Account:/slipstream/home/slipstream:/bin/bash\ngalaxy:x:1000:1000::/slipstream/galaxy:/bin/bash\npostfix:x:105:111::/var/spool/postfix:/bin/false\nsgeadmin:x:106:113::/var/lib/gridengine:/bin/false\nslipuser:x:1006:1006:SlipStream-User-Account:/slipstream/home/slipuser:/bin/bash\nmichael:x:1007:1007::/slipstream/home/michael:/bin/bash"}, {"role": "system", "text": "\n\n\n MichaelC:\n\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# su galaxy\ngalaxy@wsu-appliance:~/production/galaxy-dist$ sh manage_db.sh upgrade -c /slipstream/galaxy/production/config/universe_wsgi.ini\nActivating virtualenv at .venv\n\n\n\nWas there any additional output after the activation of the .venv when running as \u2018galaxy\u2019? Your copy&paste goes right into you entering su michael."}, {"role": "user", "text": "No.  It went to the prompt as shown without any hesitation."}, {"role": "user", "text": "Just in case I had done something that caused it to bounce out of virtualenv - I did a rollback to the previous snapshot and then repeated the whole update process except after the reboot I logged into \u201cGalaxy\u201d (not root) and this is what I got:\ngalaxy@wsu-appliance:~/production/galaxy-dist$ sh manage_db.sh upgrade -c /slips                                                                                             tream/galaxy/production/config/universe_wsgi.ini\nActivating virtualenv at .venv\nTraceback (most recent call last):\nFile \u201c./scripts/manage_db.py\u201d, line 23, in \ninvoke_migrate_main()\nFile \u201c./scripts/manage_db.py\u201d, line 19, in invoke_migrate_main\nmain( repository=repo, url=db_url )\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/shell.py\u201d, line 209, in main\nret = command_func(**kwargs)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/api.py\u201d, line 186, in upgrade\nreturn _migrate(url, repository, version, upgrade=True, err=err, **opts)\nFile \u201c\u201d, line 2, in _migrate\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/util/init.py\u201d, line 160, in with_engine\nreturn f(*a, **kw)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/api.py\u201d, line 345, in _migrate\nchangeset = schema.changeset(version)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/schema.py\u201d, line 82, in changeset\nchangeset = self.repository.changeset(database, start_ver, version)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/repository.py\u201d, line 225, in changeset\nchanges = [self.version(v).script(database, op) for v in versions]\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/repository.py\u201d, line 189, in version\nreturn self.versions.version(*p, **k)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site                                                                                             -packages/migrate/versioning/version.py\u201d, line 163, in version\nreturn self.versions[VerNum(vernum)]\nKeyError: <VerNum(135)>\ngalaxy@wsu-appliance:~/production/galaxy-dist$ ^C"}, {"role": "user", "text": "I forgot to say is that I also added: find . -name '*.pyc' -delete before \u201cgit fetch\u201d - since I had seen this mentioned as a possible solution.  When I didn\u2019t add this I just got \u201cActivating virtualenv at .venv\u201d and nothing else.\nSo - the current procedure that got me to the last output was:\ncd /slipstream/galaxy/production/galaxy-dist\nfind . -name '*.pyc' -delete\ngit fetch origin && git checkout release_17.01 && git pull --ff-only origin release_17.01\n./scripts/common_startup.sh\nreboot\nsu galaxy\ncd /slipstream/galaxy/production/galaxy-dist\nsh manage_db.sh upgrade -c /slipstream/galaxy/production/config/universe_wsgi.ini"}, {"role": "system", "text": "Did you perhaps attempt to update to a Galaxy version greater than 17.01 before this? The maximum database version for 17.01 is 133, and it looks like your database might be at 135.\nFWIIW, 17.09 has db version 135."}, {"role": "user", "text": "Yes.  Initially I did try to update to 19.01 (latest version at the time) using the instructions left for me.  It wouldn\u2019t start so I rolled back using ZFS snapshot. I didn\u2019t try upgrading again until recently - after getting advice here that I should try one version at a time - so I tried to upgrade to 17.01.  Looking at the root history file though it does appear that there was a snapshot file pre- upgrade to 17.09 that was deleted.  So I suspect that the previous administrator had upgraded to 17.09.  Looking at my emails from them it does seem that the last version they upgraded to was 17.09.   Not sure why the current version is 16.07.  But it is always possible that that I did it (downgraded to 16.07??) at some point."}, {"role": "user", "text": "Thanks for this.  I updated to 17.09 and now the database version and the migrate versions directory are now matched (135).  I was then able to restart Galaxy.\nI then tried to update to the next version - 18.01.  The database migration worked fine.  The tool config migration came up with an error:\nsh ./scripts/migrate_tools/0011_tools.sh install_dependencies\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/config.py\u201d, line 7, in     import ipaddress\nImportError: No module named ipaddress\nAnd then it wouldn\u2019t start (abnormal termination).\nI noted in the release notes that there are quite a few changes including \" The default web server used by Galaxy has changed from Paste to uWSGI and the default configuration file for Galaxy is now config/galaxy.yml instead of config/galaxy.ini .\"  The config file here is still universe_wsgi.ini.   So - somehow the universe_wsgi.ini settings need to be moved to the sample galaxy.yml file??  There was also mention of moving settings from the xml to the new xml.sample files??  Wow.  This is not an easy upgrade process it seems.\nI looked for tutorials in server administration - but they only refer to newer versions (20 plus) and mainly refer to using \u201cAnsible\u201d.\nIs there any resource to guide upgrades from 17.09??"}, {"role": "user", "text": "Since last Monday I successfully migrated from \u201cuniverse_wsgi.ini\u201d to \u201cgalaxy.ini\u201d; and also updated the \u201ctool_conf.xml\u201d , \u201ctool_data_table_conf.xml\u201d and \u201cdatatypes_conf.xml\u201d files using the .sample files.  After rebooting Galaxy started fine.  The current version is 17_09 and database 35.\nI then tried to update to the next version - 18_01 with the following outcome:\ngit fetch origin && git checkout release_18.01 && git pull --ff-only origin release_18.01\n./scripts/common_startup.sh\nScreen output:\nActivating virtualenv at .venv\nRequirement already satisfied: pip>=8.1 in ./.venv/lib/python2.7/site-packages\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nSNIMissingWarning*\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nInsecurePlatformWarning*\nCertificate did not match expected hostname: pypi.org. Certificate: {\u2018notAfter\u2019: \u2018Nov 23 18:41:10 2022 GMT\u2019, \u2018subjectAltName\u2019: ((\u2018DNS\u2019, \u2018www.python.org\u2019), (\u2018DNS\u2019, '.python.org\u2019), (\u2018DNS\u2019, \u2018docs.python.org\u2019), (\u2018DNS\u2019, \u2018downloads.python.org\u2019), (\u2018DNS\u2019, \u2018pypi.python.org\u2019)), \u2018subject\u2019: (((\u2018commonName\u2019, u\u2019www.python.org\u2019),),)}*\nRequirement already satisfied: numpy==1.9.2 in ./.venv/lib/python2.7/site-packages (from -r requirements.txt (line 4))\nRequirement already satisfied: bx-python==0.7.3 in ./.venv/lib/python2.7/site-packages (from -r requirements.txt (line 5))\nCollecting MarkupSafe==1.0 (from -r requirements.txt (line 6))\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nSNIMissingWarning*\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nInsecurePlatformWarning*\nCould not fetch URL https://wheels.galaxyproject.org/simple/markupsafe/: There was a problem confirming the ssl certificate: [Errno 1] _ssl.c:510: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol - skipping*\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nInsecurePlatformWarning*\nCertificate did not match expected hostname: pypi.org. Certificate: {\u2018notAfter\u2019: \u2018Nov 23 18:41:10 2022 GMT\u2019, \u2018subjectAltName\u2019: ((\u2018DNS\u2019, \u2018www.python.org\u2019), (\u2018DNS\u2019, '.python.org\u2019), (\u2018DNS\u2019, \u2018docs.python.org\u2019), (\u2018DNS\u2019, \u2018downloads.python.org\u2019), (\u2018DNS\u2019, \u2018pypi.python.org\u2019)), \u2018subject\u2019: (((\u2018commonName\u2019, u\u2019www.python.org\u2019),),)}\n\nCould not fetch URL https://pypi.python.org/simple/markupsafe/: There was a problem confirming the ssl certificate: hostname \u2018pypi.org\u2019 doesn\u2019t match either of \u2018www.python.org\u2019, '.python.org\u2019, \u2018docs.python.org\u2019, \u2018downloads.python.org\u2019, \u2018pypi.python.org\u2019 - skipping\n\nCould not find a version that satisfies the requirement MarkupSafe==1.0 (from -r requirements.txt (line 6)) (from versions: )*\nNo matching distribution found for MarkupSafe==1.0 (from -r requirements.txt (line 6))\n/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see Advanced Usage - urllib3 2.0.0.dev0 documentation.\n\nInsecurePlatformWarning*\nCertificate did not match expected hostname: pypi.org. Certificate: {\u2018notAfter\u2019: \u2018Nov 23 18:41:10 2022 GMT\u2019, \u2018subjectAltName\u2019: ((\u2018DNS\u2019, \u2018www.python.org\u2019), (\u2018DNS\u2019, '.python.org\u2019), (\u2018DNS\u2019, \u2018docs.python.org\u2019), (\u2018DNS\u2019, \u2018downloads.python.org\u2019), (\u2018DNS\u2019, \u2018pypi.python.org\u2019)), \u2018subject\u2019: (((\u2018commonName\u2019, u\u2019www.python.org\u2019),),)}*\n\nreboot\nsh manage_db.sh upgrade -c /slipstream/galaxy/production/config/galaxy.ini\nOutput:\nActivating virtualenv at .venv\n135 \u2192 136\u2026\nMigration script for collections and workflows connections.\ndone\n136 \u2192 137\u2026\nAdd copied_from_job_id column to jobs table\ndone\n137 \u2192 138\u2026\nAdd version column to  history_dataset_association table\ndone\n138 \u2192 139\u2026\nMigration script to add the history_dataset_association_history table.\nCreated history_dataset_association_history table\ndone\n139 \u2192 140\u2026\nAdd dataset_version column to job_to_input_dataset table\ndone\nsh ./scripts/migrate_tools/0011_tools.sh install_dependencies\nOutput:\nTraceback (most recent call last):\n\nFile \u201c./scripts/migrate_tools/migrate_tools.py\u201d, line 17, in *\nfrom tool_shed.galaxy_install.migrate.common import MigrateToolsApplication*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/tool_shed/galaxy_install/migrate/common.py\u201d, line 8, in *\nimport galaxy.config*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/config.py\u201d, line 7, in *\nimport ipaddress*\nImportError: No module named ipaddress\n\n\nHowever galaxy then failed to start (abnormal termination) with the following Output from handler0.log:\nTraceback (most recent call last):*\n\nFile \u201c./scripts/paster.py\u201d, line 27, in *\nserve.run()*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/serve.py\u201d, line 1055, in run*\ninvoke(command, command_name, options, args[1:])*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/serve.py\u201d, line 1061, in invoke*\nexit_code = runner.run(args)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/serve.py\u201d, line 226, in run*\nresult = self.command()*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/serve.py\u201d, line 642, in command*\napp = loadapp(app_spec, name=app_name, relative_to=base, global_conf=vars)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 293, in loadapp*\nreturn loadobj(APP, uri, name=name, *kw)\n\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 318, in loadobj*\nglobal_conf=global_conf)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 343, in loadcontext*\nglobal_conf=global_conf)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 367, in _loadconfig*\nreturn loader.get_context(object_type, name, global_conf)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 508, in get_context*\nsection)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 567, in _context_from_explicit*\nvalue = import_string(found_expr)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/util/pastescript/loadwsgi.py\u201d, line 118, in import_string*\nreturn pkg_resources.EntryPoint.parse(\u201cx=\u201d + s).load(False)*\nFile \u201c/slipstream/galaxy/production/virtualenv/local/lib/python2.7/site-packages/pkg_resources/init.py\u201d, line 2268, in load*\nreturn self.resolve()*\nFile \u201c/slipstream/galaxy/production/virtualenv/local/lib/python2.7/site-packages/pkg_resources/init.py\u201d, line 2274, in resolve*\nmodule = import(self.module_name, fromlist=[\u2018name\u2019], level=0)*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/web/buildapp.py\u201d, line 3, in *\nfrom galaxy.webapps.galaxy.buildapp import app_factory*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/webapps/galaxy/buildapp.py\u201d, line 12, in *\nimport galaxy.app*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/app.py\u201d, line 18, in *\nfrom galaxy.tools.cache import (*\nFile \u201c/slipstream/galaxy/production/galaxy-dist/lib/galaxy/tools/init.py\u201d, line 16, in *\nimport packaging.version*\nImportError: No module named packaging.version\n\n\nAny ideas on what to try next would be greatly appreciated!!"}, {"role": "system", "text": "Looks like setuptools might be missing. You may want to try moving your .venv directory to a temporary name (e.g. .venv.old), and then start Galaxy and see if it will rebuild a working .venv."}, {"role": "user", "text": "Sure.  I also provided the pip.log output.  It looks like the problem is related to:\u201c403 Client Error: SNI is required\u201d\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# mv .venv .venv.old\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# cd .venv\nbash: cd: .venv: No such file or directory\nroot@wsu-appliance:/slipstream/galaxy/production/galaxy-dist# ./scripts/common_startup.sh\nAlready using interpreter /usr/bin/python\nNew python executable in .venv/bin/python\nInstalling setuptools, pip\u2026done.\nActivating virtualenv at .venv\nDownloading/unpacking pip>=8.1\nCannot fetch index base URL Simple index\nCould not find any downloads that satisfy the requirement pip>=8.1\nCleaning up\u2026\nNo distributions at all found for pip>=8.1\nStoring debug log for failure in /root/.pip/pip.log\n\n/slipstream/galaxy/production/galaxy-dist/.venv/bin/pip run on Tue Nov 30 14:09:55 2021\nDownloading/unpacking pip>=8.1\nGetting page Links for pip\nCould not fetch URL https://pypi.python.org/simple/pip/: 403 Client Error: SNI is required\nWill skip URL Links for pip when looking for download links for pip>=8.1\nGetting page Simple index\nCould not fetch URL https://pypi.python.org/simple/: 403 Client Error: SNI is required\nWill skip URL Simple index when looking for download links for pip>=8.1\nCannot fetch index base URL Simple index\nURLs to search for versions for pip>=8.1:\n\n\nLinks for pip\nGetting page Links for pip\nCould not fetch URL https://pypi.python.org/simple/pip/: 403 Client Error: SNI is required\nWill skip URL Links for pip when looking for download links for pip>=8.1\nCould not find any downloads that satisfy the requirement pip>=8.1\nCleaning up\u2026\nRemoving temporary dir /slipstream/galaxy/production/galaxy-dist/.venv/build\u2026\nNo distributions at all found for pip>=8.1\nException information:\nTraceback (most recent call last):\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/basecommand.py\u201d, line 122, in main\nstatus = self.run(options, args)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/commands/install.py\u201d, line 278, in run\nrequirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/req.py\u201d, line 1177, in prepare_files\nurl = finder.find_requirement(req_to_install, upgrade=self.upgrade)\nFile \u201c/slipstream/galaxy/production/galaxy-dist/.venv/local/lib/python2.7/site-packages/pip/index.py\u201d, line 277, in find_requirement\nraise DistributionNotFound(\u2018No distributions at all found for %s\u2019 % req)\nDistributionNotFound: No distributions at all found for pip>=8.1\n"}, {"role": "user", "text": "Just in case anyone reads this and is trying to upgrade Galaxy from pre-Galaxy 20 versions (in my case 16) with older pre-Ubuntu 20 versions (in my case 14) )\u2014 it is not possible since Galaxy and Ubuntu are co-dependent through this time period.  Ubuntu is very difficult/impossible to upgrade successfully.  Similarly Galaxy\u2019s requirements (mainly Python) change greatly through this period. I ended up copying raw data off my Galaxy installation; wiping the server; installing Ubuntu 20; and then the Ansible version of Galaxy 21 \u2013 the latter based on these tutorials.\nNew install of Galaxy\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Galaxy Installation with Ansible\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAdd on Ansible \u2013 install Proftpd\n  \n      \n\n      usegalaxy.be\n  \n\n  \n    \n\nGalaxy Training: Enable upload via FTP\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAnsible does make it easy - although there are a couple of errors and poorly explained sections in the tutorials that required some headscratching and tweaking.\n\nCross-posted topic: toolshed not working anymore with old galaxy?"}], [{"role": "user", "text": "Hi, I submitted three datasets to the MethylDackel tool and it is running for the past two days (other users said it took them 2-3 hours to get results). does anyone know what\u2019s the problem?\nThanks"}, {"role": "system", "text": "Hi @Guy_Haim,\nI have this here in your logs.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_load] build FASTA index.\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:0-1000000!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:1000000-2000000!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:3000001-4000001!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:4000001-5000001!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:5000001-6000001!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:6000001-7000001!\nNote that the output will be truncated!\n[fai_fetch_seq] The sequence \"chr1\" not found\n[fai_fetch_seq] The sequence \"chr1\" not found\nfaidx_fetch_seq returned -2 while trying to fetch the sequence for tid chr1:10000002-11000002!\nNote that the output will be truncated!\n\nAre you supplying your own index? Which reference genome are you using?\nCiao,\nBjoern"}, {"role": "user", "text": "Yes, my own index.\n\n13: GRCh38.primary_assembly.genome.fa uncompressed\n\ndo you think I should use another index?"}, {"role": "system", "text": "Any reason you can not use \u2026\n"}], [{"role": "user", "text": "Yesterday I had a project in my account in perfect condition. Today I log in and all I find is: \u201cThis history is empty. You can load your own data or get data from an external source\u201d.\nWhat can I do to recover my history?"}, {"role": "system", "text": "I am having the same issue. Any solution?"}, {"role": "system", "text": "Same thing happened to me this morning\u2026 all my histories are empty, but still say they contain items when I look at the list of histories. I tried \u201cresume all paused jobs\u201d and \u201cunhide all hidden content\u201d but nothing has happened."}, {"role": "system", "text": "usegalaxy.eu is currently getting prepared for the update to the latest release of Galaxy.\nThis causes a bit more issues than expected, and we\u2019re sorry for the inconvenience."}, {"role": "system", "text": "This issue has been dealt with. Sorry again for the inconvenience."}], [{"role": "user", "text": "Hi,\nI launched a job on Friday for alignment on SPADES on usegalaxy.org\nI used FASTQ to check the quality of my uploaded sequences and Trimmomatic to trim my data beforehand. I used the \u201cpaired\u201d output from Trimmomatic to launch SPADES.\nI launched 24 alignments at the same time (all from a same pair-end Illumina run; 24 samples) and with the same settings (run only assembly; careful correction; k-mers to use 33,55,99; coverage cutoff auto).\nTwo jobs are still ongoing, 12 succeeded and 10 failed.\nFor the failed jobs, the error reported is:\n\ntool error\nAn error occurred with this dataset:\nRemote job server indicated a problem running or monitoring this job.\n\nI\u2019m not too sure what that means.\nI don\u2019t understand why 10 jobs failed as from the quality analysis (FASTQ) it did not seem to me that there was any difference between the 12 that worked and the 10 that failed.\nCan this be a server issue? Is it worth it to re-launch the jobs that failed?\nThanks for your help\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nYes, there were some server issues and SPAdes was one impacted tool. Not all runs, just some.\nThe error message you posted can be produced by a server/cluster issue or an input issue. Given what you state and the timing, this seems to be likely server related. Rerun the failed jobs and see if that works.\nAt least one rerun is a good idea anyway \u2013 unless the input problem is obvious. Due to the high volume of work done at this server, some small fraction of jobs will fail even under normal conditions. But about half of a batch failing usually means more is going on \u2013 so glad you asked about it!\nFAQ: https://galaxyproject.org/support/tool-error/#type-cancelled-by-admin-or-a-cluster-failure\nIf the reruns fail again, please let us know:\n\nSend in a bug report from any of the new errored results and include a link to this Galaxy Help topic in the comments (for context)\nPlease do not delete the inputs or outputs\nQuickly post back here that you sent the bug report in, to make sure we find it\n\nThanks!"}, {"role": "user", "text": "Thank you !!\nGalaxy is down for scheduled maintenance at the moment so I will try to rerun the failed jobs when everything is back to normal and will keep you posted.\nThanks again\nVanina"}, {"role": "system", "text": "Yes, please try again after the scheduled maintenance is completed.\nFor others curious about downtime for server upgrades (now or later), this is where to find out about usegalaxy.* server and related core component status:\nhttps://galaxyproject.statuspage.io/"}, {"role": "user", "text": "Hi!\nSo I tried to re-run the 10 failed jobs.\nI now had 6 successful jobs and 4 that still failed.\nBecause it seems a bit inconsistent, I reported a bug as you suggested, flagging this galaxy help link.\nThanks for helping\nVanina"}, {"role": "system", "text": "@vanina.guernier Thanks! reviewing now"}, {"role": "user", "text": "Hi\nI wanted to let you know that I just tried to run some of the same data with Unicycler as I had both short and long reads for 9 samples.\nAll of them failed in a few minutes.\nAgain it\u2019s showing the same error:\n\ntool error\nAn error occurred with this dataset:\nRemote job server indicated a problem running or monitoring this job.\nOf note, I did put\n\n\nthe Illumina data R1 and R2 as first and second set of reads\nMinIon files as long reads (but output from filtlong instead of raw data as I had some issues before with these).\nAll settings were otherwise by default.\n\nShould I open this as a separate request for help??\nThanks\nVanina"}, {"role": "system", "text": "No need to open another topic here, let\u2019s keep it all together for context.\nIf the Unicycler runs are in the same history, I\u2019ll load it up again to capture the new runs and review it with the others. If it is in a different history, please send in another bug report \u2013 that makes it easy to make sure we are reviewing the same data \nThanks for doing more testing and reporting problems!!"}, {"role": "user", "text": "If I am not mistaken, I have only one history.\nBut just to be sure, I reported one of the bug and linked to this chat again.\nOf note, as I ran into trouble before when using trimmed sequences (with Trimmomatic) in Unicycler, I tested a few different options for a same sample:\n\ntrimmed Illumina reads + filtered Nanopore reads (using filtlong)\ntrimmed Illumina reads + raw Nanopore reads\nraw Illumina reads + filtered Nanopore reads (using filtlong)\nraw Illumina reads + raw Nanopore reads\n\nNone of it worked and the jobs were not queued, it failed straight away in minutes or sometimes in seconds (while for example, when my jobs failed with Spades, it was queued for days before finally failing).\nThanks for your help\nVanina"}, {"role": "system", "text": "Thanks, I saw your report plus can reproduce the \u201cfails immediately\u201d issue with test data across a set of specific tools.\nThere is a problem with one of our dedicated clusters. Unicycler, Spades, plus a few other tools are impacted. Our administrator is working on it. We\u2019ll post back an update tomorrow, even if not fully resolved yet.\nIf your work is urgent, you might want to try the usegalaxy.eu server as a short-term alternative for now. Most public Galaxy servers are very very busy, so using more than one is common. Usage terms are one account per person per public Galaxy server \u2013 so it is fine to use both, plus any others that host the tools you are interested in: https://galaxyproject.org/use/\nAll the feedback appreciated!"}, {"role": "user", "text": "Thanks! It\u2019s kind of reassuring to know that it\u2019s a server issue rather than a problem with my data.\nI\u2019m guessing that if I want to use http://usegalaxy.eu I will need to upload all my raw data again on the european server? (the different servers are not \u201clinked\u201d right?)\nVanina"}, {"role": "system", "text": "Update: The usegalaxy.org server now has a status banner for the impacted tools. When that clears or updates, then tools not listed will run again.\n\nCertain large memory tools are not functioning due to issues with the Bridges supercomputer at PSC. Affected tools include Trinity, SPAdes, Unicycler, and RNA STAR.\n\n\nThe problem is still being resolved, so using usegalaxy.eu is a good idea for now.\nAnd yes, accounts are distinct between these two servers (and most public Galaxy servers). There are some domain-specific sub-servers where the same account is used across all, but you\u2019ll be able to tell by the server\u2019s URL.\nExample: covid19.usegalaxy.eu is a subdomain server of usegalaxy.eu and your account at both would be the same, but different from your account at usegalaxy.org. Accounts at any usegalaxy.eu or usegalaxy.org servers (including subdomain servers) are distinct from any account at usegalaxy.org.au.\nHow to move data around between Galaxy servers by URL (avoiding local download/upload) https://galaxyproject.org/support/#data-options\n\n\nDatasets can be copied directly between servers. Capture the link from the disc icon in the \u201cfrom\u201d dataset then paste that into the Upload tool at the \u201cto\u201d server. The data will load into the active history. If you have many datasets, copy the URLs into some text editor scratch space, then paste all in at the same time and the data will load in batch.\n\n\nHistories can be exported as an archive at the \u201cfrom\u201d server (history menu option) then loaded by URL at the \u201cto\u201d server (User > Histories > Import). Be aware that very large histories will take some time to create an archive from, and then more time load at the other server. To reduce an original history\u2019s size, copy just the datasets you need into a new history, then archive/transfer that.\n\n\nWorkflows can also be transferred between servers by URL (Workflow view for both). Generate a share link for the workflow on the \u201cfrom\u201d server and paste that URL into the \u201cto\u201d server (Workflow > Import). Note that different public Galaxy servers host different tools/versions \u2013 so be sure to open, review, and update any workflows as needed before running them.\n\n"}, {"role": "user", "text": "Ok, thanks for the update.\nI\u2019m afraid I\u2019m not quite clear on the transfer of datasets though:\n\nCapture the link from the disc icon\n\nWhat is the disc icon? Whatever I click on does not change the URL on the web browser.\nThe only \u201cdisc\u201d I can see is this on the upper left when I click on a data in my history:\n\nBut it just allows to download the data as shown on the picture, so I don\u2019t find a link.\nI tried to find the information elsewhere but I\u2019m stuck, sorry.\nVanina"}, {"role": "system", "text": "Hold the control key down and click on the disc icon. A menu will appear with an option to copy the link.\nhttps://galaxyproject.org/support/#basics >> https://galaxyproject.org/support/download-data/ plus others in that group explain functions within datasets, icons, and related."}, {"role": "user", "text": "Hi\nI managed to transfer the needed data to usegalaxy.eu\nAll the Unicycler runs that I launched worked!!\nSo I will probably try to do the same thing with SPAdes for my 4 remaining failed jobs.\nThanks a lot for your help, that was really useful.\nVanina"}, {"role": "system", "text": "\n  \n    \n    \n    run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210 usegalaxy.org support\n  \n  \n    run Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n  \n\n"}], [{"role": "user", "text": "Hi all,\nsince some months we are having an issue with the toolshed. It\u2019s not installing any tools anymore printing the following error:\n\u201cRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser.\u201d\nI tried the enabling of third party cookies but it does not change anything.\nOur instance is quite old (16.07) but a the moment we cannnot update.\nIs there anything I can do to get access to the toolshed again?\nI found a number of related topics, which are not helping with my problem:\n  \n    \n    \n    Repository installation is not possible due to an invalid Galaxy URL \n  \n  \n    Hi \nI\u2019m trying to install a tool from the Toolshed but I got this error: \n\nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser. \n\nThe cookies are OK (tested on Firefox and Chrome). \nI\u2019ve found this similar topic: \nhttps://biostar.usegalaxy.org/p/8548/index.html \nSo I checked the admin_users parameter in galaxy.yml file. It was empty. \n-> First of all it\u2019s weird that I could connect as admin mode (with admin@galaxy\u2026\n  \n\n\n  \n    \n    \n    Error attempting to retrieve installation information from tool shed \n  \n  \n    Hi @yukieymd \nBoth the Main and Test ToolSheds were recently undergoing updates. Issues around that are resolved as of earlier today. \nRelated Q&A: \n\nThanks!\n  \n\n\n  \n    \n    \n    trouble in galaxy shed \n  \n  \n    I\u2019ve tried installing blast2go in the browser version of Galaxy. Yet, I always get the same error (mentioned below). \nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser. \nI have allready checked my cookie settings in both google chrome as in mozilla firefox and both browsers allow the third-party cookies. However, the error still occurs\u2026 Any advice\n  \n\n\nCheers\nJochen"}, {"role": "system", "text": "\n\n\n bimbam23:\n\n16.07\n\n\nHi Jochen @bimbam23\nI\u2019ve asked your question on the developer\u2019s gitter channel to see if there are any more potential solutions. They may answer here or there and you can certainly join in the chat: galaxyproject/dev - Gitter\nMy guess is that the upgrades that occurred this week are incompatible with the version of Galaxy you are using \u2013 or are incompatible right now \u2013 but let\u2019s get more advice.\nGood to see you (virtually) again!"}, {"role": "user", "text": "Hej Jenn,\nthanks for helping and good to hear from you too \nI will have a look at the gitter channel\nCheers Jochen"}, {"role": "system", "text": "Hi Jochen,\nI have exactly the same problem - older Galaxy version (16.?) and cannot install any new tools.  What did you find out and/or end up doing??\nBest\nMichael"}, {"role": "user", "text": "Hi Michael,\nI could not solve the problem. I might have to install a new galaxy version in the future\u2026 Updating from 16 \u2026might be a bit difficult I guess.\nCheers\nJochen"}, {"role": "system", "text": "Thanks for the information!  Yes - I tried to update from version 16 (I think to 19) last year and could it kept failing.\nWe purchased Galaxy already installed on a Linux server from Bioteam - who maintained the updates until 2019 (i.e. Version 16).  Unfortunately they stopped supporting us (and everyone else I gather) - but kindly left instructions on how to update it - which didn\u2019t work.  Since then we haven\u2019t needed to update - until now when the Toolshed stopped working.\nIf you having any advice on updating - let me know.\nThanks\nMichael"}, {"role": "system", "text": "For some details check out release 20.09 possibly broke the ability to install tools \u00b7 Issue #11746 \u00b7 galaxyproject/galaxy \u00b7 GitHub\ntldr: you have to update"}, {"role": "system", "text": "To add, if you\u2019re running into problems updating your Galaxy instances, let us know.\nFor such old releases I\u2019d recommend doing year-by-year updates.\nSo if you start on release_16.01, I\u2019d recommend updating to release_17.01, doing the database migrations (./manage_db.sh -c config/galaxy.ini), start up Galaxy and verify the interface works, and then going to release_18.01 and so on. Once you reach release_20.01 you have to migrate to Python 3, instructions for updating this are here: Supported Python versions \u2014 Galaxy Project 21.01 documentation"}], [{"role": "user", "text": "Hi everyone!\nI\u2019m trying to use the tool Stacks2 : process_radtags to demultiplex paired-end data. I\u2019ve got 2 huge files, R1 and R2, each one at about 45 Gb, in fastqsanger.gz format. The problem I encounter is that when I select \u201cpaired-end files\u201d in the first line of the tool, it can\u2019t find my data files. On the other end, if I leave it at \u201cSingle-end files\u201d, it finds both of the files with no problem. Should I interlace my two files for this tool to work?\nCan anyone help me with this small issue?\nThanks!"}, {"role": "system", "text": "Hi @Joelle99,\nyou need to create a paired collection. In order to do it you need to select Operation on multiple datasets, then select your paired files and finally Build a Dataset Pair.\nScreenshot from 2021-02-24 11-36-57540\u00d7925 61.3 KB\nRegards"}, {"role": "user", "text": "Hi, and thank you for your answer!\nSadly, I had already tried beforehand to build a dataset pair, but it still can\u2019t find my file with my two reads\u2026"}, {"role": "system", "text": "Have you double checked that the format that was assigned by Galaxy is what the tool requires?"}, {"role": "user", "text": "Yes I did ! Here\u2019s what they ask for :\n\nBoth of my files are in fastqsanger.gz, which should then be fine I guess\u2026"}, {"role": "system", "text": "Is that the value Galaxy assigned to the uploaded files? You can double check by clicking the edit attributes pencil icon on the dataset, then going to the \u201cDatatypes\u201d tab"}, {"role": "user", "text": "Yes it is! Here\u2019s one file for example :\n"}, {"role": "system", "text": "Can you do a screen cap of the collection you had created?"}, {"role": "user", "text": "There you go!\n\nimage286\u00d7669 20.4 KB\n"}, {"role": "system", "text": "I\u2019ll admit I have zero experience working with paired collections. Are you sure this is a \u2018list:pair\u2019 and not somehow a \u2018pair\u2019.\nSomeone else is going to have to weigh in on this."}, {"role": "user", "text": "Well, I followed @gallardoalba  instructions so I guess it must be okay? To be honest, this is my first time analysing sequencing data, so I\u2019m just starting to learn how Galaxy (and Rad-Seq analysis) works\u2026"}, {"role": "system", "text": "Hi Joelle!\nThank you for reporting your issue. It seems to me there is something wrong somewhere\u2026 First, on the process radtags tool (-> maybe it can be of interest for us to know if are you using the last version of the tool ? / in which Galaxy instance ?) , if you have paired-end data, you need to select as input, R1 file(s) (or collection) in the first \u201cpaired-end reads infile(s)\u201d \u201c\u201cbox\u201d\u201d and R2 file(s) (or collection) in the second \u201cpaired-end reads infile(s)\u201d \u201c\u201cbox\u201d\u201d. So here, it seems to me you created a collection with both R1 and R2 files, so it\u2019s a good dataset pair ;), but with this tool, you need to use separately R1 and R2. After saying that, normaly this tools is not \u201clooking\u201d at the name or content of the files when you are \u201cjust\u201d selecting it\u2026 so\u2026 normally, if the datasets formats are ok, the tool will display these datasets on the \u201cselect box\u201d\u2026 One exaplanation for ma can be that there is an issue using fastqsanger.gz WHEN in a collection\u2026 so MAYBE, you need to use Fastqsanger or fastqsanger.gz input files (so not in a collection) OR use fastqsanger files if in a collection\u2026 I have to test it!\nSo, please, can you try to \u201cjust\u201d select your fastqsanger.gz from your history (not in collection) ? And please, pay attention to the fact that your files need to be named something like\"name_R1_001.fastq\" and \u201cname_R2_001.fastq\u201d so the execution of the tool can work \nAdditionally, don\u2019t hesitate to share your history so I can have a look."}, {"role": "user", "text": "Hi! Thanks a lot for your return.\nSo I first used Stacks:process_radtags (Version 1) with no problem with the same files. But, for any reason, it won\u2019t detect my files (even if the files are separate, i.e not in a collection) with Stacks2:process_radtags (Version 2). I am working in the EU instance of Galaxy. Here are my two files :\n\nimage274\u00d7595 19.2 KB\n\nThank you! "}, {"role": "system", "text": "OK! It appears there is a pb with the last stacks2 process radtags tool \u2026 And I don\u2019t know why, there is no more process radtags tool named as the last one I used. Can you try to access and use this last version who is working for me on eu but seems to be ~hidden : the last procrad tool who is working properly for me"}, {"role": "user", "text": "Hi again \nYes, this is also the version I used (1.46), it works well! I based my analysis on this one since I wasn\u2019t able to make the new one (version 2.4) work. I just wished to be able to use the new one in the future as it seems a few improvements were done. But if there\u2019s a true bug, I at least know that it\u2019s not my fault if I can\u2019t get it to work!\nI\u2019ll keep checking in the future to see if the bug is resolved and the tool usable for my analysis.\nThank you! "}, {"role": "system", "text": "Hi again \nIn fact, it appears that to work properly with the 2.4 tool version, you need to use the \u201cBuild list of Dataset Pairs\u201d function, not \u201cBuild a Dataset Pair\u201d, even if you only have one pair\u2026 I think this will be ok!"}], [{"role": "user", "text": "Hello,\nI\u2019m using Bowtie2 via Galaxy to align my RNAseq reads against a published transcriptome. If I want more stringent conditions for the alignment, what parameters I should adjust from the default?\nThank you very much"}, {"role": "system", "text": "Bowtie2 already reports the \u201cbest\u201d alignment(s) by default.\nTo end up with a set of \u201cmore stringent alignments\u201d than the default, you can either tune parameters for the mapper, filter the result, or both. What type of data you are mapping (single-end/paired-end) will impact which mapping parameters or result filters can be applied.\nTo tune mapping parameters, see the tool form for a summary of advanced options and what each does. Tool form help is based on the manual, which is also worth reviewing: http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml\nTo filter the output result, retaining reads up a certain MAPQ value (30 indicates a unique alignment) and/or only properly-paired alignments (if using paired-end reads) are commonly applied. Example tool: Bamtools >> Fiilter."}, {"role": "user", "text": "Thank you jennaj for your reply.\nThis is a paired end RNAseq. and I wish to use the aligner to eliminate reads that are originated from a contamination, so they won\u2019t be included in the assembly. So i\u2019m trying to align against the in silico transcriptome of the contaminant organism, and further use only the unaligned reads to the assembly.\nbut when I try to blast in NCBI random results from the \u2018unaligned reads\u2019 output of the Bowtie2 I still get results for the same contaminant. Moreover, if I just test and run again for the second time, the unaligned reads to the same in silico transcriptome of the contaminant, I still get high alignment rates. so something doesn\u2019t make sense.\nI\u2019m not sure which parameter I should change, as there are so many options\u2026"}, {"role": "system", "text": "It sounds like you are not capturing enough hits with Bowtie2. Meaning, you need more permissive alignment results, not stricter.\nBowtie2 and a contaminate reference transcriptome target isn\u2019t the best way to positively identify all contaminate reads. NGS mappers are designed to find and retain high-quality, same organism, matches. Any reads (contaminate or not) with lower quality or insufficient coverage won\u2019t be retained as a positive hit.\nOptions:\n\n\nIf the target organism genome/transcriptome does not exist, then using a more permissive mapper to identify contaminates would be a better choice. Is there a reason why you are not using BLASTN for this step?\n\n\nIf the target organism genome does exist, do the filtering the other way around by mapping against it with a spliced-mapper like HISAT2. Contaminate reads won\u2019t pass mapping criteria (along with any failed reads from the target organism).\n\n"}, {"role": "user", "text": "\nyes, the genomes are available for both the contaminate organism and the organism-of-interest. BUT the genome of the organism-of interest is considered as a draft genome and is not complete, that is why I preferred to do a de novo assembly, rather than aligning to the existing genome.\nI gave both Hisat2 and Bowtie2 a try, and run them against the contaminant in silico transcriptome, got the similar results.\nyou reckon that BLASTN is a better tool for this? with the genome or in silico transcriptome?\nbut how should I set the threshold for a good match? eval<0.0005 / % coverage / % identity ? or change something from the default parameters before running the blastn?\nThanks for all your help!\n"}, {"role": "user", "text": "what would you use than: BLASTN/HISAT2/BOWTIE2 with the reads? and what parameters / thresholds to achieve a more permissive alignments?\nThanks"}, {"role": "system", "text": "BLASTN as wrapped for Galaxy is already set to be very permissive (no minimum percent ID or coverage). E-value (p-value) is not a useful metric in my opinion, it is based on the query/target size and HSP-specific. I would stick with the default for that one: https://www.ncbi.nlm.nih.gov/BLAST/tutorial/Altschul-1.html\nIt may be too permissive, but that depends on how similar the two organisms are, whether it is important for you to remove all of the contaminates at the risk of losing any target genome reads that map to both (IF any do). Raise (strict, perfect complete matches) then lower the percent identity/coverage at increments, keeping in mind that contaminate reads could be degraded in some way, it depends on factors like what they represent and how they were introduced during library construction or during sequencing.\nThe more similar the two organisms, the more likely some cross-over mapping is to occur at some rate, but I wouldn\u2019t expect HISAT2 to capture high MAPQ/Properly paired result that includes contaminant reads.\nSo, you\u2019ll need to test out thresholds, examine the results, and make some decisions. This is not an exact process, especially since the target genome is in an unfinished state.\nExample: You might choose to keep reads that map well to the target genome first (with HISAT2), and then evaluate the remainder of reads as potential contaminants based on the mapping results to the other transcriptome. You might also want to test different thresholds at this point, assemble, the re-evaluate how those longer assembled contigs map to both targets to make your final decision."}, {"role": "user", "text": "Thanks for all your advice!\nI\u2019m thinking maybe the way to go is to create a database of both the contaminant and the organism of interest and plus a close species to the organism of interest that its genome is more complete, and to use that to blastn my reads against. than filter only the desired ones.\nBut someone also recommended me on STAR aligner. The output I\u2019m getting is only a bam file, any idea if there is an option to output also a fastq file with the mapped reads and a fastq with the unmapped reads? I could not find that option in the settings in Galaxy.\nCheers"}, {"role": "system", "text": "\n\n\n kml:\n\nThe output I\u2019m getting is only a bam file, any idea if there is an option to output also a fastq file with the mapped reads and a fastq with the unmapped reads? I could not find that option in the settings in Galaxy.\n\n\nTry these tools to manipulate BAM results. Alternatives can be found with keyword searches at the top of the left tool panel:\n\nFilter BAM\nConvert, Merge, Randomize BAM\nSamToFastq\n"}, {"role": "user", "text": "Thank you jennaj.\nI have assembled the reads using Trinity, after filtering them according to the blastn results. but now that I try to align back those filtered reads to the assembled transcriptome using Bowtie2 (to assess the quality of the assembly), I get an error message: \" Error, fewer reads in file specified with -2 than in file specified with -1\". I noticed that the number of reads in each forward doesn\u2019t match the number of its reverse.\nWhat shall I do??"}, {"role": "system", "text": "The paired reads must be a match for the R1(forward) and R2(reverse) input to use Bowtie2 or Trinity.\nDuring your processing, if one read from a pair was filtered out with BLASTN (determined to be contamination) you will also need to filter out the other read from the pair before continuing.\nOther tools (besides Trimmomatic) that can create paired fastq output.\n\n\nFastQ Interlacer followed by FastQ Deinterlacer\n\n"}], [{"role": "user", "text": "Hi\nI\u2019m trying to run the Blast2GO tool in https://proteomics.usegalaxy.eu but keeps on giving me this error:\nAn error occurred with this dataset:\nformat tabular database ?\nBlast2GO JAR file not found: /opt/b2g4pipe_v2.5/blast2go.jar\nPlease advise."}, {"role": "system", "text": "Welcome, @Lizex\nThis looks like a technical problem, and might be related to the server status.\n\nBlast2GO JAR file not found: /opt/b2g4pipe_v2.5/blast2go.jar\n\nPlease follow this topic or the EU chat for updates: Jobs not running usegalaxy.eu\nOnce that is cleared up, try a rerun first. If that also fails, to help you more the moderators will need input/parameter setting to assist with troubleshooting. See Troubleshooting errors"}, {"role": "system", "text": "Hi @Lizex\nPlease try the tool again now."}, {"role": "user", "text": "Hi Jenna\nThe error still persists:\nAn error occurred with this dataset:\nformat tabular database ?\nBlast2GO JAR file not found: /opt/b2g4pipe_v2.5/blast2go.jar"}, {"role": "system", "text": "\n\n\n jennaj:\n\nOnce that is cleared up, try a rerun first. If that also fails, to help you more the moderators will need input/parameter setting to assist with troubleshooting. See Troubleshooting errors\n\n\nHi @Lizex The EU moderators will be back Monday their time. This error could be technical or some problem with the data/parameters.\nPlease post back these details: screenshots or a shared history link. You can also send in a bug report directly from the red error dataset at the same time \u2013 please include a link to this topic in the free text comments so they can link the two."}, {"role": "user", "text": "Thanks Jenna\nI tried again running the Blast2GO tool this morning but got the same error.\nLizex"}, {"role": "user", "text": "Hi\nI want to use the Blast2GO tool. I did a Blast P search for my sequences and downloaded the .xml file that is needed. I uploaded the file into Galaxy and hit the run button. I still get the same error since I reported the issue.\nAn error occurred with this dataset:\nformat tabular database ?\nBlast2GO JAR file not found: /opt/b2g4pipe_v2.5/blast2go.jar\nI\u2019ve been reporting this problem since last week and still no solution?"}, {"role": "system", "text": "\n\n\n Lizex:\n\nI\u2019ve been reporting this problem since last week and still no solution?\n\n\nWe can\u2019t help more without more details. The FAQ I posted explains how to do all of these.\n\n\nSharing your history is one option. This is the fastest option for your case.\n\n\nPosting back the full contents of the Job information page is another, specifically the Tool Parameters and Job Information sections with expanded stderr/stdout, plus screenshots of the expanded input datasets (to reveal the datatype, database, and \u201cpeek\u201d view of a few lines).\n\n\nSubmitting a bug report is another. Not all get a direct reply: instead server admins review these for potential server-side/tool bug issues (then apply appropriate fixes) then if there is time get back about potential usage help (content, format, metadata, parameters).\n\n\nWhy is this needed? The job details matter to isolate what exactly is going wrong. Errors like the one you have could be due to input/parameter problems (likely would result even if running the tool outside of Galaxy) OR an actual technical bug (tool wrapper, server, dependencies). Most issues are the first but some can be the latter. You can try to troubleshoot input/parameters issues yourself then ask for advice. Server/tool issues need to be fixed by the server admins or tool developers."}, {"role": "user", "text": "Hi Jenna\nThanks for your reply. Does the Blast2GO tool work on your end if you would run it on any random set of sequences. I don\u2019t do any thing different then uploading the .xml from NCBI Blastp and run the tool as default.\nI already reported (bug report) the issue.\nSorry if I can\u2019t give more information since I cannot post a link in this post.\nRegards"}, {"role": "system", "text": "Update:\nPing @wm75 @gallardoalba \u2013 the dependency is actually missing. Not sure if licensing is a factor, or if that recently became a factor. Last wrapper update was Oct 2022, and the development repo for the Galaxy wrapper is not linked from the ToolShed so I can\u2019t dig into this more.\nThanks!\n\nHi @Lizex\nThis tool is a little bit special: no tool test data, older indexes (and only one, from 2013), and the wrapper was contributed by the developer community. It isn\u2019t even hosted at all UseGalaxy.* servers \u2026 just this EU server.\nThe EU team is running an administrative training event this week, and those are the same people who scan bug reports and address that server\u2019s technical issues.\nI\u2019ll start up a direct message that you should be able to post a share link into. What I can help with is adjusting any content/format/datatype issues with the input and confirming a missing index is the root problem. If for some reason the local index is actually not available, or doesn\u2019t support your specific data, that will need to wait for the EU admins."}, {"role": "user", "text": "Hi Jenna\nJust a follow up on this issue. Could the EU team resolve the issue on the Blast2GO problem?"}, {"role": "system", "text": "Hi @Lizex\nI don\u2019t have any updates.\nAnother way to ask the EU admins questions is through their chat (found at the bottom of their homepage). You can include a link to this question for extra context but also send in a bug report, then tell them when it was sent in and the tool name + error message again to make it easy to get oriented in a chat format."}, {"role": "user", "text": "Hi Jenna\nThanks for the advice. Will do that."}, {"role": "user", "text": "Hi Jenna\nThe EU admin/team never responded to my chat. What more can I do?"}, {"role": "system", "text": "\n\n\n Lizex:\n\nWhat more can I do?\n\n\nConsider using alternative methods?\n\nReview tool choices in the Annotation section of the tool panel.\nOr, do some direct data manipulations with tabular formatted data for the query and target. This is how everyone originally did Bioinformatics   When you come up with a custom process you like, consider creating a reusable workflow. You can \u201cfavorite\u201d workflows and they will show up in the tool panel just like any other tool. Under the hood this is what all tools really are: groupings of manipulations.\n\nReferences\n\nData Manipulation Olympics\n\nSearch Tutorials (query=workflow)\n"}], [{"role": "user", "text": "Hi there, I have seen several posts pertaining to annotation of the DESEQ2 output GeneID with gene name (ENSRNOG00000046834 etc) and for mice and humans it works easily with the AnnotateMyIDs tool.  I work with rat tissue.  I have installed this AnnotateMyIDs on our server, but it does not support rats. I have tried to fiddle with the code to get it to accept rats too, but have failed so far and am worried about stuffing the server!\nIs there a way to add rat support to this, as admin?  IF not is it possible to make a feature request somewhere for this? \u2026It was developed by IUC. I don\u2019t know who that is. I ran  Uniprot mapping and this gives a list of the converted gene names, but not in the same file (unless I did it wrong). Is there an easier way? Thanks.\nI have written a program to do this in Python outside of Galaxy. Be nice to all be inside though!"}, {"role": "user", "text": "Found a way to do this around the houses; uniprot ID mapping (creating a standalone dataset) followed by \u201cfind in reference\u201d tool, followed by some text clean-up.  Would  still be brilliant if annotateMYIDs would do rats though."}, {"role": "system", "text": "I think this is possible, it just means that we need to store and download another GB of annotation. I will look at this asap, promised."}, {"role": "user", "text": "\u2026thanks\u2026\nBW\nR."}, {"role": "system", "text": "@RichardBJ can you try it for me please?"}, {"role": "user", "text": "@bjoern.gruening  \u2026sorry, try what/which?  Did you alter annotateMYids?  The Galaxy ToolSched AnnotateMYids does seem to have updated (for my installation) and when I tried annotatemyids on usegalaxy.org it still just has humanoids, mice, flies and zebras\u2026  I am assuming I have missed something\u2026 sorry\u2026"}, {"role": "system", "text": "Try it at the Galaxy EU https://usegalaxy.eu server.\nIf it works there, then the other usegalaxy.* servers will be updated as well."}, {"role": "user", "text": "Ah! EU Galaxy \nYes, so I created an account there, uploaded a rat DESEQ2 and it worked beautifully.\nIf you do update on other Galaxies, any chance you could pop up simple instructions for setting that up through our local installations too?\nThank you guys so much!!!"}, {"role": "user", "text": "@bjoern.gruening thanks, so yes that did work. Is it possible to install this tool on our own installations?"}, {"role": "system", "text": "@RichardBJ, @bjoern.gruening will see your reply and give instructions.\nThe tool itself has not been updated yet to include Rat for use in a local Galaxy. He was probably waiting for your feedback first, then is finding time to update the tool itself so everyone can update (public + local).\nRepository: https://toolshed.g2.bx.psu.edu/view/iuc/annotatemyids/ecc913a7334b"}, {"role": "system", "text": "Update: I also ping\u2019d the IUC (tool authors) to see if they can offer more help/advice soon-ish. Feel free to join in on the conversation: https://gitter.im/galaxy-iuc/iuc?at=5d0be4c91e35ef14b687c982"}, {"role": "system", "text": "I\u2019ve added rat to annotatemyids and submitted a PR to the IUC repo here, let\u2019s see if it\u2019s accepted https://github.com/galaxyproject/tools-iuc/pull/2462\nBy the way there is also this deg_annotate tool that can annotate deseq2 output with a GTF so maybe that\u2019s another option? https://toolshed.g2.bx.psu.edu/repository?repository_id=e33c4ee81be2b1b4"}], [{"role": "user", "text": "Need an AI ML based variant calling tools for Illumina and Oxford data, please. Is there any Galaxy instance I could not find?"}, {"role": "system", "text": "Hi,\nYou can find all available tools and their description in the Gaxy Tool Shed."}, {"role": "system", "text": "Hi @player_777,\nI think that there\u2019s not currently any tool with those characteristics. Have you in mind any of them? We can create the tool wrapper if required.\nRegards"}, {"role": "user", "text": "DeepVariant,\nthanks"}, {"role": "user", "text": "DeepSV\nDAVI\nClarivoyant\nVariant Works\nCoVacs are also good tools to see on Galaxy"}, {"role": "system", "text": "Thanks! I\u2019ll start working in DeepVariant and will include the rest in the tool request list. I will try to make it available for next week.\nRegards"}, {"role": "user", "text": "Where I can access these tools now? Thanks"}, {"role": "system", "text": "Hi!\nI\u2019m still working on it. I\u2019ll try to make it available for the next week.\nRegards"}, {"role": "user", "text": "Is there any progress and results available? Thanks."}, {"role": "system", "text": "HI @player_777,\nI\u2019ll try to make it available this week.\nRegards"}, {"role": "system", "text": "Hi @player_777,\nI have opened a PR for including the DeepVariant tool DeepVariant PR.\nRegards"}], [{"role": "user", "text": "Hello,\nI am trying to assemble a plant genome using ONT data with Flye. Prior to launch the assembly with all my data (about 37 Gb in fastqsanger.gz) I recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. I then launched with all the available sequences (37 Gb in fastqsanger.gz) and it\u2019s now been running for 5 days. I am just wondering if I am using more ressources than allowed for this tool or if it\u2019s still running. What makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ?\nThank you very much.\nBen"}, {"role": "system", "text": "Hi @BBOACHON,\nDid your analysis finish successfully? Sorry for the late response.\nRegards"}, {"role": "user", "text": "Hi, no I just cancelled it yesterday after 2 weeks running. I thought that the job was requesting too much memory and time. I could finally run the job in 24 hours on our lab server. I have tried to run an annotation with Funannotate with it is also talking probably way too long and some plant databases are not available (VIRIDIPLANTAE).\nKind regards."}, {"role": "system", "text": "How many cores are you using? Probably it will be necessary to fine tune it in the Galaxy server.\nRegards"}, {"role": "user", "text": "Hello, To run Flye on our server I have been using from 10 to 24 threads and it was using about 300 Go of RAM. On the Galaxy server I don\u2019t think we can tune these parameters right ?\nFor the Funannotate Job, it is still running on the Galaxy server since December 26, so I guess it is bugged for using more RAM than attibuted. Although I could not use the closest database to my organism (plant) although several plant databases are available. I got this error message \u201c[Dec 25 01:14 PM]: ERROR: solanales_odb10 busco database is not found, install with funannotate setup -b solanales_odb10\u201d and finally launched it using EUKARYOTA."}, {"role": "system", "text": "Hi, I am having a similar problem/question and although the topic appeared as solved, I am not sure if you found a solution to deal with these \u201cbig\u201d assemblies with Galaxy-Flye. I am trying to assemble about 20 Gbp of an ONT soil metagenome and the process is still running since December 31 (10 days ago). I am not sure if I should continue waiting or there was an error with this process and should cancel it to don\u2019t waste resources.\nThank you in advance"}, {"role": "user", "text": "Hello, Unfortunately my problem was not solved with Flye\u2026 The job was still running after a while and I decided to stop it. Same for Funannotate, there were missing databases and job was taking forever. I have issued this several times, jobs are not seeing as failed but still running. When using another instance (Use Galaxy France) I had the same problems but the jobs would fail after a while with this error message \u201cThis job was terminated because it used more memory than it was allocated\u201d. I tried to report the problem, they allocated more memory but it failed again. You may try to ask for more RAM allocation for the tool but in my experience I considered that the project was probably oversized for the GALAXY serveurs, and I finally did it in command lines using our lab serveur. At least GALAXY was good for teaching the bases of assembly pipelines !!"}, {"role": "system", "text": "Hi @Andres_Esteban_Marco @BBOACHON,\nwe are working in order to dynamically optimize the resources assigned to Flye according to the inputs.\nIs your process still running @Andres_Esteban_Marco?\nRegards"}, {"role": "user", "text": "Thank you for the answer. This is great news.\nIs there a way to see if the jobs are in pause or failed even if still running since weeks on Galaxy europe? In other instance when the job takes too long it fails.\nConcerning the same issue with Funannote, are you going to try ressource optimization too ? Or should I open another topic related to it. It\u2019s a very convenient tool.\nRegards"}, {"role": "system", "text": "Hi! Regarding the first question, you can ask in the Gitter channel about the state of your process. And yes, if you consider Funnotate requires it, I\u2019ll work on it. It would be very useful if you could provide us with some details about the resources that Funnotate requires depending on the input file (something similar to this).\nRegards"}, {"role": "user", "text": "Hello, Thank you for your answer. Unfortunately I could not find  examples of performance regarding the use of Funannotate. I am not sure this help, but in the manual (\u2013memory : RAM to use for Jellyfish. Default: 50G, CPU 2 by Default : I found genome size is about \u223c275.42 Mb).\nI am trying to install Funannotate on our server, but it is quite difficult. If I suceed I will give it a try with my data and provide you the perfomance.\nRegards"}, {"role": "user", "text": "Hi, you probably figured by yourself that I was not giving pertinent info, since jellyfish mentionned in the manual of funannotate is a tool, not the organism\u2026 sorry for that\u2026 Still learning "}, {"role": "system", "text": "Hello, thanks for your answer. I just checked (today) and the job is still running.\nBest regards"}, {"role": "system", "text": "We updated the funannotate databases and provided more cores to this tool.\nYou can see how many cores and memory every tools gets here: infrastructure-playbook/tool_destinations.yaml at master \u00b7 usegalaxy-eu/infrastructure-playbook \u00b7 GitHub"}], [{"role": "user", "text": "Hello,\nI am developing galaxy tools, and constructed a workflow to chain them together. The problem I am encountering comes from one of the tools executing before a previous tool finishes to run.\nAttached are two images. One with the workflow where circled in red (RP Selenzyme) is the tool that executes too early (before RP Thermodynamics finishes). The second image is the side panel showing that the tool RP Selenzyme returns an error due to an empty input since it executes earlier than it should.\n\nScreenshot from 2020-02-24 12-54-391908\u00d7907 133 KB\n \nScreenshot from 2020-02-24 12-47-371339\u00d7538 112 KB\n\nAny help would be very welcomed. Do not hesitate to ask me for any logs. I tried this on the Galaxy 19.05 and 20.01 releases and the same error occurs.\nThank you,"}, {"role": "system", "text": "Without the tools and workflow we can\u2019t help you there, but keep in mind that parallel execution may be misleading here, the first job whose inputs are ready to run will run. In your history I see multiple parallel executions, so it is not clear to me that the input wasn\u2019t already finished and failed for another reason."}, {"role": "user", "text": "Thanks a lot for your response.\nMisinterpretation of parallel runs is exactly my problem!\nThe tool circled in red is run in parallel to the other first tools to run when it should not. The \u201cRP Selenzyme\u201d tool requires two inputs, \u201cRP Thermodynamics\u201d and the underlined input.  However, you can see from the first image that \u201cRP Thermodynamics\u201d is in grey, indicating that it has not finished and yet \u201cRP Selenzyme\u201d executes.\nIf there a way for me to manually specify when to execute a tool in a workflow you think?"}, {"role": "system", "text": "No, I think the input dataset to the failed job is hidden and completed correctly. You can see the hidden datasets by clicking on \u201c59 hidden\u201d. You can also see more information if you click on the failed dataset and then the i symbol. The input dataset numbers will be listed there, and they will probably not correspond to any grey dataset."}, {"role": "user", "text": "Thanks for suggesting the debug.\nPlease find attached the screenshot from the information tag of the failed tool. Unfortunately, the dataset is not hidden, corresponds to the right tool and as I described before fires before the previous dataset is generated (run number 352 that us still grayed out).\n\nScreenshot from 2020-02-24 16-36-501614\u00d7874 128 KB\n"}, {"role": "system", "text": "Can you share tools and workflows somewhere ?"}, {"role": "user", "text": "As of now I cannot share the tools nor the workflow. I\u2019ll ask my collaborators if we can share our Galaxy test server temporarily with you.\nThanks a lot for helping, I\u2019ll get back to you."}, {"role": "system", "text": "No, that will not help, I\u2019ll try and see if I can reproduce this."}, {"role": "user", "text": "Hello,\nThe problem persists. You are welcomed to go on the projects galaxy and create an account to inspect the problem:\nhttps://synbiocad.micalis.inra.fr/test/\nHere are two workflows, one that work fine (rpranker-2) and another that contains the bug that I mentioned (rprankerdebug):\nhttps://synbiocad.micalis.inra.fr/test/u/mdulac/w/rpranker-2\nhttps://synbiocad.micalis.inra.fr/test/u/mdulac/w/rprankerdebug\nTo run the workflow import the E.Coli model in the \u201cshared data\u201d space and fill the required input with:\n\nGEM SBML: E.Coli\nTarget InChI: InChI=1S/C6H6O4/c7-5(8)3-1-2-4-6(9)10/h1-4H,(H,7,8)(H,9,10)/p-2/b3-1-,4-2-\nMaximal Pathway Length: 3\n\nEssentially the rpSelenzyme tool is the culprit and the only difference between the two workflows is an additional input to that tool. Please let me know if you want me to post logs or anything.\nCheers"}, {"role": "system", "text": "I see, the text input is short-circuiting the inputs ready check."}, {"role": "system", "text": "So connecting a text parameter to a select is a new feature in 20.01, this won\u2019t work on 19.09. Then there is the issue that I think there are 2 parameters called input in RpSelenzyme. Can you try changing the one Taxonomy JSON input to a different name and rebuild and re-run the workflow ?"}], [{"role": "user", "text": "I am trying to do the minimap2 with my own rat genome assembly against the reference rat genome. It went through well when I mapped against rn6 which is on the reference list. But when I tried to map agaisnt rn7 which is not on the list and had to upload as a seperate dataset, it failed constantly whatever parameters I have changed. The error message showed out of memory. Is there any solution for this circumstance? Or may I ask if the rat rn7 could be added on the reference list on the server?"}, {"role": "system", "text": "Hi @daikez\nLarger genomes tend to exceed processing resources when used as a custom genome. Why? The genome has to be loaded then indexed first. After that is done the tool is run against it and the index is discarded after each run.\nWhen the query is another entire genome, potentially fragmented, that also adds to the memory and time the job requires.\nThe rn7 genome is new (officially released May 4, 2021 by UCSC UCSC Genome Browser: News Archives) and isn\u2019t indexed for tools at usegalaxy.* servers yet.\nUseGalaxy.org is undergoing testing for the upcoming release, so adding a new genome would be problematic right now.\nThat said, UseGalaxy.eu might be able to index the genome sooner, it has started to become requested more often. ping @bjoern.gruening @gallardoalba\nOur plans are to unify the reference data across usegalaxy.* servers but that project is still a work-in-progress. So, if the EU server can add it, you can work there.\nWhat you might be able to do meanwhile:\n\nMake sure that your query doesn\u2019t include very short sequences that represent a single read. Meaning, you might need to clean up/filter the assembly first.\nLoad up the genome (use the UCSC version Index of /goldenPath/rn7/bigZips \u2013 chose the rn7.fa.gz file) and try running at UseGalaxy.eu instead. They have a few clusters that scale for even large jobs. Just be aware that the job may queue, run, then auto-rerun if the first attempt fails for resources.\nDownload the same rn7 fasta, uncompress, reduce the file so that only the primary chromosomes are remaining (remove all the haplotype/alt/unmapped sequences). Then load that up to either/both servers and try to map against that. This reduces the size of the genome \u2013 and primary chromosome hits are usually what are most important anyway.\nOr, pick the version of the genome from NCBI that only includes primary chromosomes: https://www.ncbi.nlm.nih.gov/genome/73\n\nYou could also wait for the genome to be indexed \u2013 the EU team will respond with their plans, likely including projected timing. It may take a day or so (time zone differences/summer vacation schedules/focusing on the Galaxy release testing, etc).\nConsider setting up and using your own Galaxy server. You\u2019ll be able to add/index any genome your want natively when you are the administrator. This is good for large work in general \u2013 the free public servers have significant resources, but not as much as you can set up yourself (for practical reasons). Galaxy scales for very large work when you run your own and allocate appropriate resources.\n\nWebinar: Use Galaxy on the web, the cloud, and your laptop too >> Webinar: Use Galaxy on the web, the cloud, and your laptop too\n\nIndexing reference genomes with Data Managers: Resources, tutorials, troubleshooting\nWays to use Galaxy: https://galaxyproject.org >> Use. Scroll down a bit to review the platform matrix to help with decisions.\nThe GVL version of Cloudman is one choice and AWS offers grants. Use a high memory base server, or you might have to start over. Cluster nodes are based on the same configuration choices as the server that hosts Galaxy. AWS Programs for Research and Education\n\n\n\n\nI also added a few tags to your post that may help with finding prior Q&A that is useful.\nThanks!"}, {"role": "user", "text": "Hi, @jennaj\nThank you very much for your detailed suggestions.\nI will check them out closely. "}, {"role": "user", "text": "\n\n\n jennaj:\n\n@bjoern.gruening @gallardoalba\n\n\nMay I ask about the possible date of indexed rat rn7.2 to be available on Galaxy? Thanks. "}, {"role": "system", "text": "Hi @daikez,\nit has been requested; I\u2019ll inform you as soon as I have some information about it.\nRegards"}, {"role": "system", "text": "Hi @daikez,\nthe rn72 indexed genome will be available on useGalaxy.eu in a few hours.\nRegards."}], [{"role": "user", "text": "Hi all,\n\nScreen Shot 2022-05-05 at 2.42.17 PM1438\u00d7860 87.5 KB\n\n\nScreen Shot 2022-05-05 at 2.42.38 PM1550\u00d71448 212 KB\n\nI am trying to align my RNA-seq reads (Bacteria), using HISAT2 to a reference genome (assembly.fna). Then,to measure the expression in the resulted bam.file, I used (featurecounts) tool, using the (gtf) annotation file. However, although both fna/gtf files belong to the same strain tested, I got this empty report;\nYour help much appreciated"}, {"role": "system", "text": "Hi @Seraph1,\nplease check if the reference genome and the gene annotation file use the same sequence name.\nKind regards,\nIgor"}, {"role": "user", "text": "\n\n\n igor:\n\nfile use the same sequenc\n\n\nHi Igor,\nThank you for your quick response, both files belong to the same reference genome.\nCould you please explain what does reference name mean;\nDo you mean both files should have same name like ( genome.fna and genome.gtf) ?"}, {"role": "system", "text": "No, not the file names, the sequence/contig names.\nClick on Preview (eye) icon of the reference genome (FASTA) file. Check the sequence name in the first line after \u2018>\u2019. Most tools consider a text string before the first space character as a name, and the rest is a comment.\nClick on Preview icon of the gene annotation file (GTF). Check the name in the first column (seqname). Sometimes the annotation file has a header. Header lines start with #. Ignore the header lines.\nBoth files should have identical sequence name.\nThis is assuming the assembly contain a single sequence. If you click on name of FASTA file, Galaxy shows number of sequences present in the file.  Some assemblies may have more than one. If it is the case, filter the genome assembly (FASTA) file using something like ^> (select lines staring with \u2018>\u2019) and check that the annotation file uses the same sequence names.\nKind regards,\nIgor"}, {"role": "system", "text": "Alternatively, you can compare sequence names in the BAM file (SQ line(s)) and the gene annotation file. I assumed you use reference genome from history, hence mentioned FASTA file."}, {"role": "user", "text": "I checked the three files (reference genome from history .fna), bam, and GTF.\n\nfna.file has several sequence/names such as (> NZ_JAFJXZ010000001.1, >NZ_JAFJXZ010000010.1 \u2026etc)\n\n*gtf.file and bam.files has also same several sequence/names (please check the attached screen shots.\nDo you think such multiple sequences may affect the alignment, if yes how can we solve it?\nThank you\n\nScreen Shot 2022-05-06 at 12.10.01 AM674\u00d71226 150 KB\n\n\nScreen Shot 2022-05-06 at 12.09.33 AM972\u00d732 6.31 KB\n\n\nScreen Shot 2022-05-06 at 12.08.57 AM318\u00d7714 18 KB\n"}, {"role": "system", "text": "So, it is not the reference data.\nfeatureCounts summary indicates majority of reads are mapped outside of the annotated genes. Sometimes RNA-Seq libraries are extremely enriched with rRNA sequences. Run Samtools idxstats on BAM. Do you see reads mapped to all contigs? Do you see a contig with unusually high number of aligned reads, something like >90% of reads?"}, {"role": "user", "text": "Hi igor,\nBoth gtf and fan files were downloaded form NCBI. I run samtools stats and idxstats (resultes in attached screen shots).\nFrom the the samtools stats report you can see ( read mapped and paired = 18495100, unmapped =339882).\n\nScreen Shot 2022-05-06 at 8.28.49 AM1806\u00d71612 433 KB\n\n\nScreen Shot 2022-05-06 at 8.27.39 AM800\u00d71488 174 KB\n\n\nScreen Shot 2022-05-06 at 8.34.55 AM830\u00d71024 105 KB\n"}, {"role": "system", "text": "Hi @Seraph1,\nthe reads are mapped to many contigs. This is a good sign.\nIf you share the history I can have a look. As you got no counts for genes, mismatch between the genome and the gene annotation is the prime suspect.\nHistory sharing: History menu (cog wheel icon at the top right corner of the History panel) >  Share or Publish > Create a link and paste in in reply.\nIf you have many samples, you can copy files for one sample (reference genome, gene annotation, BAM and featureCounts outputs) and share only one sample.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nThanks for the follow up,\nI repeated the steps, and this time, I did not use any trimming tools.\nPlease use this link to access the workflow;\nurl: https://usegalaxy.org/u/seraph2/h/rna-seq\nSeraph"}, {"role": "system", "text": "Hi @Seraph1,\noutput of featureCounts #14 has counts for 157 genes, so it does count reads against some genes. The small number of genes is an unintended consequence of the gene annotation. Tools such as featureCounts and htseq-count count reads against a feature in 3d column of GTF file and aggregate the results using an attribute from the last column. For example, feature in the 3d column can be \u2018exon\u2019, and attribute is \u2018gene_id\u2019. The tool counts reads against axons and returns count for a value (gene ID) in \u2018gene_id\u2019. Your GTF file does not have \u2018exon\u2019 feature for some genes. Filter the GTF file on column 3 using Filter data on any column using simple expressions with c3==\u2018exon\u2019. You\u2019ll see 157 records. As bacterial genes are not spliced, the featureCounts estimated read counts for genes with available \u2018exon\u2019 feature. You can select a feature used by featurCounts for counting in Advanced options. If you  set it to CDS, featureCounts returns 5092 lines. With CDS you\u2019ll miss some ncRNAs. Try different features, examine the annotation, and select a feature most appropriate for purpose of your analysis.\nYou can unshare the history if you don\u2019t have any other questions: History menu > Share or Publish > Unshare it.\nKind regards,\nIgor"}], [{"role": "user", "text": "Starting server in PID 30796.\nserving on http://127.0.0.1:8080\ngalaxy.queue_worker INFO 2022-01-06 13:49:48,449 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Instance \u2018main.web.1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,449 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel default.\ngalaxy.model.database_heartbeat DEBUG 2022-01-06 13:49:48,560 [pN:main.web.1,p:30796,w:1,m:0,tN:database_heartbeart_main.web.1.thread] main.web.1 is config watcher\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,992 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel default finished (542.171 ms)\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,992 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel ontology:edam_operations.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,175 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel ontology:edam_operations finished (182.193 ms)\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,175 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel ontology:edam_topics.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,538 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel ontology:edam_topics finished (362.471 ms)"}, {"role": "user", "text": "\nimage941\u00d7496 8.83 KB\n"}, {"role": "system", "text": "Hi @user1\nThis type of connection error can come up for many reasons, including firewall settings on your computer and/or browser, but before investigating more you might just need to stop the server and restart it. Example how-to:\n\n\n\nlocalhost connection issue\n\n\n\nStop galaxy\nrun the command sudo lsof -i tcp:8080\n\nKill the PID\u2019s of galaxy sudo kill 23001 (The PID of yourself, 23001 is an example)\nstart galaxy again\ntest if it solved your problem\n\n\n\nGalaxy resources\n\nQuick start: Get Galaxy - Galaxy Community Hub\n\nTutorials: Galaxy Training!\n\nDocs: Galaxy Documentation \u2014 Galaxy Project 21.09.1.dev0 documentation\n\n\nIf you need more help, please explain a bit more about your server. Include anything relevant: Version of Galaxy? Where sourced? Default configuration or ? Is this the first time starting it up?\nLet\u2019s start there, thanks!"}, {"role": "user", "text": "Sorry. I still can\u2019t open it.\nThank you for your reply"}, {"role": "system", "text": "Is galaxy running on the same computer as where you are trying to access it? Or are you running galaxy on a (remote) server and you are trying to access it with another laptop/computer?"}], [{"role": "user", "text": "Hi,\nI have two assembled fasta files, one for control and second for disease condition. I need to fine differentially expressed genes in both of them. Plz guide me how to perform this type of analysis.\nRegards"}, {"role": "system", "text": "Hi @humaira,\nit depends on the goal of your analysis and the type of data you have.  When you talk about assembled fasta files, you mean assembled transcriptomes generated by RNA-seq?\nRegards"}, {"role": "system", "text": "You have two Fasta or FASTQ files ? FASTA are for references. I assume you have 2 FASTQ files between two conditions. If this is true, you should align it with HISAT2 then Featurecount for quantification and Use deseq2 or edgeR for DEG. Other tools are useful too.\nBest"}, {"role": "user", "text": "I have two fasta files. one for normal and second for diseased condition."}, {"role": "system", "text": "how many repeats do you have for each condition ?\nBest"}, {"role": "user", "text": "AOA.\n133,900 reads for normal and 132,900 for diseased.\nRegards."}, {"role": "system", "text": "you are telling the number of your reads. i meant the number of your datasets\nYou can use your fastas as reference genome not your main samples as i think i told you before. i assume you build your reference genome.\nWhats your analysis about? is it in homo sapiens fields or else ?!"}, {"role": "user", "text": "I am working on transcriptome analysis of plant and no reference genome exists for this plant."}, {"role": "user", "text": "yes assembled transcriptomes generated by RNA-seq."}, {"role": "system", "text": "Hi @humaira,\nyou can use Salmon Quant for performing the differential expression analysis. I recommend you to have a  look at this tutorial: Quantification of gene expression: Salmon.\nRegards"}], [{"role": "user", "text": "I\u2019m a relatively savvy Galaxy user, however just today, we are experiencing issues in downloading any file from Galaxy Main larger than 1Gb. Irrespective of file size, the resulting downloaded archive is 1.08Gb, and fails to expand on opening.\nI am downloading using a good internet connection (University-based) and am doing nothing different than usual!\nPlease help!\nBest wishes,\nD"}, {"role": "user", "text": "So, I\u2019ve checked locally and there is no traffic/network shaping in place. I\u2019ve also repeated the below on two different computers;\n(1) Tried to download collection >1Gb using API key as previous using both curl and wget - all fail at 1.08Gb. Attempting to download a single file from within a collection, without API fails due to authentification.\n(2) Tried to download single dataset using direct wget/curl without API - wget fails, but curl works\n(3) Tried three different web browsers to download either a collection or a single file >1Gb - all fail at 1.08Gb\nUsing MacOS Mojave"}, {"role": "system", "text": "Hi @KEELE\nSorry that you are having problems and thanks for doing all of the troubleshooting.\nWhich Galaxy server are you working at? You state Galaxy Main but we\u2019d like to confirm that with the URL.\nAlso, to help expedite the troubleshooting, share back some of the command strings that you used and note which failed/worked. It is Ok to xxx mask out the actual dataset http address and API keys, but leave all other content, including punctuation, intact. Use \u201cPre-formatted\u201d text so that any spaces, etc are preserved.\nWe can follow up from there, either here publically or in a direct message based on your reply."}, {"role": "user", "text": "Many thanks for your response  I have done some further investigation;\n(1) I\u2019m using usegalaxy.org\nCommand to download a collection [FAILS];\nwget -O MyTestArchive.tgz 'https://usegalaxy.org/api/dataset_collections/bcaddaXXXe5aafbb/download\u2019?key=90a1876062110873XXXXX38a426cca8a\nNB: I\u2019ve redacted the data ID and my api key using XXX - I can confirm the correct details were provided in the command.\nRunning this command twice, results in two different file sizes as below, none of which are anywhere near the expected size - I have 20 files in the collection, many of which are several Gb each;\nMyTestArchive.tgz       [    <=>             ] 175.44M  6.60MB/s    in 43s\nMyTestArchive.tgz       [               <=>  ] 208.08M  3.18MB/s    in 48s\nCommand to download a single file from within a collection [WORKS];\nwget -O SingleFileWithinCollection.tgz \u2018https://usegalaxy.org/datasets/bbdXXXXXcb8906b5fc7e7eced1d68b4e/display?to_ext=data&hdca_id=bcadda227e5aafbb&element_identifier=ID1-DZ_A_TTACCGAC-CGTATTCG_L008.fastq.gz\u2019?key=90a1876062110873XXXXX38a426cca8a\nConclusions; individual files from within a collection appear to be accessible and download correctly using wget or curl. However, when attempting to download the entire collection (by capturing the collection url from the disk icon), the download stops at an apparently random point. Yesterday I found this to be ~ 1.08Gb, whereas today, this appears to be ~ 200Mb.\nYour help is most appreciated."}, {"role": "user", "text": "Deleted for clarity; only partial first response published.\nFor everyone\u2019s benefit, adding a hard line to an email cuts off the response! "}, {"role": "user", "text": "Deleted for clarity; only partial first response published."}, {"role": "user", "text": "Can anyone help with the download of collections please?"}, {"role": "system", "text": "@KEELE\nDaniel, we can help in the morning here. Apologies for the delay. This should be working, but we can sort out why it isn\u2019t.\nIf you want to send me a direct message with a share link to the history with the collection you are trying to download (be sure to share the \u201cobjects\u201d) that will help to jump-start the troubleshooting. Or, if your registered email here is the same as at https://usegalaxy.org, just message the history name and dataset number (direct message is fine for that as well). I\u2019m an administrator at both places but need to know where to look. I\u2019m pretty sure that I know your account email from prior help but that is still good to confirm and can be done privately.\nReminder: Never share your account password with anyone. An admin wouldn\u2019t need it."}, {"role": "user", "text": "Dear JennaJ,\nMany thanks indeed.\nI have done so.\nBest wishes,\nD"}, {"role": "system", "text": "@KEELE Yes, the data is large. The connection dropped for me too.\nI remembered that this has come up before. See this prior Q&A on how to \u201cresume\u201d downloads with curl and wget and see if one of those works for you.\n\n  \n    \n    \n    Downloading large files usegalaxy.org support\n  \n  \n    Yes, this could be a network issue. A slower connection can drop. Your internet provider could have download limits. Other connection issues are possible but those are two most common reasons we have had reported when this has come up before. \nTry adding this argument to your wget string: -c \nFor more explanations, Google \u201cresume curl\u201d or \u201cresume wget\u201d for much discussion. Plus, the man page for each tool will have official argument descriptions. Both can resume but in slightly different ways th\u2026\n  \n\n\nYour other option is to create a History archive and download that. Once uncompressed, you\u2019ll find your data inside of it. Tip: Copy just the data you need into a new history to make it smaller/faster to process this way. Copies of dataset you already have in your account do not consume any additional account quota space. I\u2019m guessing that you don\u2019t need everything in the original history, just the results, but either way should work. It just takes longer to create then download or import-to-another-Galaxy-server a really large history archive.\n\nThe option to create and download/generate a link to a History archive is under the History menu (gear icon).\nThe option to import a History archive (already downloaded archive file or URL from a publically accessible Galaxy server) is at the top of the Saved History page, if you have a local Galaxy and want to store data in context.\n\nBe sure to create a share link to the history (and objects) \u2013 just a link is fine, you don\u2019t need to publish it) \u2013 before creating the archive if you decide to go this route. Sharing after (while the archive is being created) doesn\u2019t work as well."}, {"role": "system", "text": "Hi, I am retrieving this older conversation because I am suffering from exactly the same issues when trying to download a collection from the usegalaxy.eu server, it fails exactly at 1.08Gb. I have tried\nwget -O pool5.tgz https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX\nand\ncurl -o pool5 https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX\nand each time I try to read the names of the content with tar -zvft , it starts reading the file names until it gets to the halfway and then I get the following error\ntar: Truncated input file (needed 101140480 bytes, only 0 available)\ntar: Error exit delayed from previous errors.\nI have tried in a Mac and a Windows laptop. I\u2019d appreciate some help with this.\nMany thanks,\nRamiro"}, {"role": "user", "text": "Dear Ramiro,\nI never did solve this problem I\u2019m afraid, so I am of little help here, with regret."}, {"role": "system", "text": "Thanks anyway, I hope someone has an idea of how to solve this."}, {"role": "system", "text": "Downloading larger datasets in a history archive (potentially subsetted to only include important data) is the current workaround. More details in this post: Files will not download completely"}], [{"role": "user", "text": "The QualiMap Multi-Sample BamQC wrapper in usegalaxy.eu does not work, showing a pink-colored flagged error message: \u201cUncaught exception in exposed API method:\u201d"}, {"role": "system", "text": "Dear @Mario_Garcia,\nThe tool should work, I tested it right now with some test data. Please make sure:\n(a) your data was correctly analyzed by QualiMap BAM QC (i.e., the files should not be empty),\n(b) your files come from the same organism and data library\n(c) and you data has no weird characters in their file name.\nIf the error still exists, then please make an error report (click on the bug-icon and send an error report, see here: Issue Reporting), so we can see the standard error and standard output of the job. You can also share your history here or with me via message (see also Issue Reporting for sharing your history).\nHave a good day and best wishes,\nFlorian"}, {"role": "user", "text": "Dear Florian,\nThank you for your prompt reply. There is nothing to check in my data because I said, the usegalaxy.eu wrapper for Qualimap Multi-sample BamQC does not run. Clicking on it just shows the error message I have described in my first post.\nBest,\nMario."}, {"role": "user", "text": "\nerror1112\u00d7600 18.3 KB\n"}, {"role": "system", "text": "I saw an error report regarding this tool I think from you today. Seems you manage to run it, but got a different error this time?"}, {"role": "user", "text": "Dear Florian. The former error still remains."}, {"role": "system", "text": "@Mario_Garcia\nI just tried loading all of these tools from the tool panel at usegalaxy.eu and each loaded correctly/successfully.\nTry refreshing your browser window, then loading the tool again. The server is quite busy right now, and the loading failure was likely transitory \u2013 refreshing your connection to the server might be needed."}, {"role": "user", "text": "Dear jennaj,\nThank you for your reply. I have just reloaded two different browsers and re-logged my user account but the same error still remains."}, {"role": "system", "text": "\n\n\n Mario_Garcia:\n\nThank you for your reply. I have just reloaded two different browsers and re-logged my user account but the same error still remains.\n\n\nVery odd. Maybe try clearing the browser cache? Or a different browser? Plus, make sure cookies are enabled. This doesn\u2019t look like a server-side issue since no one else can reproduce it (?) \u2013 or at least three other people cannot: one in the EU (@Flow), one in the USA (me), and another person working at this server during the GCC2021 training going on right now (not sure from where!).\nI\u2019m not sure how I can help more \u2013 but the EU group will be back online in their morning "}, {"role": "system", "text": "I almost suspect, that this has something todo, with the data you have seleced. Can you share your history? Either with me directly via message or via link here in this post (but make sure you allow the data to be assessed). It would be of interest for us to check if there is a bug in the wrapper.\nCheers,\nFlorian"}, {"role": "user", "text": "Dear Florian,\nThank you for your reply. As I\u2019ve explained before, I do not select any data, just click on the tool at the left tools panel and the error window is showed.\nBest."}, {"role": "system", "text": "Hi all,\nI verified the issue. It happens only in the Mario\u2019s account.\nReally odd.\nI check the logs, hoping to find some explanation."}, {"role": "system", "text": "@Mario_Garcia You have fallen into a bug.\n  \n\n      github.com/galaxyproject/galaxy\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      null / -1 HID? \n    \n\n    \n      \n        opened 01:15PM - 28 May 21 UTC\n      \n\n\n      \n        \n          \n          hexylena\n        \n      \n    \n\n    \n    \n  \n\n\n  \n    Spotted today during a class.\n\n![image](https://user-images.githubusercontent.\u2026com/458683/119989389-68a56900-bfc7-11eb-99fd-5f58f1348fea.png)\n\n![2021-05-28_15-12](https://user-images.githubusercontent.com/458683/119989403-6e02b380-bfc7-11eb-9344-a985f7dbbe24.png)\n\nhistory id b798433517f2aa48 for the european admins.\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAs you can verify, the first steps of your history don\u2019t have a number, but null.\nHow did you produce those steps? Will be really useful to fix the bug if you can provide some details (in the issue page better).\nDid you get there by clicking on re-run on an element of a mapped over collection ? This is an action we know to create this issue.\nAs a temporary workaround, I can suggest creating a new history, move the needed datasets there and run the tool."}], [{"role": "user", "text": "Hi, this is my first time reporting an issue, so I apologize if I overlook info, can\u2019t format code correctly in this flavor of markdown, etc.  I\u2019ve submitted a bug report via the Galaxy server, but, at the risk of redundancy, wanted to ask for help here to try to get the issue resolved sooner in case someone knows what might be going wrong.  I can\u2019t find a previous mention of this particular problem in GalaxyHelp or via Google.\nI\u2019m trying to assemble a genome on the Galaxy main server (\u201cversion_major\u201d: \u201c18.09\u201d), and have trimmed my reads (SRR5906897, Trimmomatic headcrop 12, slidingwindow 4, 25, minlen 100).  When I attempt a de novo assembly via Unicycler, however, it consistently fails with the message:\nError: SPAdes crashed! Please view spades.log for more information.\n\nAfter issuing the bug report, I received an email with the additional stdout code that appears to have the main issues in the following lines:\n\nlibgomp: Thread creation failed: Resource temporarily unavailable\n\n== Error == system call for: &quot;['/pylon5/mc48nsp/xcgalaxy/conda/envs/__unicycler@0.4.6/share/spades-3.12.0-1/bin/spades-hammer', '/pylon5/mc48nsp/xcgalaxy/main/staging/21785278/working/spades_assembly/read_correction/corrected/configs/config.info']&quot; finished abnormally, err code: 1\n\nI\u2019m uncertain whether the crucial issue is the \u201cResource temporarily unavailable\u201d line or whether spades-hammer can\u2019t access the config.info file indicated.  I\u2019ve tried re-running the job with \" Skip SPAdes error correction step\" set to \u201cYes\u201d, but I appear to receive the same error.  I\u2019m assuming this is a legitimate bug that will be hard to resolve on my end, but would appreciate any received wisdom as to the best course of action.  Can anyone advise?\nThanks!"}, {"role": "system", "text": "Hi, Thanks for reporting the problem. I was looking at your history and the inputs are Ok. Am running a quick test job to see if the problem is server side or cluster, plus looking at the input closer just to rule out format/content issues.\nFeedback once done"}, {"role": "system", "text": "Unicycler is working fine on test data. For your failure, it was due to the input reads not being paired up.\nMake sure to only input matched forward/reverse reads. The tools  Fastq Interlacer  and  De-interlacer  can be used to filter out reads that are not paired. Then, try another rerun.\nGalaxy tutorials for Assembly: https://galaxyproject.github.io/training-material/topics/assembly/"}, {"role": "user", "text": "Hmm\u2026  Thanks for looking into it, but I don\u2019t think that\u2019s the problem.  I hadn\u2019t bothered to check that there weren\u2019t any unpaired reads in the inputs, since they were output by Trimmomatic as the relevant paired read files, but when I put it through the Fastq Interlacer today, the tool outputs the message:\nThere were 0 single reads.\nInterlaced 471127 pairs of sequences.\n\nI then put the reads through the De-Interlacer, and submitted those read files to Unicycler, and it outputs the same error for me.  So I can\u2019t account for the difference from the results generated with your test data, but I\u2019m pretty sure it\u2019s not because of the presence of unpaired reads."}, {"role": "system", "text": "The fastq datasets were of different sizes with a differing number of reads (originally). They inputs how have the same number of reads.\nHowever, the new run had the \\2 reads entered for forward and \\1 reads entered for reverse on the tool form. The ordering should be swapped. I rerunning your job that way to see what results. You could also try this now - I am expecting it to work.\nI am also running some other tests. This tool was updated and I want to make sure that:\n\ncompressed fastqsanger.gz inputs are not a problem (shouldn\u2019t be, or that\u2019s something to fix)\nthe sequence identifiers containing a space is not a problem (again, shouldn\u2019t be)\n\nI\u2019ll write back once all the tests complete."}, {"role": "system", "text": "\n\n\n jennaj:\n\nHowever, the new run had the \\2 reads entered for forward and \\1 reads entered for reverse on the tool form. The ordering should be swapped. I rerunning your job that way to see what results. You could also try this now - I am expecting it to work.\n\n\nThis did work. So, please also try reversing the order of the reads in your rerun job.\nThe other items I am still testing but given that result do not expect any problems (the first test rules out the other issues)."}, {"role": "user", "text": "Ok, I\u2019m running the Unicycler again with the interlaced-then-de-interlaced reads in the reverse order now, and will report back if it doesn\u2019t work (these runs have usually taken a while to error out, in my experience).  However, I still have some questions:\n\nI see now that you\u2019re right that the de-interlaced reads were in reverse order, with the output file that received the automatically-generated name \u201cFASTQ de-interlacer left mates\u2026\u201d contains reads whose IDs all end with \u2018/2\u2019, indicating they were originally the reverse read from the original set.  However, when I ran the FASTQ interlacer tool in the first place, I made sure to pass the \u201c\u2026897_1.fastq.gz\u201d to \u201cLeft-hand mates\u201d, and \u201c\u2026897_2.fastq.gz\u201d to \u201cRight-hand mates\u201d.  If the FASTQ de-interlacer tool is going to automatically append names to the output as \u201cleft mate\u201d and \u201cright mate\u201d, it\u2019s confusing that it doesn\u2019t maintain the same order of those names as was passed in arguments to the interlacer tool.  Unless I made a mistake somewhere in there, it seems to be unfortunate behavior on the part of the tool.\nI still don\u2019t get different numbers of reads in the output from Trimmomatic; can you tell me how you\u2019re detecting that?  Whether I check by loading the files into BioPython and then checking the number of records, or if I use the more crude measure of wc -l from Bash, I count the same number of forward and reverse reads, whether comparing them among the untrimmed files, as well as for the paired outputs from Trimmomatic.  I definitely see that the two files are of different sizes, but you seem to be suggesting that the problem SPAdes is encountering is that the number of reads passed is different between the two, and I can\u2019t confirm that on my end.\n\n\u2026And in the time it took to write all of that, Unicycler has crashed again, with the same error message about SPAdes, even for the job in which I had passed \u201c63: FASTQ de-interlacer right mates\u2026\u201d to \u201cSelect first set of reads\u201d, and so on; the reverse orientation as I had tried before.  So I\u2019m still not sure what\u2019s going wrong."}, {"role": "system", "text": "Thanks for writing back. Here are the stats and I found out what the issues are around the failures. Some tools are picky about format. The testing became complex but there is a solution.\n\nMatched reads must be provided in the forward and reverse inputs (only)\nThose reads cannot contain internal white spaces in the header. This seems to cause problem both with the Deinterlacer and Unicycler tool.\n\nInputs/QA processing\nOriginal read count, using FastQC:\n\nSRR5906897 forward (R1): 861794\nSRR5906897 reverse (R2): 861794\n\nPost-Trimmomatic (run1 parameters applied) read count, using FastQC:\n\nSRR5906897 forward (R1) paired: 732201\nSRR5906897 reverse (R2) paired: 732201\nSRR5906897 forward (R1) unpaired: 42532\nSRR5906897 reverse (R2) unpaired: 6298\n\nPost-Trimmomatic (run2 parameters applied) read count, using FastQC:\n\nSRR5906897 forward (R1) paired: 471127 > input to failed Unicycler assembly\nSRR5906897 reverse (R2) paired: 471127 > input to failed Unicycler assembly\nSRR5906897 forward (R1) unpaired: 156090\nSRR5906897 reverse (R2) unpaired: 18560\n\nPost interlacer read count, on Trimmomatic run2, using FastQC:\n\nSRR5906897 Pairs: 942254 total seqs, 471127 pairs\nSRR5906897 Singles: 0\n\nPost deinterlacer read count, on Trimmomatic run2 interlaced result, using FastQC:\n\nSRR5906897 left mates (ended up with R2 reads): 471127 > input to failed Unicycler assembly\nSRR5906897 right mates (ended up with R1 reads): 471127 > input to failed Unicycler assembly\nSRR5906897 left singles (R2): 0\nSRR5906897 right singles (R1): 0\n\nHow to fix the data to get a successful run\nUsing the interlacer/deinterlacer doesn\u2019t seem to matter, Trimmomatic took care of that part. The problem is with the spaces contained within the sequence identifiers.. Both the Deinterlacer and Unicycler tool have trouble interpreting these sequences.\n\nStart with either the Trimmomatic or Deinterlacer paired fastqsanger.gz datasets. Trimmomatic/Deinterlacer paired results are identical in content.\nPick which you want to use (Trimmomatic is fine).  Click on the pencil icon for each of the datasets, then under the \u201cConvert\u201d tab, and uncompress the fastq data. The next tool needs uncompressed plain text input.\n\nRemove the space from the sequence identifiers in the uncompressed fastq data. I replaced it with an underscore, using the tool Text transformation with sed and the sed program: s/ ([0-9])/_\\1/\n\nInput those final, paired, no-spaces-in-identifier fastqsanger results to Unicycler. You can input the R1/R2 reads in either order and get a successful run, and I didn\u2019t compare those, yet would recommend putting the R1 reads first (forward) and the R2 reads second (reverse) to run the tool as this is consistent with the actual data content.\n\nAll other tests with different data variations/formatting failed.\nNone of this is extra manipulation is ideal so I\u2019ll bring it up with the developers to see what can be done (if anything). 3rd party wrapped tools often have specific formatting expectations. Sometimes the Galaxy wrapper around the tool can adjust formats that trigger errors and sometimes not, requiring that the data be prepared to be in the correct format using other methods in upstream steps."}, {"role": "user", "text": "Ok, thanks for the detailed breakdown, Jennifer.  I\u2019ll try modifying the read seq IDs and re-running on my end, and will report back if it doesn\u2019t work.\nIt\u2019s too bad that the SRR files sometimes contain spaces in those IDs while a lot of tools might not be designed to tolerate them; if a Galaxy-based wrapper script can\u2019t be instituted to parse such situations, I might suggest elevating this kind of input-format-consideration to a part of the site\u2019s FAQ/common issues docs so that users can be aware of it and perform such a workaround on their own; if it\u2019s already there, I must have missed it.\nI\u2019m glad to hear I\u2019m not crazy about thinking the inputs had equal numbers of reads present, but I am curious why your earlier trick of swapping the forward and reverse read inputs worked on your end and not mine, if the real issue ended up being the read seq ID formatting.  Were you using different reads as a starting point?\nAlso, while I was using another tool to get around the problem with Unicycler not working, I came across another issue with another tool on Galaxy that I\u2019d like to note; should I do that via bug report built into the Galaxy Main server, or via this forum?  Are there best practices guidelines as to how we interact with your team?"}, {"role": "system", "text": "\n\n\n dmack33:\n\nI am curious why your earlier trick of swapping the forward and reverse read inputs worked on your end and not mine\n\n\nI had just mixed up the tests, had about 20 running! All finished up then I went through everything all over again to figure out the root problem.\n\n\n\n dmack33:\n\nI might suggest elevating this kind of input-format-consideration to a part of the site\u2019s FAQ/common issues docs so that users can be aware of it and perform such a workaround on their own; if it\u2019s already there, I must have missed it.\n\n\nCorrect, this specific issue about EBI SRA identifiers with spaces and Deinterlacer/Unicyler/Spades usage is not covered. The Unicycler tool was just updated and revealed the problem. Deinterlacer has always been picky about formats but the output swap has never come up before either (and is very strange, I need to examine the root cause more). I\u2019d like to explore getting all this addressed at a higher level (tool level) and not require users to do a workaround. But if that doesn\u2019t happen, then an FAQ for EBI SRA similar to this one for NCBI SRA data would be the next step: NCBI SRA Fastq - Galaxy Community Hub. If you wanted to contribute an FAQ to the hub, that would be welcomed.\nI\u2019m not sure yet how many tools might have odd behavior due to this specific formatting issue and what new formatting would resolve the problem for all or most tools (if just removing the space is enough). Any tool that interprets sequence identifier /1 and /2 content could be impacted. When I run into tool errors, I try to reduce the data down to the most common format, regardless of type, to figure out what the tool author was expecting \u2013 these types of formatting variation requirements are rarely documented in tool manuals.\nThanks for all the feedback and hope your runs do better this time!"}, {"role": "system", "text": "Dear Team,\nI have utilized Ion torrent based sample data in fastq format and have also performed groomer to get standard fastq information. I had started assembly with single sample still it failed. Below is the error reported.\n\u201clibgomp: Thread creation failed: Resource temporarily unavailable\u201d.\nPlease can someone help in resolving this issue."}, {"role": "system", "text": "Hi Team,\nI\u2019m also running into problems using Unicycler- I get the general error \u2018Remote job server indicated a problem running or monitoring this job\u2019, and when looking at the bug report I get the notification that spades has crashed.\nI am using fastqsanger inputs, and have been inputting these directly into Unicycler (using all defaults) and then running it. The problem is, this was working beautifully until around the 20th of December (roughly) and gave me some great assemblies. After that however, Unicycler has been crashing each and every time I try and use it. Even when I return to histories where Unicycler worked and gave me an assembly, run the same data under the same conditions, Unicycler crashes.\nI am very, very new to using both Galaxy and Unicycler so I apologise if I am missing something very obvious- it is possible I am! I tried using the interlacer and de-interlacer as suggested in this thread, but again, I am not sure what I am doing so I could very well be doing things wrong.\nThanks for the help that has already been provided- I will try and follow it and see if it works, I just wanted to bring up that Unicycler was working great previously in case something has happened.\nIf anyone can provide any further help on what might be happening, I would be incredibly grateful!\nThanks!"}, {"role": "system", "text": "Thanks for reporting the problem again!\nUnicycler is now also failing on known tests cases that should be successful.\nWe are looking into the problem and will post back if this can be resolved quickly. If an issue ticket is created (meaning, this can\u2019t be fixed quickly), I\u2019ll post that link back here so everyone can track the issue/progress/resolution.\n\nUpdate: Issue is now ticketed Uncycler 0.4.6.0 failing at PCS Bridges, Trinity runs fine \u00b7 Issue #185 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub\nUpdate2: Resolved some time ago, closing"}, {"role": "system", "text": "\n  \n    \n    \n    run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210 usegalaxy.org support\n  \n  \n    run Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n  \n\n"}], [{"role": "user", "text": "Hi. I have been using LEfSe in the past and was successfully able to run the full pipeline with the Galaxy LEfSe tool. Now when I am trying to run the pipeline using new tabular files (I believe to be formatted correctly) and even old ones that successfully worked in the past I run into two errors:\nStep A) Info\nERROR:galaxy.datatypes.data:Unable to count lines in file /export/galaxy-central/database/files/004/dataset_4265.dat\nStep B) Error message:\nNumber of significantly discriminative features: 36 ( 36 ) before internal wilcoxon\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, 'lefse_ru\nI have tried to resolve these problems many different ways, but find myself very stuck. If anyone has any reconditions. It is greatly appreciated."}, {"role": "system", "text": "Hello @garretta\nPlease try again from the start (Upload?).\nThe UseGalaxy.org server has some issues over the weekend. That likely impacted your analysis. Please see here for details: Cluster issue at UseGalaxy.org -- Resolved 06/05/2023. Rerun failed (red) or leave queued (gray) - #3 by jennaj\nYour first error message \u201cInfo\u201d indicates that the server couldn\u2019t \u201cread\u201d the input, and I\u2019m guessing that the file is not actually empty. That\u2019s why I am suggesting to reload the data again. \nPlease give that a try and we can follow up more if needed. A shared history link or screenshots with more details would be helpful: Troubleshooting errors && What information should I include when reporting a problem?"}, {"role": "user", "text": "\n\n\n jennaj:\n\nnfo\u201d indicates that the server couldn\u2019t \u201cread\u201d the input, and I\u2019m guessing that the file is not actually empty. That\u2019s why I am suggesting to reload the data again. \n\n\nHi @jennaj Thank you for the quick answer: I tried starting from the upload step again and run into the same errors. Here is a screenshot of the issues. Getting the data is green (that step did not fail).\n\nScreenshot 2023-06-05 at 10.24.29 AM572\u00d7928 69.3 KB\n"}, {"role": "system", "text": "Are you working at UseGalaxy.org?"}, {"role": "user", "text": "I am using: http://galaxy.biobakery.org/\nI am not sure what the difference between the two are. I am more familiar with the format of http://galaxy.biobakery.org/"}, {"role": "system", "text": "Hi @garretta\nOk \u2013 thanks for clarifying. You can work at any server you want to but if there is a server issue going on, only the administrators of the server can help with that.\nOthers have reported problems with that other public server over the last few days, and those admins do not track this forum. They do have contact information in an email address, and run a local forum (or did, so please double check my advice).\nClick into this other topic for more details about what is known:\n\n  \n    \n    \n    Failled Data upload \n  \n  \n    Hi @Virginia_Perez \nYou\u2019ll need to contact the admins of the server about new issues. They do not track this forum and when I contact them it uses these same routes. If you would like to post back what happens, that would be great!\n  \n\n"}, {"role": "user", "text": "Thank you for the help. I will look into the other forums!"}, {"role": "system", "text": "Hi,  @jennaj , I have the same error when I run new date into the pipeline, using http://galaxy.biobakery.org/, for know where is the problem, I tried the date which was successfully able to run,  but now it shows this error too. I don\u2019t understand. I report this error, for any possible help. Greatly appreciated and thanks a lot.\nThe ERROR is:\nNumber of significantly discriminative features: 396 ( 396 ) before internal wilcoxon\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, 'lefse_"}, {"role": "system", "text": "Welcome, @Xueqian_Yin\nWhen working at that server, you should use the resources they host to ask questions, especially for errors that seem to be server side.\nI just checked again and they do have a dedicated forum. The link is on the server\u2019s homepage. I needed to copy and paste it into a new browser window to get the connection correct. Maybe try that way too? Or, just go here: https://forum.biobakery.org/"}, {"role": "system", "text": "Thanks a lot @jennaj !"}, {"role": "system", "text": "Update: Looks like they are working on it, and suggest alternative tools too. Leaving this link here for anyone who happens to run into the same issue. LEfSe issues and errors - #5 by Kelsey_Thompson - LEfSe - The bioBakery help forum"}], [{"role": "user", "text": "Hi Everyone,\nVery new to galaxy. I\u2019m using my university\u2019s big computer called OSCAR to run galaxy, and I\u2019m having a few issues. I was able to install and run galaxy so I had my own instance. However, some sources say I need to become a root user of the computer to be able to make myself admin and restart galaxy so that I can add tools, but I\u2019m not allowed to have root access. (for the record, galaxy is in its own directory within home) I was able to add myself as an admin in the galaxy.yml file and restart without using \u201csudo\u201d but I\u2019m not sure if that\u2019s good enough. there are no options for admin on my galaxy instance. Is it possible to make myself a galaxy admin without root? I can\u2019t use docker on OSCAR for the same reason. Additionally, partially because I can\u2019t become admin, I can\u2019t import the toolshed I need, mentioned in this page: https://snvphyl.readthedocs.io/en/latest/install/galaxy/#overview I really need these SNVPhyl tools for my project. Can anyone help me with these problems?\nBest,\nCS"}, {"role": "system", "text": "Hello,\nIt is definitely possible to run Galaxy without being root. And having root access has nothing to do with becoming \u201can admin\u201d in Galaxy.\nI added at few tags to your post, including \u201cbecome-an-admin\u201d. Review the prior Q&A with same links to find out how to get set up with a basic local install with admin configured. You\u2019ll also find more links to our FAQs, tutorials, and help for advanced configuration.\nThanks!"}], [{"role": "user", "text": "Hi,\nDoes anyone have any pointers for indexing a reference sequence for use with GATK tools? I have indexed reference sequences that I use for BWA and minimap2, but I can\u2019t get the reference sequences to appear with GATK. I receive an error when I use the \u201cGATK-sorted picard indexes builder\u201d.\nThanks!"}, {"role": "system", "text": "Hi @jsalem\nNearly all GATK tools, including the Data Manager, as wrapped for Galaxy, are considered deprecated. These were all based on earlier releases of GATK.\nCurrently, the only GATK4 tool that has been wrapped is:\n\n\nGATK4 Mutect2  - Call somatic SNVs and indels via local assembly of haplotypes\nToolShed Repository: Galaxy | Tool Shed\n\n\nFor native genome fasta files to be accessible to this tool (option: Choose the source for the reference list) \u2013 adding the genome with the fetch DM is probably enough, although to really make new genomes useful there is a short-list of core recommended indexes. All have Data Mangers. You can certainly run more DMs after those (GATK4 Mutect2 won\u2019t need more\u2026 but other tools can). See the topic below for help \u2013 the same process applies to all genomes you plan to index, not just the one referenced in that particular Q&A.\n\n  \n    \n    \n    Indexing reference genomes with Data Managers: Resources, tutorials, troubleshooting \n  \n  \n    Hi, \nThe mm10 reference genome/build is sourced from UCSC. \nYou\u2019ll need to index the genome on your server with Data Manager tools. \nInstall the genome with Data Managers (sourced from the ToolShed, all are in a separate category there). Install DMs like any other tool using the Admin functions. \nYou\u2019ll need these DMs at a minimum. Execute them in this order first: \n\n\nFasta fetcher \u2013 has an option to pick UCSC as the data source.\nSAM indexer\nPicard indexer\n2bit (twoBit) indexer\n\nThen get the DMs\u2026\n  \n\n"}, {"role": "user", "text": "Hi @jennaj,\nThanks for getting back to me. I was hoping to use GATK indel realigner, which requires a reference sequence. Do you know of any other good realigner tools?"}, {"role": "system", "text": "\n\n\n jsalem:\n\nrealign\n\n\nHi \u2013 Review the LoFreq tools"}, {"role": "system", "text": "I am using GATK4 Mutect2 and I select a cached reference and every time I get the error \u201cA USER ERROR has occurred: Argument reference was missing: Argument \u2018reference\u2019 is required.\u201d with no reference passed to -R argument."}, {"role": "system", "text": "Hi @BJWiley233\nThanks for sending in the bug report, it helped to spot the problem.\nThe reads were mapped to hg18, but the genome selected on the GATK4 Mutect2 form was hg19. Your other errored jobs (including Freebayes) also have this conflict.\nIt is important to use the same reference genome throughout an analysis."}, {"role": "system", "text": "Yes I think I selected hg18 my accident.  I was wondering why the chom.sizes didn\u2019t match.  Rookie mistake."}, {"role": "system", "text": "\n\n\n BJWiley233:\n\nYes I think I selected hg18 my accident\n\n\nGlad we could help \nEveryone does some version of this kind of mixup. Mismatched inputs are one of the first things to check, along with format, whenever errors come up. Often much easier to spot in other people\u2019s work than your own."}, {"role": "system", "text": "Hi Jenna,\nAfter realigning to hg19 the calls worked with freebayes but mutect2 is still missing the --reference flag when selecting from locally cached.  I send the bug report.\nBrian"}, {"role": "system", "text": "Same.\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/038/551/38551877/_job_tmp -Xmx7g -Xms256m\n13:40:56.325 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cvmfs/main.galaxyproject.org/deps/_conda/pkgs/gatk4-4.1.7.0-py38_0/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\nOct 25, 2021 1:40:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine\nINFO: Failed to detect whether we are running on Google Compute Engine.\n13:40:57.841 INFO  GetSampleName - ------------------------------------------------------------\n13:40:57.841 INFO  GetSampleName - The Genome Analysis Toolkit (GATK) v4.1.7.0\n13:40:57.841 INFO  GetSampleName - For support and documentation go to https://software.broadinstitute.org/gatk/\n13:40:57.842 INFO  GetSampleName - Executing as g2main@galaxy-28.novalocal on Linux v3.10.0-1127.8.2.el7.x86_64 amd64\n13:40:57.842 INFO  GetSampleName - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01\n13:40:57.842 INFO  GetSampleName - Start Date/Time: October 25, 2021 1:40:55 PM UTC\n13:40:57.842 INFO  GetSampleName - ------------------------------------------------------------\n13:40:57.842 INFO  GetSampleName - ------------------------------------------------------------\n13:40:57.843 INFO  GetSampleName - HTSJDK Version: 2.21.2\n13:40:57.843 INFO  GetSampleName - Picard Version: 2.21.9\n13:40:57.843 INFO  GetSampleName - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n13:40:57.843 INFO  GetSampleName - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n13:40:57.843 INFO  GetSampleName - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n13:40:57.843 INFO  GetSampleName - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n13:40:57.843 INFO  GetSampleName - Deflater: IntelDeflater\n13:40:57.843 INFO  GetSampleName - Inflater: IntelInflater\n13:40:57.843 INFO  GetSampleName - GCS max retries/reopens: 20\n13:40:57.843 INFO  GetSampleName - Requester pays: disabled\n13:40:57.843 INFO  GetSampleName - Initializing engine\n13:40:58.847 INFO  GetSampleName - Done initializing engine\n13:40:58.900 INFO  ProgressMeter - Starting traversal\n13:40:58.901 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Records Processed   Records/Minute\n13:40:58.903 INFO  ProgressMeter -             unmapped              0.0                     0              NaN\n13:40:58.903 INFO  ProgressMeter - Traversal complete. Processed 0 total records in 0.0 minutes.\n13:40:58.903 INFO  GetSampleName - Shutting down engine\n[October 25, 2021 1:40:58 PM UTC] org.broadinstitute.hellbender.tools.GetSampleName done. Elapsed time: 0.06 minutes.\nRuntime.totalMemory()=416284672\nUsing GATK jar /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-2d71e7294258e0d6557ab09c36c8336300de41c90ae1e21e30396938913db7d4/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar\nRunning:\njava -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-2d71e7294258e0d6557ab09c36c8336300de41c90ae1e21e30396938913db7d4/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GetSampleName --input=tumor.bam --output=samplename.txt\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/038/551/38551877/_job_tmp -Xmx7g -Xms256m\nUSAGE: Mutect2 [arguments]\nCall somatic SNVs and indels via local assembly of haplotypes\nVersion:4.1.7.0\nRequired Arguments:\n\u2013input,-I:String             BAM/SAM/CRAM file containing reads  This argument must be specified at least once.\nRequired.\n\u2013output,-O:File              File to which variants should be written  Required.\n\u2013reference,-R:GATKPathSpecifier\nReference sequence file  Required.\nOptional Arguments:\n\u2013add-output-sam-program-record,-add-output-sam-program-record:Boolean\nIf true, adds a PG tag to created SAM/BAM/CRAM files.  Default value: true. Possible\nvalues: {true, false}\n\u2013add-output-vcf-command-line,-add-output-vcf-command-line:Boolean\nIf true, adds a command line header line to created VCF files.  Default value: true.\nPossible values: {true, false}\n\u2013af-of-alleles-not-in-resource,-default-af:Double\nPopulation allele fraction assigned to alleles not found in germline resource.  Please see\ndocs/mutect/mutect2.pdf fora derivation of the default value.  Default value: -1.0.\n\u2013alleles:FeatureInput        The set of alleles to force-call regardless of evidence  Default value: null.\n\u2013annotation,-A:String        One or more specific annotations to add to variant calls  This argument may be specified 0\nor more times. Default value: null. Possible Values: {AlleleFraction,\nAS_BaseQualityRankSumTest, AS_FisherStrand, AS_InbreedingCoeff,\nAS_MappingQualityRankSumTest, AS_QualByDepth, AS_ReadPosRankSumTest, AS_RMSMappingQuality,\nAS_StrandBiasMutectAnnotation, AS_StrandOddsRatio, BaseQuality, BaseQualityHistogram,\nBaseQualityRankSumTest, ChromosomeCounts, ClippingRankSumTest, CountNs, Coverage,\nDepthPerAlleleBySample, DepthPerSampleHC, ExcessHet, FisherStrand, FragmentLength,\nGenotypeSummaries, InbreedingCoeff, LikelihoodRankSumTest, MappingQuality,\nMappingQualityRankSumTest, MappingQualityZero, OrientationBiasReadCounts,\nOriginalAlignment, PossibleDeNovo, QualByDepth, ReadPosition, ReadPosRankSumTest,\nReferenceBases, RMSMappingQuality, SampleList, StrandBiasBySample, StrandOddsRatio,\nTandemRepeat, UniqueAltReadCount}\n\u2013annotation-group,-G:String  One or more groups of annotations to apply to variant calls  This argument may be\nspecified 0 or more times. Default value: null. Possible Values:\n{AlleleSpecificAnnotation, AS_StandardAnnotation, ReducibleAnnotation, StandardAnnotation,\nStandardHCAnnotation, StandardMutectAnnotation}\n\u2013annotations-to-exclude,-AX:String\nOne or more specific annotations to exclude from variant calls  This argument may be\nspecified 0 or more times. Default value: null. Possible Values:\n{AS_StrandBiasMutectAnnotation, BaseQuality, Coverage, DepthPerAlleleBySample,\nDepthPerSampleHC, FragmentLength, MappingQuality, OrientationBiasReadCounts, ReadPosition,\nStrandBiasBySample, TandemRepeat}\n\u2013arguments_file:File         read one or more arguments files and add them to the command line  This argument may be\nspecified 0 or more times. Default value: null.\n\u2013assembly-region-out:String  Output the assembly region to this IGV formatted file  Default value: null.\n\u2013assembly-region-padding:Integer\nNumber of additional bases of context to include around each assembly region  Default\nvalue: 100.\n\u2013base-quality-score-threshold:Byte\nBase qualities below this threshold will be reduced to the minimum (6)  Default value: 18.\n\u2013callable-depth:Integer      Minimum depth to be considered callable for Mutect stats.  Does not affect genotyping.\nDefault value: 10.\n\u2013cloud-index-prefetch-buffer,-CIPB:Integer\nSize of the cloud-only prefetch buffer (in MB; 0 to disable). Defaults to\ncloudPrefetchBuffer if unset.  Default value: -1.\n\u2013cloud-prefetch-buffer,-CPB:Integer\nSize of the cloud-only prefetch buffer (in MB; 0 to disable).  Default value: 40.\n\u2013create-output-bam-index,-OBI:Boolean\nIf true, create a BAM/CRAM index when writing a coordinate-sorted BAM/CRAM file.  Default\nvalue: true. Possible values: {true, false}\n\u2013create-output-bam-md5,-OBM:Boolean\nIf true, create a MD5 digest for any BAM/SAM/CRAM file created  Default value: false.\nPossible values: {true, false}\n\u2013create-output-variant-index,-OVI:Boolean\nIf true, create a VCF index when writing a coordinate-sorted VCF file.  Default value:\ntrue. Possible values: {true, false}\n\u2013create-output-variant-md5,-OVM:Boolean\nIf true, create a a MD5 digest any VCF file created.  Default value: false. Possible\nvalues: {true, false}\n\u2013disable-bam-index-caching,-DBIC:Boolean\nIf true, don\u2019t cache bam indexes, this will reduce memory requirements but may harm\nperformance if many intervals are specified.  Caching is automatically disabled if there\nare no intervals specified.  Default value: false. Possible values: {true, false}\n\u2013disable-read-filter,-DF:String\nRead filters to be disabled before analysis  This argument may be specified 0 or more\ntimes. Default value: null. Possible Values: {GoodCigarReadFilter, MappedReadFilter,\nMappingQualityAvailableReadFilter, MappingQualityNotZeroReadFilter,\nMappingQualityReadFilter, NonChimericOriginalAlignmentReadFilter,\nNonZeroReferenceLengthAlignmentReadFilter, NotDuplicateReadFilter,\nNotSecondaryAlignmentReadFilter, PassesVendorQualityCheckReadFilter, ReadLengthReadFilter,\nWellformedReadFilter}\n\u2013disable-sequence-dictionary-validation,-disable-sequence-dictionary-validation:Boolean\nIf specified, do not check the sequence dictionaries from our inputs for compatibility.\nUse at your own risk!  Default value: false. Possible values: {true, false}\n\u2013downsampling-stride,-stride:Integer\nDownsample a pool of reads starting within a range of one or more bases.  Default value:\n1.\n\u2013exclude-intervals,-XL:StringOne or more genomic intervals to exclude from processing  This argument may be specified 0\nor more times. Default value: null.\n\u2013f1r2-max-depth:Integer      sites with depth higher than this value will be grouped  Default value: 200.\n\u2013f1r2-median-mq:Integer      skip sites with median mapping quality below this value  Default value: 50.\n\u2013f1r2-min-bq:Integer         exclude bases below this quality from pileup  Default value: 20.\n\u2013f1r2-tar-gz:File            If specified, collect F1R2 counts and output files into this tar.gz file  Default value:\nnull.\n\u2013founder-id,-founder-id:String\nSamples representing the population \u201cfounders\u201d  This argument may be specified 0 or more\ntimes. Default value: null.\n\u2013gatk-config-file:String     A configuration file to use with the GATK.  Default value: null.\n\u2013gcs-max-retries,-gcs-retries:Integer\nIf the GCS bucket channel errors out, how many times it will attempt to re-initiate the\nconnection  Default value: 20.\n\u2013gcs-project-for-requester-pays:String\nProject to bill when accessing \u201crequester pays\u201d buckets. If unset, these buckets cannot be\naccessed.  Default value: .\n\u2013genotype-germline-sites:Boolean\n(EXPERIMENTAL) Call all apparent germline site even though they will ultimately be\nfiltered.  Default value: false. Possible values: {true, false}\n\u2013genotype-pon-sites:Boolean  Call sites in the PoN even though they will ultimately be filtered.  Default value: false.\nPossible values: {true, false}\n\u2013germline-resource:FeatureInput\nPopulation vcf of germline sequencing containing allele fractions.  Default value: null.\n\u2013graph-output,-graph:String  Write debug assembly graph information to this file  Default value: null.\n\u2013help,-h:Boolean             display the help message  Default value: false. Possible values: {true, false}\n\u2013ignore-itr-artifacts:BooleanTurn off read transformer that clips artifacts associated with end repair insertions near\ninverted tandem repeats.  Default value: false. Possible values: {true, false}\n\u2013initial-tumor-lod,-init-lod:Double\nLog 10 odds threshold to consider pileup active.  Default value: 2.0.\n\u2013interval-exclusion-padding,-ixp:Integer\nAmount of padding (in bp) to add to each interval you are excluding.  Default value: 0.\n\u2013interval-merging-rule,-imr:IntervalMergingRule\nInterval merging rule for abutting intervals  Default value: ALL. Possible values: {ALL,\nOVERLAPPING_ONLY}\n\u2013interval-padding,-ip:IntegerAmount of padding (in bp) to add to each interval you are including.  Default value: 0.\n\u2013interval-set-rule,-isr:IntervalSetRule\nSet merging approach to use for combining interval inputs  Default value: UNION. Possible\nvalues: {UNION, INTERSECTION}\n\u2013intervals,-L:String         One or more genomic intervals over which to operate  This argument may be specified 0 or\nmore times. Default value: null.\n\u2013lenient,-LE:Boolean         Lenient processing of VCF files  Default value: false. Possible values: {true, false}\n\u2013max-assembly-region-size:Integer\nMaximum size of an assembly region  Default value: 300.\n\u2013max-population-af,-max-af:Double\nMaximum population allele frequency in tumor-only mode.  Default value: 0.01.\n\u2013max-reads-per-alignment-start:Integer\nMaximum number of reads to retain per alignment start position. Reads above this threshold\nwill be downsampled. Set to 0 to disable.  Default value: 50.\n\u2013min-assembly-region-size:Integer\nMinimum size of an assembly region  Default value: 50.\n\u2013min-base-quality-score,-mbq:Byte\nMinimum base quality required to consider a base for calling  Default value: 10.\n\u2013mitochondria-mode:Boolean   Mitochondria mode sets emission and initial LODs to 0.  Default value: false. Possible\nvalues: {true, false}\n\u2013native-pair-hmm-threads:Integer\nHow many threads should a native pairHMM implementation use  Default value: 4.\n\u2013native-pair-hmm-use-double-precision:Boolean\nuse double precision in the native pairHmm. This is slower but matches the java\nimplementation better  Default value: false. Possible values: {true, false}\n\u2013normal-lod:Double           Log 10 odds threshold for calling normal variant non-germline.  Default value: 2.2.\n\u2013normal-sample,-normal:StringBAM sample name of normal(s), if any.  May be URL-encoded as output by GetSampleName with\n-encode argument.  This argument may be specified 0 or more times. Default value: null.\n\u2013panel-of-normals,-pon:FeatureInput\nVCF file of sites observed in normal.  Default value: null.\n\u2013pcr-indel-qual:Integer      Phred-scaled PCR SNV qual for overlapping fragments  Default value: 40.\n\u2013pcr-snv-qual:Integer        Phred-scaled PCR SNV qual for overlapping fragments  Default value: 40.\n\u2013pedigree,-ped:File          Pedigree file for determining the population \u201cfounders\u201d  Default value: null.\n\u2013QUIET:Boolean               Whether to suppress job-summary info on System.err.  Default value: false. Possible\nvalues: {true, false}\n\u2013read-filter,-RF:String      Read filters to be applied before analysis  This argument may be specified 0 or more\ntimes. Default value: null. Possible Values: {AlignmentAgreesWithHeaderReadFilter,\nAllowAllReadsReadFilter, AmbiguousBaseReadFilter, CigarContainsNoNOperator,\nFirstOfPairReadFilter, FragmentLengthReadFilter, GoodCigarReadFilter,\nHasReadGroupReadFilter, IntervalOverlapReadFilter, LibraryReadFilter, MappedReadFilter,\nMappingQualityAvailableReadFilter, MappingQualityNotZeroReadFilter,\nMappingQualityReadFilter, MatchingBasesAndQualsReadFilter, MateDifferentStrandReadFilter,\nMateDistantReadFilter, MateOnSameContigOrNoMappedMateReadFilter,\nMateUnmappedAndUnmappedReadFilter, MetricsReadFilter,\nNonChimericOriginalAlignmentReadFilter, NonZeroFragmentLengthReadFilter,\nNonZeroReferenceLengthAlignmentReadFilter, NotDuplicateReadFilter,\nNotOpticalDuplicateReadFilter, NotProperlyPairedReadFilter,\nNotSecondaryAlignmentReadFilter, NotSupplementaryAlignmentReadFilter,\nOverclippedReadFilter, PairedReadFilter, PassesVendorQualityCheckReadFilter,\nPlatformReadFilter, PlatformUnitReadFilter, PrimaryLineReadFilter,\nProperlyPairedReadFilter, ReadGroupBlackListReadFilter, ReadGroupReadFilter,\nReadLengthEqualsCigarLengthReadFilter, ReadLengthReadFilter, ReadNameReadFilter,\nReadStrandFilter, SampleReadFilter, SecondOfPairReadFilter, SeqIsStoredReadFilter,\nSoftClippedReadFilter, ValidAlignmentEndReadFilter, ValidAlignmentStartReadFilter,\nWellformedReadFilter}\n\u2013read-index,-read-index:String\nIndices to use for the read inputs. If specified, an index must be provided for every read\ninput and in the same order as the read inputs. If this argument is not specified, the\npath to the index for each input will be inferred automatically.  This argument may be\nspecified 0 or more times. Default value: null.\n\u2013read-validation-stringency,-VS:ValidationStringency\nValidation stringency for all SAM/BAM/CRAM/SRA files read by this program.  The default\nstringency value SILENT can improve performance when processing a BAM file in which\nvariable-length data (read, qualities, tags) do not otherwise need to be decoded.  Default\nvalue: SILENT. Possible values: {STRICT, LENIENT, SILENT}\n\u2013seconds-between-progress-updates,-seconds-between-progress-updates:Double\nOutput traversal statistics every time this many seconds elapse  Default value: 10.0.\n\u2013sequence-dictionary,-sequence-dictionary:String\nUse the given sequence dictionary as the master/canonical sequence dictionary.  Must be a\n.dict file.  Default value: null.\n\u2013sites-only-vcf-output:Boolean\nIf true, don\u2019t emit genotype fields when writing vcf file output.  Default value: false.\nPossible values: {true, false}\n\u2013tmp-dir:GATKPathSpecifier   Temp directory to use.  Default value: null.\n\u2013tumor-lod-to-emit,-emit-lod:Double\nLog 10 odds threshold to emit variant to VCF.  Default value: 3.0.\n\u2013tumor-sample,-tumor:String  BAM sample name of tumor.  May be URL-encoded as output by GetSampleName with -encode\nargument.  Default value: null.\n\u2013use-jdk-deflater,-jdk-deflater:Boolean\nWhether to use the JdkDeflater (as opposed to IntelDeflater)  Default value: false.\nPossible values: {true, false}\n\u2013use-jdk-inflater,-jdk-inflater:Boolean\nWhether to use the JdkInflater (as opposed to IntelInflater)  Default value: false.\nPossible values: {true, false}\n\u2013verbosity,-verbosity:LogLevel\nControl verbosity of logging.  Default value: INFO. Possible values: {ERROR, WARNING,\nINFO, DEBUG}\n\u2013version:Boolean             display the version number for this tool  Default value: false. Possible values: {true,\nfalse}\nAdvanced Arguments:\n\u2013active-probability-threshold:Double\nMinimum probability for a locus to be considered active.  Default value: 0.002.\n\u2013adaptive-pruning-initial-error-rate:Double\nInitial base error rate estimate for adaptive pruning  Default value: 0.001.\n\u2013allele-informative-reads-overlap-margin:Integer\nLikelihood and read-based annotations will only take into consideration reads that overlap\nthe variant or any base no further than this distance expressed in base pairs  Default\nvalue: 2.\n\u2013allow-non-unique-kmers-in-ref:Boolean\nAllow graphs that have non-unique kmers in the reference  Default value: false. Possible\nvalues: {true, false}\n\u2013bam-output,-bamout:String   File to which assembled haplotypes should be written  Default value: null.\n\u2013bam-writer-type:WriterType  Which haplotypes should be written to the BAM  Default value: CALLED_HAPLOTYPES. Possible\nvalues: {ALL_POSSIBLE_HAPLOTYPES, CALLED_HAPLOTYPES}\n\u2013debug-assembly,-debug:Boolean\nPrint out verbose debug information about each assembly region  Default value: false.\nPossible values: {true, false}\n\u2013disable-adaptive-pruning:Boolean\nDisable the adaptive algorithm for pruning paths in the graph  Default value: false.\nPossible values: {true, false}\n\u2013disable-tool-default-annotations,-disable-tool-default-annotations:Boolean\nDisable all tool default annotations  Default value: false. Possible values: {true, false}\n\u2013disable-tool-default-read-filters,-disable-tool-default-read-filters:Boolean\nDisable all tool default read filters (WARNING: many tools will not function correctly\nwithout their default read filters on)  Default value: false. Possible values: {true,\nfalse}\n\u2013dont-increase-kmer-sizes-for-cycles:Boolean\nDisable iterating over kmer sizes when graph cycles are detected  Default value: false.\nPossible values: {true, false}\n\u2013dont-use-soft-clipped-bases:Boolean\nDo not analyze soft clipped bases in the reads  Default value: false. Possible values:\n{true, false}\n\u2013emit-ref-confidence,-ERC:ReferenceConfidenceMode\nMode for emitting reference confidence scores (For Mutect2, this is a BETA feature)\nDefault value: NONE. Possible values: {NONE, BP_RESOLUTION, GVCF}\n\u2013enable-all-annotations:Boolean\nUse all possible annotations (not for the faint of heart)  Default value: false. Possible\nvalues: {true, false}\n\u2013force-active:Boolean        If provided, all regions will be marked as active  Default value: false. Possible values:\n{true, false}\n\u2013force-call-filtered-alleles,-genotype-filtered-alleles:Boolean\nForce-call filtered alleles included in the resource specified by --alleles  Default\nvalue: false. Possible values: {true, false}\n\u2013gvcf-lod-band,-LODB:Double  Exclusive upper bounds for reference confidence LOD bands (must be specified in increasing\norder)  This argument may be specified 0 or more times. Default value: [-2.5, -2.0, -1.5,\n-1.0, -0.5, 0.0, 0.5, 1.0].\n\u2013independent-mates:Boolean   Allow paired reads to independently support different haplotypes.  Useful for validations\nwith ill-designed synthetic data.  Default value: false. Possible values: {true, false}\n\u2013kmer-size:Integer           Kmer size to use in the read threading assembler  This argument may be specified 0 or more\ntimes. Default value: [10, 25].\n\u2013max-mnp-distance,-mnp-dist:Integer\nTwo or more phased substitutions separated by this distance or less are merged into MNPs.\nDefault value: 1.\n\u2013max-num-haplotypes-in-population:Integer\nMaximum number of haplotypes to consider for your population  Default value: 128.\n\u2013max-prob-propagation-distance:Integer\nUpper limit on how many bases away probability mass can be moved around when calculating\nthe boundaries between active and inactive assembly regions  Default value: 50.\n\u2013max-suspicious-reads-per-alignment-start:Integer\nMaximum number of suspicious reads (mediocre mapping quality or too many substitutions)\nallowed in a downsampling stride.  Set to 0 to disable.  Default value: 0.\n\u2013max-unpruned-variants:Integer\nMaximum number of variants in graph the adaptive pruner will allow  Default value: 100.\n\u2013min-dangling-branch-length:Integer\nMinimum length of a dangling branch to attempt recovery  Default value: 4.\n\u2013min-pruning:Integer         Minimum support to not prune paths in the graph  Default value: 2.\n\u2013minimum-allele-fraction,-min-AF:Double\nLower bound of variant allele fractions to consider when calculating variant LOD  Default\nvalue: 0.0.\n\u2013num-pruning-samples:Integer Number of samples that must pass the minPruning threshold  Default value: 1.\n\u2013pair-hmm-gap-continuation-penalty:Integer\nFlat gap continuation penalty for use in the Pair HMM  Default value: 10.\n\u2013pair-hmm-implementation,-pairHMM:Implementation\nThe PairHMM implementation to use for genotype likelihood calculations  Default value:\nFASTEST_AVAILABLE. Possible values: {EXACT, ORIGINAL, LOGLESS_CACHING,\nAVX_LOGLESS_CACHING, AVX_LOGLESS_CACHING_OMP, EXPERIMENTAL_FPGA_LOGLESS_CACHING,\nFASTEST_AVAILABLE}\n\u2013pcr-indel-model:PCRErrorModel\nThe PCR indel model to use  Default value: CONSERVATIVE. Possible values: {NONE, HOSTILE,\nAGGRESSIVE, CONSERVATIVE}\n\u2013phred-scaled-global-read-mismapping-rate:Integer\nThe global assumed mismapping rate for reads  Default value: 45.\n\u2013pruning-lod-threshold:DoubleLn likelihood ratio threshold for adaptive pruning algorithm  Default value:\n2.302585092994046.\n\u2013recover-all-dangling-branches:Boolean\nRecover all dangling branches  Default value: false. Possible values: {true, false}\n\u2013showHidden,-showHidden:Boolean\ndisplay hidden arguments  Default value: false. Possible values: {true, false}\n\u2013smith-waterman:Implementation\nWhich Smith-Waterman implementation to use, generally FASTEST_AVAILABLE is the right\nchoice  Default value: JAVA. Possible values: {FASTEST_AVAILABLE, AVX_ENABLED, JAVA}\nConditional Arguments for readFilter:\nValid only if \u201cMappingQualityReadFilter\u201d is specified:\n\u2013maximum-mapping-quality:Integer\nMaximum mapping quality to keep (inclusive)  Default value: null.\n\u2013minimum-mapping-quality:Integer\nMinimum mapping quality to keep (inclusive)  Default value: 20.\nValid only if \u201cReadLengthReadFilter\u201d is specified:\n\u2013max-read-length:Integer     Keep only reads with length at most equal to the specified value  Default value:\n2147483647.\n\u2013min-read-length:Integer     Keep only reads with length at least equal to the specified value  Default value: 30.\n\nA USER ERROR has occurred: Argument reference was missing: Argument \u2018reference\u2019 is required.\n\nSet the system property GATK_STACKTRACE_ON_USER_EXCEPTION (\u2013java-options \u2018-DGATK_STACKTRACE_ON_USER_EXCEPTION=true\u2019) to print the stack trace.\nUsing GATK jar /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-2d71e7294258e0d6557ab09c36c8336300de41c90ae1e21e30396938913db7d4/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar\nRunning:\njava -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-2d71e7294258e0d6557ab09c36c8336300de41c90ae1e21e30396938913db7d4/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar Mutect2 --QUIET --tumor-sample Galaxy11-ob__HISAT2_on_97.1.1__cb.bam --input tumor.bam --input normal.bam --output output.vcf.gz"}, {"role": "system", "text": "\n\n\n mitchellgc:\n\nA USER ERROR has occurred: Argument reference was missing: Argument \u2018reference\u2019 is required.\n\n\nDo the inputs have the same exact reference genome assigned as was selected on the GATK4 tool form? Which database (genome) for each? It wasn\u2019t added to the command string but should be if specified on the form."}, {"role": "system", "text": "That\u2019s what\u2019s strange. hg19 was selected but wasn\u2019t passed to the command string."}, {"role": "system", "text": "Thanks @mitchellgc for the extra feedback.\nThe version installed at ORG has problems. We are working to fix that now.\nMeanwhile, you can run the job at UseGalaxy.eu with this version of the tool. It was updated with some important changes finalized and published on Oct 6, 2021.\n\nGATK4 Mutect2 - Call somatic SNVs and indels via local assembly of haplotypes (Galaxy Version 4.1.7.0+galaxy1)\n\nAny other version of the tool could have problems. This was complicated to fix by the tool-dev group (since January) but all the changes are now incorporated.\nTracking ticket: Update GATK4 Mutect2 \u00b7 Issue #345 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub"}], [{"role": "user", "text": "Hello Galaxy folks,\nI have been using galaxy for a while to analyse RNAseq and I think it\u2019s incredible. Now I was wondering if there would be any workflow to work with single cell RNAseq data from FASTQ files.\nI have been trying to follow-up these tutorials\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Pre-processing of Single-Cell RNA Data\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\nBut so far I couldn\u2019t finish it so I was wondering if any of you know about a complete workflow.\nI liked this one from the EMBL-EBI https://www.youtube.com/watch?v=1w4sA-qyO3g\nbut they use data from GXA as the source, so the preprocessing of the data it\u2019s not done and I am struggling with it.\nHope you can help me.\nBest,"}, {"role": "system", "text": "When you say \u201ccouldn\u2019t finish\u201d, do you mean that the workflow failed? And if so, how did it fail?"}, {"role": "user", "text": "Hello Astrov, thanks for replying.\nIt failed because I tried to make a hybrid workflow from the pre-processing data analysis workflow and the one explained in the youtube video from EMBL-EBI by Wendy. What I fail at is in the pre-processing analysis from the Fastq files.\nI followed this workflow:\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Pre-processing of 10X Single-Cell RNA Datasets\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\nIn which they specify the fastq files \u201cread1\u201d and \u201cread2\u201d as the containers of the barcodes and the sequence respectively. And also a Cell ranger whitelist which can be provided by them in the Zenodo Link. (picture attached)\nScreenshot 2020-04-21 at 09.44.071570\u00d7922 321 KB\n\nWhen I try to do these steps, Galaxy says wrong input file and this is where my first very naive question comes. I think I don\u2019t fully understand the outcome of the sequencing from the scRNAseq analyses.\nI have sent two samples and what I\u2019ve received is 8 fastq files.\nI was counting with 2 fastq files (pair end sequencing) per sample but I have 4 per sample, i.e (picture attached):\n\nScreenshot 2020-04-21 at 10.05.421308\u00d7332 258 KB\n\nLooking at other workflows that allows you to use pre-loaded scRNAseq files like this one 10X STAR SOLO workflow (picture attached). I could understand that every read generate two files that would be 4 files per sample but in my case I have 8 and I don\u2019t fully understand why \nScreenshot 2020-04-21 at 10.07.441630\u00d71178 332 KB\n\nAny ideas about this?\nThanks in advance.\nJavi"}, {"role": "system", "text": "Hi Javi,\nWhen you preview your FASTQ files (the eye icon), can you read them in plain text or is it garbled?\nIf garbled, can you change the datatype to make them fastqsanger.gz.\nIf not garbled, can you change the datatype to fastqsanger\nIf you private message me your galaxy history, I can further debug this a bit more.\nBest,\nMehmet"}, {"role": "user", "text": "Hello mtekman,\nI think they are not garbled, it\u2019s the normal structure of a FASTQ file. I tried to send you a private message with the history but it didn\u2019t allow me, dunno why. sorry man.\nThanks!\n\nScreenshot 2020-04-21 at 11.54.291966\u00d71228 356 KB\n"}, {"role": "system", "text": "Can you share your history with me"}, {"role": "system", "text": "Table of Contents\n_________________\n\n1. Determining the 10x inputs\n.. 1. Order of inputs\n.. 2. Barcode Size and Chemistry\n2. Obtaining annotation data\n.. 1. GTF and Whitelist\n3. Running the tool\n.. 1. Setting Basic Parameters\n..... 1. Setting the input pairs\n..... 2. (Optional) Multiple input pairs\n..... 3. GTF and Whitelist\n.. 2. Setting Advanced Parameters\n\n\nHi Javi, thanks for sharing your history with me.\n\n\n1 Determining the 10x inputs\n============================\n\n1.1 Order of inputs\n~~~~~~~~~~~~~~~~~~~\n\n  The first thing you need to do is to determine which of your paired\n  reads is the cDNA read, and which is the barcode read. Typically\n  barcode is Read1 and cDNA is Read2, but we should confirm this\n  manually:\n\n  * By picking two _read1.fastq's at random, we can see that these are\n    the barcoded reads since most of sequences end with poly-T tails.\n  * By picking two _read2.fastq's at random, we can infer that these are\n    cDNA reads because the sequence is more varied.\n\n\n1.2 Barcode Size and Chemistry\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Next, we need to find out what 10x chemistry we are using, and we can\n  determine that by looking at the size of our barcodes, as given in the\n  [tutorial].\n\n  * By picking a few random reads in a _read1.fastq file, we can count\n    from the start of a sequence to the first 'TTTT' (allowing for a few\n    small mismatches), and we see that the barcode size is 26bp, meaning\n    this is v2 10x chemistry.\n\n\n[tutorial]\n<https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-preprocessing-tenx/tutorial.html#10x-chemistries>\n\n\n2 Obtaining annotation data\n===========================\n\n  To run the input FASTQ data on STARsolo, we need the whitelist of cell\n  barcodes and the GTF file with the gene annotations.\n\n\n2.1 GTF and Whitelist\n~~~~~~~~~~~~~~~~~~~~~\n\n  From the [Zenodo link]:\n  * Copy the urls of the *3M-february-2018.txt.gz* and\n    *Homo_sapiens.GRCh37.75.gtf*\n  * Paste them into Galaxy using the upload data dialog\n  * Wait for them to be imported into your history\n\n\n[Zenodo link] <https://zenodo.org/record/3457880>\n\n\n3 Running the tool\n==================\n\n  Finally we can run the tool using all the inputs and additional files\n  we have obtained.\n\n\n3.1 Setting Basic Parameters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n3.1.1 Setting the input pairs\n-----------------------------\n\n  Since your FASTQ files are individual files and not collections, we\n  set the *Input Type* to \"Single files\". Then, for each Input Pairs we\n  set\n  * *Barcode reads* to a <prefix>_read1.fastq.gz file\n  * *cDNA reads* to a <prefix>_read2.fastq.gz file\n\n  where the <prefix> should be identical for each pair of inputs.\n\n\n3.1.2 (Optional) Multiple input pairs\n-------------------------------------\n\n  If you have multiple input pairs you can insert them all at once by\n  clicking on the *+ Insert Input Pairs* button and repeating the above\n  for a different <prefix>. This approach comes with some\n  advantages/disadvantages:\n\n  * Pro - Multiple input pairs at once has the benefit of having one\n    large all-inclusive count matrix at the end of the run.\n  * Con - If just *one* of your inputs is malformed, then the entire\n    tool fails and it will be hard to know which dataset was\n    responsible.\n\n\n  Normally one should first try setting all inputs at once, and if that\n  run fails, the sequential runs approach.\n\n\n3.1.3 GTF and Whitelist\n-----------------------\n\n  Now we need to use our additional files. For *RNA-Seq Cell Barcode\n  Whitelist* set this to your \"3M-february-2018\" dataset.\n\n  Set the *Custom or built-in reference genome* to \"Use a built-in\n  index\", and set the *Reference genome with or without an annotation*\n  to \"use genome reference without builtin gene-model\".\n\n  For *Select reference genome* set this \"Human Dec. .... hg19\", since\n  this is the version of our GTF file.\n\n  Then for *Gene model (gff3,gtf) file for splice junctions* set this to\n  our GTF file.\n\n\n3.2 Setting Advanced Parameters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Now, under the *Configure Chemistry Options* set this \"Cell Ranger v2\"\n  as we determined from our barcode size before.\n\n  Finally we need to tell RNA STARsolo that the barcode size is variable\n  due to all the poly-T bases in the barcode sequence. If you scroll\n  down in the RNA STARsolo tool there is an option *Barcode Size is same\n  size of the Read*, which is set to Yes by default. Set this to \"No\".\n\n  After that hit execute, and your jobs should commence!\n\n  Best, Mehmet"}], [{"role": "user", "text": "Md simulation stopped suddenly and xtc file have not been produced. I restarted several times but got same error. Please suggest me to overcome this error."}, {"role": "system", "text": "Hi @asif0067\nThis is how to start investigating why an error is occurring: Galaxy Training!\nIf you are not sure how to interpret the error, please post back:\n\nServer URL where you are working\nFull tool name and version\nThe error message details, including the stderr and stdout\n\n\nThanks!"}, {"role": "user", "text": "Galaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2020.4+galaxy0\nTool error\nAn error occurred with this dataset:\nRemote job server indicated a problem\nGROMACS:      gmx grompp, version 2020.4\nExecutable:   /data/share/tools/_conda/envs/__gromacs@2020.4/bin.AVX2_256/gmx\nData prefix:  /data/share/tools/_conda/envs/__gromacs@2020.4\nWorking dir:  /data/share/staging/26133001/working\nCommand line:\ngmx grompp -f ./md.mdp -c ./inp.gro -r ./inp.gro -n ./index.ndx -p ./top_input.top -o outp.tpr\nIgnoring obsolete mdp entry \u2018title\u2019\nIgnoring obsolete mdp entry \u2018ns_type\u2019\nSetting the LD random seed to -1827399811\nGenerated 2926 of the 2926 non-bonded parameter combinations\nGenerating 1-4 interactions: fudge = 0.5\nGenerated 2926 of the 2926 1-4 parameter combinations\nExcluding 3 bonded neighbours molecule type \u2018Protein_chain_A\u2019\nExcluding 3 bonded neighbours molecule type \u2018UNL\u2019\nExcluding 2 bonded neighbours molecule type \u2018SOL\u2019\nExcluding 1 bonded neighbours molecule type \u2018NA\u2019\nNumber of degrees of freedom in T-Coupling group Protein_UNL is 8336.85\nNumber of degrees of freedom in T-Coupling group Water_and_ions is 153435.16\nEstimate for the relative computational load of the PME mesh part: 0.15\nNOTE 1 [file ./md.mdp]:\nThis run will generate roughly 3862 Mb of data\nThere was 1 note\nGROMACS reminds you: \u201cTry to calculate the numbers that have been\u201d (The Smoke Fairies)\nturning H bonds into constraints\u2026\nturning H bonds into constraints\u2026\nturning H bonds into constraints\u2026\nturning H bonds into constraints\u2026\nDetermining Verlet buffer for a tolerance of 0.005 kJ/mol/ps at 310 K\nCalculated rlist for 1x1 atom pair-list as 1.297 nm, buffer size 0.097 nm\nSet rlist, assuming 4x4 atom pair-list, to 1.224 nm, buffer size 0.024 nm\nNote that mdrun will redetermine rlist based on the actual pair-list setup\nCalculating fourier grid dimensions for X Y Z\nUsing a fourier grid of 60x60x60, spacing 0.156 0.156 0.156\n GROMACS - gmx mdrun, 2020.4 (-:\n                        GROMACS is written by:\n Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen\nPar Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    \n\nAldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray\nGerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang\nAleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis\nPeter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson\nJustin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund\nPascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall\nSander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov\nAlfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen\nChristian Wennberg    Maarten Wolf      Artem Zhmurov\nand the project leaders:\nMark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel\nCopyright (c) 1991-2000, University of Groningen, The Netherlands.\nCopyright (c) 2001-2019, The GROMACS development team at\nUppsala University, Stockholm University and\nthe Royal Institute of Technology, Sweden.\ncheck out http://www.gromacs.org for more information.\nGROMACS is free software; you can redistribute it and/or modify it\nunder the terms of the GNU Lesser General Public License\nas published by the Free Software Foundation; either version 2.1\nof the License, or (at your option) any later version.\nGROMACS:      gmx mdrun, version 2020.4\nExecutable:   /data/share/tools/_conda/envs/__gromacs@2020.4/bin/gmx_gpu\nData prefix:  /data/share/tools/_conda/envs/__gromacs@2020.4\nWorking dir:  /data/share/staging/26133001/working\nCommand line:\ngmx_gpu mdrun -ntmpi 1 -nb gpu -pme gpu -nt 8 -deffnm outp\nReading file outp.tpr, VERSION 2020.4 (single precision)\nChanging nstlist from 20 to 100, rlist from 1.224 to 1.345\n1 GPU selected for this run.\nMapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:\nPP:0,PME:0\nPP tasks will do (non-perturbed) short-ranged interactions on the GPU\nPP task will update and constrain coordinates on the CPU\nPME tasks will do all aspects on the GPU\nUsing 1 MPI thread\nNon-default thread affinity set, disabling internal thread affinity\nUsing 8 OpenMP threads\nWARNING: This run will generate roughly 3843 Mb of data\nstarting mdrun \u2018Protein-Ligand in water\u2019\n50000000 steps, 100000.0 ps.\nWriting final coordinates.\n           Core t (s)   Wall t (s)        (%)\n   Time:   731308.351    91413.557      800.0\n                     1d01h23:33\n             (ns/day)    (hour/ns)\n\nPerformance:       94.516        0.254\nThanx for Using GROMACS - Have a Nice Day\ntitle                   = Protein-ligand complex MD simulation\n; Run parameters\nintegrator              = md        ; leap-frog integrator\nnsteps                  = 50000000   ; 2 * 50000000 = 100000 ps (100 ns)\ndt                      = 0.002     ; 2 fs\n; Output control\nnstenergy               = 5000      ; save energies every 10.0 ps\nnstlog                  = 5000      ; update log file every 10.0 ps\nnstxout-compressed      = 5000      ; save coordinates every 10.0 ps\n; Bond parameters\ncontinuation            = yes       ; continuing from NPT\nconstraint_algorithm    = lincs     ; holonomic constraints\nconstraints             = h-bonds   ; bonds to H are constrained\nlincs_iter              = 1         ; accuracy of LINCS\nlincs_order             = 4         ; also related to accuracy\n; Neighbor searching and vdW\ncutoff-scheme           = Verlet\nns_type                 = grid      ; search neighboring grid cells\nnstlist                 = 20        ; largely irrelevant with Verlet\nrlist                   = 1.2\nvdwtype                 = cutoff\nvdw-modifier            = force-switch\nrvdw-switch             = 1.0\nrvdw                    = 1.2       ; short-range van der Waals cutoff (in nm)\n; Electrostatics\ncoulombtype             = PME       ; Particle Mesh Ewald for long-range electrostatics\nrcoulomb                = 1.2\npme_order               = 4         ; cubic interpolation\nfourierspacing          = 0.16      ; grid spacing for FFT\n; Temperature coupling\ntcoupl                  = V-rescale                     ; modified Berendsen thermostat\ntc-grps                 = Protein_UNL Water_and_ions    ; two coupling groups - more accurate\ntau_t                   = 0.1         0.1                     ; time constant, in ps\nref_t                   = 310         310                     ; reference temperature, one for each group, in K\n; Pressure coupling\npcoupl                  = Parrinello-Rahman             ; pressure coupling is on for NPT\npcoupltype              = isotropic                     ; uniform scaling of box vectors\ntau_p                   = 2.0                           ; time constant, in ps\nref_p                   = 1.0                           ; reference pressure, in bar\ncompressibility         = 4.5e-5                        ; isothermal compressibility of water, bar^-1\n; Periodic boundary conditions\npbc                     = xyz       ; 3-D PBC\n; Dispersion correction is not used for proteins with the C36 additive FF\nDispCorr                = no\n; Velocity generation\ngen_vel                 = no        ; continuing from NPT equilibration"}, {"role": "system", "text": "Hi @asif0067\nThanks for providing the details. You are using the most current tool version at UseGalaxy.eu. The job appears to be running out of resources during execution (~4 GB of data is very large for any public Galaxy server to process).\nPlease submit a bug report for this error and include a link to this topic in the comments. The UseGalaxy.eu admin/support team should be able to help you more. My guess is that the job will be too large as is, but they can confirm.\ncc @bjoern.gruening @wm75 @gallardoalba\nTutorial: Running molecular dynamics simulations using GROMACS"}, {"role": "user", "text": "I reported for this error but there is no reply from chemoinformatics expert via emails."}, {"role": "user", "text": "I restarted several times but no xtc file was generated so failed again and again. Please solve this issue."}, {"role": "system", "text": "Hi @asif0067,\nI have checked the error reports, but didn\u2019t find the one corresponding to GROMACS. Could you try to submit it once again?\nRegards."}, {"role": "user", "text": "Ok. I am submitting again."}, {"role": "user", "text": "Resubmissited\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2020.4+galaxy1\n\n\n\n\n\nJob API ID:\n11ac94870d0bb33a89cd26bce070f612\n\n\n\n\n\n\nCommand Line:\nln -s '/data/share/staging/29031595/inputs/dataset_a5b73911-0cc4-4c57-9630-499070226db9.dat' ./md.mdp &&  ln -s '/data/share/staging/29031595/inputs/dataset_230c9ef5-e546-409d-8892-4eeb8d0b3d56.dat' ./inp.gro && ln -s '/data/share/staging/29031595/inputs/dataset_9f6e79c1-e1d4-44e8-b298-038bb5270172.dat' ./top_input.top &&  ln -s '/data/share/staging/29031595/inputs/dataset_0f7fbca9-2d70-481e-97f9-b4ec87c48209.dat' ./posres.itp &&   ln -s '/data/share/staging/29031595/inputs/dataset_3bfdfcae-8ae2-4b71-9f1c-08bf1016052a.dat' ./index.ndx &&  gmx grompp -f ./md.mdp -c ./inp.gro -r ./inp.gro -n ./index.ndx -p ./top_input.top -o outp.tpr &>> verbose.txt &&  gmx mdrun -nt \"${GALAXY_SLOTS:-4}\" -deffnm outp &>> verbose.txt   && cat md.mdp &>> verbose.txt\n\n"}, {"role": "user", "text": "Please check it"}, {"role": "system", "text": "Sorry, we haven\u2019t got any report. Have you used the bug icon?\n"}, {"role": "user", "text": "Yes I used this icon to submit the error in the beginning of this occurence"}, {"role": "system", "text": "I tried rerunning your job, just changing the number of steps to 500, and it completed without any error. So the problem doesn\u2019t appear to be on the GROMACS side.\nI don\u2019t have any helpful suggestion, but I recommend you use your own resources for conducting these kind of simulations.\nAll the best,\nSimon"}], [{"role": "user", "text": "Hello everyone!\nI\u2019m trying to do QC on my RNAseq samples, and after mapping with STAR (no gene counts, 2-pass, hg38 with GFF file for spli junctions) and passing the BAM file to Qualimap RNA-seq QC, I get a error.\nThe problem is that the program doesn\u2019t even spit the error, as seen in the image.\n\nimage1581\u00d7716 41.3 KB\n\n\"\nJava memory size is set to 1200M\nLaunching application\u2026\ndetected environment java options -Djava.awt.headless=true -Xmx7680m\nQualiMap v.2.2.2-dev\nBuilt on 2019-11-11 14:05\nSelected tool: rnaseq\nInitializing regions from features.gtf\u2026\nInitialized 1\n\"\nUpon further inspection, this is the error:\n\"\n\nDataset Error Report\nAn error occurred while running the tool  toolshed.g2.bx.psu.edu/repos/iuc/qualimap_rnaseq/qualimap_rnaseq/2.2.2d+galaxy1 .\n\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 255 ()\nTool generated the following standard error:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/031/100/31100658/_job_tmp -Xmx7g  -Xms256m\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/031/100/31100658/_job_tmp -Xmx7g  -Xms256m\nFailed to run rnaseq\njava.lang.IllegalArgumentException: Comparison method violates its general contract!\nat java.base/java.util.TimSort.mergeLo(TimSort.java:781)\nat java.base/java.util.TimSort.mergeAt(TimSort.java:518)\nat java.base/java.util.TimSort.mergeForceCollapse(TimSort.java:461)\nat java.base/java.util.TimSort.sort(TimSort.java:254)\nat java.base/java.util.Arrays.sort(Arrays.java:1441)\nat org.bioinfo.ngs.qc.qualimap.common.TranscriptDataHandler.createHelperMaps(TranscriptDataHandler.java:652)\nat org.bioinfo.ngs.qc.qualimap.common.TranscriptDataHandler.constructTranscriptsMap(TranscriptDataHandler.java:175)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.loadRegionsFromGTF(ComputeCountsTask.java:702)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.initRegions(ComputeCountsTask.java:608)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.run(ComputeCountsTask.java:479)\nat org.bioinfo.ngs.qc.qualimap.process.RNASeqQCAnalysis.run(RNASeqQCAnalysis.java:68)\nat org.bioinfo.ngs.qc.qualimap.main.RnaSeqQcTool.execute(RnaSeqQcTool.java:221)\nat org.bioinfo.ngs.qc.qualimap.main.NgsSmartTool.run(NgsSmartTool.java:190)\nat org.bioinfo.ngs.qc.qualimap.main.NgsSmartMain.main(NgsSmartMain.java:113)\n\"\nI\u2019ve used the BAM file that STAR generates and the same GFF reference I used to align with STAR.\nAny ideas?"}, {"role": "system", "text": "Hi. Were you able to figure out how to get rid of the error? I\u2019m getting a similar warning when I try to run Qualimap on the terminal."}, {"role": "user", "text": "In the end I ended up using other tools, couldn\u2019t solve this in the online instance of galaxy "}, {"role": "system", "text": "Hi there,\nI\u2019m getting a similar error when trying to run QualiMap BAM QC on a collection of mapped reads from RNA STAR. Format is bam, database is hg38. All jobs failed and I get the following tool error message: \u201cRemote job server indicated a problem running or monitoring this job.\u201d or on some jobs: \u201cFailed to communicate with remote job server.\u201d\nMy history is now filled with 11 failed jobs for each of the datasets in my collection (they all have the same name, see picture attached).\nAm I getting the inputs wrong or what could be the reason?\nCheers,\nSven\n\nimage1673\u00d71051 92.7 KB\n"}, {"role": "system", "text": "@clsven, which Galaxy instance you\u2019re using?"}, {"role": "system", "text": "I\u2019m using the main server usegalaxy.org"}, {"role": "system", "text": "@clsven, I\u2019ve just tested QualiMap BamQC on small datasets with default parameters in both usegalaxy.org and usegalaxy.eu; the .org jobs didn\u2019t even run, while .eu jobs ran perfectly in a short time.\nI\u2019d suggest you test the European instance and tell us if you have better luck."}, {"role": "system", "text": "@David  Thanks a lot for testing that!\nWhat\u2019s the best way to transfer my datasets (i.e. the entire collection) to the European server? Neither sharing the history with my .eu account, nor downloading a dataset from the URL seems to work."}, {"role": "system", "text": "@clsven, How exactly didn\u2019t work? Can you show us the error? Are the files too big? You can also copy some datasets to a new history and download this whole history."}, {"role": "system", "text": "@clsven\nUPDATE: The smaller datasets completed running ok in the .org instance, although it took much more time to start/complete. The bigger one is still running on .org.\nAlso there are interesting differences about the org/eu runs:\n\nEu\n\nhas Command Line and Tool Standard Output text; Tool Standard Error empty:\n\nimage789\u00d7209 10.1 KB\n\n\nOrg\n\nhas Command Line empty, but has Tool Standard Output and Tool Standard Error text:\n\nimage716\u00d7216 9.46 KB\n\nOutput values are the same:\n\nEu\n\n\n\n\n\nReference size\n12,567,654\n\n\n\n\nNumber of reads\n267,167\n\n\nMapped reads\n267,167 / 100%\n\n\nUnmapped reads\n0 / 0%\n\n\nMapped paired reads\n0 / 0%\n\n\nSecondary alignments\n0\n\n\nSupplementary alignments\n1,737 / 0.65%\n\n\nRead min/max/mean length\n30 / 151 / 94.83\n\n\nOverlapping read pairs\n0 / 0%\n\n\nDuplicated reads (flagged)\n0 / 0%\n\n\nDuplicated reads (estimated)\n48,903 / 18.3%\n\n\nDuplication rate\n16.42%\n\n\nClipped reads\n143,727 / 53.8%\n\n\n\n\nOrg\n\n\n\n\n\nReference size\n12,567,654\n\n\n\n\nNumber of reads\n267,167\n\n\nMapped reads\n267,167 / 100%\n\n\nUnmapped reads\n0 / 0%\n\n\nMapped paired reads\n0 / 0%\n\n\nSecondary alignments\n0\n\n\nSupplementary alignments\n1,737 / 0.65%\n\n\nRead min/max/mean length\n30 / 151 / 94.83\n\n\nOverlapping read pairs\n0 / 0%\n\n\nDuplicated reads (flagged)\n0 / 0%\n\n\nDuplicated reads (estimated)\n48,903 / 18.3%\n\n\nDuplication rate\n16.42%\n\n\nClipped reads\n143,727 / 53.8%\n\n\n\n"}], [{"role": "user", "text": "Good morning,\nI run roary for 59 genomes, I got 600 core genes, but the alignment file is empty.\ncan someone please help me understand why, I ran roary two times and I got the same result.\nthank you"}, {"role": "user", "text": "for update, I rerun with roary tool in galaxy europe, and got the same result empty file for alignment, I check my gff files no problem with them, if some can guide me what to do I will be grateful."}, {"role": "system", "text": "Hello everyone,\nI\u2019m responding to this topic because i\u2019m experiencing the same issue. Roary runs, but output is empty (0 lines for \u201cSummary statistics\u201d and \u201cGene Presence Absence\u201d tabular files and 0 bytes for the \u201cCore Gene Alignment\u201d). It worked fine when I tested few genomes, but the problem comes when I use more (552 in my case).\nI got same results with different \u201cminimum percentage identity for blastp\u201d (90, 85, 65, 60 or 50%).\nAnyone has a solution?\nThanks in advance."}, {"role": "system", "text": "Up: the problem I reported is only for usegalaxy.eu. It worked with useglaxy.org. Still no idea why."}, {"role": "system", "text": "Hi @Cosmo,\nthanks for the report; we are investigating the cause of the problem. Could you share a Galaxy history with me in order to reproduce it?\nRegards"}, {"role": "system", "text": "Hi @gallardoalba,\nThis issue of getting empty core genome alignment file is still not resolved in usegalaxy.eu and usegalaxy.org. I performed roary for 1000 genomes and got 1978 core genome. But the core genome alignment file is missing/empty.\nThank you in advance."}, {"role": "system", "text": "Update 2023-03-07\nFile naming issue resolved. Use version 3.13.0+galaxy2 or later.\n\nHi @Ashikha @Cosmo @Ola_Alessa\nRoary is one of the few tools that interprets dataset names by necessity. Most other tools don\u2019t \u2013 metadata like the datatype, or the actual dataset contents are used instead.\nWorkarounds that tend to solve most issues with weird results from this tool are listed in the issue ticket here: Roary 3.13.0 fails at usegalaxy.org -- likely installation issue \u00b7 Issue #293 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub\nIf those don\u2019t work for some reason, we would definitely like an example in a shared history link. A small public example is best as we might want clone it to include in that ticket\u2019s workaround or reassess whether any part of the issue can be addressed. And if the tool is actually silently failing for suspected resource reasons with larger data/stricter params, an example would be helpful. Post the shared link here as a reply (public), or ask @gallardoalba or me to start up a private message to share it in (if not shared already!).\nEvery example of odd failures or odd outputs I\u2019ve reviewed was not dependent on the number/size of inputs \u2013 all were dataset naming issues. But that could be a biased set. While it is certainly possible to create a really massive computationally expensive job that fails to produce results at a public site (could happen at one or multiple \u2013 each uses different resources), those usually result in \u201cred\u201d error results. Very few produce empty (putatively successful) \u201cgreen\u201d results, and I don\u2019t think that is a known yet about this particular tool.\nThanks in advance for the feedback!"}], [{"role": "user", "text": "Hi Galaxy team,\nI would like to use HiSat or Star for aligning Zebrafish RNAseq data. Under Reference Genome there is a note \u201cIf your genome of interest is not listed, contact the Galaxy team (\u2013genomeDir)\u201d. If this is not the right contact, kindly redirect me.\nThank you."}, {"role": "system", "text": "@Lena_Ho on https://usegalaxy.eu we have installed the Zebrafish Genome. We try to harmonize all reference genomes in the near future.\nHope that helps!\nBjoern"}], [{"role": "user", "text": "I am currently in the midst of performing DGE analysis through a HISAT2, StringTie and DeSeq2 approach. I am at the stage where I need to use feature counts for my HISAT2 and StringTie merge, but everytime I try to run the program I get an error with this messagw \u201cUnable to run this job due to a cluster error, please retry it later\u201d. I then ran stringtiemerge again and the same message appeared. Prior to today when I used StringtieMerge with the same dataset it worked. I\u2019m just wondering is this a server issue with galaxy?"}, {"role": "system", "text": "I\u2019ve been getting the same error with featurecounts since last night."}, {"role": "system", "text": "I have been having the same error with several methods as well.  I also have several jobs that seem to be hung."}, {"role": "user", "text": "I saw a similar post where they used the same quote of cluster error. The response was that the Galaxy server had issues. So I\u2019m thinking it might be the same in this case.\nhttps://biostar.usegalaxy.org/p/21454/"}, {"role": "system", "text": "I figured it was an issue on there end.  I was just hoping that this would help get a fix in place quickly."}, {"role": "user", "text": "It\u2019s working for me now I think."}, {"role": "system", "text": "Which Galaxy server was everyone using? Galaxy Main https://usegalaxy.org or a different URL?\nI don\u2019t know of any current server issues here, but can check in with our administrator."}, {"role": "user", "text": "It was usegalaxy.org for me."}, {"role": "system", "text": "Thanks, we confirmed there was a problem impacting some jobs that is now resolved.\nFor anyone that ran into this problem, please rerun any jobs that failed for this reason.\nJobs that are currently queued and seem \u201chung\u201d should process on their own as cluster resources are available. The cluster is quite busy, so some queue wait-time is expected. Jobs will process faster if allowed to stay queued.\nHowever, if any job stays queued longer than 24 hrs, a rerun is probably Ok for this specific situation. Warning: this is normally not recommended \u2013 if a job is queued, any delete/rerun just puts the new rerun job back at the end of the queue, increasing wait time. But a rerun might be needed for some of these (due to the nature of the original problem)."}], [{"role": "user", "text": "Hi all,\nI\u2019m getting the following error.\n\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid\n\nThe analysis on a dataset collection results in some completing fine, the rest with this error. Thanks for any help!"}, {"role": "system", "text": "Hi @ssayson\nWe fixed this a bit earlier today at UseGalaxy.org. Please try a rerun for any jobs that failed this way.\nThanks for reporting the issue!"}], [{"role": "user", "text": "Hi, I have a gene expression matrix that is taken from the logarithm of gene expression values.\nNow I want to return the logarithm of values to the normal state, how to convert the value from logarithm to normal?\nAnd what tool should I use?"}, {"role": "system", "text": "Hi @maryam-gh99\nit seems Datamesh does not have power transformation option but you can try awk. Assuming you have log2 values, calculate raw values with 2^$column_number_with_log_values and print columns you need.\nHere is an example:\nhttps://usegalaxy.org.au/u/igor/h/powertransformation\nLog values are in column 2, and I printed the whole line plus additional column with raw/non-transformed values.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi there,\nI\u2019m following the tutorial for ATAC-seq analysis for the platform. They use Genrich for peak calling but it doesn\u2019t show up when I look for it in \u201ctools\u201d. Isn\u2019t it available yet?\nThanks!"}, {"role": "system", "text": "Welcome @Chiqui\nNot all tools are available at all public Galaxy servers. Some are domain-specific. Some have unique tools. If running your own server, tools are usually in the ToolShed https://usegalaxy.org/toolshed and can be installed, but not always.\nThat is a lot of \u201csometimes\u201d. So let\u2019s break this down into smaller pieces and work on a solution from there. Specifically, these details would help:\n\n\nWhat exact tutorial are you following? Link/URL?\n\n\nWhich Galaxy server are you currently working at? URL please if public. Describe the source/server type if not public (it is Ok if you don\u2019t know the non-public full details, just share what you can).\n\n\nThanks!"}, {"role": "user", "text": "Hi! Thanks for the reply!\n\nI\u2019m following this tutorial: https://galaxyproject.github.io/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html\n\nI\u2019m using the web-base platform (usegalaxy.org) so it seems it works with CyVerseat the Texas Advanced Computing Center\n\nIs there any other way to work with Galaxy in a user-friendly manner that doesn\u2019t depend on the website? I am not a bioinformacitian and even though I have some knowledge of R, it\u2019s hard for me to analyze data using scripts. That\u2019s why I was amazed when I found and started using the website.\nWell, I hope you can help me!\nBest!"}, {"role": "system", "text": "Hi,\nI am very happy to see that this tutorial is used.\nWhen you go to the tutorial webpage, in the overview frame, you have an icon written Galaxies. If you click on it, you will see where you can run the full pipeline. As you will see, you can run it on usegalaxy.eu.\nI hope it helps,\nBest"}], [{"role": "user", "text": "Hi!\nI have mounted the singularity containers from the galaxyproject with cvmfs and am wondering what the expected speed and responsiveness should be. Currently listing the files takes a while the first time, but I think that is expected. But using a container is also a bit slow. I am in the EU and I thought it would use the Freiburg mirror (cvmfs1-ufr0), also because I enabled the GEOAPI function. But looking at the network traffic it seems to connect the penn state / psu server, which is off course far away. Is this expected? Or should I change something in the setup.\nPart of the config\nCVMFS_SERVER_URL='http://cvmfs1-psu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-iu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-tacc0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org;http://cvmfs1-mel0.gvl.org.au/cvmfs/singularity.galaxyproject.org'    # from /cvmfs/cvmfs-config.galaxyproject.org/etc/cvmfs/domain.d/galaxyproject.org.conf\nCVMFS_SHARED_CACHE=yes    # from /etc/cvmfs/default.conf\nCVMFS_STRICT_MOUNT=no    # from /etc/cvmfs/default.conf\nCVMFS_SYSLOG_FACILITY=\nCVMFS_SYSLOG_LEVEL=\nCVMFS_SYSTEMD_NOKILL=\nCVMFS_TIMEOUT=5    # from /etc/cvmfs/default.conf\nCVMFS_TIMEOUT_DIRECT=10    # from /etc/cvmfs/default.conf\nCVMFS_TRACEFILE=\nCVMFS_TRUSTED_CERTS=\nCVMFS_USER=cvmfs    # from /etc/cvmfs/default.conf\nCVMFS_USE_CDN=yes    # from /etc/cvmfs/default.local\nCVMFS_USE_GEOAPI=yes    # from /etc/cvmfs/default.local\n\nOutput from iftop and nethogs:\n# iftop\nserver     => cvmfs1-psu0.galaxyproject.org             23.1Kb  32.0Kb  39.1Kb\n      <=                                                      1.74Mb  2.56Mb  3.31Mb\n# nethogs\n  13093 cvmfs    /usr/bin/cvmfs2      ens2f0      5.861     456.206 KB/sec\n\nThanks for sharing experience and insights!"}, {"role": "system", "text": "Welcome, @mattias\nGood questions. I\u2019ve cross-posted over to the Admin chat to hopefully get more eyes on this. They may reply here or there, and feel free to join! You're invited to talk on Matrix"}, {"role": "user", "text": "Thanks for cross-posting. Re-enabling a matrix account so I can follow over there as well "}, {"role": "system", "text": "@mattias Did you try swapping the order of servers in CVMFS_SERVER_URL and check whether that has any effect? Something I\u2019ve encountered in the past is that if, say the Freiburg server is down, the cvmfs client automatically fails over to the next closest one. It could be the case that a restart of the client could kick off a GEOAPI lookup again.\nYou might also be able to use this: ansible-cvmfs/main.yml at main \u00b7 galaxyproject/ansible-cvmfs \u00b7 GitHub, which is the playbook use by the usegalaxy.* federation, to crosscheck your configuration."}, {"role": "user", "text": "Thanks for your answer. I did try switching some urls, but it did not have effect. I did run wipecache and now I notice the geo api is active in the syslog:\nJun  7 15:11:20 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) switching proxy from (none) to DIRECT (set proxies)\nJun  7 15:11:20 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) geographic order of servers retrieved from cvmfs1-iu0.galaxyproject.org\nJun  7 15:11:20 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) switching proxy from (none) to DIRECT (cloned)\nJun  7 15:11:20 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) switching host from http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/cvmfs-config.galaxyproject.org to http://cvmfs1-psu0.galaxyproject.org/cvmfs/cvmfs-config.galaxyproject.org (host returned HTTP error)\nJun  7 15:11:21 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) CernVM-FS: linking /cvmfs/cvmfs-config.galaxyproject.org to repository cvmfs-config.galaxyproject.org\nJun  7 15:11:40 nioo0009 mount.cvmfs: configuration repository directory does not exist: /cvmfs/cvmfs-config.galaxyproject.org/etc/cvmfs/config.d\nJun  7 15:11:40 nioo0009 cvmfs2: configuration repository directory does not exist: /cvmfs/cvmfs-config.galaxyproject.org/etc/cvmfs/config.d\nJun  7 15:11:40 nioo0009 cvmfs2: configuration repository directory does not exist: /cvmfs/cvmfs-config.galaxyproject.org/etc/cvmfs/config.d\nJun  7 15:11:41 nioo0009 cvmfs2: (singularity.galaxyproject.org) switching proxy from (none) to DIRECT (set proxies)\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) geographic order of servers retrieved from cvmfs1-mel0.gvl.org.au\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) switching proxy from (none) to DIRECT (cloned)\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) whitelist lifetime verification failed, expired\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) whitelist verification failed (5): expired whitelist\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) failed to fetch manifest (8 - bad whitelist), trying another stratum 1\nJun  7 15:11:42 nioo0009 cvmfs2: (singularity.galaxyproject.org) switching host from http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org to http://cvmfs1-psu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org (manually triggered)\n\nWhat I notice is 1) a config folder on the mount does not exist 2) the fr0 host is tried but switched to psu0 because of a host returned HTTP error.\nIt is not the case that the freiburg stratum1 is down at the moment?\nI have this setup:\n/etc/cvmfs/default.local\nCVMFS_HTTP_PROXY=DIRECT\nCVMFS_CLIENT_PROFILE=single\nCVMFS_QUOTA_LIMIT=5000\nCVMFS_USE_GEOAPI=yes\n\n/etc/cvmfs/config.d/cvmfs-config.galaxyproject.org.conf\nCVMFS_USE_GEOAPI=yes\nCVMFS_SERVER_URL=\"http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/@fqrn@;http://cvmfs1-psu0.galaxyproject.org/cvmfs/@fqrn@;http://cvmfs1-iu0.galaxyproject.org/cvmfs/@fqrn@;http://cvmfs1-tacc0.galaxyproject.org/cvmfs/@fqrn@\"\nCVMFS_KEYS_DIR=\"/etc/cvmfs/keys/galaxyproject.org/\n\nI did already look at the ansible configs but went for directly creating the config files. Maybe I have something not correctly set, but I don\u2019t see it myself yet."}, {"role": "system", "text": "\n\n\n mattias:\n\nJun  7 15:11:20 nioo0009 cvmfs2: (cvmfs-config.galaxyproject.org) switching host from http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/cvmfs-config.galaxyproject.org to http://cvmfs1-psu0.galaxyproject.org/cvmfs/cvmfs-config.galaxyproject.org (host returned HTTP error)\n\n\n\n@bjoern.gruening Is singularity hosted on the Freiburg cvmfs servers? I see a 404 when I visit the above link in the browser, but a 403 when I try the same with psu and mel servers."}, {"role": "user", "text": "I am using the cvmfs-config to get the keys etc, but it seems with my config it tries to get this form the freiburg server but gets a 404 and therefore goes to the pennstate one. But the singularity folder is there at freiburg, but since the host switched maybe it is never tried? Should I manually configure things witthout cvmfs-config?\nHere is a list of status codes I get:\nlink | status |\n---- |----\nhttp://cvmfs1-ufr0.galaxyproject.eu/cvmfs/cmfs-config.galaxyproject.org/ | 404\nhttp://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org/ | 403\nhttp://cvmfs1-psu0.galaxyproject.org/cvmfs/cvmfs-config.galaxyproject.org/ | 403\nhttp://cvmfs1-psu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org/ | 403\n"}, {"role": "system", "text": "\n\n\n mattias:\n\nhttp://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org/\n\n\nI think it\u2019s worth a try. The last time I tried, I didn\u2019t have much luck with cvmfs-config. @hxr may be able to point to the issue."}, {"role": "user", "text": "I removed the cvmfs-config repo from the setup, but I still have issues accessing the freiburg mirror. If I remove the psu mirror it first tries the freiburg one, fails, and then goes to the Indiana one. That one is already much faster in response time, but still I think the freiburg one should be better since it is much closer. Here the logs:\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) switching proxy from (none) to DIRECT (set proxies)\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) switching proxy from (none) to DIRECT (cloned)\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) whitelist lifetime verification failed, expired\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) whitelist verification failed (5): expired whitelist\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) failed to fetch manifest (8 - bad whitelist), trying another stratum 1\nJun 23 14:22:55 server cvmfs2: (singularity.galaxyproject.org) switching host from http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org to http://cvmfs1-iu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org (manually triggered)\nJun 23 14:23:12 server cvmfs2: (singularity.galaxyproject.org) CernVM-FS: linking /cvmfs/singularity.galaxyproject.org to repository singularity.galaxyproject.org\n\nError seems to be whitelist lifetime verification and according to this thread ([CVM-1828] .ch) whitelist verification failed (6) - SFTJIRA) is because of an expired key. Is there a different key for the freiburg one? Seems not if I look at the ansible configs. Or is there something not up to date at the freiburg mirror, since the other mirrors work? @bjoern.gruening, can you maybe help here?"}, {"role": "system", "text": "We will have a look at this next week. The monitoring looks like it should work Grafana"}, {"role": "user", "text": "Thanks for the dashboard link. For me this confirms that:\n\nThere is no cvmfs-config mirror at fr0\nThe singularity mirror is up at fr0, or at least pings \n\nMaybe because there is no cvmfs-config I misconfigured something? But it seems the other mirrors work with the same config. @bjoern.gruening, can you confirm that the public key for fr0 is the same as for the others, so like this:\n# /etc/cvmfs/keys/galaxyproject.org/singularity.galaxyproject.org.pub\n-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAuJZTWTY3/dBfspFKifv8\nTWuuT2Zzoo1cAskKpKu5gsUAyDFbZfYBEy91qbLPC3TuUm2zdPNsjCQbbq1Liufk\nuNPZJ8Ubn5PR6kndwrdD13NVHZpXVml1+ooTSF5CL3x/KUkYiyRz94sAr9trVoSx\nTHW2buV7ADUYivX7ofCvBu5T6YngbPZNIxDB4mh7cEal/UDtxV683A/5RL4wIYvt\nS5SVemmu6Yb8GkGwLGmMVLYXutuaHdMFyKzWm+qFlG5JRz4okUWERvtJ2QAJPOzL\nmAG1ceyBFowj/r3iJTa+Jcif2uAmZxg+cHkZG5KzATykF82UH1ojUzREMMDcPJi2\ndQIDAQAB\n-----END PUBLIC KEY-----\n\nThis is also the same public key as configured for the ansible role: ansible-cvmfs/defaults/main.yml at main \u00b7 galaxyproject/ansible-cvmfs \u00b7 GitHub"}, {"role": "system", "text": "Hi!\nThe public key that you pasted is correct. There was a problem with our server. At the moment, I expect data.galaxyproject.org, main.galaxyproject.org, sandbox.galaxyproject.org, singularity.galaxyproject.org, refgenomes-databio.galaxyproject.org and usegalaxy.galaxyproject.org to work on the Freiburg server. Can you try again?"}], [{"role": "user", "text": "Hello!\nI am running Roary for a pangenome analysis on Prokka output. The jobs are automatically being paused. I have tried several time to \u201cresume jobs\u201d via the history options with no success.\n\nScreenshot 2023-03-08 172551371\u00d7935 62.6 KB\n"}, {"role": "system", "text": "Hi @feanor\nAre the inputs in an error state? If so, then you\u2019ll need to fix the upstream problem.\nThat may just be a rerun. A new option \u201cto resume downstream jobs\u201d will be above the Submit button on that rerun tool form.\nPlease give that a try, thanks!"}, {"role": "user", "text": "Hi @jennaj\nThe input (a collection of prokka-annotated genomes) is not in error state. I have tried to rerun the Roary tool and again the jobs are first shown to be on queue and then they are paused.\nMore screens attached. Thanks!\n\nscreen_1350\u00d7848 55.3 KB\n\n\nscreen_21728\u00d71165 192 KB\n\n\nscreen_31912\u00d71218 198 KB\n"}, {"role": "system", "text": "Sometimes an error in just one of the inputs can be hidden inside the collection, i.e. one of your 2024 gff3 inputs might be showing in red when you step inside the collection.  Rerunning with \u201cresume dependent jobs\u201d as suggested by @jennaj might fix your issue.\nFrom the eu side we can also offer to take a look at this right in your history, but we\u2019d need to know your eu user name to do so."}, {"role": "user", "text": "Hi @wm75\nWhen I select \u201cRun Job Again\u201d for the paused Roary job, I see no option to \u201cresume dependent jobs\u201d as you and @jennaj suggested. Perhaps I am missing something. Here\u2019s a screenshot of the \u201cRun Job Again\u201d page.\n\nscreen_42540\u00d71242 434 KB\n\nEDIT: Same for Galaxy Version 3.13.0+galaxy2, I have tried both current and previous version:\n\nscreen_52546\u00d71232 434 KB\n"}, {"role": "system", "text": "This is not about the paused job @feanor, but about the inputs to that job. Look inside the collection of gff3s!"}, {"role": "user", "text": "Got it, thanks @wm75. I checked manually, all gff3 files are in OK status (green). I also downloaded and inspected the collection, all files are there with expected-size data written in them.\nI may have identified the issue. I used the filter collection tool to filter out 3 gff from my initial 2027 gff collection. I tried to run Roary on this collection and it is queued but not paused. So this may a bug of the filter collection tool afterall. Will update further.\nEDIT: the job on the initial collection (n=2027) was also paused"}, {"role": "user", "text": "@wm75 @jennaj\nIs there anything else I could try? Should I share with you my User ID as @wm75 suggested?\nThanks!"}, {"role": "system", "text": "@feanor yes, from my point of view further debugging would require me to take a look at your actual history, and for this I\u2019d need you to share your history either via link or privately with me as explained here. For sharing privately you can use the email from my profile here on the help forum.\nAlternatively, if your history also contains a failed job, you can simply submit a bug report for that dataset and mention in there what the bug really is about. The bug report will then allow me to discover your history, too."}, {"role": "user", "text": "Hi @wm75,\nWhere exactly can I get your email address? I can\u2019t seem to find it here. Thanks again!"}, {"role": "system", "text": "Ah sorry, that won\u2019t work. Messaged you privately."}, {"role": "system", "text": "So, this turned out to be a non-trivial case, but:\nthe key to finding out what\u2019s wrong was to expand the view of one of your paused jobs. When you do that it says: \u201cInput dataset \u2018IHIT32037\u2019 was deleted before the job started. To resume this job fix the input dataset(s).\u201d\nThat\u2019s rather clear, but the challenge is to locate that dataset inside your large input collections. How I did this was via the advanced search interface (downwards-facing arrows at the top of the history panel) and configure it like shown in the screenshot (filter by name: IHIT32037; deleted: Yes).\nThis will reveal two deleted datasets from two different collections (the two datasets are actually at index number 1494 inside their collections, but unfortunately the current UI doesn\u2019t have a way to show the deleted state from inside the collection - that\u2019s why @jennaj\u2019s and my own previous advice wasn\u2019t really helpful). If you undelete these, you should be able to resume the paused jobs or to run the tool again without getting into the paused step.\n\nScreenshot from 2023-03-15 10-50-33284\u00d7646 27.2 KB\n\nNo idea how you arrived at this situation, but that should allow you to proceed finally."}], [{"role": "user", "text": "I am trying to run DESeq2 with featureCounts files from a single cell sequencing project. I have 8 different samples, so 8 collections of tabular files. Each file has a column with Geneid and a column with the count. When I try to run DEseq (4 samples from one group vs 4 samples in another group) I get an error message saying:\nError in data.frame(\u2026, check.names = FALSE) :\narguments imply differing number of rows: 135280, 235928, 226602, 203217, 210838, 214950, 279139, 133575\nCalls: get_deseq_dataset \u2026 eval \u2192 eval \u2192 eval \u2192 cbind \u2192 cbind \u2192 data.frame\nThe report also states that:\nThe tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\nHow can I resolve this?\nMany thanks!"}, {"role": "system", "text": "Hi @carolyn_nielsen,\ndid you follow the same pipeline for generating the gene counts for each sample?"}, {"role": "system", "text": "Dear carolyn_nielsen,\nI assume that you may have picked samples twice or more for the same factor? Or you may have selected some input files that are nout count files?\nSo please check your input file setting (factor and factor levels).\nCheers,\nFlorian"}, {"role": "system", "text": "Hi @carolyn_nielsen\nBoth of the replies from @gallardoalba and @Flow could be potential issues. I also added a few tags to the topic that lead to prior Q&A about DEseq2/Featurecounts/GTFs. Those topics include troubleshooting help and tutorials.\nThis FAQ may also help: Help for Differential Expression Analysis"}, {"role": "user", "text": "Yes all the samples were processed using the same workflow:\nTrimmomatic \u2192 RNAStar \u2192 Filter BAM datasets on a variety of attributes \u2192 StringTie \u2192 StringTie Merge \u2192 GffCompare \u2192 FeatureCounts"}, {"role": "user", "text": "\nimage557\u00d7881 29.5 KB\n\nHi there - sample input attached here. Each of the collections is the output of FeatureCounts after I removed the first duplicate row. Each collection is a different sample, containing a different number of counts files as samples didn\u2019t all have the same number of cells."}, {"role": "user", "text": "Thanks will look at these!"}, {"role": "system", "text": "Can you check if your count files have the same number of rows and order. Meaning, have you used the same annotation for all your files?"}, {"role": "user", "text": "It should have been the same annotation as the same workflow and reference genome was used for each collection. But, when I open each of the collections shown above I can see that the count files they contain sometimes have different numbers of rows to the other collections, which I think relate (very loosely?) to the numbers in the error message above: 135280, 235928, 226602, 203217, 210838, 214950, 279139, 133575.\nFor example, the files in the first collection (n=49) seem to all say approx 220,00 lines (I haven\u2019t manually opened every single one to check):\n\nSecond (n=88) also approx. 220,000:\n\nThird (n=87) is approx 130,000:\n\nFourth (n=59) is approx 240,000:\n\nFifth (n=88) approx 230,000:\n\nSixth (n=52) approx 290,000:\n\nSeventh (n=81) approx 210,000:\n\nEighth (n=37) approx 130,000:\n\nAny thoughts on why the lane #s are different and how to resolve?"}, {"role": "system", "text": "Hi @carolyn_nielsen\n\n\n\n carolyn_nielsen:\n\nTrimmomatic \u2192 RNAStar \u2192 Filter BAM datasets on a variety of attributes \u2192 StringTie \u2192 StringTie Merge \u2192 GffCompare \u2192 FeatureCounts\n\n\nTry this order instead, and omit GffCompare. The original protocol is creating a unified GTF to base counts on after the counts have been generated and is likely the source of the problem.\n\nTrimmomatic\n\nRNAStar (or HISAT2) If using HISAT2 be sure to use the output setting to create a BAM dataset compatible with Stringtie. This option is located under Advanced Options > Spliced alignment options > Transcriptome assembly reporting > select the radio button for \u201cReport alignments tailored for transcript assemblers including StringTie\u201d)\n\nFilter BAM datasets on a variety of attributes \u2013 this is optional, and usually used to restrict the BAM contents. I\u2019m assuming you are doing this on purpose and with a purpose specific to your analysis goals.\n\nStringTie Merge \u2013 enter just your reference annotation GTF under the input Reference annotation to include in the merging. This \u201cgrooms\u201d the GTF to a standardized format Stringtie can interpret.\n\nStringtie \u2013 using the groomed GTF from step 4\n\nStringTie Merge \u2013 enter the groomed GTF from step 4 plus all of the GTFs produced by Stringtie in step 5 (one per sample). This step is optional and depends on how you want to do the counting.\n\nIf you are only interested in known transcripts/genes, skip step 6.\nIf you are interested in both known transcripts/genes + novel transcripts/genes within your reads, run step 6 to create a unified GTF. Be aware that any novel transcripts/genes that do not merge with known transcripts/genes will be named by Stringtie.\n\n\n\nFeatureCounts \u2013 run this with the GTF output of either step 4 (known only) or step 6 (known + novel), along with all BAM mapped read results.\nProceed to DEseq2 using the same exact GTF used in step 7 and the current error about count inputs having a different number of lines (genes) will resolve.\nNote: The message about the same input count file being input twice is a common usage problem, so that warning is presented to help with troubleshooting whenever the tool fails. From your screenshot and description of the inputs to DEseq2, this doesn\u2019t seem to be what is going on for your case.\nWhy are are removing extra header lines from GTFs created by Stringtie is unclear. The reference GTF for known transcript/genes may contain extra comment lines, and those should be removed, but Stringtie GTFs should contain one header line (contains sample IDs). It looks like the sample ID header line is in your data \u2013 so maybe I misinterpreted what you are doing.\n\nCreating a unified GTF (known only, or known + novel) and using that to generate counts will then be based on the same set of transcripts/genes, and the count files will have the same number of rows (one per gene included in the unified GTF). GffCompare is a useful tool but not for DEseq2 (or edger or limma).\nPrior Q&A: Searching the Galaxy\nThe first few results from that search points to Q&A that describes the solution (on our prior forum):\n\nStringTie and StringTie merge - when to apply the Guide gff (reference annotation file)?\nStringtie Merge\n\nOne issue ticket that describes the problem with the usage protocol above included (in part). This is very technical and isn\u2019t really needed \u2013 the same information is in this post and in the prior Q&A linked above: Stringtie 1.3.3 errors when the option to output Deseq2/EdgeR is used \u00b7 Issue #1322 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\nFAQ:Extended Help for Differential Expression Analysis Tools\nFor others reading (@carolyn_nielsen does have this data): If you do not have a known transcript/gene GTF, that can be omitted. But you should still Stringtie merge all of the Stringtie GTFs before running Featurecounts to generate counts. For this type of usage, all transcript/gene names will be assigned by Stringtie (random but unique identifiers \u2013 the MSTRG.N content in your data are those types of identifiers, and many would resolve/merge into Known Genes if the protocol above is followed).\nTutorials: Galaxy Training!\nHope that helps!"}], [{"role": "user", "text": "Dear Galaxy community,\nI\u2019am new to the Galaxy project. I tried using the Volcano plot tool. Everytime I got th efollowing error message: [1] \u201cHeader row detected\u201d\nError in log10(pvalue) : non-numeric argument to mathematical function\nCalls: print \u2026  \u2192 f \u2192 sca\nDoes anybody knows what to do? I\u2019am not familiar with bioinformatics or programming and really do not know what to do.\nThanks in advance.\nCS"}, {"role": "system", "text": "Dear @CSm,\nYour input-file contains a header line, which might break the tool. You can remove the header line with \" Remove beginning of a file\".\nKind regards,\nFlorian"}, {"role": "user", "text": "Dear Florian,\nthank you very much for your reply. I tried already and removed the first line in the file, but the result is almost the same. Error message: Error in log10(pvalue) : non-numeric argument to mathematical function\nCalls: print \u2026  \u2192 f \u2192 scales_add_defaults \u2192 lapply \u2192 FUN\nBest regards\nCS"}, {"role": "system", "text": "Dear @CSm,\nPlease check if your inut file is correclty formatted. I assume that one or multiple lines (pvalues) are empty or contain NaN, Inf, or something else, which cannot be applied to a mathematical function, such as, log10.\nCheers,\nFlorian"}, {"role": "user", "text": "Dear Florian,\nthank you very much for your help. I checked already all lines for all parameter, but there are numbers only. From that point of view, it should be fine.\nBest regards\nCSm"}, {"role": "system", "text": "Can you share your history? Then I could take a brief look."}, {"role": "user", "text": "Of course I can. Here it is: Galaxy | Europe | Accessible History | hPASMC Volcano plot\nThere are different files I tested with. Some are with and some are without header. Thank you very much for your help.\nBest regards\nCSm"}, {"role": "system", "text": "It looks like you have numbers for your p-values, but from a programming standpoint, you do not have. Your p-values columns contain \u201cvalues\u201d, such as, \u201c3,22212E-15\u201d, which will be converted into a factor or string in R. Your values contain commas, which is not a float/double notation.\nYou have to save your table in a numeric format.\nKind regards,\nFlorian"}, {"role": "user", "text": "Dear Florian,\nthank you very much for your advice. Is it like in the history number 20 what you mean? The \u201c3,22212E-15\u201d format looks like in the training material for the volcano plot. Or do you think I have to substitute the comma by point?\nBest regards\nCSm"}, {"role": "system", "text": "\nOr do you think I have to substitute the comma by point?\n\nYes"}, {"role": "system", "text": "Dear Galaxy community,\nI tried using the Volcano plot tool. Everytime I got th efollowing error message: [1] \u201cHeader row detected\u201d\nError in log10(pvalue) : non-numeric argument to mathematical function\nCalls: print \u2026 \u2192 f \u2192 sca\nDoes anybody knows what to do? My database looks like the picture.\nThanks in advance.\nPS\n\nCaptura de pantalla 2023-04-21 a las 15.15.461204\u00d7944 184 KB\n"}, {"role": "system", "text": "Hi @Pol_Siso_Camarasa\nDid you apply the adjustments that @Flow suggested? The screenshot shows those are not done yet.\n\nRemove the header line\nReplace commas with dots in numerical values\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi, I am having following goseq error while using two inputs shown below:\n\nimage295\u00d7697 5.8 KB\n\n\nAs a  genome file I used both gtf and fasta files of the potato genome but the error message has not changed (the file format was not indicated in the tutorial).\nHow should I proceed?"}, {"role": "system", "text": "Hi @aaak,\nthis history reproduces the analysis in which you have faced difficulties: history goseq. I suggest you have a look at it, probably you will find the cause of the problem. If you cannot find the problem, please share your history with me.\nRegards"}, {"role": "user", "text": "@galladoalba I could not find what the problem is. I tried the analysis on eu server as you shared; however, no idea about why I am having this problem. One more thing, you said :\n\n\n\n gallardoalba:\n\nthis history reproduces the analysis in which you have faced difficulties: history goseq\n\n\nwhat do you mean with that? Does the analysis in EU server affects analysis in AU server?"}, {"role": "user", "text": "I am leaving the link in case that you want  to see the history of the analysis :\nhttps://usegalaxy.org.au/u/f_kurt/h/potato37fbtolerant"}, {"role": "system", "text": "Thanks, I\u2019ll check it."}, {"role": "system", "text": "Hi @aaak,\nthe problem is that Limma goseq requires inputs ( differentially expressed genes file and gene lengths file) of the same number of entries; in order to fix the problem, you need to run Limma without filtering low expressed genes.\nRegards"}, {"role": "user", "text": "\n\n\n gallardoalba:\n\nthe problem is that Limma goseq requires inputs ( differentially expressed genes file and gene lengths file) of the same number of entries; in order to fix the problem, you need to run Limma without filtering low expressed genes\n\n\nNow, I understand in my length file I have more than 30000 genes whilst only about 25000 genes are available\u2026 I will run the analysis again with equal number of genes, I will let you know if it works\u2026 heartfelt thanks."}, {"role": "user", "text": "Nope\u2026 it did not work out."}, {"role": "system", "text": "Could you share your history with me again, please?"}, {"role": "user", "text": "To help you understand my doings, let me explain how I proceeded.\nI merged all data files as you can see in 11. and I created status and length files by cutting necessary columns. Then I ran the goseq\nthe result was the same error message. The link is below.\nhttps://usegalaxy.org.au/u/f_kurt/h/unnamed-history"}, {"role": "system", "text": "Hi @aaak,\nin that case the problem is that the gene annotation file should have this format: annotation file.\nThis paper provides some information about how to obtain the GAF (Go Annotation File) corresponding to S. tuberosum.\nRegards"}], [{"role": "user", "text": "Hello\nI download some data from EBI website. So these datas are small rna seq data and the types are Fastq.gz. Is there any way to converting them to fastqsanger, i mean some how remove the gun zip(gz). ? I try that with changing their data set manually but in mapping step it make some errors.\nThanks"}, {"role": "system", "text": "Just unpack it\u2026\nFor example with the command:\ngunzip myfile.fastq.gz\nThis is not really a galaxy question and a very basic thing to do so it is possible I understand your question wrong.\nEDIT:\nIf you are on windows I think you could use 7zip."}, {"role": "user", "text": "I am asking because i do not actually download the files. Im downloading it directly in to galaxy. So i have not any data to use gunzip or 7zip.\nI was asking about Gunzip files in GALAXY. (to get results faster, otherwise i know if i download them, i can use gunzip on ubuntu"}, {"role": "system", "text": "Then indeed I did not fully understood it, already had that feeling. Most mapping tools can handle .gz files. Maybe if you share which tool you used and the error people can better help you. Or maybe it helps if you download the files and check whats in it."}, {"role": "user", "text": "My data is fastq.gz . so the bowtie doesnt recogonize this type of dataset.  So i wondered if maybe i can convert them with some tools in galaxy"}, {"role": "system", "text": "@amir please tell Galaxy that this datatype is fastqsanger.gz I think then it will work."}, {"role": "user", "text": "i tried that. but makes some errors in mapping steps"}, {"role": "system", "text": "What kind of errors? Can you share a history?"}, {"role": "user", "text": "it says my data isnt fastq file.  it happens when i do what you said"}, {"role": "user", "text": "If still not clear i can share the history"}, {"role": "system", "text": "Hi amir - I know this is a rather belated response but bowtie2 requires that it\u2019s input files be in the .fastqsanger/.fastqsanger.gz format which is effectively the same as fastq but provides a guarantee that the PHRED quality encoding is the more recent version. You can designate your file as fastqsanger at upload, edit their format from the history or feed them through the fastq groomer tool to convert them to fastqsanger"}], [{"role": "user", "text": "I\u2019m having an issue mapping paired-end data when the data resides in a collection. I can upload 6 pairs of fastq files, make the collection, perform FastQC and Trimming. However, when I go to run minimap2, only 1 of the 6 pairs is successfully mapped; the other 5 pairs fail.  They all have the same the error:\nCould not display BAM file, error was:file has no sequences defined (mode=\u2018rb\u2019) - is it SAM/BAM format? Consider opening with check_sq=False\nI can map the 6 pairs individually, but I would much rather use collections. Why is this error specific to collected datasets?"}, {"role": "system", "text": "Hi @jsalem\nAre you running the job at a public Galaxy server? What is the server\u2019s URL?\nThanks!"}, {"role": "user", "text": "Hi Jennifer,\nThanks for your reply. I am running a local instance of Galaxy (20.0.2) with the URL http://127.0.0.1:8080."}, {"role": "system", "text": "Hi @jsalem\n\n\n\n jsalem:\n\nI am running a local instance of Galaxy (20.0.2)\n\n\nThat is not a release branch of Galaxy. Stable releases are listed here with release notes: Galaxy Documentation \u2014 Galaxy Project 22.01.dev0 documentation\nApart from that, it looks as if Samtools was not installed OR the resulting BAMs had no hits (are empty). So \u2013 this is a configuration or resource issue.\nThat same docs site has help about how to manage dependencies, custom configurations, scaling. Stable releases will now install Samtools when the instance is first started up when the defaults are used.\nPrior Q&A that may also help: Example step-by-step for a basic local Galaxy install with an admin account created"}, {"role": "user", "text": "My apologies for the Galaxy version typo (20.01). You are right that the resulting BAM files are empty, but not for all of the paired-end sets in the collection. One of the paired sets within the collection always maps correctly, while the remaining paired-end sets generate empty BAM files. When the paired sets are process individually and not put into a collection, they map just fine. I will check out the links you sent. Thank you!"}, {"role": "system", "text": "@jsalem\nI asked our dev team for some advice. This may not be a technical server issue. Note: the options below assume that your server is not hosted publically, but if it is, you could generate a share link to the history with the odd results (inputs + outputs) and post that back (here, or in a direct message).\nThings to check:\n\nAre you using the most current version of the tool? If not, upgrade and see if that resolves the problem:\n\n\n\nMap with minimap2  A fast pairwise aligner for genomic and spliced nucleotide sequences (Galaxy Version 2.17+galaxy2)\nhttps://toolshed.g2.bx.psu.edu/view/iuc/minimap2/8c6cd2650d1f\n\n\nIs the paired-end dataset collection structured correctly? Does that match the collection input options set on the tool form? There are three different ways to run paired-end data through this tool using a collection, and two different collection types (interleaved paired fastq or distinct forward/reverse paired fastq).\n\nThe database logs, and the stderr or stdout report (on the Job Details form \u2013 the \u201ci\u201d info icon) also may give some clues. If you want to post back that content we may be able to help that way.\nIf you want to post some screenshots back that show the tool settings (form plus Job Details) and the dataset collection itself plus the included datasets in the history (will be hidden by default) we might be able to help more from that info, too.\nThe final option is to see if you can reproduce this behavior at a public Galaxy server (usegalaxy.org, usegalaxy.eu, usegalaxy.org.au). A collection with at least two paired-end elements would be enough. That would allow you to generate a share link to the history and allow for a quicker review \u2013 plus if the result is different (BAMs are not empty) \u2013 is another way to narrow down what might be going wrong.\nThanks!"}, {"role": "system", "text": "Hi @jsalem,\nI\u2019ve tried reproducing your issue with three different versions of minimap2 (going back as far as version 2.12, but all of them worked just fine for me with a collection of 2 PE sequenced samples.\nSo I doubt this is a problem with the tool, but rather a problem with the collection structure as @jennaj guessed already.\nAre you sure each PE dataset in the collection contains the fw and rv reads of one sample?"}, {"role": "user", "text": "Thank you both for the feedback. I am trying on the public server today to see if the problem arises there. I suspect the problem is specific to my setup.\nI agree, the problem is not specific to a version of minimap2, or minimap2 itself (BWA has the same issue for me). The issue arises when >1 alignments are executed at one time.\nAs I described above, when fastq files (e.g. SampleA_F1 and R1, SampleB_F1 and R1) were uploaded as a \u201clist-of-pairs\u201d, Only SampleB generated alignment data, while SampleA was empty. Not only are there no alignments for SampleA but, the stdout and stderr are empty as well.\nTo troubleshoot, I instead selected \u201cpaired\u201d for the collection type, and uploaded each set individually (so SampleA is one collection and SampleB is another collection). If I wait for the minimap2 job on SampleA to finish prior to staring another minimap2 job, then both alignments work. But, if I start a minimap2 job for SampleB while minimap2 for SampleA is running, then SampleB generates an empty bam file and SampleA works.\nCould it have something to do with the reference sequence? Can 2 jobs utilize the same reference sequence file at the same time?"}, {"role": "system", "text": "\n\n\n jsalem:\n\nCould it have something to do with the reference sequence? Can 2 jobs utilize the same reference sequence file at the same time?\n\n\nThis should work fine, in general. With minimap2, specifically, and also with BWA-MEM (if you are using a fasta genome from your history), both jobs will build separate index structures for the reference, which can require lots of memory.\nSo one potential issue could be that your setup tries to run both jobs on the same machine and that machine does not have enough memory for both. Maybe one of the two gets killed without writing anything to stdout/stderr. Pure speculation though since I don\u2019t know anything about your setup.\nAnyway, the place to look for more information would be at the level of Galaxy then, not at the tool level. You may want to look in Galaxy\u2019s log, or in the terminal from which you run Galaxy if you\u2019re not redirecting to a log file."}, {"role": "user", "text": "Mapping collections worked correctly on public galaxy\u2026\nI am using a built-in reference sequence. Do you know how much memory I need? (my virtualbox uses 20 of the 32 GB on my PC). Is there anything I should look for in the terminal? Where is the log file stored?"}, {"role": "user", "text": "I think it is a memory issue. I dropped the allocated memory to my virtualbox from 20 to 10 GB and, instead of 1 alignment working, both failed. If anyone knows a way to decrease minimap memory usage, please let me know."}], [{"role": "user", "text": "Hi,\nI\u2019m having trouble running the GeneFamilyClassifier tool on the main galaxy server. I have no problem running the command line version of this tool.\nHere is what I see. I can\u2019t even find what the problem is\u2026\n\nScreen Shot 2022-10-27 at 2.23.19 PM580\u00d7692 40.8 KB\n\n\nAny help is appreciated.\nBest"}, {"role": "user", "text": "I\u2019m trying to run this tool again, and the it has been queued for an hour\u2026"}, {"role": "user", "text": "\nScreen Shot 2023-03-07 at 12.35.22 PM1322\u00d7911 84.1 KB\n\nThis is the job information from my last attempt in October"}, {"role": "user", "text": "\nScreen Shot 2023-03-07 at 12.34.22 PM1289\u00d7787 73.9 KB\n\nThis is the job information from my attempt today."}, {"role": "system", "text": "Hi @Huiting120\nWould you please send in a bug report from the most recent error? Thanks!"}, {"role": "user", "text": "Hi @jennaj\nThanks! Where can I find the bug report?"}, {"role": "system", "text": "\n\n\n Huiting120:\n\nWhere can I find the bug report?\n\n\nClick on the ladybug icon at the bottom of a red error dataset. You can add a link to this topic for context in the free comment section.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Troubleshooting errors\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "I do not see the ladybug icon when I opened the red error dataset.\n\n\nScreen Shot 2023-03-09 at 12.06.26 PM286\u00d7925 10 KB\n"}, {"role": "system", "text": "Ah, ok. I missed that. It is a collection without any outputs, so no individual datasets or bug icons.\nThe other option is to generate and post back a history share link. Please leave all inputs and outputs undeleted. Sharing your History"}, {"role": "user", "text": "Here is a link to the history: Galaxy\nFor the two latest errors (boxes 17 and 18), the input data is in box 16. Box 17 was run in Oct 2022 and Box 18 was ran this week.\nThank you very much!"}, {"role": "system", "text": "Ok, thanks. I\u2019ve updated an issue ticket here with the information from this tool and the error messages: Fix dependency for plant_tribes_kaks_analysis 1.0.3.0 \u00b7 Issue #535 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub\nThe other tool in that ticket appears to be working Ok, but theoretically has an update pending.\nThis fix won\u2019t be immediate. Please follow that ticket for updates. Even if we split it off, all will be linked together. You can unshare your history now."}], [{"role": "user", "text": "Hello, I am trying to locally install an instance of Galaxy on a MacOSX (High Sierra), and am getting the following error when I try the following command. Please help!\nsh run.sh\nInstalling node into /usr/local/Cellar/galaxy/.venv with nodeenv.\n * Install prebuilt node (10.13.0) .Traceback (most recent call last):\n  File \"/usr/local/Cellar/galaxy/.venv/bin/nodeenv\", line 10, in <module>\n    sys.exit(main())\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 1076, in main\n    create_environment(env_dir, opt)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 905, in create_environment\n    install_node(env_dir, src_dir, opt)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 692, in install_node\n    download_node_src(node_url, src_dir, opt, prefix)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 542, in download_node_src\n    dl_contents = io.BytesIO(urlopen(node_url).read())\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 564, in urlopen\n    return urllib2.urlopen(req)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 154, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 429, in open\n    response = self._open(req, data)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 447, in _open\n    '_open', req)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 407, in _call_chain\n    result = func(*args)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1241, in https_open\n    context=self._context)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1198, in do_open\n    raise URLError(err)\nurllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:727)>\n"}, {"role": "system", "text": "Not sure I understand what\u2019s going on with the paths here \u2013 is this a brew (based on /usr/local/Cellar) or system (/Library/Frameworks/\u2026) Python?\nA bit of digging leads me to think this error can arise when Python isn\u2019t compiled with SSL support."}, {"role": "user", "text": "This is a System Python, v2.7 installed in /Library/Frameworks and usually comes with SSL support.\nThe galaxy was cloned into /usr/local/Cellar though, would this affect it?"}, {"role": "system", "text": "Ok, that clears up the path question.  /usr/local/Cellar is usually only used for homebrew related stuff, so I probably wouldn\u2019t leave Galaxy here long term, but I don\u2019t think that\u2019s the problem here.\nFrom /usr/local/Cellar/galaxy can you execute these two commands?\nsource .venv/bin/activate python -c \"import ssl; print(ssl.OPENSSL_VERSION)\""}, {"role": "user", "text": "I get the following:\n\nOpenSSL 1.0.2q  20 Nov 2018\n\nAs far as I can tell, the SSL support is working, but it is unable to connect to the URL?"}, {"role": "system", "text": "Yeah, seems like your python isn\u2019t likely the culprit based on that.  Any institutional or local firewalls?  Is it the same error when you retry?\nCan you manually source .venv/bin/activate and then try to do a nodeenv -p -n 10.13.0, and see what that output is?"}, {"role": "user", "text": "I am at a University Institution - if we have firewalls, I am not sure what they are.\nAgain, I get the same error. It has not changed during any of these attempts to sh run.sh\n\n\nInstall prebuilt node (10.13.0) .Traceback (most recent call last):\nFile \u201c/usr/local/Cellar/galaxy/.venv/bin/nodeenv\u201d, line 10, in \nsys.exit(main())\nFile \u201c/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\u201d, line 1076, in main\ncreate_environment(env_dir, opt)\nFile \u201c/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\u201d, line 905, in create_environment\ninstall_node(env_dir, src_dir, opt)\nFile \u201c/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\u201d, line 692, in install_node\ndownload_node_src(node_url, src_dir, opt, prefix)\nFile \u201c/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\u201d, line 542, in download_node_src\ndl_contents = io.BytesIO(urlopen(node_url).read())\nFile \u201c/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\u201d, line 564, in urlopen\nreturn urllib2.urlopen(req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 154, in urlopen\nreturn opener.open(url, data, timeout)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 429, in open\nresponse = self._open(req, data)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 447, in _open\n\u2018_open\u2019, req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 407, in _call_chain\nresult = func(*args)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 1241, in https_open\ncontext=self._context)\nFile \u201c/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\u201d, line 1198, in do_open\nraise URLError(err)\nurllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:727)>\n\n"}, {"role": "user", "text": "Update on this, my issue was that I have urllib3 installed, but it was looking for urllib2, hence the errors. I have updated the galaxy path to use urllib3 and it is now installed successfully"}], [{"role": "user", "text": "Hello,\nI am having a weird issue trying to push my tool (a wrapper for alevinQC) to the main toolshed. I used planemo to prepare my submission to the toolshed.\nI have a working version on the test toolshed: https://testtoolshed.g2.bx.psu.edu/view/florian.wuennemann/alevinqc/49fe36f5ad92\nI tried pushing to the main toolshed using planemo and it created an entry but somehow that entry is not findable by others in the toolshed and I can\u2019t install it in my galaxy from the main toolshed:\nhttps://toolshed.g2.bx.psu.edu/view/fwuennemann/alevinqc/000000000000\nWhen I try to interact with the main toolshed entry via planemo, it tells me it doesn\u2019t exist, but when I try to create a new toolshed entry, it tells me I own a tool with that name\u2026 (see code below)\nThanks for the help!\nplanemo shed_create --shed_target toolshed\nUnexpected HTTP status code: 400: {\"err_msg\": \"You already own a repository named <b>alevinqc<\\/b>, please choose a different name.\", \"err_code\": 400008}\n\nplanemo shed_diff --shed_target toolshed\nshed_diff: Repository [alevinqc] does not exist in the targeted Tool Shed.\n\n planemo shed_update --check_diff --shed_target toolshed\nRepository [alevinqc] does not exist in the targeted Tool Shed.\nFailed to update repository 'alevinqc' as it does not exist on the main Tool Shed.\n"}, {"role": "system", "text": "Hi @florian.wuenneman!\nCan you please try planemo shed_upload --shed_target toolshed ./alevinqc/\nHope that works,\nBjoern"}, {"role": "user", "text": "Thank you for the help, Bjoern!\nI tried to run shed_upload inside the tool folder from my local machine and outside the folder specifying it. In both cases, it returned the following:\ngit rev-parse HEAD\ngalaxy.util.commands WARNING: Passing program arguments as a string may be a security hazard if combined with untrusted input\ngit diff --quiet\nRepository [alevinqc] does not exist in the targeted Tool Shed.\n\nDo you have any other suggestions?\nThanks,\nFlorian"}, {"role": "system", "text": "\n\n\n florian.wuenneman:\n\nalevinqc\n\n\nAppears to be something weird going on inside the toolshed - when I following this link on the toolshed to your repository found with a name search: https://toolshed.g2.bx.psu.edu/repository/browse_repositories?f-free-text-search=alevinqc&sort=name&operation=view_or_manage_repository&id=4ad514afab719a60\nI see a strange cascade of windows - never seen that. Looks like the toolshed is in a state of confusion about that tool "}, {"role": "user", "text": "Thanks for confirming that this weird cascading effect is going on in the toolshed @fubar !\nI think because the upload of the tool to toolshed failed, it\u2019s stuck in this state of having the entry but not having actual metadata associated with it. I can see something for this package when I am logged in to my toolshed account\u2026\nIs there a way to delete the current alevinqc from the main toolshed and just restart the process? @bjoern.gruening or anyone else familiar with the toolshed?"}, {"role": "system", "text": "I think this is a different bug due to the rewrite."}, {"role": "system", "text": "Can you add --force_repository_creation to your command?"}, {"role": "user", "text": "Unfortunately, still getting an error:\ngit rev-parse HEAD\ngalaxy.util.commands WARNING: Passing program arguments as a string may be a security hazard if combined with untrusted input\ngit diff --quiet\nUnexpected HTTP status code: 400: {\"err_msg\": \"You already own a repository named <b>alevinqc<\\/b>, please choose a different name.\", \"err_code\": 400008}\nRepository [alevinqc] does not exist in the targeted Tool Shed.\n"}, {"role": "user", "text": "Does anyone else have any other suggestions? It seems like I am stuck at this point unfortunately, unable to publish the tool on the main toolshed right now\u2026\nOne idea is to just resubmit the tool with a slightly different name to make it work?\nFor example alevinqc_galaxy?\nThank you for the help everyone!"}, {"role": "system", "text": "The resubmitting with different repo name will most probably work. At this point I suspect the mercurial repository corresponding to your repo on the toolshed side is in a bad shape and needs some manual reset/fixing."}, {"role": "user", "text": "I was finally able to fix this issue, by logging into the toolshed and uploading a tarball directly on the defective repo page for alevinqc. Now the entry seems to be correct and installable via toolshed. Any confirmation of correct install from a second source would be appreciated!\nhttps://toolshed.g2.bx.psu.edu/view/fwuennemann/alevinqc/5f0da20666fa"}], [{"role": "user", "text": "I am following the workflow on RNA-seq data analyses in galaxy but I have been having some challenges. I ran trimmomatic on my fastq files but it always return an error, even when the files did not give any error when I ran fastqc on them. I went further to align them to the human genome using hisat2 and it came out well, but when I ran the differential analyses with DESeq2, I encountered same error, please what could be the problem and how can I resolve this? Thank you."}, {"role": "system", "text": "I don\u2019t know if I can help but also others will not be able to help because your question is missing the error. Can you share which error you get exactly?"}, {"role": "user", "text": "Hi,\nPlease see the error message I received after running it. Thank you.\nStderr: Trimmomatic did not finish successfully\n\nStdout\n\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/jetstream2/scratch/main/jobs/45084131/_job_tmp -Xmx28g -Xms256m\nTrimmomaticSE: Started with arguments:\n -threads 8 fastq_in.fastqsanger.gz fastq_out.fastqsanger.gz ILLUMINACLIP:/usr/local/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:4:20\nUsing PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'\nILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nError: Unable to detect quality encoding\n"}, {"role": "system", "text": "Not sure if you can do anything about it now. I came across this one Trimmomatic Error: Unable to detect quality encoding and I don\u2019t see the option to choose the phred encoding. There is already an github isseu Trimmomatic error: unable identify Phred quality encoding \u00b7 Issue #79 \u00b7 fls-bioinformatics-core/galaxy-tools \u00b7 GitHub. I think for now you could maybe try to use another tool like cutadapt or Trim Galore. (Trim galore uses cutadapt in the background but it has an \u201cautomatic detection feature\u201d)"}], [{"role": "user", "text": "Hi there,\nI have been running a Trinity assembly to create a reference transcriptome and a gene-to-transcript map. It has been running for almost two weeks and I am wondering how I can see the progress or an estimated time for completion?\nRecently, it switched from \u201cthis job is currently running\u201d to \u201cthis job is waiting to run\u201d, and I would also like to know why this happened. I have been on a time crunch for a project and would really really love to have some sort of time estimate for when this tool can finish.\nThank you for your help!\nJulie"}, {"role": "system", "text": "Hi @jradfor8,\nunfortunately, is not possible to evaluate the progress of your process; which Galaxy instance are you using?\nRegards"}, {"role": "user", "text": "Thanks for getting back to me. I am not sure what you mean by which \u201cinstance\u201d, but I am on the usegalaxy.org server. Let me know if you need more information.\nThanks again,\nJulie"}, {"role": "system", "text": "I had the same question here. My Trinity jobs are still waiting to run."}, {"role": "system", "text": "I\u2019ll try to provide both of you with more information as soon as possible.\nRegards"}, {"role": "system", "text": "Hi @jradfor8 & @Sammy\nThe cluster at UseGalaxy.org that runs Trinity is very busy.\nThe best strategy is to start up the jobs, then allow them to complete. If you delete/rerun, that new job goes back at the end of the queue, extending wait time.\nOnly delete/rerun if you already know the inputs have problems and need to rerun anyway. Stalled jobs can also be due to issues with upstream jobs, and sending those outputs as inputs to Trinity. Please check to make sure upstream/input datasets are in a successful state (\u201cgreen\u201d).\nFew more tips:\nMake sure that you ran some QA/QC on the inputs. Trinity requires that paired inputs are still paired after QA. Trimmomatic can do both and you can learn about your input quality with FastQC.\n\nGTN Tutorials that include QA steps: Galaxy Training!\n\nGTN Tutorial about Trinity (not published yet, may still be helpful): De novo transcriptome assembly, annotation, and differential expression analysis\n\nGalaxy Help posts about QA/QC in various contexts: Search results for 'qa-qc' - Galaxy Community Help && Search results for 'trimmomatic' - Galaxy Community Help\n\n\nThanks @gallardoalba for triaging these questions "}, {"role": "user", "text": "Hi there!\nI have not deleted or rerun the job at all.\nMy input files are in the successful state, and have already been QC\u2019d appropriately. Is there any possible way that you can unpause this job? It is stuck in the \u201cwaiting to run\u201d phase, even though previous to being stopped it was running nonstop for two weeks (in the yellow phase).\nThanks for your reply and help."}, {"role": "system", "text": "Hi @jradfor8\n\n\n\n jradfor8:\n\nIs there any possible way that you can unpause this job?\n\n\nA \u201cgray\u201d colored job is waiting for resources to become available. Meaning, it is in the job queue already.\n\n\n\n jradfor8:\n\nIt is stuck in the \u201cwaiting to run\u201d phase, even though previous to being stopped it was running nonstop for two weeks (in the yellow phase).\n\n\n\n\n\n jradfor8:\n\nI am on the usegalaxy.org server\n\n\nA \u201cyellow\u201d job is actively executing. It will end in either a \u201cgreen\u201d (successful) or \u201cred\u201d (error) state at UseGalaxy.org. Some other public Galaxy servers do this a bit differently, and will automatically rerun a job one or more times if it originally failed. An example public Galaxy server that automatically reruns all failed jobs one time is UseGalaxy.eu.\nThat said, the cluster that runs Trinity jobs at UseGalaxy.org did undergo some configuration changes over the last month. Some jobs were impacted and administratively rerun. My guess is that is what happened in your case as you explain it, and from what I can see in your account.\nThe Trinity job was started on Feb 12. It is batched with 8 total paired-end inputs, pooled = yes, and no QA/QC was done on the reads, or rather, no QA was included in the history where this work is included. The data appears to have been downloaded from SRA and directly input to Trinity (?).\n\u201cPooled\u201d can be set to either yes or no.\n\nFor \u201cpooled = yes\u201d, that means all inputs will be combined into one assembly. The datasets are quite large to be pooled, between ~10-20 GB for each end of each pair. An assembly at a public Galaxy server (any) is unlikely to be successful with that much data run as \u201cpooled = yes\u201d. It will run out of resources and fail, for either runtime or memory reasons.\nFor \u201cpooled = no\u201d, there will be one assembly job/result per paired-end read input. Some of your pairs would probably assemble, but it is unlikely that all would, especially the first pair in the collection as it is now (the largest, with no QA).\n\nWe granted you some extra quota space already, so sent you links about the ways to use Galaxy, but I also added them below again for others reading. Public servers usually not a good choice for any single job that involves a very large amount of data. Many smaller individual jobs are usually fine. Quota space is storage space, and unrelated to the amount of memory/resources a tool uses when executing. Public Galaxy servers have significant computational resource allocations but not as much as what you can set up yourself in your own Galaxy (for practical reasons).\nWhat to do from here:\nIf you didn\u2019t intend to pool the data, then you should rerun it as \u201cpooled = no\u201d. You can still use a collection input. Definitely do some QA first. If any of the jobs fail, then you\u2019ll either need to adjust the inputs (downsample the reads with a tool like Seqtk) or run the job at a Galaxy server with more resources allocated (your own).\nIf you did intend to pool the data, expect this job to fail for resource reasons. QA won\u2019t make a difference by itself at UseGalaxy.org, but you should do some QA if/when you decide to run the job at your own Galaxy server. Downsampling + QA might help at UseGalaxy.org, but you would need to test that to see how much is needed to get a successful job \u2013  and decide if the assembly using that much downsampling is still of value to you. The maximum total size of all inputs sent to a Trinity job is between 20-35 GB when working at public servers. This is not exact since the size of the data is just one factor \u2013 the read content is another (and is why a QA step is strongly recommended).\nWays to use Galaxy:\n\n\nWebinar: Webinar: Use Galaxy on the web, the cloud, and your laptop too\n\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources\n\n\nDecision matrix: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources\n\n\nThe GVL version of Cloudman is one choice and AWS offers grants. AWS Programs for Research and Education\n\n\nHope that helps you to make some decisions about how to proceed.\nJen"}], [{"role": "user", "text": "Hi,\ngenome.ucsc.edu is down due to a disk array failure last night. It\u2019ll come back soon, but in the meantime, can Galaxy users open their files on genome-euro.ucsc.edu ? Is there any workaround for them to get the URL to the file on Galaxy and paste it on the genome-euro.ucsc.edu ?\nThanks!\nMax"}, {"role": "user", "text": "A UCSC user wrote to our support desk:\n\nI have tried using the eu site, but I need to be able to add custom tracks from Galaxy. When opening the tracks Bia galaxy, the default is to open them in the main version of UCSC, therefore I cannot get them to open.\nI have tried adding the tracks manually, but one is a BAM file and the other is a VCF, and when I try to upload these they both either come out blank or display an error. Do you have any advice on how I can get galaxy data to be displayed in the eu version rather than the main browser, as I think that would fix my issue?\n\nDoes this mean that Galaxy blocks requests based on IP address? Could you unblock genome-euro.ucsc.edu and genome-asia.ucsc.edu ?"}, {"role": "system", "text": "\n\n\n Maximilian_Haeussler:\n\nGalaxy users open their files on genome-euro.ucsc.edu\n\n\nHi @Maximilian_Haeussler\nWhat to try:\n\nHave the user set the Galaxy history to a shared state\nAttempt to copy/paste the dataset URL\nIP address wouldn\u2019t be a factor as far as I know\nThe type of data would matter. I don\u2019t think any UCSC sites support a BAM upload, and not sure about VCF. Or maybe that changed on your end? These data are usually hosted at Galaxy, then the direct link to UCSC transfers slices of data and not the entire file at once.\n\nPlease let me know what you think. "}, {"role": "user", "text": "Yes, I understand that UCSC does not store data, it\u2019s loaded via URL from Galaxy.\nOK, the outage is resolved now, but this will come up again: what is the easiest way for a user to open a datafile on genome-euro.ucsc.edu and not genome.ucsc.edu ?\nI suspect that the \u201copen in UCSC\u201d button in Galaxy has some special sharing to allow the file to be opened by genome.ucsc.edu only? Opening a file in UCSC doesn\u2019t require the user to put it into a shared state, right?"}, {"role": "system", "text": "Hi @Maximilian_Haeussler\n\n\n\n Maximilian_Haeussler:\n\nYes, I understand that UCSC does not store data, it\u2019s loaded via URL from Galaxy.\n\n\nThis is how I also understood it. The direct link-out hosts the data in slices. Uploading directly by URL fails.\nFrom what I could guess by testing loading data to UCSC (custom track view) by URL:\n\n\nVCF worked fine for a very small file (header + one data line). I didn\u2019t test a larger file. Maybe there is a data size limit and that is what the UCSC user was attempting?\n\nBAM failed each attempt, even with a small file. It seems the function is looking for a myfile.bam.bai index file to upload along with the myfile.bam. If I added in both URLs into the same data loading submission, that also failed. It seems the that function might be looking in the same place as the bam URL location for the associated index but that is just a guess.\n\n\n\n\n Maximilian_Haeussler:\n\nOK, the outage is resolved now, but this will come up again: what is the easiest way for a user to open a datafile on genome-euro.ucsc.edu and not genome.ucsc.edu ?\n\n\nThere is a global server setting to point to a \u201cUCSC server\u201d for all functions. Right now, that is a single hardcoded value. Some ideas that came up yesterday are below. I\u2019m not stuck on a specific solution  and maybe we can discuss with stakeholders + developers.\n\nMake a change on the Galaxy side to store multiple UCSC targets.\n\n\nHave those all accessible to the admins for quick updates. This seems like it would be difficult to communicate to all server admins within a timeframe that would be worth it and would take some work to do (backend + UI/UX). It expects admins to be available for those \u201cquick updates\u201d and users would have to wait.\nAdd in a user preference (per account) that the user can set, to control the target UCSC server for both link-outs to UCSC and getting data from the Table Browser. I like this idea the best but it would take some work to implement.\n\n\nMake a change on the UCSC side to auto-redirect to the preferred UCSC server.\n\n\nThe redirect would controlled by UCSC admins in real time and would involve some work.\nMight be complicated to tune the handshake, on both sides.\nUser would not have control over this but is also has the least friction versus how it works now.\n\n\n\n\n Maximilian_Haeussler:\n\nI suspect that the \u201copen in UCSC\u201d button in Galaxy has some special sharing to allow the file to be opened by genome.ucsc.edu only? Opening a file in UCSC doesn\u2019t require the user to put it into a shared state, right?\n\n\nThe open in UCSC link-outs involve a handshake between the servers. The links will only show up in Galaxy if the datatype and database/dbkey for that data are appropriate for UCSC. Sharing state is not factor for this type of transfer, instead: is that data in a Galaxy account that is currently logged into and is the target UCSC server available. If yes, the data is sent.\nTransferring data by URL is a bit different. Those URLs are valid for anywhere. If you know the link, the data can be transferred and read at the destination. The reason the URL data transfer might need to have the history sharing state set is because of the administrative \u201cGDPR-mode\u201d some servers apply. Default for GDPR is any data retrieval by URL is restricted/private unless specifically granted by setting the history (or account) permissions.\nUsers can specify the data sharing state (by URL) as an account level user preference \u2013 or per history. I wasn\u2019t sure where that user was working so suggested setting the history to shared in case permissions were the problem on the Galaxy side. Sharing state might have been an issue for the user\u2019s VCF (or it was too large?) but it wasn\u2019t for my test file. Their BAM would have failed since what UCSC is trying to read and the single URL for the data don\u2019t quite match up for what UCSC needs. Solving the data/URL send/read for the BAM file type is another thing that could be tuned up, but maybe not as the primary solution since the file sizes will be a limit. Addressing the link-out function seems more important but all this can be discussed "}, {"role": "user", "text": "\n\n\nVCF worked fine for a very small file (header + one data line). I didn\u2019t test a larger file. Maybe there is a data size limit and that is what the UCSC user was attempting?\n\n\nI don\u2019t think that Galaxy uses that. Yes, you can upload small VCF files, but not big ones. We are not Galaxy, we cannot store big files for users for too long. We\u2019ll add something like this, but don\u2019t have the feature yet.\n\n\n\nBAM failed each attempt, even with a small file. It seems the function is looking for a myfile.bam.bai index file to upload along with the myfile.bam. If I added in both URLs into the same data loading submission, that also failed. It seems the that function might be looking in the same place as the bam URL location for the associated index but that is just a guess.\n\n\nYou cannot upload BAM files at all into UCSC. You can only paste a URL to a BAM file into the custom track box, and yes, it expects the .bai next to it, but you can use the setting bigIndexUrl= to point to the bai file, if it\u2019s not stored next to the bam.\nA line like this should work:\ntrack=test type=bam bigDataUrl=URL-to-BAM bigIndexUrl=Url-to-BAI\nthis is documented here, but should probably more widely highlighted, I added notes to various doc pages now. Thanks for bringing this up.\nhttps://genome.ucsc.edu/goldenPath/help/trackDb/trackDbHub.html#bigDataIndex\n\nAdd in a user preference (per account) that the user can set, to control the target UCSC server for both link-outs to UCSC and getting data from the Table Browser. I like this idea the best but it would take some > work to implement.\n\nThis seems technically the easiest route to me, as a galaxy-naive programmer. Has the added advantage that local server admins can point to the EU server, e.g. if data protection is an issue. Has the added advantage that user could point to their \u201cown\u201d UCSC mirror on-site when they work with Galaxy.\n(It\u2019s a lot easier now than in the past to setup a UCSC mirror site. We have a download+click VM image now and a docker container. You can have your own mirror soon with a single \u201cdocker run\u201d command)\n\n\nMake a change on the UCSC side to auto-redirect to the preferred UCSC server.\n\n\nWe cannot redirect when the UCSC server is down.\n\n\nThe open in UCSC link-outs involve a handshake between the servers. The links will only show up in Galaxy if the datatype and database/dbkey for that data are appropriate for UCSC. Sharing state is not factor for this type of transfer, instead: is that data in a Galaxy account that is currently logged into and is the target UCSC server available. If yes, the data is sent.\n\n\nPlease note that now we have thousands more genomes available than before, a lot of GCA_ and GCF_ genomes we can handle now. We have many many more genomes than shown on the tree on hgGateway: https://hgdownload.soe.ucsc.edu/hubs/. This should probably a different ticket: support NCBI Assemblies for UCSC linkouts. We have a text file with these accessions (see the URL before) and Galaxy could pull the list once per night. But yes, different ticket.\n\nTransferring data by URL is a bit different. Those URLs are valid for anywhere. If you know the link, the data can be transferred and read at the destination. The reason the URL data transfer might need to have the history sharing state set is because of the administrative \u201cGDPR-mode\u201d some servers apply. Default for GDPR is any data retrieval by URL is restricted/private unless specifically granted by setting the history (or account) permissions.\n\nThis makes sense. I didn\u2019t know about GDPR mode. Thanks!\n\nUsers can specify the data sharing state (by URL) as an account level user preference \u2013 or per history. I wasn\u2019t sure where that user was working so suggested setting the history to shared in case permissions were the problem on the Galaxy side. Sharing state might have been an issue for the user\u2019s VCF (or it was too large?) but it wasn\u2019t for my test file. Their BAM would have failed since what UCSC is trying to read and the single URL for the data don\u2019t quite match up for what UCSC needs. Solving the data/URL send/read for the BAM file type is another thing that could be tuned up, but maybe not as the primary solution since the file sizes will be a limit. Addressing the link-out function seems more important but all this can be discussed \n\nI don\u2019t fully understand, but it sounds as if BAM loading onto UCSC is broken fundamentally, it shouldn\u2019t be. If this is really true (I have trouble believing it, I\u2019m relatively sure that I displayed a BAM file from Galaxy on UCSC years ago\u2026), then this sounds like another ticket, to add the bigDataIndex=xxx to the custom track line."}, {"role": "system", "text": "\n\n\n Maximilian_Haeussler:\n\nA line like this should work:\ntrack=test type=bam bigDataUrl=URL-to-BAM bigIndexUrl=Url-to-BAI\n\n\nGreat! I didn\u2019t know about that option.\nThe submission was successful for smaller pair of BAM + BAI files. Both URLs are available in the Galaxy UI. But, there was an error message. Do you recognize what is going wrong? Or, are you able to load a BAM from a Galaxy history by URLs? A working example would be helpful.\nTest history: Galaxy\n\nScreen Shot 2023-04-21 at 10.29.50 AM2012\u00d7360 100 KB\n\nWhat I entered in the second box of the custom track form:\n\ntrack=test type=bam bigDataUrl=https://usegalaxy.org/api/datasets/f9cad7b01a472135b6ce5d10a7ecd18b/display?to_ext=bam bigIndexUrl=https://usegalaxy.org/api/datasets/f9cad7b01a472135b6ce5d10a7ecd18b/metadata_file?metadata_file=bam_index\n\n\nTo check if that particular BAM was a problem, I tried with another. The form will not submit. Maybe I trigged some kind of block with too many failures.\nThis is the string for that data (dataset 5 in the test history). The URLs are captured from the disk icon.\n\ntrack=testbam type=bam bigDataUrl=https://usegalaxy.org/api/datasets/f9cad7b01a47213532ebe265768e88e2/display?to_ext=bam bigIndexUrl=https://usegalaxy.org/api/datasets/f9cad7b01a47213532ebe265768e88e2/metadata_file?metadata_file=bam_index\n\nAnd, I tried going from another server. The submission form went through, but the data didn\u2019t show up on the \u201cManage Custom Tracks\u201d view. So, three different results . I\u2019m probably doing something wrong.\nHistory: Galaxy | Europe\nSting used at UCSC\n\ntrack=test type=bam bigDataUrl=https://usegalaxy.eu/api/datasets/4838ba20a6d86765d1ffc92a1647911d/display?to_ext=bam bigIndexUrl=https://usegalaxy.eu/api/datasets/4838ba20a6d86765d1ffc92a1647911d/metadata_file?metadata_file=bam_index\n"}, {"role": "user", "text": "Hi Jennifer,\nI typed too quickly, and should have tested the custom, my colleagues looked at it, and this one is correct:\ntrack type=bam bigDataUrl=https://usegalaxy.org/api/datasets/f9cad7b01a472135b6ce5d10a7ecd18b/display?to_ext=bam bigDataIndex=https://usegalaxy.org/api/datasets/f9cad7b01a472135b6ce5d10a7ecd18b/metadata_file?metadata_file=bam_index\nIt\u2019s called \u201cbigDataIndex\u201d (as I linked in the docs, but I mistyped the tag in the example).\nBut it seems that what prevents this from working is the part after the \u201c?\u201d. QA tried downloading the files into a local webserver and that works, so it\u2019s not the files. While we\u2019re looking into it, does Galaxy have a way to access files without the \u201c?\u201d query parameters?"}, {"role": "system", "text": "\n\n\n Maximilian_Haeussler:\n\nWhile we\u2019re looking into it, does Galaxy have a way to access files without the \u201c?\u201d query parameters?\n\n\nNot that I know of. Let\u2019s bring in others to confirm. Chat link: You're invited to talk on Matrix"}, {"role": "system", "text": "Hello,\nThank you for using the UCSC Genome Browser and reporting your issues.\nAn engineer ran the following tests and no issues were reported:\n\nFetch test on the Galaxy BAM file\nFetch test via a Genome Browser HTTP library on the Galaxy BAM file\nFetch test on the Galaxy BAM index file\n\nHowever, a fetch test via a Genome Browser HTTP library on the Galaxy BAM index file failed. It appears that the metadata response does not support the HEAD command, which can be seen with \u201ccurl -I\u201d:\ncurl -I \u201chttps://usegalaxy.eu/api/datasets/4838ba20a6d86765d1ffc92a1647911d/metadata_file?metadata_file=bam_index\u201d HTTP/1.1 404 Not Found\nThe HEAD method doe work on the main Galaxy BAM file:\ncurl -I \u201chttps://usegalaxy.eu/api/datasets/4838ba20a6d86765d1ffc92a1647911d/display?to_ext=bam\u201d HTTP/1.1 200 OK\nWe recommend adding support for the HEAD method for metadata.\nI hope this is helpful. If you have any further questions, please reply to genome@soe.ucsc.edu.\nAll messages sent to that address are archived on a publicly accessible Google Groups forum.\nIf your question includes sensitive data, you may send it instead to genome-www@soe.ucsc.edu.\nJairo Navarro\nUCSC Genome Browser"}, {"role": "system", "text": "\n\n\n jnavarr5:\n\nWe recommend adding support for the HEAD method for metadata.\n\n\nGreat, thank you @jnavarr5 for sorting out the exact issue!\nIssue ticket: Enhancement: Add functionality for direct BAM index retrieval via URL from Galaxy to UCSC \u00b7 Issue #16074 \u00b7 galaxyproject/galaxy \u00b7 GitHub"}, {"role": "system", "text": "the HEAD request for metadata files has been implemented in Allow HEAD request for requesting metadata files by martenson \u00b7 Pull Request #16113 \u00b7 galaxyproject/galaxy \u00b7 GitHub and will be available in the next release"}], [{"role": "user", "text": "Silly, simple question,  but I can\u2019t find a good solution.\nI have 2 VCF files being produced in a workflow, and I\u2019d like to combine these files as a single VCF.  The bcf_concat tool will do it, but how do I feed both files to the tool in a workflow?\nCheers,\nMat"}, {"role": "system", "text": "For a tool with a multiple data input, you should be able to drag multiple datasets onto the same input circle in the workflow editor. For example:\n\nimage1872\u00d7870 124 KB\n"}, {"role": "user", "text": "Apologies, I should have been more specific.  The inputs start out as a paired collection, the aligner combines into a single bam, and then I would like to run 2 variant callers and combine the output vcf\nSomething like this\u2026\n\nimage1452\u00d7778 88.8 KB\n\nThis doesn\u2019t seem to work on a collection. What am I doing wrong  ?\nCheers,\nMat"}, {"role": "system", "text": "Looks like you don\u2019t have two VCF files but two collections of VCF files (ribbons as inputs). Do you want to merge the collections element wise, creating a new collection of combined VCFs?"}, {"role": "user", "text": "yep "}, {"role": "system", "text": "Ideally you could do with collections what @jxtx showed for regular datasets (two or more datasets going into an input that accepts multiple datasets). It does become ambiguous if you try to do this with collections (should the input collections be fully consumed and you get a single output dataset, or treated element-wise and you get a single output collection back). If element-wise, should we iterate over the top level collection or the lowest level collection ?)\nThere is one trick that should work now. You can use the Zip Collection tool, which will create a list of pairs from your two VCF collections. You can then use the Apply Rule to Collection tool where you can take this list of pairs collection as input, click edit in the tool form, and then click Rules -> Add / Modify Column Definitions. Click on Add Definition -> List Identifier and select column A, then click on Assign another column and select column B. Click on Apply, Save and run the tool. You\u2019ll get a list of lists, where the inner list contains 2 elements, one for each variant caller. You can use this list in bcftools concat to get a single collection of combined VCFs.\nUnfortunately the user interface in the workflow editor doesn\u2019t let you do this at the moment, but you can extract this operation from the history. I\u2019ve done this here: https://usegalaxy.org/u/marius/w/workflow-constructed-from-history-test-1.\nHope that helps!"}, {"role": "system", "text": "I actually do this a bit differently than what @mvdbeek suggested.\nUse the Merge Collections tool to merge all of your collections into one big one, but set the\n\u2018How should conflicts (or potential conflicts) be handled?\u2019 advanced option to \u2018Append suffix to every element identifer\u2019 and the default suffix \u2018_#\u2019. Then send that into the Apply Rules tool with the following rule:\n{\n    \"rules\": [\n        {\n            \"type\": \"add_column_metadata\",\n            \"value\": \"identifier0\"\n        },\n        {\n            \"type\": \"add_column_regex\",\n            \"target_column\": 0,\n            \"expression\": \"\\\\d+$\"\n        }\n    ],\n    \"mapping\": [\n        {\n            \"type\": \"list_identifiers\",\n            \"columns\": [\n                1,\n                0\n            ],\n            \"editing\": false\n        }\n    ]\n}\n\nThe benefit of this method is it works with any amount of input collections. You can then use the Relabel Collection tool to change the identifiers from numbers if need be."}], [{"role": "user", "text": "I tried Create a collection from a list of datasets, then the error shows:\n\u00d7The following selections could not be included due to problems:\n\nPaired-end data (fastq-dump) is a collection, this is not allowed\nPaired-end data (fastq-dump) is a collection, this is not allowed\n\nAt least one element is needed for the collection. You may need to [cancel ](javascript:void(0))and reselect new elements.\nCould anybody let me know how to do that in Galaxy?\nThank you,"}, {"role": "system", "text": "Hi @crisprucd\nTry using Collection tools to transform.\nIf your data is currently in two collection \u201clists\u201d (single end each) \u2013 one for forward reads and one for reverse reads \u2013 create a paired end collection using the tool Zip collections.\nShould that not be a match for what you want to do \u2026what kind of collections do you already have? There is probably a tool. Modifying collection types is super important, and used all the time.\nTechnically, you could unhide all of the current collection\u2019s datasets, then start over from scratch but the only reason to do that is if the original collection groupings were incorrect for some reason.\nCollection tutorials, plus the tool forms have short descriptions https://training.galaxyproject.org"}, {"role": "user", "text": "Hello Jennaj,\nThank you for the help. I downloaded 6 SRR files using download and extract reads in FASTQ format from NCBI SRA. The files showed a single-end data(fastq-dump), which is empty, and a paired-end data(fastq-dump) for each download in history. I want to combine the paired files into one collection file.\nI tried a few ways in Galaxy, but no success."}, {"role": "system", "text": "The output from fasterq_dump is in collections already.\nThe paired end collection output will contain one pair per accession.\n\n\n\n crisprucd:\n\nI want to combine the paired files into one collection file.\n\n\nDid you fetch the accessions all in one job? Or did you run that tool one time per accession? You can input a list instead to get them all at once.\nThere are still ways to manipulate what you have if that is true. Unzip collection \u2192 Merge collections (twice, once per end) \u2192 then Zip collection. The \u201cmerge\u201d expects list collections. There are reasons around this that are too much to put here\u2026\nIf that does not help enough, maybe some screenshots or a shared history will me to understand what is going on."}, {"role": "user", "text": "I tried paste/fetch data for the 6 SRR files together and it didn\u2019t go through. So I had to use the tool of download and extract reads in FASTQ format from NCBI SRA, which did individually. Now I am trying to use FASTQ Splitter to separate them into single end files, then I will try the Zip collection to put them together.\nThanks,"}, {"role": "system", "text": "\n\n\n crisprucd:\n\nI tried paste/fetch data for the 6 SRR files together and it didn\u2019t go through.\n\n\nThe list of accessions should be in a plain text file. Just the SRR or ERR identifiers with no extra content, one per line.\nYou can create that with the Upload tool, or load up a file you created on your computer. You can even edit it more once in your history: click on the \u201cVisualize\u201d icon for the dataset and search with the keyword \u201cedit\u201d to find the function.\n\n\n\n crisprucd:\n\nSo I had to use the tool of download and extract reads in FASTQ format from NCBI SRA, which did individually. Now I am trying to use FASTQ Splitter to separate them into single end files, then I will try the Zip collection to put them together.\n\n\nThat sounds good except for the Fastq Splitter part. That is for paired reads that are joined into the same sequence. These reads are in separate sequences but interleaved/interlaced. Try using FASTQ de-interlacer on paired end reads(Galaxy Version 1.1.5) instead. Hope it works out\n\nNGS data logistics << explains the different fastq formats with examples/tools\n"}, {"role": "user", "text": "Thank you so much.\nI will correct that."}, {"role": "user", "text": "Hi Jennaj,\nI unzipped the individual file and the forward and reverse are separated. Then I tried to use zip connection to put them together directly, but the multiply datasets icon can not recognize the  files. Should I use a tool to put the forward files and reverse files as 2 connections at first, then the zip connection would work to put the forward connection and reverse connection as one file?\nThank you,"}, {"role": "system", "text": "I\u2019m confused, maybe misunderstanding. Unzipping a list collection does not create two files. A list collection contains only one list, not two paired lists, so there is nothing to unzip.\nWhat you are trying to do is super tedious. While getting the data reorganized is certainly possible (a few different ways, not just what we discussed so far), it seems well worth your time to start over. Learning how to do this will serve you now, and going forward.\nTry retrieving the sequences from NCBI again. Put all of the accessions to retrieve into a simple text file (one accession per line, no extra spaces or tabs), use the tool Faster Download and Extract Reads in FASTQ format from NCBI SRA, and select that file with the accession list instead of entering accessions directly on the form. All will be sort out correctly into collections in a usable format with zero chance of any mixups."}, {"role": "user", "text": "Hi Jennaj,\nThank you for the suggestions.\nThe forward reads and revers read are paired for each sample. However the zip connection cant recognize them, which cant add them together as a connection file.\nI tried FASTQ de-intertracer to separate them into individual forward or reverse file, but it didn\u2019t go through.\nSo I tried unzip connection, which looked worked and separated them into several forward and revers files.\nThen I tested Zip connection, but it does not recognize the files through the multiply dataset. The separated files are still recognized as a individual collection files for Zip collection. I may used it by a wrong way for the zip collection tool and it may just work on 2 separated connection files, and add them together.\nI will try The Faster Download and Extract Reads in FASTQ format from NCBI SRA tool to download the dataset together. But if we have same situation which we just have individual paired files, how we can add them together as one connection file.\nThank you for your time.\n6313AF93968847589BB8FAFA41EEFEBC.png708\u00d71 96 Bytes"}, {"role": "system", "text": "\n\n\n crisprucd:\n\nBut if we have same situation which we just have individual paired files, how we can add them together as one connection file.\n\n\nIf data are ever in a collection, organized in a way that you don\u2019t want to use, and the collection manipulation tools are not working for some reason, you can try this:\n\nGo into the Hidden tab of your history\nUnhide the datasets you want to restructure\nCreate new collections\n\nCollections are like folders. The files inside them are datasets. It is fine to delete collection folders that you don\u2019t need \u2013 just be sure to not deleted the dataset files inside of collections or you will need to upload the data again."}], [{"role": "user", "text": "hi there, I uploaded to usegalaxy.org some fasta files with mozilla, but when I click to start the import of those file in the \u2018choose FTP file\u2019 I get a warning: bad request (400) error. tried to reupload those files with filezilla but still the same error. can someone help me?"}, {"role": "system", "text": "I can confirm this is broken, probably due to today\u2019s update of Galaxy. I believe we will fix it tomorrow. Sorry for the inconvenience."}, {"role": "system", "text": "This looks to be fixed now to me, just uploaded a file. Please give it a try now."}, {"role": "system", "text": "Update: We are still having a bit of trouble.\nPlease see our gitter chat post for details now, and how to check for updates ongoing: https://gitter.im/galaxyproject/Lobby?at=5d83aca2e45b6d47321e5164\nBut if anything is unclear, can always ask here."}, {"role": "system", "text": "I am having this same problem. I uploaded 16 files of the same set a week earlier and now I get the 400 error when I try uploading the last 8 files."}, {"role": "system", "text": "Hi @Kuksi \u2013 if you are working at UseGalaxy.org, the server is being upgraded to the 20.05 pre-release for testing today. Please try again now.\nI\u2019ll also run a quick test and will post back with the result, and check whether this is a known issue or not being addressed (pending or done).\nMore feedback after the checks. Thanks for reporting the issue!"}, {"role": "system", "text": "Great! Thanks. Will try in a bit when I get to my computer next."}, {"role": "system", "text": "@Kuksi\nGreat, please do.\nI just tested FTP and things are working. Yours probably will too. But if this happens again over the next week or so, wait a bit (5-10 minutes) then try a rerun. The release testing involves restarting the server a few times, which can impact this particular function (external data upload). If the function seems to be down for longer time span, please write back if you have time. Things can be broken while others get fixed "}, {"role": "system", "text": "Ok. Thanks.  Just tried twice and I am still getting the same error. Will keep testing every 15 minutes or so and will let you know how it goes."}, {"role": "system", "text": "Just to specify what I am doing: 1) I upload my fq.gz files to the ftp server using file-zilla.  That step executes without problem and it is very fast.  2) I click on the upload data button (usegalaxy.org) and when the dialog appears I click on choose FTP files. All my files are there. 3) I click on select all then I click on start. 4) I immediately get a 400 error.\nI also tried uploading two files just by drag and drop into the upload area. The first file uploaded (surprisingly fast), but the second file did not even start until I canceled about an hour later."}, {"role": "system", "text": "Update 4/21/2021\nFTP Upload has been corrected at UseGalaxy.org\nThanks again for reporting the problem!\n\nHi @Kuksi \u2013 Ok, I can confirm the problem. A few files will load, others fail for the same reason as you report.\nThis is likely related to the reconfiguration going on right now but I\u2019ll confirm that with the dev team.\nAn immediate solution would be to use a different public Galaxy server right now if your work is urgent."}], [{"role": "user", "text": "As the Galaxy tools use the CPU, but some of the tools (GROMACS) and similar are able to use the GPU for better performance as well.\nIs it possible to run the galaxy or specific tools on GPU?\nKindly suggest some tutorials or help to run GROMACS on GPU through Galaxy."}, {"role": "system", "text": "@vmevada102 yes this is in theory possible. However, at least the European Galaxy server has way more CPUs than GPUs. This means while a single user can maybe profit from GPU support we can run more jobs in parallel on CPUs at the moment. So overall all users profit currently more from CPU-Gromacs.\nIf you really need GPU because you are running a bigger project please get in contact with us and we can find a solution. The crucial part is to setup a container with GPU support and GROMACS that can run on the given infrastructure. This is not really a Galaxy problem.\nHope that helps a little bit,\nBjoern"}, {"role": "user", "text": "Thanks for your reply,\nSometimes, GROMACS required 1 day or more than that to run the job on CPU. If we can process it using GPU, we might be reducing the time period to run such jobs.\nThere are serveral users having their own system with more GPU. They might be also benfited like me.\nVishal"}, {"role": "user", "text": "Hi bjoern.gruening,\nWe discussed the usage of the Gromacs on GPU.\nIf you have found some resolution. Kindly share with me to run this on my server as well.\nWe have dedicated NVIDIA GPU card for the calculations."}, {"role": "system", "text": "Can you submit jobs to your GPU host? We have created VMs ontop of our GPUs, so that every VM just get one GPU. Then we simply submit jobs into those VMs."}, {"role": "user", "text": "Hi bjoern.gruening\nActually, we have NVIDIA GPU (Tesla K40c). All CUDA drivers were installed properly in the system.\nThe system consist one Galaxy installed run the programs in local system.\nBut unfortunately, The galaxytools does not recognise the GPU in the system. Or there might some configuration to detect(utilise) the same.\nAs Gromacs Simulation Version 2020.4+galaxy0 in http://usegalaxy.eu  the new parameter available.  \" Job Resource Parameters\" Stating the information to execute command on GPU.\nHowever, this parameter was not available in my local system with the same version of tool installed in the system.\nPlease help me run the same on the GPU available in the system"}, {"role": "system", "text": "Hi,\nwhat you see on EU is a special Galaxy feature. You can add some additional parameters to a tool form that can influence where your tool is executed, here GPU or CPU.\nPlease set the ENV var in your Galaxy destination to GPU_AVAILABLE=1 and than the tool should use it. I will loop our expert @simonbray in so that he can make sure I don\u2019t say stupid things."}, {"role": "system", "text": "\nPlease set the ENV var in your Galaxy destination to GPU_AVAILABLE=1 and than the tool should use it. I will loop our expert @simonbray in so that he can make sure I don\u2019t say stupid things.\n\nNo, I think this is accurate. It\u2019s cool that you\u2019re trying to get the GROMACS tools set up with GPU @vmevada102! Let us know about your progress."}, {"role": "user", "text": "Thank you for kind support @bjoern.gruening  & @simonbray,\nI am core programmer, so, some confusions will be raised as if.\nHowever, It looks silly question regarding GPU. But the feature you have set is extraordinary to analyse our data with GPU.\nAs per the suggestion, I have added the GPU_AVAILABLE=1 in the Variable.  But still have problem,\nThe option of choosing parameter for GPU is missing in our local Galaxy,\n\neu1459\u00d7806 294 KB\n \nlocal1382\u00d7649 234 KB\n\nSo, is there any specific configuration or change some parameters to enable this feature in Local Galaxy.\nWithout his selection option, i am not able to proceed further.\nVishal."}, {"role": "system", "text": "Yes, this is a general Galaxy feature.\nPlease have a look at this file: galaxy/job_resource_params_conf.xml.sample at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub\nAnd here the part for the job_conf.xml file. galaxy/job_conf.xml.sample_advanced at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub\nPlease also note that we have the entire usegalaxy.eu confuration online on GitHub. See here for example: infrastructure-playbook/job_resource_params_conf.xml at master \u00b7 usegalaxy-eu/infrastructure-playbook \u00b7 GitHub\nHope that helps,\nBjoern"}], [{"role": "user", "text": "run Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d\nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler?\nThanks."}, {"role": "system", "text": "Hi @Meadowlawn\nI reproduced the issue with Unicycler. I hope it will be fixed soon.\n@jennaj It seems there is an issue with Unicycler on usegalaxy.org.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nThank you very much for quick reply. I tired today and found it still does not work.\nCurrent error is \u201cline 10: unicycler: command not found\u201d. The error changed from \u201cline 110 command not found\u201d to \u201cline 10 command not found\u201d\nThanks again,\nGuowu"}, {"role": "system", "text": "Hi Guowu,\nif you need results urgently, maybe copy FASTQ files to another Galaxy server, such as usegalaxy.eu, and complete assembly over there. It is OK to have accounts on several public Galaxy servers.\nTo copy a file, click on dataset name in history on \u2018source\u2019 Galaxy, click at Copy Link (chain icon), on \u2018target\u2019 Galaxy activate Upload menu, switch to Paste/Fetch Data tub, paste the link and hit Start.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nThank you for the instructions. I will try it. When do you think it will be fixed?\nThanks again,\nGuowu"}, {"role": "system", "text": "Hi Guowu,\nunfortunately, I don\u2019t know, but I copied my reply to Jenifer.\nKind regards,\nIgor"}, {"role": "system", "text": "Update Jan 24 2022\nAll test runs for the three most current versions of Unicycler were successful. I\u2019m going to consider this issue resolved/closed unless a bug report for a job run from now forward is sent in. If doing that, comment here and include a link to this topic in the bug report comments to help find it.\nPublic test history based on tutorial data: test-history-unicycler\n\nHi @igor\nI started up some tests but had to restart again. Still running now.\n@Meadowlawn \u2013 would you please\n\ntry a rerun now in a clean history\nbe sure to use the most current version of the tool\nif you have not done QA steps on the reads, do that first, then assemble\nif the job still fails, please submit a bug report from that history and the errored run. It would be helpful if you posted in the URL for this question into the comments field.\n\nUnicycler requires uncompressed intact pairs of short reads (Illumina) and optionally long reads (ONT, etc). If the tool cannot read the inputs, it can error with the message you received.\nTutorials\n\nQuality Control\nUnicycler Assembly\n"}, {"role": "system", "text": ""}, {"role": "system", "text": ""}, {"role": "system", "text": "Hi @jennaj\nI am still getting errors with Unicycler on the main Galaxy server; the last on was on January 25th:\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI don\u2019t have problems with the same datasets, both fastqsanger and compressed versions, in Europe. The same version of Unicycler was used with the default settings:\nhttps://usegalaxy.eu/u/igor/h/unicycler-test\nI know I am absent-minded, but I could not figure out what I am doing wrong: select Unicycler, select input files, click Execute or Run Tool. The only difference I found is in Galaxy versions: the main server has 23.0.rc1, Europe uses 22.05. I have a normal user account on the main server and in Europe. Any chance it works for admin accounts only?\nKind regards,\nIgor"}, {"role": "system", "text": "Thanks @igor\nWe did some more adjustments this morning at UseGalaxy.org. All tests were successful \nAnyone that had a recent failure with this tool should please try again now."}], [{"role": "user", "text": "Hi\nI have been running into issues when using Unicycler on https://usegalaxy.eu/ (job fails in seconds).\nSo I have been wanting to try to repeat the job on https://usegalaxy.org/ as sometimes something won\u2019t work on a server but will on another one.\nTo do so I needed my long reads so I used the \u201ccopy link\u201d to import my data to the US server.\nBut I get an error message after a few seconds:\n\nerror\nAn error occurred with this dataset:\nThe uploaded file contains invalid HTML content\n\nWhat I tried to transfer is a fastq file originating from filtlong (ran on the European server with no error).\nCan anybody help?\nThanks\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nStart by double-checking that the data is really in fastq format and contains sequences (only).\nIf not, a format/content issue could explain both problems.\nFAQs that can help more: https://galaxyproject.org/support/#getting-inputs-right"}, {"role": "system", "text": "@vanina.guernier I\u2019d say it depends on which link you copied.\nThe one that will work reliably is the one you get while hovering over the download icon.\nThe one you get from the eye icon, in contrast, will work for some files, but, e.g.,  not for large fastq files (cause it links to the preview of the files, which is the html-formatted page with the warning that you\u2019re viewing a large file).\nHope that helps,\nWolfgang"}, {"role": "user", "text": "Thanks @wm75 . I used the download icon and it worked well for other large data, just not this specific data.\nVanina"}, {"role": "user", "text": "I don\u2019t see anything wrong with the data.\nThe format is fastqsanger which should be fine I think.\nI managed to import the MinIon data and do the filtlong analysis again on the US server.\nBut the Unicycler analysis that I launched has been on hold for about 5 days now.\nAny troubles with this tool at the moment? I know I can sometimes expect delays, but would like to make sure it\u2019s normal.\nThe same analysis failed straight away on https://usegalaxy.eu\n\nShould I put a specific request for Unicycler error message on https://usegalaxy.eu? (and it\u2019s not related to the copy pasting topic)\nThis is the first time I get the message in blue, not sure what this means.\nThanks\nVanina"}, {"role": "system", "text": "\n\n\n vanina.guernier:\n\nThanks @wm75 . I used the download icon and it worked well for other large data, just not this specific data.\n\n\nHave you tried this more than once? It is possible one of the servers was just busy. How large was the data? Is it in a collection or an individual dataset?\n\n\n\n vanina.guernier:\n\nThe format is fastqsanger which should be fine I think.\n\n\nYou can double-check that the format is actually fastqsanger by redetecting the datatype. Use the pencil icon to reach: Edit attributes > Datatype > use the button to \u201cdetect datatype\u201d. If you do not get fastqsanger as the result there is a content or format problem that makes the data out-of-specification. Some of these tools also process fasta data \u2013 and if the data is in that format, the correct datatype should be assigned.\n\n\n\n vanina.guernier:\n\nI managed to import the MinIon data and do the filtlong analysis again on the US server.\nBut the Unicycler analysis that I launched has been on hold for about 5 days now.\n\n\nYes, the queue is quite long for jobs that run on the cluster that processes Unicyler. The same is true for Trinity and RNA-Star. Confirm that the inputs are Ok, and if they are, then allow any queued jobs to stay queued to ensure the fastest processing. If you delete and rerun, that will place the new jobs back at the end of the queue again, extending wait time. If the inputs are problematic \u2013 fix as needed, then go ahead and rerun.\n\n\n\n vanina.guernier:\n\nShould I put a specific request for Unicycler error message on https://usegalaxy.eu? (and it\u2019s not related to the copy pasting topic)\n\n\nConfirming your inputs are Ok can help here too. Also, make sure you are using the most current version of the tool (and all tools). A job failing so quickly is a strong indicator of an input problem. If the inputs are Ok, then you can submit a bug report. Putting some details into the comments would help \u2013 including a link to this Galaxy Help topic for context. Or @wm75 may ask for something different (he is an admin for the EU server).\n\n\n\n vanina.guernier:\n\nThis is the first time I get the message in blue, not sure what this means.\n\n\nThe usegalaxy.eu server automatically reruns jobs that fail \u2013 and that is what the message/blue box is notifying you about. This helps to eliminate transient cluster/server-side issues (rare, but happen!). A rerun for any odd failure is a good idea. Not all public Galaxy servers do this automatically \u2013 but you can rerun yourself."}, {"role": "system", "text": "\n\n\n vanina.guernier:\n\nThe same analysis failed straight away on https://usegalaxy.eu\n\nShould I put a specific request for Unicycler error message on https://usegalaxy.eu?\n\n\nYes, @vanina.guernier, please report that separate issue using the bug icon and submitting an error report to us from the usegalaxy.eu team. Maybe it also helps us diagnose what\u2019s wrong with the data transfer, in which case I would post the findings back here."}, {"role": "user", "text": "Hi. I reported both problems:\n\nfailed importation of MinIon data in fastqsanger (original data imported and filtered with filtlong on the EU server)\nfailed Unicycler jobs on the EU server\nI tried to run the same Unicycler job on the US server see if the error is consistent but still queuing.\nThanks for help!\n"}, {"role": "system", "text": "\n\n\n vanina.guernier:\n\nfailed importation of MinIon data in fastqsanger (original data imported and filtered with filtlong on the EU server)\n\n\nThanks for sending in the bug report. Using default server-settings at usegalaxy.eu required that the history and the datasets inside it were set to a \u201cshared by link\u201d state in order to transfer a dataset from usegalaxy.eu to usegalaxy.org. @wm75 may have comments about whether that is expected or not.\nTo be clear \u2013 you don\u2019t need to  publish  the history/data \u2013 just set the history/datasets as  shared by link. Only you or anyone you give that share link to would have access to your data (or server administrators). Do this under the History menu (gear icon) > choose \u201cShare or Publish\u201d > and on that form click the button to share by URL. Be sure to check the box to \u201calso share datasets\u201d. Once data is transferred, you can unshare as wanted.\nMore about data privacy, and how you can configure your own data permissions at any Galaxy server (assuming it is running a current release, as both of these public servers are), plus how to share Galaxy objects are all covered in these FAQs. You have full control over permissions and sharing. Galaxy Support - Galaxy Community Hub\n\nSharing and Publishing your work\nData Privacy Features\n\nAs an aside, I didn\u2019t notice anything obviously problematic with your data formatting or content \u2013 this is definitely long read fastqsanger. But the EU team can provide more feedback based on your bug report to them regarding the failed Unicyler job there. The job at ORG is still queued.\nHope that helps!"}, {"role": "system", "text": "\n\n\n jennaj:\n\nUsing default server-settings at usegalaxy.eu required that the history and the datasets inside it were set to a \u201cshared by link\u201d state in order to transfer a dataset from usegalaxy.eu to usegalaxy.org. @wm75 may have comments about whether that is expected or not.\n\n\nThat\u2019s a very good observation @jennaj (learnt something about our server config from it ;))\nWhether or not accessing (and thus uploading to a different server) datasets via known links works or not by default is controlled by your settings in User -> Preferences -> Set Dataset Permissions for New Histories. In there you have an access category, and users that you have listed under it will have link-based access to any of your datasets. Now if you leave this list empty, it means any  user has access.\nApparently, the default setting for newly created accounts has changed on usegalaxy.eu (maybe also on usegalaxy.org?) at some point (about 2 years ago it seems). Old accounts (like mine) have an empty list, while newer accounts start out with just the individual user\u2019s email.\nIf you have an account with restricted access, you will have to Share via Link to make data tarnsfers work as @jennai pointed out. If you have the unrestricted setting, data transfers will work out of the box (at the cost of your datasets being potentially discoverable by anyone).\nAlso note that the setting is called \u201cSet Dataset Permissions for New Histories\u201d. Whatever changes you make to it will, thus, not change properties of datasets in existing histories!"}], [{"role": "user", "text": "Hi Galaxy team!\nI started training on Label-free data analysis using MaxQuant.\nI have a problem with \u201cVisualize this data\u201d on the cut tool result.\nWhen I selected box plot(jqPlot) it did not visualize data.\nInstead a message \u201cPlease confirm the settings before rendering the results\u201d appeared.\nWith best regards,\nArman"}, {"role": "system", "text": "Hi @Arman_Kulyyassov,\nthank you for your report; I contacted the person in charge of this training in order to check the problem.\nRegards"}, {"role": "system", "text": "Hi Arman,\nthe Galaxy interface has changed a bit. When you see the message \u201cPlease confirm the settings before rendering the results\u201d you should see two small arrows on the upper right corner of the screen. Once you click on them the menu appears which allows you to set the specified parameters.\nBest,\nMelanie"}, {"role": "user", "text": "Dear Melanie,\nthank you very much for your letter and helpful advice. I followed instructions but instead of a visualization plot I get this:\n\nimage1920\u00d71080 275 KB\n"}, {"role": "system", "text": "Hello Arman,\nI\u2019m facing the same issue now. I tried 3 different browsers, but it\u2019s not working.\nHow did you do to visualize the boxplot?\nBest regards,\nRaquel Neres"}, {"role": "system", "text": "\n\n\n Raquel_Neres:\n\nHow did you do to visualize the boxplot?\n\n\nWelcome, @Raquel_Neres\nLook under User \u2192 Visualizations\nThe data will take variable time to render \u2026 and the \u201cPlease wait\u2026\u201d message will appear on the left side of the window. Let that complete to see the graphic.\nWhich web browser you are using shouldn\u2019t matter too much since the data is server side. Safari, Chrome, and Firefox are good choices. Avoid Internet Explorer. Make sure cookies are allowed (that is how Galaxy \u201cknows\u201d that you are accessing your own data, so privacy reasons).\nReference: Label-free data analysis using MaxQuant\nThe GTN is hosting a training session all next week. Everyone is welcome to join to interact with the authors and trainers. It is fine to still register. Sm\u00f6rg\u00e5sbord 3: A week of Free, Online, Galaxy Training supported by the Global Galaxy Training Community"}, {"role": "system", "text": "I\u2019m still having this issue after following your advices (with a different training).\nSo I created a visualization using Box Plot (jqPlot) and it is then listed under User \u2192 Visualizations. However, when I click the visualization, the following two messages are displayed subsequently:\n\nYour job has been queued. You may close the browser window. The job will run in the background.\n\nand then:\n\nYour job is running in the background and you may close the browser tab. Results will be available in your saved visualizations list.\n\nWhen the second message vanishes, the window remains empty. No visualization is shown:\n\nBildschirmfoto 2023-06-05 um 14.34.132618\u00d72154 324 KB\n\nThere are also three data series configured.\nI also noticed that in the History pane, the visualization item is generated as of \u201cformat tabular\u201d, which seems wrong to me?"}, {"role": "system", "text": "Hi @kostrykin\nThe UseGalaxy.org server had some problems that were just resolved. This is what probably caused your delays as well: Cluster issue at UseGalaxy.org -- Resolved 06/05/2023. Rerun failed (red) or leave queued (gray) - #3 by jennaj\n\n\n\n kostrykin:\n\nI also noticed that in the History pane, the visualization item is generated as of \u201cformat tabular\u201d, which seems wrong to me?\n\n\nThis tool will generate a tabular output to hold the visualization information (what is specified on those details screens where you are specifying how to plot the data). Then, when the visualization is viewed, that table is parsed into a graphic, on demand. You might need to do that \u201cgenerate graphic artifact\u201d process over again \u2013 or maybe that part was completed and the graphic will render now.\nLet us know how that goes, and we can follow up. If you want to share a history link back for outstanding issues, we can take a look to make sure more is not going on. Sharing your History"}, {"role": "system", "text": "Hi @jennaj thanks four your quick response, however, the problem still persists, as described above, I only get a blank screen when using the \u201cBox Plot (jqPlot)\u201d visualization tool (I\u2019m using the usegalaxy.eu server).\nHere is the link to my history: Galaxy | Europe\nI\u2019m trying to produce a visualization of 219, while following this tutorial: Analyse HeLa fluorescence siRNA screen"}, {"role": "system", "text": "Ok, thanks for clarifying. I\u2019m looking now\u2026\nHi @kostrykin\nThe how-to for the job seems to loop, and never produces a graphic. It cannot even be downloaded. I tried modifying the chart metadata to make the naming \u201cunique\u201d but that was not enough.\nThis seemed specific to certain chart types, and I found an issue ticket. The correction will be available at the UseGalaxy.* servers after the next release. No \u2026 sorry, that was a different chart type. I opened up a new issue, and used your example from the tutorial (copied over to the usegalaxy.org server to make sure not server-specific problem).\n\n  \n\n      github.com/galaxyproject/galaxy\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      Visualizations/Charts: Boxplot fails to render\n    \n\n    \n      \n        opened 07:03PM - 05 Jun 23 UTC\n      \n\n\n      \n        \n          \n          jennaj\n        \n      \n    \n\n    \n    \n  \n\n\n  \n    **Describe the bug**\nBoxplot fails to render. Tabular output is generated, but \u2026no graphic. Either in Galaxy or when downloaded (\"blank png\").\n \nSeems related to https://github.com/galaxyproject/galaxy/issues/13437\n\n**Galaxy Version and/or server at which you observed the bug**\nGalaxy Version: (check <galaxy_url>/api/version if you don't know)\nCommit: (run `git rev-parse HEAD` if you run this Galaxy server)\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create a boxplot graphic\n2. Wait for that to process\n3. No graphic\n4. Seems to loop back into creating the graphic if that is access from User > Visualizations\n\n**Expected behavior**\nCreate a graphic. Other charts do plot on the same input, including the formatted output from this chart type.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Additional context**\n\nHistory with output from the boxplot tool. Never renders.\nhttps://usegalaxy.org/u/jen-galaxyproject/h/test-ghelp-8152\n\nThis is the blank graphic produced...\n![New Chart](https://github.com/galaxyproject/galaxy/assets/8999735/b2b7da34-bc35-4d87-88d9-37e2c7e3eff9)\n... in between here.\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI\u2019m not sure of an exact workaround \u2026 but the tabular file is an appropriate input. There is likely a way to use the application directly as an alternative for the graphing portion. http://www.jqplot.com/\nApologies for missing this earlier!!"}], [{"role": "user", "text": "I\u2019m having trouble viewing the 512 successfully aligned TCR clonotypes from MiXCR. See 1) report of successful MiXCR run with 513 clonotypes and 2) empty clonotype file\n\n\n\n\nAnalysis date: Thu Oct 28 16:38:14 CEST 2021\nInput file(s): mixcr_analysis.clna\nOutput file(s): mixcr_analysis.contigs.clns\nVersion: 3.0.5; built=Mon Feb 25 00:04:20 CET 2019; rev=af538f7; lib=repseqio.v1.5\nCommand line arguments: --report mixcr_analysis.report mixcr_analysis.clna mixcr_analysis.contigs.clns\nAnalysis time: 59.16s\nInitial clonotype count: 513\nFinal clonotype count: 513 (100%)\nLongest contig length: 553\nClustered variants: 0 (0%)\nReads in clustered variants: 0.0 (0%)\nReads in divided (newly created) clones: 0.0 (0%)\n\n\n\n\n"}, {"role": "system", "text": "Hi @OrangeOwl23,\ncould you share your history with me? I\u2019ll have a look at it.\nRegards"}, {"role": "user", "text": "Do I need your email in order to do that?"}, {"role": "user", "text": "Hi @gallardoalba !\nAny luck with troubleshooting this?\nKindly,\nJohn"}, {"role": "system", "text": "Hi,\nI\u2019ll provide a feedback today; sorry for the delay.\nRegards"}, {"role": "user", "text": "Thank you!!"}, {"role": "system", "text": "Hi @OrangeOwl23,\nI\u2019ll update the tool; I hope that it could solve the problems. It will be probably available at the beginning of the next week.\nRegards"}, {"role": "user", "text": "Sounds good, thank you! Let me know when you\u2019d like me try running it again."}, {"role": "user", "text": "I just ran the MiXCR pipeline again, and got the same results. Successful run, but an empty clonotype file. Any other troubleshooting ideas?"}, {"role": "system", "text": "Hi @OrangeOwl23,\ncan you share your history with the new tool version? I\u2019ll have a look.\nRegards"}, {"role": "system", "text": "Hi, I am one of the developers of MiXCR. If you are still having that issue please try the latest version (v.4.2) of mixcr and let me know if there are still any issue. Also, since the interface changed a bit I can help you with writing the command if needed.\nAll the best"}], [{"role": "user", "text": "Hi, my input is 9M reads. I ran it through samtools mpileup following BWA alignment. I set the max per-file depth to 1M and I got an output of about 250,000 reads. I know I didn\u2019t lose that much to quality, so why is my output number higher? I also noticed that if I set the max per file depth to anything between 300,000-1,000,000, I still get 250,000 as my output. Does samtools have a read limit?"}, {"role": "system", "text": "I never used samtools mpileup on galaxy but how did you counted the number of reads in the mpileup file?"}, {"role": "user", "text": "Oh I counted later down the pipline. I use compute and I end up with a table with read number included"}, {"role": "system", "text": "Did you used the output option --output-QNAME?\nI am asking because although it is possible the get the total amount of reads from a mpileup file it is not that easy. As far as I can think of. And also as far as I know (but can be wrong) you can not do that with the compute tool. And again I can be wrong but I have the feeling you are misunderstanding the mpileup output."}, {"role": "user", "text": "I did not use the output option --output-QNAME. Well  if the max-per-file depth does not determine how many reads at each base position, what does it do? Column 4 shows read number at each position? or am I completely wrong?"}, {"role": "system", "text": "Yes column 4 shows the depth (amount of reads) mapped on that specific position so that is correct. But just summing up that column will not give you the correct total amount of mapped reads. Because let\u2019s say you have position 500 and 501, a (big) part of the reads mapped on those positions are the same reads and counted double. And if you have 9M reads, they will be spread out over the full reference.\nIf you want to know the total amount of mapped reads you need to use Samtools flagstat."}, {"role": "user", "text": "You are right. I used Samtools flagstat and I got 9 M reads aligned (99.5% aligned). Is there anyway of checking how much reads I have after Samtools?"}, {"role": "user", "text": "I still only have max 250,000 reads at a position following samtools."}, {"role": "system", "text": "So you are expecting more then 250,000 reads per position. You could double check this with Samtools depth. Also in samtools mpileup you can disable the max depth by using a value of 0.\nEDIT:\nYou also may need to rephrase your question.\n\nI also noticed that if I set the max per file depth to anything between 300,000-1,000,000, I still get 250,000 as my output.\n\nIf the max is 300,000 and the depth is lower then that (250,000) why would it change? I would expect a change if you set the max-depth to a lower then 250,000 value. Like 8000 or something"}], [{"role": "user", "text": "I ran into this error when running Trinotate on Galaxy Europe:\n\nThe job creating this dataset has been resubmitted\nAn error occurred with this dataset:\nThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource. memory memory memory memory * Running CMD: wget \"ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/unipr\n\nDoes this mean the job is running again, or do I need to resubmit?"}, {"role": "system", "text": "@jaredbernard You can see if the job is running by its color:\nhttps://galaxyproject.org/support/how-jobs-execute/#dataset-states\nYou can try rerun it."}, {"role": "user", "text": "Thanks, @David. The error message was red, but the phrasing made me think it was going to automatically rerun. Also I thought Galaxy Europe sometimes reran jobs that fail. But I waited several hours and it was not rerunning, so I started it again myself.\nI noticed a few others have posted this error for various tools, but no one specifically addressed what this error message means, so I thought it would be good if a help issue specifically asked the meaning of the message."}, {"role": "system", "text": "@jaredbernard, I agree. I\u2019ve found a more direct answer from the gitter / usegalaxy-eu/Lobby:\n\nHelena Rasche @erasche jun 19 2019 09:01\nThis means the job has failed, and we have automatically retried to run it with more memory.\nIf your jobs fail, please check the (i) information icon, and see the stderr or stdout, and if those do not tell you the issue, then please usethe bug report icon to submit errors\n\nMaybe this would be a more official answer for now"}, {"role": "user", "text": "Thanks for the answer @David and @erasche. Unfortunately, my restarted job ended with the same error message."}, {"role": "system", "text": "Any chance your input is not ok?"}, {"role": "user", "text": "I\u2019ve wondered that, as a possibility. But I think it is okay.\nI\u2019ve done things virtually identical to the workflow in this de novo transcriptome assembly tutorial and advice on this Galaxy Help page, but using my own data."}, {"role": "system", "text": "@jaredbernard Can you provide the full error text message and the bug text?\n\n\n\n jaredbernard:\n\n\"ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/unipr\n\n\nMaybe it is similar to what happened here:\n  \n    \n    \n    Trinotate running Error usegalaxy.org support\n  \n  \n    I was running Trinotate and got error as like this \n\nError, cannot locate version database file. Be sure it\u2019s in your current directory. at /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-d301214ea0391aec86b2f63c2c1ac14ef0598be5ae4d143b7405cc0414beaa71/bin/Trinotate line 53. \n\nThe error that occurred during the download of pfam2go database \n\nRunning CMD: wget \u201cftp://ftp.geneontology.org/pub/go/external2go/pfam2go\u201d \n\u20132021-04-27 05:21:06--  ftp://ftp.geneontology.org/pub/go/external2go/pf\u2026\n  \n\n"}, {"role": "user", "text": "Thanks for looking into this, @David.\nThe message in my first post was my whole error message.\nUnder Tool Standard Error, the message is:\n\nError, cmd: wget \u201cftp://ftp.geneontology.org/pub/go/external2go/pfam2go\u201d  died with ret 1024 at /usr/local/tools/_conda/envs/mulled-v1-d301214ea0391aec86b2f63c2c1ac14ef0598be5ae4d143b7405cc0414beaa71/lib/site_perl/5.26.2/Pipeliner.pm line 102.\nPipeliner::run(Pipeliner=HASH(0x55d0b25ea838)) called at /usr/local/tools/_conda/envs/mulled-v1-d301214ea0391aec86b2f63c2c1ac14ef0598be5ae4d143b7405cc0414beaa71/bin/Build_Trinotate_Boilerplate_SQLite_db.pl line 120\n"}, {"role": "system", "text": "Hi there,\nI\u2019m having a similar error:\nError, cmd: wget \u201cftp://ftp.geneontology.org/pub/go/external2go/pfam2go\u201d  died with ret 1024 at /usr/local/tools/_conda/envs/mulled-v1-d301214ea0391aec86b2f63c2c1ac14ef0598be5ae4d143b7405cc0414beaa71/lib/site_perl/5.26.2/Pipeliner.pm line 102.\nDid you find any solution?"}, {"role": "system", "text": "Update\nIssue ticket: Bug: Trinotate (Galaxy Version 3.2.1) \u00b7 Issue #3875 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\n\nHI @melisa123\nThis looks like an actual bug. Specifically, the environment created for the job is failing to retrieve one of the required inputs (from a 3rd party\u2019s site).\nYou are working at UseGalaxy.eu, correct? If so, please submit your error as a bug report and include a link to this topic at Galaxy Help in the comments. Both provide technical context. Step 4 here explains how: Troubleshooting errors"}], [{"role": "user", "text": "Hello.  Twice in the month of July I\u2019ve been unable to successfully run RNA STAR 2.7.8a on the usegalaxy.org server.  During the first week of July I had this issue and deleted my job after a week of waiting.  I\u2019m currently attempting another job, which has remained queued for the last 6 days.  I\u2019m curious if this is expected behavior for this application.  Thank you and I apologize for the bother."}, {"role": "system", "text": "Dear @pad59,\nUnfortunately, it is. Depending on the organism, your input data, some settings, and the day, STAR can take very long.\nFor example:\n\nMapping to human in comparison to cyanobacteria takes longer.\nThe deeper you sequence the longer it will take to map your data.\n\nUse 2-pass mapping for more sensitive novel splice junction discovery activating this function also takes longer.\nIf you use also a non-standard genome/transcriptome, which is not supported by Galaxy, then STAR takes especially longer, because it has to first generate an index file and then runs the mapping.\nIt could be that you have used Galaxy on days, when a workshop, conference or training was given. During those peak times, jobs can take a bit longer.\n\nI hope I could clarify, why your job might have taken so long. Maybe to speed up your process here are few things you can do:\n\nRe-run your job, like you have done already.\nCheck a few settings for STAR to speed up the process (like the 2-pass mode, if you have selected it).\nIf you have used a non-standard genome/transcriptome, then maybe it is better to first run RNA STAR locally once to generate the index file, which can then be provided for Galaxy.\n\nI hope I could help.\nHave a good week and best wishes,\nFlorian"}], [{"role": "user", "text": "Hello!\nI would like to do a metagenomics analysis with the \" MetaPhlAn2 to profile the composition of microbial communities \" tool from Galaxy and using as input file, one that is the output of \" FASTQ joiner on paired end reads \" tool. Because I would like to use the forward and reverse fastq sequences from the same sample at the same time.\nHowever, I always get an error and failure when I try this. It seems to work only with the forward or the reverse fastq sequence, but no when they\u00b4re together.\nSomeone could help me? Is there specific parameters that I should have in mind?\nThanks!"}, {"role": "system", "text": "I think you should always post here what the error exactly says. Now it is hard to help you.\nBut! Check the output format of FASTQ joiner, this is fastqsanger. Change this to fastq by clicking on that pencil icon and then run Metaphlan2. Think this will do the trick.\nBtw, if you have amplicon data also take a look at FLASH."}, {"role": "user", "text": "Thanks for the suggestion, but it did not work \nNow, I include the error here. Sorry, I didn\u00b4t see before the window where the error is explained (I am new in Galaxy\u2026:S)\nthe error says that:\nOriginal error: usage: metaphlan2.py --input_type\n{fastq,fasta,multifasta,multifastq,bowtie2out,sam}*\n[\u2013mpa_pkl MPA_PKL] [\u2013bowtie2db METAPHLAN_BOWTIE2_DB]*\n[\u2013bt2_ps BowTie2 presets] [\u2013bowtie2_exe BOWTIE2_E*\n2nd error (after changing with the pencil the format of file to fastq): Error: reads file does not look like a FASTQ file\nError: Encountered exception: \u2018Unidentified exception\u2019\nCommand: /usr/local/tools/_conda/envs/mulled-v1-c5867e29ea0fba532ec8dc4a557d8798445dccb6ecf21f67e09143751a79b65d/bin/bowtie2-align-s --wrapper basic-"}, {"role": "system", "text": "I expected this to be your first error. And my solution should solve it, at least it did for me when I tested it.\n\n\n\n SumTot:\n\nError: reads file does not look like a FASTQ file\n\n\nIs it possible to try it again? Did you changed the datatype of the right file? So the output of FASTQ joiner. Maybe you can test it also on the public galaxy server to check if it is not a local problem. When I replied on your question I used https://usegalaxy.eu/\nEDIT:\nI just read your error better.\nError: reads file does not look like a FASTQ file\nTry to open and examine the file. Maybe it has to do with the FASTQ Header Style option"}, {"role": "user", "text": "Hello!\nThanks for all your answers \nI tried again several times and finally MetaPhlAn2 worked well with the file produced by FASTQ joiner. I didn\u00b4t change anything\u2026except that I uncompressed the file. Maybe that was the main error.\nHowever now the result gives me as output: 100% unclassified. I don\u00b4t understand why\u2026Any idea?\nThanks again!"}, {"role": "system", "text": "With very limited to no knowledge about metaphlan2 but still want to help you I would say that there are just no known markers in your input. Why they are not there I don\u2019t know. Because this is a specific question about the analyses and not galaxy itself you can maybe get an answer at biostars.\nHere the website of the tool:\nhttps://huttenhower.sph.harvard.edu/metaphlan\nGood luck"}, {"role": "user", "text": "Thank you very much! "}, {"role": "user", "text": "I have a last question, about the input used for FASTQ joiner tool.\nThe input fastq files must be a fastqsanger type or similar, no? But what about fastq files from Illumina? I would like to join forward and reverse fastq files from Illumina. So, my question is if  I am selecting the correct tool for doing that.\nMaybe there is another tool that I should use and because of that I am having so many errors\u2026"}, {"role": "system", "text": "Hi @SumTot\n@gbbio May have more ideas (is a pro at troubleshooting!) \u2026 but I would suggest posting back the first two fastq reads from both the forward and the reverse inputs to help with that. Quote the content to preserve the format. Also note the currently assigned datatype (copy/paste to ensure it is exact).\nThe tool is picky about the input sequence and quality score lines. We can help with any other fix-ups or the appropriate parameters that might work with the data \u201cas-is\u201d.\nIllumina reads come in a few different \u201cflavours\u201d \u2013 it depends on the version. The end goal will be to get to the point where all is in a standardized fastqsanger datatype variant. And to make sure the assigned datatype/compression is a match for the actual dataset content. Most tools require fastq data to be in a fastqsanger variant format \u2013 mostly due to how the quality scores will be interpreted. If the quality score scaling is not what is expected, tools can error in odd ways, or not output data at all, but that is usually easily resolved.\nOne important part about content is that both inputs contain the same base \u201cread\u201d names. If there are unpaired reads in either, that will cause problems. QA steps can remove one end of a pair. Some (example: Trimmomatic) will split those out \u2013 four outputs \u2013 2 for those that remain paired (forward + reverse), 1 for unpaired forward, and 1 for unpaired reverse. Tools in the group Seqtk can also be used to filter out any unpaired reads \u2013 some tools are single manipulation functions, and one (tool:Seqtk seq) can apply multiple manipulations all at the same time.\nAlso, be sure that you are using the most current version of the tool. At https://usegalaxy.org, this will be: FASTQ joiner  on paired end reads (Galaxy Version 2.0.1.1+galaxy0)\nThe first few FAQs here may also help. Review if you want to \u2013 all of it is good reference info that helps to resolve the most common input problems. https://galaxyproject.org/support/#getting-inputs-right\nThanks!!"}, {"role": "system", "text": "If your method by joining the paired reads and execute methaplhan2 is the right method I don\u2019t know. But Illumina data should just work. As far as I know fastq and fastqsanger is basically the same format. But the datatype \u201cfastqsanger\u201d is a galaxy way to make sure it got the ASCII_BASE 33 quality scores. Only it is possible that the galaxy tool or in other words the wrapper is created to give the output the galaxy datatype fastqsanger. But it is just a FASTQ file.\nSo now you will think, why does it not work then if fastq and fastqsanger is the same format. This is because metaphlan2 has a parameter --input_type and will get the value of the galaxy datatype of the input file.\nIf you are interested you can see it here: https://github.com/galaxyproject/tools-iuc/blob/master/tools/metaphlan2/metaphlan2.xml\nIn summary Illumina data just works but after FASTQ joiner you need to change the galaxy datatype to fastq because the metaphlan2 galaxy wrapper is created that way. It has nothing to do with your data itself but with galaxy.\nIf, but I highly doubt it you got ASCII_BASE 64 quality score you could use something like https://toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer\nHere a page with the scores:\nhttps://drive5.com/usearch/manual/quality_score.html"}], [{"role": "user", "text": "Keep getting \u201cThis is a new dataset and not all of its data are available yet\u201d for all plots I used (histogram, Spectra plot) and pretreatments. how can i fix this?"}, {"role": "system", "text": "Hi @lem, you see this when the input dataset is not yet available - for instance, when it hasn\u2019t uploaded yet, or is the output of another job which is still running. The input will turn green when it is ready."}], [{"role": "user", "text": "I aligned RNA-seq reads against a custom-build reference genome using Bowtie2. I use the produced BAM with a \u201cchromosom\u201d txt file to generate a WIG file using \u201cBAM to Wiggle\u201d but I get a message error\u2026\nExecution resulted in the following messages:\n\u201cFatal error: Exit code 1 (An error occured during execution, see stderr and stdout for more information)\u201d\n\u201cFatal error: An error occured during execution, see stderr and stdout for more information\u201d\nTool generated the following standard error:\n\u201cProcessing CP004075.1 \u2026\nProcessing CP001179.1 \u2026\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/bin/bam2wig.py\u201d, line 105, in \nmain()\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/bin/bam2wig.py\u201d, line 102, in main\nobj.bamTowig(outfile = options.output_prefix, chrom_sizes = chromSizes, chrom_file = options.chromSize, q_cut = options.map_qual, skip_multi=options.skip_multi,strand_rule = options.strand_rule, WigSumFactor=norm_factor)\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/lib/python2.7/site-packages/qcmodule/SAM.py\u201d, line 2597, in bamTowig\nif strandRule[key] == \u2018+\u2019:Fwig[pos] +=1.0\nKeyError: \u2018+\u2019\u201d\nCould anyone help me please?\nThanks,\nAlicia"}, {"role": "system", "text": "Hi @AliciaNevers,\naccording to the source code, there is a problem with the StrandRule; which kind of reads are you using (single or paired) and which output did you get from Infer experiment.\nRegards"}, {"role": "user", "text": "Thanks for your answer.\nI didn\u2019t try the infer experiment, is it mandatory?\nMy reads are single for this experiment. But I tried to run again a previous job  with Paired-end reads from another experiment using \u201cBam to wiggle\u201d code. I did\u2019nt work either this time whereas it gave me nice coverage files a year ago\u2026\nRegards\nAlicia"}, {"role": "user", "text": "\nCapture d&#39;\u00e9cran 2021-06-14 22.40.391920\u00d71080 271 KB\n\n\nCapture d&#39;\u00e9cran 2021-06-14 22.42.001920\u00d71080 263 KB\n"}, {"role": "system", "text": "\n\n\n AliciaNevers:\n\n. But I tried to run again a previous job with Paired-end reads from another experiment using \u201cBam to wiggle\u201d code. I did\u2019nt work either this time whereas it gave me nice coverage files a year ago\u2026\n\n\nHi @AliciaNevers,\ncould you repeat the analysis with the previous tool version? Bam to Wiggle 2.4. If it works then we can confirm that there\u2019s a bug in the last update.\nRegards"}, {"role": "user", "text": "I\u2019m sorry but I can\u2019t use this previous update on the regular usegalaxy.org and I don\u2019t know how to transfer my account to Galaxy europe\u2026 When I try to log in with my informations on Galaxy europe i\u2019m not recognized.\nCould you send me the link for usegalaxy.org please?\nThanks a lot,\nAlicia"}, {"role": "system", "text": "Hi @AliciaNevers,\nunfortunately, this is not available in usegalaxy.org, but if you share with me your history I can test it in the European instance. I\u2019ll provide you my mail by DM.\nRegards"}, {"role": "user", "text": "Hi,\nOk, I understand\u2026 What is a DM?\nCan I copy the link here or you wouldn\u2019t do like that?\nThanks in advance,\nRegards,\nAlicia"}, {"role": "system", "text": "Hi @AliciaNevers,\nI repeated the analysis in the previous version and it ran fine, then the problem seems to be related to the last update. I\u2019ll review it.\nRegards"}], [{"role": "user", "text": "Hi !\nI try to access the UseGalaxy (.eu and .org) webpage on my old 2013 iMac, and I can\u2019t reach it\u2026 It\u2019s leading me to a blank page. I cleared my cache and it didn\u2019t help.\nCould it be due to the version of my iMac? Is there anything I can do?"}, {"role": "system", "text": "Welcome, @Emm!\nCan you try with another machine to see what happens? What\u2019s the browser\u2019s version?"}, {"role": "system", "text": "Sorry forgot to mention I am using Chrome Version 91.0.4472.77"}, {"role": "system", "text": "Welcome, @juliayork!,\nCan you tell us how are you reaching that link?\nIf your aim is download the dataset, you can go here, then click on the View data (eye) button of the dataset you want, a new tab will open; change the\n\nusegalaxy.org/datasets/bbd44e69cb8906b5da669214049c29b0/display/?preview=True\n\nto\n\nusegalaxy.org/datasets/bbd44e69cb8906b5da669214049c29b0/display/\n\nThis will download the respective dataset."}, {"role": "user", "text": "Hello ! I tried on my 2017 macbook, and it works well, so that\u2019s why I feel like it\u2019s due to the old iMac\u2019s version\u2026\nThe bowser\u2019s version I used on the old iMac is Firefox 49.0.2."}, {"role": "system", "text": "Ok, @Emm and @juliayork, if everything works we can mark this topic as solved."}, {"role": "user", "text": "Well, it works on my laptop, but I would like it to work on my iMac, which doesn\u2019t "}, {"role": "system", "text": "@Emm, Can you try update iMac\u2019s browser and test it?"}, {"role": "system", "text": "It is solved in that I got access to the data I needed but the issue of the link loading a blank page is still there. It is not clear to me how the next person who tries to access the data from the link in the journal will find the link you suggested, I couldn\u2019t find it.\nCan you load any of the data from the link I posted above?"}, {"role": "system", "text": "@juliayork , Yes, look\n\nimage1366\u00d7768 73 KB\n\n This is the link I provided to you, after I open the link you provided before."}, {"role": "user", "text": "Hi ! Sorry, it took me a while to answer.\nI updated Firefox, like you suggested, and it worked ! \nThank you very much !!"}], [{"role": "user", "text": "Fastp jobs that were queued over 16 hrs ago have still not started running."}, {"role": "system", "text": "Update 09/26/2022\nThe UseGalaxy.org server is still very busy.\nLeave jobs queued for the fastest processing.\n\nUpdate 09/23/2022\nAt usegalaxy.org we were able to confirm the delays and fixed a small technical issue.\nPlease leave queued jobs queued. Avoid deleting and rerunning as that will lose the job\u2019s original place in the queue.\nThanks to everyone who reported the problem!\n\nHi @imperialadmiral121\nThe public servers might just be busy. https://status.galaxyproject.org/\nAre you working at usegalaxy.org? There aren\u2019t any known issues as of now, so this prior Q&A would apply. If you think more is going on, please post screenshots or share your history. If we find something on our own, we will post an update.\n\n  \n    \n    \n    jobs are not running or currently running for a long time usegalaxy.eu support\n  \n  \n    Hi @berger_juliette \nAre these jobs queued (gray) or executing (yellow)? understanding-job-statuses \n\nA single trimmomatic job that has been queued for five days is probably Ok.\nA single trimmomatic job that has been executing for five days would be unusual.\n\nJobs run after any upstream jobs complete (inputs for the tool are ready to use) and as public shared resources appropriate for the tool become available. Some of your jobs run, then some of other people\u2019s jobs run, then more of yours, repe\u2026\n  \n\n\nAnd, someone else reported longer than usual delays at usegalaxy.eu a few hours ago \u2013 no update yet: FastQC not running"}], [{"role": "user", "text": "I am trying to align my fastqsanger.gz files with HISAT2 but it is not working. The problem arose after I had successfully ran an initial set of samples for DEGs, using HISAT2 as part of it. Now that I uploaded the rest of my files, the concatenated fastqsanger.gz files will not go through HISAT2. I even try running my previously successful files with no avail, which makes me think it is in fact a HISAT2 or galaxy issue.  Can anyone help? Thanks in advance!"}, {"role": "system", "text": "Hi @japiz\nTry one more rerun now. We\u2019ve had reported issues of mapping jobs failing but that was unreproducible in tests and others have had success today.\nIf your rerun today fails, please send me a direct message with a share link to your history, a link to this post, and note which dataset is the problem.\nMore details about the prior reported problems, how to send a direct message, and how to generate a share link are in this post: bowtie2 not working\nThanks!"}, {"role": "user", "text": "Hi @jennaj,\nThanks for getting back to me. The problem still  persists today. I\u2019d be happy to send you the info, but I couldn\u2019t  find how to send you a direct message. I didn\u2019t see an  envelope when clicking on your \u201c@\u201d and couldn\u2019t start a new direct message from my Account>>Messages section either. Maybe I\u2019m missing something? Thanks again for your help.\nBest,\nJ. Apiz"}, {"role": "system", "text": "Ah, Discourse (how this site is run) updated the UI a bit. Try this:\n\ndirect-message-ghelp.png2468\u00d7612 104 KB\n\nBug reporting directly through the usegalaxy.org web interface is still problematic, or we could use that instead \u2026 so, that\u2019s why we are using share links/direct messages. I\u2019m curious about a potential cluster/job server-side issue."}, {"role": "user", "text": "\nimage.png1920\u00d7919 32.7 KB\n\nFor some reason I seem to be missing the new message button all together. Tried in two different browsers. Perhaps replying to a direct message from you might be a temporary way around this.\nBest,\nJ. Apiz"}, {"role": "system", "text": "Odd. It may be a setting I can adjust.\nMeanwhile, I just sent you a direct message.\nPlus, I am rerunning HISAT2 tests against all 3 clusters. Twice each. Will take some time to complete."}, {"role": "system", "text": "Ok, I found the problem and was able to reproduce it.\nThe \u201cJetstream\u201d cluster is the problem.\nNear the bottom of the tool form is an option to select the cluster to run the job on. You were using the default, which means Galaxy decides where to send the job. It will never send a mapping job to Stampede (and expect mapping jobs to fail there anyway, for a different reason). Galaxy will send mapping jobs to Jetstream \u2013 and since that is problematic, there is a way to avoid it.\nSet up your tool form like this:\nYou\u2019ll see this option first:\n\njob-resource-parameters-1.png1162\u00d7264 12.9 KB\n\nIn the pull-down menu switch to \u201cSpecify job resource parameters\u201d and then select the \u201cGalaxy Cluster (default)\u201d option:\n\njob-resource-parameters-2.png1162\u00d7700 47 KB\n\nI will alert our admin about this. More feedback soon. Meanwhile, this will allow you to get your jobs to run successfully.\nThanks for taking the time to follow through and share your history, much appreciated!"}, {"role": "system", "text": "Update: Ticket for the issue is now here https://github.com/galaxyproject/usegalaxy-playbook/issues/257"}, {"role": "user", "text": "I\u2019m glad to say this did fix my issue!\nInitially I was hesitant, as upon changing the Job Resources Parameters setting to the Specify Job Resource Parameters  option, the Galaxy cluster (default) option was already preset under Computer Resource. If I understood correctly,the reason this makes sense is because when leaving the default Use default job resource parameters option in Job Resource Parameters, Galaxy will use the Jetstream cluster, while changing to  Specify Job Resource Parameters will in turn use the Galaxy Cluster option as default, successfully redirecting the job to a different cluster.\nThanks for your help @jennaj !"}, {"role": "system", "text": "Super, glad it worked.\nAnd yes, you are interpreting the cluster selections correctly.\nUsing all defaults == Galaxy decides which cluster to send the job to. Factors are based on the tool itself, cluster load distributions, and the like. In short, get the job executed on the most appropriate cluster(s) as resources are available.\nSpecifically sending to the \u201cGalaxy cluster (default\u201d) == means that there is no \u201cGalaxy\u201d decision making based on the tool itself or cluster/load related factors. It is definitely possible to pick a cluster that will \u201cnot work\u201d \u2013 and for mapping and other tools that use indexed genomes, Stampede is not appropriate. Both \u201c(beta)\u201d clusters should never be used by most people. Galaxy will never send jobs to an inappropriate cluster, but people can, by manipulating this setting.\nThere isn\u2019t a good way to clarify all of this in the user interface, although we made an attempt at it in this FAQ: https://galaxyproject.org/main/. However, that can also be difficult to interpret.\nSo, glad you wrote in, we isolated the problem, and now have a successful workaround. When the ticket closes (when using \u201cDefault-let-galaxy-choose-the-cluster\u201d or specifying \u201cJetstream\u201d both work), I\u2019ll post back to this thread and the other.\nThen all can decide what you want to do. Jetstream is a larger cluster resource and jobs will often \u201crun faster there\u201d (same execution time, but shorter queue time) but not always. Letting \u201cGalaxy decide\u201d is generally best, and that is why it is the \u201cdefault\u201d on the tool form\u2026 but that is not an option until the Jetstream issues are resolved for this problem. Resolution should be soon but I cannot offer any accurate estimates, yet. \nI appreciate that you took the time to write back AND explain your confusion. Usage confusion is important feedback, thank you!"}, {"role": "system", "text": "Update: Fixed\n@japiz\n\n\n\nTophat not working???\n\n\nNote to all end users running mapping: Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.\nTicket: https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708\n\n"}], [{"role": "user", "text": "Hi,\nI was not able to upload fastq.gz files from my PC (size around 1.5 Gb) to the usegalaxy.eu. I tried it many times with different files; the message was: Warning: Upload request failed. (0)\nIs it possible to fix it somehow?"}, {"role": "system", "text": "Hi @CC_Tue\nTo Upload larger files to Galaxy, or multiple files in batch, from a local computer \u2013 try using FTP.\n\n\nGeneral FTP Help: Galaxy FTP Upload - Galaxy Community Hub\n\n\nUseGalaxy.eu FTP Help: Galaxy Europe | FTP Configuration\n\n\nGTN tutorials, start here: Galaxy Training!\n\n\nNote that the FTP URL differs by the server and is linked to an activated, existing registered account at that server (email plus password). The connection can be resumed if it stalls out due to an interrupted or slower internet upload speed. The latter is common for home networks (usually optimized for data download speeds).\nI added some tags to your post that cover related Q&A and troubleshooting.\nHope that helps!"}, {"role": "user", "text": "Dear jennaj\nthanks a lot, I will try this possibility (need to learn first, as I am new to FTP).\nI hope it will work."}, {"role": "system", "text": "\n\n\n CC_Tue:\n\nneed to learn first, as I am new to FTP\n\n\nFew tips: Install and try using a simple application like Filezilla. All default settings should work fine. At the top, type in the target server\u2019s URL for FTP then your email + password (same as used to log into Galaxy at that server) and click on the button for \u201cquick connect\u201d. Certificate pop-ups can be reviewed and accepted. Then it is as simple as navigating to the files on your computer on the left side and dragging them over to the server on the right side. The transfer status will be reported in the bottom tabs.\nDon\u2019t quit out of the application or let your computer \u201csleep\u201d until the full data transfer is completed. Should a connection drop or a file partially transfer: click on quick-connect again, accept server certificate pop-ups, then click on \u201cresume transfer\u201d.\nYou can get fancier with a command-line function/string or by custom pre-configuring an FTP application (instructions are in the links I sent before), but that isn\u2019t needed for most public Galaxy servers."}, {"role": "user", "text": "Dear jennaj,\nIt is working!\nAgain many thanks for your help and best wishes!"}, {"role": "system", "text": "Hi!\nI already tested all options with FTP transfer files for Galaxy EU but Filezilla showed a critical error and I cannot connect with Galaxy EU server, here it is an Screenshot of this error.\nCaptura de Pantalla 2021-11-08 a la(s) 2.19.332382\u00d7430 107 KB\n\nI check several times my password and is correct, since i can login into galaxy server with chrome without any problem"}, {"role": "system", "text": "Hi Diego,\nthere is an interval of time between when you create an account in the web interface and when that account is replicated into the FTP server.\nI checked, and it\u2019s now correctly present into the FTP server.\nPlease make another try"}, {"role": "system", "text": "Hello,\nI have the same problem: I want to upload fastq.gz files (1.5-2.5GB each) to usegalaxy.eu and get the warning upload request failed. It works fine for smaller sized files (~500MB).\nI tried using FileZilla using galaxy.uni-freiburg.de as server, as well as ftp.usegalaxy.eu\nUsing this galaxy.uni-freiburg.de, I get a timeout connection error. For ftp.usegalaxy.eu, I get the following:\n\n\n\n\nStatus:\nAufl\u00f6sen der IP-Adresse f\u00fcr ftp.usegalaxy.eu\n\n\n\n\n\nStatus:\nVerbinde mit 132.230.223.103:21\u2026\n\n\nStatus:\nVerbindung hergestellt, warte auf Willkommensnachricht\u2026\n\n\nAntwort:\n\n220-incoming.galaxyproject.eu FTP server\n\n\nAntwort:\n\n\n\nAntwort:\nUnauthorized access is prohibited\n\n\nAntwort:\n220 FTP Server ready.\n\n\nBefehl:\nUSER (my email address)\n\n\nAntwort:\n550 SSL/TLS required on the control channel\n\n\nFehler:\nHerstellen der Verbindung zum Server fehlgeschlagen\n\n\n\nI would appreciate any help."}, {"role": "system", "text": "Hi tsa.s,\nthe instruction for using our FTP service are on Galaxy Europe | FTP service instructions\nPlease take a look at the manual and make sure you are following all the directions."}], [{"role": "user", "text": "I tried running a FASTA sequence on AlphaFold2 and it showed me this error:\nE0902 16:02:23.065268 22797118093120 hhsearch.py:56] Could not find HHsearch database /data/pdb70/pdb70\nTraceback (most recent call last):\nFile \u201c/app/alphafold/run_alphafold.py\u201d, line 445, in \napp.run(main)\nFile \u201c/opt/conda/lib/python3.7/site-packages/absl/app.py\u201d, line 312, in run\n_run_main(main, args)\nFile \u201c/opt/conda/lib/python3.7/site-packages/absl/app.py\u201d, line 258, in _run_main\nsys.exit(main(argv))\nFile \u201c/app/alphafold/run_alphafold.py\u201d, line 353, in main\ndatabases=[FLAGS.pdb70_database_path])\nFile \u201c/app/alphafold/alphafold/data/tools/hhsearch.py\u201d, line 57, in init\nraise ValueError(f\u2019Could not find HHsearch database {database_path}')\nValueError: Could not find HHsearch database /data/pdb70/pdb70\nThis is the sequence I tried to run:\n\n33\nMPAPAPLLIILRRRIRKQAHAHSKGGGGSMR\nTSYLLLFTLCLLLSEMASGGNFLTGLGHRSD\nHYNCVSSGGQCLYSACPIFTKIQGTCYRGKA\nKCCKEAAAKFPGVGFPGVGFPGVGKPKPKPL\nGIYVYVRRKKSLSGCYWCLKKFSWWDPGFFI\nKKGLPIRASRLVFKKKRRCAVLLYKKVIFSN\nISNWVYKKRLGNGTFYKKLQVQFAVFKKNSP\nYIPYLLKKSFQAWLTNQPYKKCAQLRYLVFK\nKFIAVIPAANFKKYAIALVAELLKKMNITGY\nTDAYKKLLLAITSNIVKKLIKGNFLEYVKKS\nLSGCYWCLIKKIYVYVRRCVLKKFAVFGILP\nCVKKDSQFTLIKPYKKPLGLWARVLAEALII\nGFSWWDPGFFIIAAYSFQAWLTNQPYASFMV\nAAYFIAVIPAANFPGVLLTQAAYSVIFSNIS\nNWVYAAYTVYFLPNSIITNVFFIAAYMVSLI\nYLASFLFKSSSTAAAYRLISTALKARALSLD\nSAAYRRRLANSLRLATVIYGKAAYEIRWSFT\nLNCSASDSEAAYEYVRILMRKLLLQITRHHH\nHHH\n\nPlease let me know if there is an error with my input or within the tool\nThank you"}, {"role": "system", "text": "Hi @sunag_parasu,\nthe problem is caused by the tool itself. I suggest you to try to run your analysis in the Australian Galaxy server; you can apply in the following link:  AlphaFold 2.0.\nRegards"}], [{"role": "user", "text": "HI\nMy trimmomatic analysis shows this error, Please what can I do?\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/jetstream2/scratch/main/jobs/47313312/_job_tmp -Xmx28g -Xms256m\nTrimmomaticSE: Started with arguments:\n-threads 8 fastq_in.fastqsanger fastq_out.fastqsanger SLIDINGWINDOW:4:20\nError: Unable to detect quality"}, {"role": "system", "text": "Hi @ERNEST_KISSI\nit looks like an issue with input files and/or datatypes. Cab you preview the input FASTQ files? What datatype (value after format:) is assigned to the file(s)?\nIf you share the history, I or someone else can have a look. History sharing: history menu (icon at the top right corner of the history panel) > Share or publish > Make history accessible > copy and paste URL here. You can unshare it later.\nKind regards,\nIgor"}, {"role": "user", "text": "Here is the link\nThe file that I am trying to trim has been named TRIM\nhttps://usegalaxy.org/u/ernest543/h/unnamed-history"}, {"role": "system", "text": "Hi @ERNEST_KISSI\nTrimmomatic (is a) flexible read trimming tool for Illumina NGS data, while you have long reads with very wide range of Phred quality scores - check it with FastQC. Do you use PacBio reads? For additional information on Phred quality score check Encoding section in FASTQ format - Wikipedia\nTry fastp instead of Trimmomatic: it does trimming and filtering for long reads and less sensitive to wide range of Phred quality scores. It also does QC. You may also check other tools designed for long reads.\nYou can unshare the history, unless you have other questions on the issue.\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you but this is my school assignment and I am supposed to use trimmomatic on a server but unfortunately I have no command line experience and I wanted to use Galaxy instead. Thank you very much for the help. Much appreciated, Igor"}, {"role": "system", "text": "hi @ERNEST_KISSI\nfastp is available on public Galaxy servers, hence no need for command line use.\nI don\u2019t see any issue with your FASTQ file, but it is not compatible with Trimmomatic in Galaxy because of extended range of Phred quality score. I reduced the range of Phred quality score on a PacBio read, to match \u2018illumina\u2019 range, and Trimmomatic has no issue with that data.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all,\nI am very naive to bioinformatics. I am using this tutorial to map a genome, but when I use multiQC tool to analyze my BAM file (MarkDuplicate Metrics) I get an error. The error is:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nModule 'picard: \u2018picard.analysis.AlignmentSummaryMetrics\u2019 not found in the file 'chlamyDNA_4_S7__001_fastq\u2019\nCould anyone help me please?"}, {"role": "system", "text": "Hi,\nthis looks like you have chosen a wrong file as your input. We have more detailed training material here, maybe that will help as well: Galaxy Training!\nCiao,\nBjoern"}, {"role": "user", "text": "\n\n\n bjoern.gruening:\n\nBjoern\n\n\nHi Bjoern,\nThanks very much for your replay, but I am sure I am using the right file.I should use the file its name ends with metrics and there is only one file with that name.\nIn the tutorial, the tutor is mapping 4 genomes at the same time, but I am mapping only one genome. Could it cause a problem when I use MultiQC tool?\nSorry if it doesn\u2019t make sense, I am very naive in bioinformatics."}, {"role": "system", "text": "HI @HamidGaikani,\nmapping just one fastq file doesn\u2019t explain your error. Please, could you share a screenshot of the files that you are working on in Galaxy?\nRegards"}, {"role": "user", "text": "Dear gallardoalba\nSorry for the late reply; I was trying to do this again, but I faced the same problem. I am attaching the screenshot. Many thanks for your help.Screen Shot 2021-01-07 at 10.34.31 PM2394\u00d71292 525 KB Screen Shot 2021-01-07 at 10.34.57 PM2398\u00d71412 497 KB"}, {"role": "user", "text": "I did every single step exactly like the tutorial, except that I uploaded my own reference genome."}, {"role": "system", "text": "Hi @HamidGaikani\nThere is an open issue with the tool that appears to be what is going wrong with your example. MultiQC fails when input datasets that have the same dataset name/label -- tool issue \u00b7 Issue #315 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub\nIn short, when inputs share the same \u201cname\u201d the tool cannot accurately define/label those inputs as distinct samples when creating the report. This creates a duplicated label conflict and fails the tool.\nSince your data is in a collection, try the tool  Collection Operations > **Relabel List Identifiers** from contents of a file to give the inputs unique names.\nOr, perhaps @bjoern.gruening @gallardoalba have a better suggestion? This wasn\u2019t an easy problem to address during the prior tool update.\nApologies for the confusion!"}], [{"role": "user", "text": "Hi,\nI would like to cite galaxy EU correctly. Regarding the information on this website Citing Galaxy one is supposed to additionally use the first publication of a public instance used. Which publication (additionally to the general primary publication) should be used to especially cite Galaxy EU with the public server in Freiburg?\nThanks a lot!\nRose"}, {"role": "system", "text": "Hi,\nwe have a small section about Acknowledgments here: Galaxy Europe\nIf you could cite the main Galaxy paper and acknowledge the EU server we would be super happy \nThanks,\nBjoern"}], [{"role": "user", "text": "I\u2019m following along the de novo transcriptome assembly Galaxy tutorial (De novo transcriptome reconstruction with RNA-Seq) with my own data.  I\u2019m stuck at the stringtie-merge step; I\u2019m getting the following error message:\nError: could not any valid reference transcripts in /corral4/main/objects/6/5/e/dataset_65e2a2c5-762d-4f88-bec9-002deee1ae23.dat (invalid GTF/GFF file?)\nIs this saying that none of the annotated sequences in my reference GTF file overlap or correspond with those in my Stringtie assembled transcripts?\nSo far, I\u2019ve checked that the chromosome names are the same in the FASTA and gtf file and I\u2019ve tried using both gtf and gff3 file formats for the reference annotation file."}, {"role": "system", "text": "Hi @julsies,\ncould you share your history with me? I can have a look at it.\nRegards"}, {"role": "user", "text": "@gallardoalba\nJust dropped you a message with a link to my history.\nSo I\u2019ve also tried using a gff3 file, but I keep getting the error:\nError parsing attribute gene_id ('\"' required for GTF) at line:\nso I\u2019ve stuck with gtf files, as they have the quotation marks (\" \") around strings in the ninth column."}, {"role": "system", "text": "Hi @julsies\n\n\n\n julsies:\n\nIs this saying that none of the annotated sequences in my reference GTF file overlap or correspond with those in my Stringtie assembled transcripts?\n\n\nTry running your GTF annotation through the Stringtie merge tool first (by itself) to adjust formatting. Then, run the Stringtie merge tool again with all of your Stringtie outputs along with the modified GTF.\nPrior Q&A with more details:\n\n\n\nStringtie Merge\n\n\nStingtie Merge can be used to do the following:\n\nPre-process an existing reference annotation GTF dataset, by itself, so it can be used with Stringtie to guide assembly, and with other downstream tools. Not all public GTFs are formatted in a way other tools can interpret and this tool can be used to \u201cgroom\u201d a GTF.\n\nCombine the per-sample GTF results of Stingtie , and optionally the reference GTF (knowns), into a unified GTF assembly that can be used with downstream differential expression tools.\n\n\n\n Why is this needed? The tool expects a slightly modified version of GTF format. The annotation file included with the tutorial is already adjusted, so that step wasn\u2019t included. Avoid GFF3 formatted annotation with this tool and most others, unless specifically noted otherwise on a tool form. More details are in the topic above, and those usually help to solve troubleshooting issues with this tool.\nOne last potential format issue to check for: A common item to correct as a data cleaning/prep step (any analysis/tool) is to remove GTF headers. Strict GTF format does not include headers, only data lines, but some data providers include one or more anyway for practical reasons (versioning, etc). So, if headers are present in your file, usually at the top and starting with a #, remove those first before using the annotation in any analysis steps. (Note: GFF3 format will have at least one header line, but that is in the specification for the datatype).\n\n\n\nKept seeing \"Illegal stran\" when using SAM/BAM to count matrix\n\n\n\u201cHow to\u201d is covered here:\n\nOverview: Extended Help for Differential Expression Analysis Tools\n\nSpecifics for GTF formatting: Common datatypes explained >> Datatypes (review the GTF \u201ctips\u201d)\n\n\n\nIf all that still fails, then more is going on and @gallardoalba can help with the history review "}, {"role": "user", "text": "Hi @jennaj\nMany thanks for your suggestions.  I\u2019ve just realized that because I\u2019ve manipulated the gtf file (I manually converted the gff3 file to a gtf) in Excel, uploading it to Galaxy has caused the ninth column to adopt many extra quotation marks.\nI\u2019d assume this is what caused the following error message when I ran the gtf file by itself through Stringtie merge:\nError: no transcripts were found in input file\n\nDo you have any suggestions on how I can manipulate the original gff3 file into a gtf so that it will be Galaxy-friendly when I upload it?"}, {"role": "user", "text": "@jennaj @gallardoalba\nI think I\u2019ve solved it!\nFirst, I used TextEdit to remove those pesky extra quotation marks.  Uploading to Galaxy confirmed they were gone for good. Then, I ran the gtf file through stringtie-merge as you suggested, and it went through! The downstream steps seem to be working.\nThank you both for your time!"}, {"role": "system", "text": "Hello,\nI tried what you suggested and ran my reference gtf through Stringtie as well before going on to the Stringtie merge step, but I am still getting the following error:\nWARNING: no reference transcripts were found for the genomic sequences where reads were mapped!\nPlease make sure the -G annotation file uses the same naming convention for the genome sequences.\nOf course, the original StringTie jobs were run without reference genomes, as I am attempting to assemble a transcriptome de novo, so I don\u2019t understand the \u201c-G\u201d comment.\nAdditonally, if I continue from here to gffcompare, I get the following error:\nNo fasta index found for ref_seq.fa.\nI thought this program used GTF inputs, not FASTA files?\nPlease help!"}, {"role": "system", "text": "\n\n\n Kanishk:\n\nOf course, the original StringTie jobs were run without reference genomes, as I am attempting to assemble a transcriptome de novo, so I don\u2019t understand the \u201c-G\u201d comment.\n\n\nHi @Kanishk Please explain the steps you have done. If you can incorporate reference annotation at the StringTie Merge step, then it can be incorporated at the StringTie step. The reference annotation needs to be based on the same exact reference genome or transcriptome used for mapping. If not, then the coordinates will not match up and a result like yours is output.\nTutorials: Search Tutorials"}, {"role": "system", "text": "Jenna,\nI\u2019m following along the de novo transcriptomics tutorial (De novo transcriptome reconstruction with RNA-Seq) with my own data.\nAs the tutorial details, when constructing de novo transcripts, StringTie should be run without a reference genome. Then these transcripts should be assembled in StringTie merge, this time using a reference genome. This is where I\u2019m starting to have issues. I\u2019ve even reformated my reference gtf through Stringtie merge to make sure the annotation is identical, but to no avail.\n\" If you can incorporate reference annotation at the StringTie Merge step, then it can be incorporated at the StringTie step.\"\nI\u2019m not sure the question is whether I can use a reference genome in the Stringtie step. According to what I\u2019m trying to accomplish, I shouldn\u2019t use one here, right? Only starting at the Stringtie merge step."}, {"role": "system", "text": "Also, I used a locally cached hg38 reference genome for the mapping portion (in HISAT2), as suggested by the tutorial, but used a downloaded hg38 gtf file in the Stringtie merge and gffcompare steps later\u2013again as suggested by the tutorial. For the sake of consistency, should I just use the gtf file in my history for HISAT2 and move forward from there? There\u2019s another problem in that case because if I try to use a reference from my history, I can\u2019t seem to select gtf files, only ones in a FASTA format\u2026"}, {"role": "system", "text": "Ok, the second reply helps me to understand.\n\n\nStringtie \u2013 the inputs are mapped reads. Whatever ever was mapped against is where the coordinates are derived from. All other inputs that are coordinate-based need to be based on that exact same reference genome.\n\n\nYou mapped against the hg38 reference genome at this step in the tutorial.\n\n\nYou can incorporate reference annotation during mapping (HISAT2) and/or transcript assembly (Stringtie) and/or transcript merging (Stringtie merge), then will also include the reference annotation for later steps (differential expression: Featurecounts and DeSeq2 if following the tutorial).\n\n\nThe reference annotation that is first used represents the known genes/transcripts for your genome. You will still capture novel transcripts if you use one. The different tool forms will state the choices: create transcripts that are known (only \u2013 those included in the annotation) OR create known plus novel. You can run it both ways in different tools and compare.\n\n\nAs you move through the analysis, the reference annotation will be updated and reformatted at certain steps. The updated version at an earlier step is what you want to use in the next downstream steps.\n\n\nThe hg38 reference annotation GTF is now best sourced from UCSC. Not from the Table Browser but from the Downloads area here. There are a few Gene tracks to choose from. You can review what each of those represents then upload the selected GTF to Galaxy with the Upload tool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\n\n\n\n\n\n Kanishk:\n\nFor the sake of consistency, should I just use the gtf file in my history for HISAT2 and move forward from there?\n\n\nI\u2019m not sure what that annotation represents or where you sourced it, but some data providers create GTF data that are not quite in the correct format. It is also possible you have annotation with chromosome identifers that are a mismatch for hg38. I would stick with the UCSC GTF if this analysis is new to you \u2013 nothing special needs to be done to prepare it for use with tools. But if you really want to use another source, Gencode is probably the best alternative. You\u2019ll need to remove the header lines and then update the datatype first. Instructions are in this prior Q&A.\nNote: you may find prior Q&A that specifically states to avoid the UCSC GTF. That was about using the version from the Table Browser before UCSC started generating properly formatted GTF data available in the Downloads area for all of their genomes that had a Gene annotation track.\nIn my opinion, the version of hg38 reference annotation in the UCSC Downloads area is now absolutely the best choice. Especially if you choose the RefSeq Gene annotation (hg38.ncbiRefSeq.gtf.gz) \u2013 as that one is updated regularly, about monthly. Other gene tracks are fixed at prior releases. Other data sources may need reformatting and some are missing important attributes. But you can review and decide. Or maybe run distinct analysis paths with the different GTF choices and compare results. However you do this \u2013 start with a specific annotation GTF then keep using that throughout the same analysis. Meaning: don\u2019t switch from RefSeq to Ensembl in the middle of an analysis or expect problems.\n\n\n\n Kanishk:\n\nThere\u2019s another problem in that case because if I try to use a reference from my history, I can\u2019t seem to select gtf files, only ones in a FASTA format\u2026\n\n\nFor this, my guess is that you are mixing up where to input a reference annotation (GTF) versus a reference genome or reference transcriptome (fasta) on the tool forms. Your analysis will include all three.\n\nA reference annotation will be selected from the history for your use case. Featurecounts does have annotation available for a few genomes, including hg38, but that is for when you don\u2019t need annotation for any other steps \u2013 and it won\u2019t match external sources \u2013 so don\u2019t mix that in for now.\nA reference transcriptome will be selected from the history (always).\nA reference genome for your case is the already indexed built-in version of hg38. Technically you can input a reference genome from the history, too, but that only works for very small genomes and there is no need if Galaxy has already indexed it.\n\nThat should cover all the questions/problems you were having. Please try this out "}], [{"role": "user", "text": "Original title: Server Indexed File (Human (homo sapiens): hg38) does not appear to be functional\nUsing bedtools GetFastaBed to get sequences in Server Indexed Files: Human (homo sapiens): hg38 underlying ChIP-seq binding sites in a collection of BED files, all returned files were 0 bytes with warnings saying \u201cWARNING. chromosome (chr1) was not found in the FASTA file. Skipping.\u201d (and many more similar warning lines)\nI\u2019m assuming this has something to do with the server maintenance and reloading of Server Indexed Files. When do you think this will be resolved?\nThank you,\nJustin"}, {"role": "user", "text": "tool still unable to find chromosomes (chr1, chrX, etc) in server indexed FASTA file and returns empty files. please help!"}, {"role": "user", "text": "Dataset Information\nNumber:\t7947\nName:\tbedtools GetFastaBed on data 7881\nCreated:\tFri Oct 16 14:11:18 2020 (UTC)\nFilesize:\t0 bytes\nDbkey:\thg38\nFormat:\tfasta\nJob Information\nGalaxy Tool ID:\ttoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_getfastabed/2.29.0\nGalaxy Tool Version:\t2.29.0\nTool Version:\tbedtools v2.29.0\nTool Standard Output:\tstdout\nTool Standard Error:\tstderr\nTool Exit Code:\t0\nHistory Content API ID:\tbbd44e69cb8906b5ee719ead5a404447\nJob API ID:\tbbd44e69cb8906b544381b625dcffda9\nHistory API ID:\t387a03a1eca44028\nUUID:\t71eb5c07-de44-44fd-8b5c-457194f301fc\nTool Parameters\nInput Parameter\tValue\nBED/bedGraph/GFF/VCF file\t\n7881: STAT5A PLS.BED\nChoose the source for the FASTA file\tpreloaded\nfasta_id\thg38\nUse the \u2018name\u2019 column in the BED file for the FASTA headers in the output FASTA file\tTrue\nReport extract sequences in a tab-delimited format instead of in FASTA format\tFalse\nForce strandedness\tFalse\nTreat split/spliced BAM or BED12 entries as distinct BED intervals when computing coverage.\tFalse\nInheritance Chain\nbedtools GetFastaBed on data 7881"}, {"role": "system", "text": "\n\n\n craigj2:\n\nGetFastaBed\n\n\nThis tool had another reported error around this some time frame (at UseGalaxy.org, with hg19).\nI\u2019m guessing that you already tried a rerun (to eliminate recent server downtime as a contributing factor).\nTesting now \u2013 more feedback soon. Thanks for reporting the problem!"}, {"role": "user", "text": "Yes, retried several times without success. Thank you!"}, {"role": "system", "text": "Update 10/21/20 1pm EST: Resolved. Please rerun prior jobs that failed.\n\nUpdate 10/20/20 7pm EST:\nThe correction is still pending. Ticket with details https://github.com/galaxyproject/usegalaxy-playbook/issues/311\nOnce confirmed to be fixed, that ticket will close out. It should be very soon\n\nCorrection in progress.\nTools with known issues (are likely more):\nBamLeftAlign\nGetFastaBed\nBWA-MEM and most other mapping tools\nFreebayes\nDetails:\nWe can confirm the problem and are correcting it right now. It might be Monday before everything is done. Write back if still a problem on Tuesday for an update.\nMeanwhile, you can do the same operation at usegalaxy.eu or usegalaxy.org.au.\nCopy the link from a dataset\u2019s \u201cdisk\u201d icon and paste it into the other server\u2019s Upload tool to move data around. Some servers require that the history/datasets are in a shared state first. Do this under User > Histories > menu (gear) > Share or Publish. Choose the \u201cshare by URL\u201d option making sure the box is checked to also share the objects inside of the history (eg datasets). Or, reset all permissions to share objects by default in user-defined ways under User > Preferences.\nEach public server has the default data permissions set a bit differently, but these FAQs cover all of the sharing/permissions in case you get stuck: https://galaxyproject.org/support/#data-options\nThanks again for the followup, appreciated"}], [{"role": "user", "text": "i was running wheat genome reference to create BAM file using BWA_MEM . it was running smoothly\nbut out file was 34.3 MB and the finaly error message was surfaced reading as:\nAn error occurred setting the metadata for this dataset Set it manually or retry auto-detection"}, {"role": "system", "text": "A few more details would help, thanks!\n\nWhere are you using Galaxy? public URL? your own server (version)?\nIf your own server, is this the first time you have run BWA-MEM?\nIs the wheat genome already indexed or are you using it as a custom genome?\nIf a custom genome, this the first time you have executed BWA-MEM with it?\n"}, {"role": "system", "text": "How to troubleshoot this type of problem in general\n\n\nClick on the link inside the dataset to re-detect metadata. In many cases, this will clear up the problem.\n\n\nCheck if the results are empty. Example: BAM results that have no data lines, only header lines.\n\n\n\nDouble check that the intended target genome was actually mapped against. Use the rerun or job details buttons to review the original genome selected, modify if incorrect, and rerun.\nIf using a custom genome, is it formatted correctly?\nReview quality metrics for fastq inputs with the tool FastQC and address content issues.\nReview the tool settings and online manuals. Does your data meet the minimum mapping criteria set by the parameters used? Is the tool appropriate for your datatype (spliced versus unspliced)? Adjust as needed. Most tool form options for most Galaxy tools are the same as when using a tool line-command.\n\n\nTest for server-side problems.\n\n\nTry at least one re-run to see if that resolves the problem (transient server/cluster issues).\nIs the tool marked as deprecated (public Galaxy server)? If so, choose an alternative tool that is not. See the Galaxy Tutorials if you are not sure of alternatives.\nLow memory resources (server/cluster that runs the jobs, different from account quota disk space) and related technical issues are possible.\n\nWhen running your own Galaxy, review the admin logs, disk space where you write outputs, etc.\nWhen using a public Galaxy server, contact the admins by email or through a bug report. Contact information is usually on the home page of a public server and often also here: https://galaxyproject.org/use\n\nProblems at a usegalaxy.* server can be reported at this forum. Specify which server by URL with your question.\n\n\n\nFull help: Troubleshooting resources for errors or unexpected results"}, {"role": "user", "text": "I am using public URL ,custom wheat  genome and and i am usinf BWA MEM first time.\nthanks\u2026for ur response"}, {"role": "system", "text": "Could you share the exact URL for the public server? Thanks!"}, {"role": "user", "text": "https://usegalaxy.org"}, {"role": "system", "text": "Thanks for clarifying.\nI reviewed the datasets with a metadata problem and all looks Ok technically.\nDid you try clicking on the link to re-autodetect the metadata? That should be enough to resolve the problem.\nThat said, I see Stringtie jobs in your history. If the reads are RNA-seq, then you\u2019ll want to use a spliced mapper like HISAT2 (instead of BWA). The custom reference genome also should be reformatted before using it with downstream tools \u2013 it contains description line content on the title lines (\">\" lines) that will cause problems when using it along with the matched reference annotation. Please see the Tutorial and FAQ links above for the how-to for all of this."}, {"role": "user", "text": "Thanks sir. i will try it and update you sir accordingly."}, {"role": "user", "text": "I think as the reference genome of wheat is large genome,so there is memory allocation problem\u2026is any solution to handle wheat genome."}, {"role": "system", "text": "I assume this is similar to a problem we\u2019ve recently had: pysam is not able to handle the large chromosomes of wheat. To index the BAM file, it can only handle contigs (chromosomes) of max 512MB. The solution provided by the wheat consortium is that they split the large chromosomes."}], [{"role": "user", "text": "Hi,\nI have been trying to annotate vcf.gz files that I have using the SnpEff tool on Galaxy. They\u2019re human samples so I\u2019m using the hg19 genome as the Snpff Genome Version Name.\nI keep getting the following error message - I\u2019m guessing from the error message that it has to do with the fact that the reference and alternate bases are not single bases and rather 2 bases (TG and CA, respectively, in the below example) - is that the case?\nHow can I resolve this error? GATK was used for variant calling.\n\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/966/40966912/_job_tmp -Xmx7g -Xms256m\nError: Error while processing VCF entry (line 176) :\nchr1 16890671 . TG CA 125.238 PASS AF=0.53012;AO=44;DP=84;FAO=44;FDP=83;FDVR=10;FR=.;FRO=39;FSAF=21;FSAR=23;FSRF=20;FSRR=19;FWDB=-0.0269551;FXX=0.0119048;HRUN=1;HS_ONLY=0;LEN=2;MLLD=170.568;OALT=CA;OID=.;OMAPALT=CA;OPOS=16890671;OREF=TG;PB=0.5;PBP=1;PPD=0;QD=6.03554;RBI=0.0269856;REFB=0.0133951;REVB=-0.00128241;RO=39;SAF=21;SAR=23;SPD=0;SRF=20;SRR=19;SSEN=0;SSEP=0;SSSB=-0.0302714;STB=0.516705;STBP=0.775;TYPE=mnp;VARB=-0.00753567 GT:GQ:DP:FDP:RO:FRO:AO:FAO:AF:SAR:SAF:SRF:SRR:FSAR:FSAF:FSRF:FSRR 0/1:91:84:83:39:39:44:44:0.53012:23:21:20:19:23:21:20:19\njava.lang.StringIndexOutOfBoundsException: String index out of range: 3\njava.lang.StringIndexOutOfBoundsException: String index out of range: 3\nat java.lang.String.substring(String.java:1963)\nat org.snpeff.snpEffect.HgvsProtein.simplifyAminoAcidsLeft(HgvsProtein.java:395)\nat org.snpeff.snpEffect.HgvsProtein.simplifyAminoAcids(HgvsProtein.java:384)\nat org.snpeff.snpEffect.HgvsProtein.toString(HgvsProtein.java:491)\nat org.snpeff.snpEffect.VariantEffect.getHgvsProt(VariantEffect.java:633)\nat org.snpeff.vcf.VcfEffect.set(VcfEffect.java:1031)\nat org.snpeff.vcf.VcfEffect.(VcfEffect.java:147)\nat org.snpeff.outputFormatter.VcfOutputFormatter.addInfo(VcfOutputFormatter.java:98)\nat org.snpeff.outputFormatter.VcfOutputFormatter.toString(VcfOutputFormatter.java:286)\nat org.snpeff.outputFormatter.OutputFormatter.endSection(OutputFormatter.java:112)\nat org.snpeff.outputFormatter.VcfOutputFormatter.endSection(VcfOutputFormatter.java:230)\nat org.snpeff.outputFormatter.OutputFormatter.printSection(OutputFormatter.java:145)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotate(SnpEffCmdEff.java:292)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotateVcf(SnpEffCmdEff.java:468)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotate(SnpEffCmdEff.java:142)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:1029)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:984)\nat org.snpeff.SnpEff.run(SnpEff.java:1183)\nat org.snpeff.SnpEff.main(SnpEff.java:162)\n\nThank you in advance for your help."}, {"role": "system", "text": "Hi @rozita,\nthe error seems to be related to this issue: String index out of range \u00b7 Issue #177 \u00b7 pcingola/SnpEff \u00b7 GitHub. SnipEff will be updated to version 5.1; I think it will fix the problem: Update PR.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba ,\nThank you for your help, from what I\u2019ve read it seems that it would take time, until at least next week, to have it updated. Is there any alternative that I can use in the meantime?"}, {"role": "system", "text": "You can try to run it in a local Galaxy instance by cloning my repository. In order to do it, you need to install planemo on your computer and run the command planemo s inside this folder. It will allow you to launch the last SnpEff version in your local Galaxy instance.\nRegards"}, {"role": "user", "text": "I\u2019m sorry, I\u2019m a beginner with the analysis so I might have a lot of questions that might seem trivial. I was using SnpEff on https://usegalaxy.org/ to do the analysis. My understanding is that I would do the above steps that you mentioned on the command line, which I did follow and I think I\u2019ve managed to install planemo and clone the repository and I can view the tools, however, I\u2019m not sure about the exact steps to be done to launch SnpEff - would you be able to assist me with that? Many thanks for your help."}, {"role": "system", "text": "Hi @rozita,\nthe error is related to the fact that the vcf line does not encode a simple snp but an mnp, yes, but is not directly caused by it.\nIt rather seems that SnpEff, after having completed the annotation of the line internally, tries to simplfy the amino acid change it\u2019s going to report by stripping off leading and trailing identical amino acids.\nLets assume for example: in your TG \u2192 CA change, the T->C is silent (leaving say an Ala unaltered), but G->A changes a Leu->Ser (completely making this up of course). Then the original AA change annotation would be AlaLeu \u2192 AlaSer, but obviously that could be simplified to just Leu \u2192 Ser.\nNow the problem is that SnpEff assumes in that simplifying code that amino acids are encoded in 3-letter code at this point. For some reason that doesn\u2019t seem to be true in your case, but you seem to arrive at that piece of code with single-letter code (like AL \u2192 AS in the example). Now when the code tries to compare the first three letters of the left string to the first three letters of the right one, it fails with that String index out of range: 3 from the error message.\nLooking for an explanation of why you arrive there with one letter codes, I can only think of the setting:\nUse one letter Amino acid codes in HGVS notation. E.g. p.R47G instead of p.Arg47Gly in the SnpEff annotation options. Maybe that is what\u2019s incompatible with MNPs?\nJust guessing here, but if you have checked that option, just try rerunning without it.\nGood luck,\nWolfgang"}], [{"role": "user", "text": "Attempting to run usegalaxy.org/u/aun1/h/covid-19-assembly and both Unicycler and Spades are failing with\ncode 17: slurm_submit_batch_job error: Invalid account or account/partition combination specified\n"}, {"role": "system", "text": "Hi @cutsort\nThis was a cluster issue we have been working on (across all Unicyler/SPAdes runs). It was resolved today. Please try a rerun.\nThanks for reporting the problem!"}, {"role": "system", "text": "Hello,\nI\u2019ve been trying to run Unicycler as part of the Sars-CoV-2 genome assembly tutorial and the job has been in que since 11th Aug 2020. I\u2019ve tried a new run aswell and have also run a subsample of the dataset. But, its the 14th of Aug and the jobs are still all on que. Are there any server issues? Please advise what I should do next.\nThanks."}, {"role": "system", "text": "Hi @nirvtul\nThe cluster that runs Unicycler from usegalaxy.org is very busy, with a longer queue than usual. There are not any known technical issues \u2013 just more queued jobs than expected.\nPlease leave your queued jobs queued and those will process as resources become available. If you delete and rerun, the new jobs will end up at the back of the queue again, further extending the wait time.\nOur administrator is double-checking why that particular cluster is so busy. If there is some problem uncovered that requires action on your part (rare!), we\u2019ll write back \u2013 but most likely the queue will simply start to move again and your existing job(s) will process.\nThanks for reporting the issue and we appreciate your patience. This is a very popular workflow under heavy usage.\nBest!"}], [{"role": "user", "text": "Hi team,\nI try many times, but I don\u2019t know how to use online reference genome.\nWhen I use bwa-mem.\nIf use own upload fasta.\nbwa-mem will index fasta every time.\nCould you told me how to fix it.\nVery thank you,\nJacky"}, {"role": "system", "text": "I am not 100% sure so do some extra research yourself. But possibly you need this: https://toolshed.g2.bx.psu.edu/repository?repository_id=d4b644c94642cd8c?\nYou can install it from your own toolshed page."}, {"role": "user", "text": "Hi gbbio,\nWhen I install data_manager_bwa_mem_index_builder.\nThan results was same.\nSource FASTA Sequence can\u2019t find fasta file.\n\n\u8a3b\u89e3 2020-02-27 1739491892\u00d7887 142 KB\n\nDo you know what problem is ?\nAnd  bwa-mem have same problem.\nThe bwa-mem can\u2019t read fasta file (no matter I was upload fai, bwt,  amb,ann, pac, sa file or not).\n\n\u8a3b\u89e3 2020-02-27 1749091899\u00d7707 165 KB\n\nI am looking forward to your reply.\nJacky"}, {"role": "system", "text": "Ah, I think I figured it out =) I got it working by installing data_manager_all_fasta_path first:\nhttps://toolshed.g2.bx.psu.edu/repository?repository_id=0a5e20872a520157\nAfter installing you have a new data manager and trough this one you can add your fasta file. I only filled in the path field for the entry field.\nEDIT:\nCan a developer now tell me how to remove the files again? =)\nhttps://biostar.galaxyproject.org/p/13591/"}], [{"role": "user", "text": "Hi,\nI\u2019m new to RNA seq and I\u2019m trying to use blastx on galaxy to blast a fasta file of possibly novel lncRNAs using the ncbi nr database. I want to remove transcripts with significant homology to known proteins with: e-value < 1e-10, target coverage > 80%, and identity > 90%, so I can preserve transcripts that are most likely lncRNAs. I tried to change the e-value/expectation value to 0.0000000001 and query coverage to 80% but I cannot change the pident setting (percentage of identical matches). How do I do this? Also, when I run the tool on default settings it is running for more than a day."}, {"role": "system", "text": "Hi @bart_joosten,\ncould you describe to me your pipeline until now? Do you have raw RNA-seq data or assembled transcripts? Which species do the samples correspond to?\nRegards"}, {"role": "system", "text": "In addition to @gallardoalba 's question -\nyes, runtime can be substantial for this tool and will depend on how many input sequences you\u2019re trying to match against the database. This is simply how it is.\nRegarding your identity threshold: % identity is part of the output you\u2019re getting? If so, you can use standard Galaxy Filter (and possibly Text Processing) tools to perform posterior filtering."}, {"role": "system", "text": "I can also add my two cents. As already mentioned it is expected that it takes long. The ncbi nt and nr database are growing insanely fast. If I need to blast a large amount of sequences I always make a subselection first. So I only blast against homo sapien sequences for example. But this may be not so easy to do in galaxy.\nAs an alternative you could check out the diamond tool. I dont have experience with it but they claim to be 2,500 times faster then blastx. I think the key to your question is what @gallardoalba is suggesting. You could reduce your input by removing duplicate sequences for example."}, {"role": "user", "text": "@gallardoalba Hi, thanks for your reply. I\u2019m trying to find novel lncRNAs. I have used fastq files from human samples and performed trimming (trimmomatic), alignment (hisat2), assembly (stringtie) and have used the merged assembled files (stringtie-merge) in the FEELnc package to find potential new lncRNAs (outputted in a GTF File). I translated the GTF file with possible novel lncRNAs into a FASTA file and uploaded this into galaxy to use with the parameters that I have specified. As has been mentioned, the running time is very long (now 2 days) because my FASTA file is very large. When I tried to use blastx from NCBI (blastx: search protein databases using a translated nucleotide query), it would only allow files with total query length of 100k, so I understand what\u2019s causing the delay.\n@wm75 yeah you are correct if I can get an output from blastx on galaxy I\u2019ll perform posterior filtering. Thanks for the suggestion."}, {"role": "user", "text": "@gbbio I like your suggestion of using the diamond tool! It has all the parameters that I want to use on blastx and should perform faster. I\u2019m trying it right now on galaxy."}, {"role": "user", "text": "@gbbio I have tried using diamond and it works well. However the diamond output in default outputs matching protein transcripts from all taxi but I am of course only interested in human transcripts but I\u2019m having some problems selecting the human taxon id in the diamond tool. Entering the human taxon id 9606 gives an error in the diamond aligner tool (use --taxonmap parameter for the makedb command).\nFrom what I understand I have to make a database using the NCBI nr protein database as a Fasta file and several other files (taxonmap, taxonnames, taxonnodes) specified in galaxy in the diamond makedb tool. However, I have some problems uploading these files. When I try to upload the file for the taxonmap in the choose remote file option (ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz) it gives me an error: Unsupported Media Type (415). So I was wondering: do you know how to solve this problem?"}, {"role": "system", "text": "Hi @bart_joosten,\nI tried to upload the file, and I didn\u2019t find any problem history. How did you try to upload it?\nRegards"}, {"role": "user", "text": "@gallardoalba I tried it with choose remote file option and it worked perfectly"}], [{"role": "user", "text": "Trying to use Wig/BedGraph-to-bigWig tool for a file generated with mapping to CHM13 T2T but the bedgraph file I am trying to convert does not allow me to designate the reference genome as CHM13 T2T (it does not come up on the list)."}, {"role": "system", "text": "Welcome, @TTP\nThe tool Wig/BedGraph-to-bigWig interprets the assigned database metadata of the input file. This is true for any input for this tool. If no database is assigned, the tool will not recognize any input datasets (a built-in quality check).\nIf you mapped in Galaxy, the output should have the database assigned as CHM13_T2T_v2.0.\nIf you generated the data outside of Galaxy, there are two choices:\n\n\nIf you sourced the genome from UCSC, then assigning that database should be OK \u2192 Changing database/build (dbkey)\n\n\nIf you sourced from somewhere else, you can do a comparison to the UCSC version.\n\n\n\nIf not an exact match (chromosome names, sizes, sequence), you can create a custom database \u2192 Adding a custom database/build (dbkey)\n\nThat creates a custom database that can be assigned to datasets, and other tool can interpret the metadata. That database is specific for your account and works just like native database assignments.\nBe sure to use that fasta as a custom genome fasta from the history with other tools. \u2192 How to use Custom Reference Genomes?\n\nNote about genome mismatch problems: tools may error or just produce weird results. The latter isn\u2019t always obvious. This would happen with any reference data mismatch, any tool, in Galaxy or otherwise. For most use cases, the important part is to use the same reference data (genome, annotation) with all tools that are part of the same analysis.\n\nI ran some tests to make sure this is all working correctly. This is the test history and it includes a few manipulations and external downloaded files (this genome has a few aliases, and can be a bit confusing). The last dataset is the result of this tool run on a BWA-MEM result. All use the index already available in Galaxy sourced from UCSC. https://usegalaxy.org/u/jen-galaxyproject/h/test-chm13-dbkey\nHope that helps!"}, {"role": "user", "text": "Thanks. Yes, mapping works fine with BWA-MEM and the correct genome is shown. I run RMDUP on this and the genome also shows fine. But when the files go into MACS2, the bedgraph files that are generated show a \u201cunspecified\u201d database/build. When I try to put in the genome to fix this, the T2T build is not there as a choice. From this point on, none of the files generated from the bedgraph files will allow me to specify T2T as a genome build. Perhaps this doesn\u2019t matter and the data is there regardless?"}, {"role": "system", "text": "Thank you for sharing more details. I also see that my last queue job finished and failed in my test history.\nAn index file is missing! That probably explains all the problems.\n/cvmfs/data.galaxyproject.org/managed/len/ucsc/CHM13_T2T_v2.0.len\nWe will need to fix this. Thank you so much for following up. I was digging for the problem with my tests. It is a bit complex.\n\nUCSC uses hs1 to represent the assembly in their browser.\n\nThey use a couple of \u201calias files\u201d for other functions (examples are downloaded into the test history) but it all works together at their site, and what is exposed is hs1.\n\n\nWe get both the assembly and these specific index files from UCSC.\nGalaxy uses one of the other aliases to name the assembly, which creates some mismatches and missing data. This includes having no direct link out to UCSC from results due to the alias mixups.\n\nThis will take some time to sort out and repair.\nYou could use the custom genome/build option instead. This would mean starting over from mapping. I would avoid using the same dbkey/database as UCSC or Galaxy to avoid clashes with whatever the final fix is. So, give it a unique name for the database metadata attribute.\nThe source data is here:\n\nTop directory Index of /goldenPath/hs1/bigZips\n\nFasta https://hgdownload.soe.ucsc.edu/goldenPath/hs1/bigZips/hs1.fa.gz\n\nReference annotation is in the /genes directory one level down. The best choice is probably https://hgdownload.soe.ucsc.edu/goldenPath/hs1/bigZips/genes/hs1.110.20220412.ncbiRefSeq.gtf.gz\n\n\nThis workaround gives you full control when using tools in Galaxy. It will not help with viewing data at UCSC, but you can use custom genomes with both IGV and IGV. Later, once Galaxy has this repaired, you could swap the database key on datasets to what we end up assigning to this same data (probably hs1). I\u2019ll mark this topic for updates as we decide what to do.\nThank you again for following up!! I tested for 2 days trying to figure it out but your example solved the riddle  This genome is newer\u2026"}, {"role": "user", "text": "Great - thanks for helping with this. Just so I am clear though, if the bedgraphs were derived from CHM13 data but then specified as hg38 then the repeat information is there or not there?"}, {"role": "system", "text": "Hi @TTP\nUpdate! We were able to create the missing index for the CHM13_T2T_v2.0 assembly. My rerun job of the failed test was successful. You should also rerun, starting from the first step that lost the database assignment (MACS2?).\nWe will be creating a brand new set of indexes for the hs1 assembly. Likely soon.\nSo, no need for the custom genome/build choice.\nDetails:\n\nThese two assemblies have the sequence content in common\nBut they have a different database/dbkey assignments and different chromosome names, and should remain distinct assemblies.\nWe\u2019ll update the longer database names to note the differences eg which to use if the UCSC genome browser will be used, or their annotations.\n\n\n\n\n TTP:\n\nif the bedgraphs were derived from CHM13 data but then specified as hg38 then the repeat information is there or not there\n\n\nI\u2019m not sure what you mean.\nOn the MACS2 tool form, the effective genome sizes are for human (the specific assembly is not noted).\nThat genome size is simplified as the general \u201ceffective size\u201d all recent human assemblies, including hg38, CHM13_T2T_v2.0, and hs1. The Galaxy indexes are all based on unmasked sequence as single strand genomic. Why? Because that is the type data the tools work with.\nWhoops! I forgot that the MACS2 effective genome size is a bit stale, for hg19.\nI couldn\u2019t find the effective genome size for CHM13_T2T_v2.0 or hs1, but this issue at the MACS Github report explains how to calculate that (and clarified what value is default on the tool form for human). Effective genome size for HG38? \u00b7 Issue #270 \u00b7 macs3-project/MACS \u00b7 GitHub\nThat length might be the full assembly \u201cungapped\u201d length, which happens to be the same as the full assembly length. Compare:\n\n\nhg38 https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.40/\n\n\nCHM13_T2T_v2.0/hs1 T2T-CHM13v2.0 - hs1 - Genome - Assembly - NCBI\n\n\nThe MACS2 tool form has another option under this same pull-down menu to enter a custom value. Maybe try using that? Apologies for misunderstanding what you were asking about! The assembly is newer to me too, and while I understand the data conceptually, I haven\u2019t worked with it directly enough to learn all of the technical bits that are useful for practical analysis yet.\n\nMACS2-effective-genome-size1990\u00d7560 67.2 KB\n\nIf you were interested in analysis that utilized the full set the CHM13_T2T_v2.0 assembly assets, I would suggest you try exploring the original assembly author publications and resources. The assembly scope for this genome is special, and the parts that work with existing tools were exacted and added to Galaxy, UCSC, IGB, IGV, etc. Keep in mind that most of that extended data will be referenced by the root accession not the database/dbkeys these applications are using to represent one slice of that data."}, {"role": "user", "text": "Thanks; I am re-running these. I have a question about a failed multibigwigsummary run. Is this also having to do with a CHM13 issue? The error shows this:\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/usr/local/lib/python3.8/site-packages/deeptools/getScorePerBigWigBin.py\", line 18, in countReadsInRegions_wrapper\n    return countFragmentsInRegions_worker(*args)\n  File \"/usr/local/lib/python3.8/site-packages/deeptools/getScorePerBigWigBin.py\", line 107, in countFragmentsInRegions_worker\n    score = bwh.stats(chrom, exon[0], exon[1])\nRuntimeError: Invalid interval bounds!\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/multiBigwigSummary\", line 14, in <module>\n    main(args)\n  File \"/usr/local/lib/python3.8/site-packages/deeptools/multiBigwigSummary.py\", line 251, in main\n    num_reads_per_bin = score_bw.getScorePerBin(\n  File \"/usr/local/lib/python3.8/site-packages/deeptools/getScorePerBigWigBin.py\", line 257, in getScorePerBin\n    imap_res = mapReduce.mapReduce((bigWigFiles, stepSize, binLength, save_file),\n  File \"/usr/local/lib/python3.8/site-packages/deeptools/mapReduce.py\", line 142, in mapReduce\n    res = pool.map_async(func, TASKS).get(9999999)\n  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n    raise self._value\nRuntimeError: Invalid interval bounds!\n/usr/local/lib/python3.8/multiprocessing/pool.py:138: ResourceWarning: unclosed file <_io.TextIOWrapper name='/corral4/main/jobs/049/800/49800431/_job_tmp/_deeptools_0vgs42ac.bed' mode='w+t' encoding='UTF-8'>\n  task = job = result = func = args = kwds = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/local/lib/python3.8/multiprocessing/pool.py:138: ResourceWarning: unclosed file <_io.TextIOWrapper name='/corral4/main/jobs/049/800/49800431/_job_tmp/_deeptools_tba_5s4y.bed' mode='w+t' encoding='UTF-8'>\n  task = job = result = func = args = kwds = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n\n\nBoth the source data files and the bed file for this tool run fine in other combinations so it seems strange that this combination fails to run.\nThanks,\n-Tanya"}, {"role": "user", "text": "Nevermind, I worked this out."}, {"role": "system", "text": "\n\n\n TTP:\n\nRuntimeError: Invalid interval bounds!\n\n\nThis part of the error indicates a mismatch between the chromosomes + coordinates between the two inputs. Double check that all are based on the exact same reference genome assembly/build.\nYou have mentioned both hg38 and CHM13_T2T_v2.0 \u2013 so I\u2019m not sure which this was based on, but using data anchored to one assembly for all inputs will work best. Those two are close enough that you might not realize there is a mismatch problem at first. Content issues do not always trigger a tool failure, instead, tools can produce odd \u201cputatively successful\u201d results that have a scientific problem."}], [{"role": "user", "text": "Hello,\nI was looking into the web hooks available in Galaxy and from what I can tell, in terms of job scheduling, the only hook available is once the job is submitted by the user. There doesn\u2019t seem to be a hook which can be triggered when a job begins running on a cluster (using Slurm for scheduling on our HPC), or when it finishes (either success or fail).\nWould it be possible for me to add something like this to my Galaxy install, or perhaps there\u2019s an easier way to do so. Ideally this would be for all tools, so it\u2019s not quite feasible to edit all the tool XMLs and add custom execution code.\nEssentially what I would want is to POST to a URL (external REST API) with a certain message (BEGIN or END FAIL or END SUCCESS, and the submitting user\u2019s username) when jobs begin or finish.\nAny suggestions are appreciated!"}, {"role": "system", "text": "There are a few options here:\nI recommend you poll the API for /api/jobs https://docs.galaxyproject.org/en/latest/api/api.html#galaxy.webapps.galaxy.api.jobs.JobController.index\nThe other possibility is to hijack the dynamic runner functionality and write a function that makes a request before assigning to another destination: https://github.com/galaxyproject/galaxy/blob/34ee279142a3b3ec60c1d579dc4e2250a229fcd9/lib/galaxy/config/sample/job_conf.xml.sample_advanced#L667-L676\nYou could also modify the runner directly to make a request: https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/jobs/runners/drmaa.py\nEverything but polling the API will break with future releases."}, {"role": "user", "text": "Thanks for the reply. I think option 2 or 3 would be favourable.\nCorrect me if I\u2019m wrong, but option 2 would only do my function upon job submission to DRM, since the dynamic runner is just in charge of scheduling/preparing the job and not following it through its life like the DRMAA runner directly.\nIs there any reason why these would break with future updates? Is Galaxy changing the way jobs are run?\nIf what I said above is true, I think I\u2019ll have to edit the DRMAA.py runner file directly to get the functionality I want."}, {"role": "system", "text": "The issue with modifying Galaxy is that your patches will need to be updated with every release that breaks them.\nIs there a reason you can\u2019t poll the API?"}, {"role": "user", "text": "I\u2019m in the process of developing an iOS app to monitor Galaxy job status across servers, among other things which can all be done via API directly. The goal is to publish it so other users can use it as well. I\u2019m implementing push notifications for job begin/completion, hence my question about Web Hooks for those statuses.\nRegarding polling the API, this isn\u2019t something iOS supports when an app is in background state. I could setup a \u201cmain\u201d external server to poll all Galaxy servers which users link in the app, however this could quickly degrade performance when scaling to many users and/or many servers.\nAt this point, I think I will go the route of making a custom (modified version of drmaa) job runner. This would be the one step users need to do manually to get this implemented with their servers, and only if they want push notifications.\nI\u2019ll just have to remind users every new release to replace the job runner with the custom one, ensuring I stay up to date with any changes in the runner files as well"}, {"role": "user", "text": "I\u2019m not sure if this is something the Galaxy Project has considered, or would like to collaborate on\u2026"}, {"role": "system", "text": "You should definitely look at an alternative to modifying Galaxy.\nMost users don\u2019t own their copy of Galaxy and you will struggle to convince sys-admins to maintain custom deployments of Galaxy.\nI also can\u2019t think of a reason someone would need immediate notification of a job completing. If the job takes longer than the person is willing to wait, they usually go off and do other things and come back when they are ready.\nGalaxy does offer the option to send an email upon job completion. You could look into making a contribution to Galaxies code base, extending this existing functionality."}], [{"role": "user", "text": "Hi there,\nI need to upload the FASTA file of my reference genome to galaxy as a dataset and change the settings in history to allow me to select that reference genome when I import gene data of the organism that I performed RNAseq analysis on. I really need help with this please, the species I need is not in the standard database that galaxy allows you to choose from when importing data. Any help will be greatly appreciated.\nKind regards\nJ"}, {"role": "system", "text": "Hi @jasmus\nFor help with custom genome formatting as fasta, please see: Datatypes - Galaxy Community Hub\nMore help:\n\nCustom Genomes - Galaxy Community Hub\nGalaxy Training!\nTopics tagged custom-genome\n"}, {"role": "user", "text": "Hi there @jennaj\nThanks, I did try and upload the fasta file of the genome I downloaded from NCBI to galaxy as a database, but the FTP upload is not working. I used FileZilla and I cannot connect to galaxy server. I made a separate post regarding this issue, but it still hasn\u2019t been resolved it seems. Any help would be greatly appreciated.\nKind regards\nJ"}, {"role": "system", "text": "Hi @jasmus\nThere are a few more details in this topic about using FTP. Maybe it will help? I also added another tag to your post that will find most prior Q&A about FTP.\n\n  \n    \n    \n    Loading larger files to public Galaxy servers: Use FTP usegalaxy.eu support\n  \n  \n    Hi, \nI was not able to upload fastq.gz files from my PC (size around 1.5 Gb) to the usegalaxy.eu. I tried it many times with different files; the message was: Warning: Upload request failed. (0) \nIs it possible to fix it somehow?\n  \n\n"}, {"role": "user", "text": "Hi @jennaj\nSorry for the trouble, but I am still not able to upload files via FTP link. My exact problem is I am wanting to do a PLS-DA on gene count data of an organism that is not in the database that galaxy offers. This is a problem as you need to specify the organism and so on when you import data sets, I understand this is so that the program can recognize genes and annotate it correctly and so on. I really just want to resolve this as soon as possible please. If you could indicate exactly how I can solve this issue in a clear step wise manner I would truly appreciate it a lot.\nP.S. The FTP link using FileZilla just gives me a timed out error or it gives me an error saying my login is not correct, but as the other poster commented there is nothing wrong with my login details and it can\u2019t be a waiting period issue either as I have not tried it again since I originally tried to upload with FTP in quite some time now.\nKind regards\nJ"}, {"role": "system", "text": "Hi @jasmus\n\n\n\n jasmus:\n\nMy exact problem is I am wanting to do a PLS-DA on gene count data of an organism that is not in the database that galaxy offers. This is a problem as you need to specify the organism and so on when you import data sets\n\n\nYou don\u2019t need to specify the \u201cdatatype\u201d or \u201cdatabase\u201d when importing data. Most of the time is actually better to leave these at default (for both) when using the Upload functions.\nWhy?\n\nIf Galaxy guesses the wrong datatype, that could indicate a format problem.\nThe exact genome build/version usually does not apply to starting data (reads \u2013 these are associated with an organism, but not a genome build/version aka database). Once mapped or other downstream tools are used, then the result would be associated with a database (coordinates differ by the original database assembly).\nMost tools do not require that database is assigned to use a custom genome. If one does, or you just want to label your data with specificity, then load the custom genome (in fasta format) and promote it to a custom build in Galaxy. This will create a new \u201cdatabase\u201d specific to your account, and you can assign it to any dataset. I added tags to this post that point to other Q&A plus FAQs that cover the \u201chow-to\u201d.\n\nBut first, you need to get the data into Galaxy! Few questions to troubleshoot FTP. Paste back what values you are using, we might be able to spot the problem.\n\nWhat is the URL of the Galaxy server you are working on?\nWhat are you entering in Filezilla for the \u201cHost\u201d field?\nAre you using your account\u2019s email address and password (same as used on that exact same server) for the \u201cusername\u201d and \u201cpassword\u201d fields? Don\u2019t post the values back here \u2013 just confirm that is what you are using. Some people try to use their \u201cpublic name\u201d in Galaxy for the \u201cusername\u201d in Filezilla \u2013 that won\u2019t work \u2013 you need to use your full email address (this is a case-sensitive value \u2013 Me@school.edu is NOT the same as me@school.edu).\n\nThe size of the dataset(s) probably do not matter until you can connect at all. For reference, the maximum size is 50 GB for most data and ~35 GB for others (BAM in particular, possibly others).\nIf Filezilla itself is connected to the server, then times out during the data transfer, you can \u201cresume\u201d the transfer \u2013 log in with Filezilla to the target server, accept any security certificates, and find the resume function to start again. Note that this won\u2019t work if you quit out of Filezilla \u2013 the app needs to stay open. If the data is large and your connection is slow, you might need to do this a few times. In those cases, sometimes this works better if you transfer only one file at a time \u2013 but much depends on your internet upload speed.\n\n\n\n jasmus:\n\nit can\u2019t be a waiting period issue either as I have not tried it again since I originally tried to upload with FTP in quite some time now.\n\n\nThere is no waiting period or throttling for data transfer rates at the usegalaxy.* servers that I know of. Other public Galaxy sites might have that in place though. Or, your internet provider might throttle.\nLet\u2019s start from there to troubleshoot this more "}, {"role": "user", "text": "Hi there @jennaj\n\n\nThe URL I am using is https://workflow4metabolomics.usegalaxy.fr/ (I basically just want to use the Workflow4Metabolomics steps to analyse my data by PLS-DA analysis).\n\n\nI enter this into the FileZilla host field ftp://usegalaxy.org\n\n\nI am using the exact details that I used to set up my account yes. I am using my educational institute email and password. It just tries to connect me and then has an \u201cerror timed out\u201d output.\n\n\nHere is the error event log:\n\n\n\n\nStatus:\nSelected port usually in use by a different protocol.\n\n\n\n\nStatus:\nResolving address of usegalaxy.org\n\n\n\nStatus:\nConnecting to 129.114.60.56:22\u2026\n\n\nError:\nConnection timed out after 20 seconds of inactivity\n\n\nError:\nCould not connect to server\n\n\nStatus:\nWaiting to retry\u2026\n\n\nStatus:\nResolving address of usegalaxy.org\n\n\n\nStatus:\nConnecting to 129.114.60.56:22\u2026\n\n\nError:\nConnection timed out after 20 seconds of inactivity\n\n\nError:\nCould not connect to server\n\n\n\nThen regarding the upload of files so there is no issue when I upload a table of gene names and their associated counts (basically a feature counts table in a CSV format) this uploads fine and it runs fine in the PCA tool. This is because the organism (S. cerevisiae) is in your database. But when I upload a similar table in csv format for a yeast species that is not in your database then it does not recognise the gene names and it makes an additional row above gene names called \u201cV1, V2, V3, V4\u2026V5689\u201d and so on. So, if I can just solve that from happening then I can do the analysis for that yeast species too. I hope this make sense and we can figure out a solution to this please. Thank you for your help and patience thus far, I appreciate it.\nKind regards\nJ"}, {"role": "user", "text": "Hi @jennaj\nI have imported the custom genome and allowed it in my list of genomes now as an option to select. I am busy trying to run PLS-DA analysis as mentioned previously as part of the Workflow4Metabolomics package that uses Galaxy servers and the ropls package. So, I am trying to do this for datasets that I have constructed exactly as the example shows. However, my datasets are in csv file format. I am getting the following error though when I try and run this analysis on Galaxy. If you could please help I would greatly appreciate it. The error reads as follows:\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :\nline 19 did not have 5 elements\nCalls: readAndCheckF \u2192 read.table \u2192 scan\nExecution halted\nAny help would really be greatly appreciated.\nKind regards\nJ"}, {"role": "system", "text": "Hi @jasmus\nThis didn\u2019t work for two reasons:\n\n\n\n jasmus:\n\n\nThe URL I am using is https://workflow4metabolomics.usegalaxy.fr/ (I basically just want to use the Workflow4Metabolomics steps to analyse my data by PLS-DA analysis).\nI enter this into the FileZilla host field ftp://usegalaxy.org\n\n\n\n\n\n\nThe FTP URL is always specific to the server you are working on, and your account there. I don\u2019t see the current FTP URL on their site, but you could ask them. Usually, it is the last part of the server URL (for this case that would be \u201cusegalaxy.fr\u201d) \u2013 but NOT always. They have a forum specific for their community where the information may be posted, see: Workflow4Metabolomics - Galaxy Community Hub\n\n\nIf you are ever working at UseGalaxy.org, the FTP URL is \u201cusegalaxy.org\u201d \u2013 without the \u201cftp://\u201d at the start.\n\n\nThis help applies to most public Galaxy servers that support FTP. The FTP URL will be the last part of the server URL, or the server, or help on their server or directory page will specify what to use or how to ask about details. All known public Galaxy servers are in the directory here (some do not use this forum, or have a supplemental forum or contact): Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\n\n\n\n jasmus:\n\nHowever, my datasets are in csv file format.\n\n\nWhat happens if you convert csv to tabular format and input that instead? Click on the pencil icon for the dataset to reach the edit attributes forms, click into the second tab named \u201cConvert\u201d, and find the option in the drop-down menu. Only a few tools interpret csv (comma-separated values) and most expect tabular (tab-separated values).\nThe tool form will usually note what is expected/supported in the help section with examples. Another quick way to find out for any tool: create a new empty history, then load up a tool form. The options on the form where input datasets are selected will list out the expected input datatype(s).\nThat server is set up a bit differently than other public servers, with special tools/workflows etc. Their forum is probably the best place to ask questions if you run into problems. There could be a known problem or you might be the first to report something they could fix.\nHope that helps!"}], [{"role": "user", "text": "Trying to use BBMap on an uploaded FASTQ dataset but it shows as unavailable. Also does this if I upload FASTQ.GZ and unzip in Galaxy. Wondering how to get around this. Data is originally pacbio.\nThanks."}, {"role": "system", "text": "Hi @TTP\nMost tools in Galaxy expect reads in fastqsanger or fastqsanger.gz format and with one of those datatypes assigned. Those datatypes represent a specific type of quality score scaling: Sanger Phred +33.\nThis is produced by Illumina 1.8+ pipelines. PacBio used Phred+33 scaling to start with and now the scaling is neutral. You can double check with a tool like FastQC or FASTQ info to see what those report.\nThere are some tools that expect just fastq or fastq.gz (scaling unspecified). But if you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:\n\nCreate a new empty history\nLoad up the tool form\nReview the input datatypes listed in the tool form\u2019s input area\nNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\nExample\n\nwhat-datatype-is-accepted-input2346\u00d7554 87.2 KB\n"}], [{"role": "user", "text": "What is going on?! I had 75% of my space quota full just yesterday and now, when I returned today everything is GONE! The history has been purged. Clean. Zero. I had not received emails, no warnings\u2026 I have just only one account with Galaxy. Admins, what is going on?"}, {"role": "system", "text": "First question: are you still logged in to Galaxy?"}, {"role": "user", "text": "Yes, I logged off and and logged back in to no avail. I can see history logs, but there is no actual data in files as they are no longer physically present. The note on top says the history has been purged.\nI am shocked to learn how unprotected Galaxy users and their data can be. Anyone else affected recently?"}, {"role": "system", "text": "@Anton_Bryantsev\nYou had this account restored back in January 2021 after it was administratively suspended then purged as a duplicate account in October 2020.\nOnce enabled again, you created more work in a history that was technically purged and already had the old datasets in it purged. Old or new data in a purged history will continue to be screened for and removed every 14 days.\nThere is a technical corner-case problem on our side but it will only present when an account has been administratively managed. Specifically, one of your histories was not labeled as purged in the GUI until this last cycle of administrative cleanup. The other histories were labeled as purged and couldn\u2019t be worked in, and now this one is, too.\nUntil we correct that, anyone with an enabled administratively managed account should not create new work in any history that has a creation date earlier than the date the account was re-enabled. Or better, simply avoid using multiple accounts to start with. Terms of use are one account per person at each distinct public Galaxy server, including UseGalaxy.org.\nGetting an account at Galaxy Main (http://usegalaxy.org)\nWe are sorry that you lost your new work in that single history with the wrong state assigned when the account was re-enabled. But the good news is that there was not much new work, and there is one new history you created/worked in after the account was re-enabled and it was not impacted. In our email communications in January 2021, you asked for the account to be enabled with or without prior data. I re-explained the terms of usage and how you can proactively resolve multiple accounts yourself before those are administratively managed for breaking the terms of service. The processes that detect and remove multiple accounts that are in use by the same person do not always capture all of the multiple accounts an individual may have. But everyone can fix this if the terms were unclear: consolidate data into one account and delete any extras under User > Preferences. Follow the terms of service and there are never problems.\n\n  \n    \n    \n    Accounts deleted from Galaxy Main to enforce posted Terms and Conditions usegalaxy.org support\n  \n  \n    My account were active last night and running data, however this morning I tried logging in to find a message saying the account had been marked as deleted. How do I recover the data that was in the account?\n  \n\n\nThanks for helping to keep UseGalaxy.org a free and fair public resource for researchers worldwide."}], [{"role": "user", "text": "Hi All,\njust new to galaxy and I am designing a workflow whenever I insert a Trimmomatic into the workflow I can not update the numbers of inputs as when I changed from single ends to paired ends(two separate inputs) nothing changes. Am I missing a step here?"}, {"role": "system", "text": "Hi @Alex_A,\nthis works just fine for me. Which browser are you using?\nWolfgang"}, {"role": "user", "text": "thanks for the reply I am using Firefox\nKind regard,\nAlex"}, {"role": "user", "text": "Just tried it in Chrome same issue"}, {"role": "system", "text": "Seems to work in Firefox and Chromium. You\u2019re trying this on usegalaxy.eu, and what you\u2019re saying is that you can change the input option to paired-end in the tool configuration on the right side of the screen, but that this doesn\u2019t have an effect on the main panel and the tool shown there?"}, {"role": "user", "text": "Thanks for the help, I was using https://usegalaxy.org but I switched to https://usegalaxy.eu/ and it works perfectly thanks for the help."}, {"role": "system", "text": "That\u2019s good to know, but it should work on .org as well. I\u2019ll let the admins there know about the issue.\nThanks for reporting."}, {"role": "system", "text": "@mvdbeek is this https://github.com/galaxyproject/galaxy/pull/10626?\nDoes .org not have the commit yet?"}, {"role": "system", "text": "It does not have the PR, I\u2019ll update now."}, {"role": "system", "text": "Alright it\u2019s all up to date and @Alex_A example works fine."}], [{"role": "user", "text": "Hi,\nI am a new user on Galaxy.eu. I have set up a run for RepeatMasker of my (large, 2.5G) de novo genome assembly. After a few days, it\u2019s still running. Is it normal, or should I have turned on some parameters such as only rodent specific repeat, or skip bacteria insert, etc to make process much quicker? I had no idea what the results would look like, so I chose the default setting. Thanks!"}, {"role": "system", "text": "Hi @daikez,\nwe have increased the assigned cores for RepeatMasker; please repeat the analysis in a few hours.\nRegards"}], [{"role": "user", "text": "Hello,  Yesterday I generated a .bam file using a local galaxy instance.  It was viewable from history.  This morning I can only see the file (.dat file) from /home/ubuntu/galaxy/database/files/000/dataset_1.dat.\nHow do I \u201crestore\u201d this file to my Galaxy UI history ?  Should I download the .dat file and re-upload into History?  Since the filesize is about 18 GB I really want to use the most efficient method for its restore.\nPlease let me know whether I should include any additional information.  There were no error messages or messages of any kind.\nThank you!"}, {"role": "system", "text": "Have you been logged in to your user account when generating the dataset yesterday, and are you still logged in today?"}, {"role": "user", "text": "Thanks for your quick reply.  Yes, I was logged into my account yesterday while generating the dataset.  I could see it in my history while still logged into galaxy.  Yesterday evening I logged out and this morning I logged back into galaxy and could not locate my dataset in history."}, {"role": "user", "text": "I will no longer logout of galaxy in the evening.  Can I retrieve the .dat file from disk and re-load it into my history ?"}, {"role": "system", "text": "Sure, that will work though it\u2019s not fixing the underlying issue, and you should definitely be able to log out of your account without losing access to data."}], [{"role": "user", "text": "Hello everyone,\nI\u2019m currently in the process of performing DEGS idenitification with DESEQ2 in the galaxy pipeline.\nI performed all my steps with Galaxy.\nI performed whole RNA transcriptome sequencing. The obtained FASTq files I imported into galaxy and then I performed the following steps:\n\nFASTP\nFASTQC\nRNA-STAR\nfeature counts\n\nAS next step I wanted to perform DESEQ2 analysis to identify DEGS between the two groups of samples.\nI input the feature counts file into the DESEq2 pipeline and I started to perform the analysis.\nAfter 10s I get the following error: \u201cError in data.frame(sample = basename(filenames_in), filename = filenames_in,  :   duplicate row.names: /data/dnb06/galaxy_db/files/f/b/0/dataset_fb0b5307-b5a9-4a57-ba8f-a3077494c305.dat\u201d.\nHowever, I don\u2019t have any duplicates. I have 58051 rows of individual genes without any duplicates.\nHas anybody encountered the same issue?\nThank you very much for your help!"}, {"role": "system", "text": "Hi @cannico\n58k is somewhat high for gene names. Do you use transcripts?\nDo you use individual count tables per sample or merged table? Do you have spaces in the sample names? Make sure the sample names and conditions start with a character, not a number and no space characters.\nIf you still have an issue, I can have a look. What Galaxy server do you use? Are you OK with the history sharing using URL?\nKind regards,\nIgor"}, {"role": "user", "text": "Hi @igor\nthank you for your reply. Yes, I use Ensemble IDs as input. I will attach a screenshot as an example file.\nFor the feature counts I used individual files for each sample. So I have 25 total samples, 15 are the positive group and 10 are the negative group. That is how all files are named: featureCounts_SF-13-516_positive. I use the galaxy europe server.\n\ntranscript id701\u00d7911 10.7 KB\n.\nI hope this helps,\nNico"}, {"role": "system", "text": "Hi @cannico\nby any chance, have you selected the same sample(s) into control and experiment conditions/group?\nI completed featureCounst and DESeq2 using ENSEMBL gene annotation in Galaxy Europe, so it looks like something in the data or the job setup.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nthank you for your reply again. I checked your suggestion and they are definitely different samples.\nI did another check and I only used 3 samples per group and it now it gives me the following error: Warning message:\nIn a$V1 == l[[1]]$V1 :\nlonger object length is not a multiple of shorter object length.\nAfter looking up that error, it means that the vectors have different lengths. I can\u2019t explain myself that error, since every file has 58,051 lines and also galaxy displays that per file. Do you have any other ideas?\nNico"}, {"role": "system", "text": "Hi @cannico\nif you share the history, I can have a look. Maybe copy the last failed job and the input files into a new history and share it. Datasets can be copied in multiple histories view mode. History sharing is in History menu (icon at the top right corner of the history panel) > Share or publish. You can paste the URL here.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi @igor\nhere is the history link.\nLet me know if you need further information and thank you for your time.\nNico"}, {"role": "system", "text": "Hi @cannico\nsample #20 is specified in both conditions.\nNote two black angles at the bottom right corner of sample selection window. Move the mouse of these two angles and drag it down to increase the window size. Do it for the both conditions and check samples used for the job.\nHope that helps.\nIf the issue is resolved, unshare/unpublish the history.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI have been working on metagenome data. I am using the vserach dereplication tool. And it has been running for two days and still it is showing the run status.\nHow much time will it usually take to complete? and is it a normal run time.\nPlease help and guide"}, {"role": "system", "text": "Hi @Ghazal_Aziz,\nit depends on your input data; did it finish successfully?\nRegards"}, {"role": "user", "text": "yes it did after 2 days. thankx"}, {"role": "user", "text": "Hi,\nI want to use GraphAln2 to view the community profile of my samples. but I am unable to. can you help ,me for the input file? what should be the input file\nthe following is the link for the GraphAln2 output which I am totally unable to understand.\nhttps://metagenomics.usegalaxy.eu/datasets/11ac94870d0bb33a2909bf94e4b31634/display?to_ext=png\nThank You"}, {"role": "system", "text": "Hi!,\ncould you share the history with me? I haven\u2019t access to the image.\nRegarding the input file, this is an example: Graphaln2.\nRegards"}, {"role": "user", "text": "https://metagenomics.usegalaxy.eu/datasets/11ac94870d0bb33a2909bf94e4b31634/display?to_ext=png"}, {"role": "user", "text": "\nurl: https://metagenomics.usegalaxy.eu/u/ghazal_aziz/h/copy-of-nplps05imutec\n"}, {"role": "user", "text": "is this url helpful?"}, {"role": "system", "text": "Yes, thanks!"}, {"role": "system", "text": "According to the error report, it seems to be a bug in this script tools-iuc/export2graphlan.xml at master \u00b7 galaxyproject/tools-iuc \u00b7 GitHub. I\u2019ll review it.\nRegards"}], [{"role": "user", "text": "Do you know the meaning of the following error messages from HISAT2 after mapping?\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of \u2018int\u2019\n(ERR): hisat2-align died with signal 6 (ABRT)\n[bam_sort_core] merging from 6 files and 1 in-memory blocks\u2026\nEspecially the \u201c(ERR)\u201d one, I\u2019m afraid my alignment might have not been completed at all."}, {"role": "system", "text": "Hi Mario\nThis is paired end data, isn\u2019t-t? And you have provided two fastq files\u2026, well, the alignment process stop (i.e.: died) because of the different number of reads in the two fastq files. Maybe one file is corrupt (e.g.: incomplete download) or something went wrong during the pre-processing?\nCheck the integrity of the files with FASTQC\nRegards, Hans-Rudolf"}], [{"role": "user", "text": "I have the metagenome data as fastq.gz files i have uploaded them and unable to form contigs (make.contigs). what would be the possible reason? what type of data is required for the contigs?"}, {"role": "system", "text": "Hi @Ghazal_Aziz\nThe tool expects reads in fastqsanger format. Try Upload without specifying the datatype. Galaxy will detect and assign the datatype that matches the best. If the upload fails or the datatype is not what you expect, this indicates a format or content problem. You may need to reload the data or possibly correct the data if the source is your own computer. QA or data manipulation tools can be used to validate most uploaded data.\n\nGTN tutorials that cover Mothur and related Metagenomics tools/workflows are here:\n\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Metagenomics\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you are not sure how to Upload read data in a way that tools can interpret, please review the NGS data logistics tutorial here\n\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Introduction to Galaxy Analyses\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nand the Quality Control tutorial here\n\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Sequence analysis\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi,\nI am trying to do single-cell data analysis, but when import data with \" Import Anndata and loom from different format\" and then do \u201cthe full data matrix\u201d inspect with \" Inspect AnnData object\", I always get the ERROR, I help you can help me to settle my problem, you can view and import my History by visiting the following URL: Galaxy | Europe | Published History | wfq"}, {"role": "system", "text": "Hi @webyoung\nWas the error generated in this same history? The error in dataset 5 is recorded as using a \u201cdataset 5\u201d for the input.\nRerun is in here \u2013 it is using dataset 4 as a input, the only valid input in this history. Galaxy | Europe | Accessible History | imported: wfq https://help.galaxyproject.org/t/error-of-inspect-anndata/8783 10-4\nThe rerun did fail, and the error message includes this. It means that the data is actually too large to parse or that there is a format problem.\n\nMemoryError: Unable to allocate array with shape (6794880, 31053) and data type float32\n\nThe parameter for the mtx version might be the problem. I started up a rerun for the h5ad creation (with an option modified to match the version header in dataset 3) then the downstream tool you had a failure with. Might reveal more about what is going on.\nWrite back when you are done with the history I shared back and I\u2019ll purge that copy."}, {"role": "user", "text": "Actually, I copied my step of running and pasted in the history, but it is the problem what I have met."}, {"role": "system", "text": "\n\n\n webyoung:\n\nActually, I copied my step of running and pasted in the history\n\n\nOk, that is what it looked like. Next time you share a history for help, please leave all of the inputs and outputs used for the original job in the same history. That might mean you need to copy the inputs into a new history \u2013 but one rerun is always recommended anyway. You could also clone the entire original history, purge anything not related to the question, then share.\nSometimes the technical details matter, especially when the error message is so odd that some help/second opinion is wanted. Leaving everything intact is how others can best help here.\nThe test jobs are still running in my copy of the history. I\u2019ll keep leaving it shared for now \u2013 unless you resolved the problem already? I think that the mtx version was set incorrectly. It is a bit tricky \u2013 there is a file version and a software version noted in the original header. The tool form just states \u201cversion\u201d. That related to the software version as far as I know, and is what the tests are for primarily. The tests also include generating some basic statistics, not just the full tabular output that you originally wanted. If the statistics can be generated but the full output cannot be, that means the file is too large to process all at once. But that isn\u2019t confirmed yet."}, {"role": "user", "text": "OK, next time, I am going to share a history for help just as you suggested. I noticed the output that you have run also showed error, I am looking forward to seeing a good method to settle this problem."}, {"role": "system", "text": "Hi @webyoung\nThe tests are done now.\n\n\nCreating an object with all of the raw data is too large to process. This might be something that can be adjusted. Send in a bug report from your error (a failed run in the same history with the exact inputs actually used) and the EU admins can investigate options. In short, the tool is being sent to a cluster node that isn\u2019t large enough to process the fully expanded data array.\n\n\nPre-filtering the raw data first does create data that works with this tool, and others, and is what you will likely want to do anyway for practical analysis reasons (quality). I filtered using defaults \u2013 but you can tune that.\n\n\nThe original history has an example of that now and the sc-RNA tutorials here have many more details.\n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello everyone,\nI have installed a local instance of Galaxy (release_16.07) on my CentOS (7.3.1611) server and added a custom tool for taxonomic analysis using qiime2.\nWhen I run the tool I get the error:\nUnicodeEncodeError: 'latin-1' codec can't encode character u'\\u2018' in position 408: ordinal not in range(256)\nbut the job command line produced by the tool actually works when typed in the terminal.\nI had noticed this type of behaviour in other different tools as well and noticed that when they produced an output like a warning or any additional output, Galaxy mistakes it as an error and produces the same error.\nNone of the tools have anything to do with MySQL, I think it\u2019s some kind of internal error from Galaxy\u2019s database or something.\nSo far I managed to get around that by redirecting stderr ( 2>/dev/null ) but in this case nothing works.\nCould you help me out?\nThank you in advance,\nThe full error is the following:\nTraceback (most recent call last):\n  File \"/home/user/galaxy/lib/galaxy/jobs/runners/local.py\", line 128, in queue_job\n    job_wrapper.finish( stdout, stderr, exit_code )\n  File \"/home/user/galaxy/lib/galaxy/jobs/__init__.py\", line 1332, in finish\n    self.sa_session.flush()\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py\", line 150, in do\n    return getattr(self.registry(), name)(*args, **kwargs)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2004, in flush\n    self._flush(objects)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2122, in _flush\n    transaction.rollback(_capture_exception=True)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\", line 60, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2086, in _flush\n    flush_context.execute()\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 373, in execute\n    rec.execute(self)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 532, in execute\n    uow\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 170, in save_obj\n    mapper, table, update)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 692, in _emit_update_statements\n    execute(statement, multiparams)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 914, in execute\n    return meth(self, multiparams, params)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 323, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1010, in _execute_clauseelement\n    compiled_sql, distilled_params\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1146, in _execute_context\n    context)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1344, in _handle_dbapi_exception\n    util.reraise(*exc_info)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1139, in _execute_context\n    context)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 450, in do_execute\n    cursor.execute(statement, parameters)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/cursors.py\", line 187, in execute\n    query = query % tuple([db.literal(item) for item in args])\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/connections.py\", line 278, in literal\n    return self.escape(o, self.encoders)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/connections.py\", line 208, in unicode_literal\n    return db.literal(u.encode(unicode_literal.charset))\nUnicodeEncodeError: 'latin-1' codec can't encode character u'\\u2018' in position 408: ordinal not in range(256)"}, {"role": "system", "text": "\n\n\n TYRANISTAR:\n\nI have installed a local instance of Galaxy (release_16.07)\n\n\nI recommend using 19.05 instead. You are missing 3 years of bugfixes and 16.07 is not really supported anymore."}, {"role": "user", "text": "At this point it is not really feasible to upgrade as I am currently using it for a research project\u2026 Is this a bug related to the specific version?"}, {"role": "system", "text": "Hard to say, you can check the release notes for encoding fixes: https://docs.galaxyproject.org/en/master/\nMy guess is you are using some (date affecting) locale that is being stored in your mysql database but fails due to encoding.\nThat said upgrading Galaxy version should not affect reproducibility of your analyses."}, {"role": "user", "text": "Well that idea certainly backfired\u2026 I tried to upgrade my galaxy instance but initialization failed as it got stuck trying to upgrade the database (it stuck around version 143). I reverted the installation to each previous setup but the upgraded version of the database was incopatible and when I tried\nsh manage_db.sh downgrade 132\nI still got an error:\nsh manage_db.sh downgrade 132\nActivating virtualenv at .venv\nTraceback (most recent call last):\n  File \"./scripts/manage_db.py\", line 22, in <module>\n    invoke_migrate_main()\n  File \"./scripts/manage_db.py\", line 19, in invoke_migrate_main\n    main( repository=repo, url=db_url )\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/shell.py\", line 209, in main\n    ret = command_func(**kwargs)\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/api.py\", line 202, in downgrade\n    return _migrate(url, repository, version, upgrade=False, err=err, **opts)\n  File \"<string>\", line 2, in _migrate\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/util/__init__.py\", line 160, in with_engine\n    return f(*a, **kw)\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/api.py\", line 345, in _migrate\n    changeset = schema.changeset(version)\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/schema.py\", line 82, in changeset\n    changeset = self.repository.changeset(database, start_ver, version)\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/repository.py\", line 225, in changeset\n    changes = [self.version(v).script(database, op) for v in versions]\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/repository.py\", line 189, in version\n    return self.versions.version(*p, **k)\n  File \"/home/omic-engine/galaxy/.venv/lib/python2.7/site-packages/migrate/versi                                                                                                             oning/version.py\", line 163, in version\n    return self.versions[VerNum(vernum)]\nKeyError: <VerNum(144)>\n\nAny ideas?"}, {"role": "system", "text": "If you did not make backups before, make backups now.\nTo be able to downgrade db you need to have the newer codebase (which has the migration script). I think you have the older code but are trying to downgrade a newer database.\nWhat version were you trying to upgrade to?"}, {"role": "user", "text": "I am not sure which database the 19.05 release was trying to upgrade to I just know it stopped at 143 and then I couldn\u2019t get back at 132 which was needed for 16.07. Anyway I installed the 16.07 from scratch and moved all my tools there again. I will try to bypass my initial error some other way."}, {"role": "system", "text": "You can always see what version your database is at by checking the migrate_version table in the database.\nfor the unicode error this could be a lead: https://github.com/galaxyproject/galaxy/issues/7421"}], [{"role": "user", "text": "Hi, everyone\nI am familiar with Galaxy, but only now have I started setting it up on our local computer (no server, just a well-built computer running in UNIX). I have succeeded to install it, set admin accounts, download reference files (i.e. reference genomes) and install new packages.\nMy problem now is, how can I set directories to store data (i.e. reference files, output files, and temp job files, if possible)? Moreover, how to set them to a location on another disk volume, other than the one galaxy was installed?\nI have tried troubleshooting from this official link and this post, but I cannot make it work.\nFor one thing, I believe the galaxy.ini file has been replaced by galaxy.yml file. Is that correct?\nIf so, I found some lines similar to what is referenced on the official link, and did the following changes:\nOn line 178, it reads:\n# on any cluster nodes that will run Galaxy jobs, unless using Pulsar.\nfile_path: /media/mol/hdmol/galaxy/database/files\n\n# Where temporary files are stored. It must accessible at the same\n# path on any cluster nodes that will run Galaxy jobs, unless using\n# Pulsar.\nnew_file_path: /media/mol/hdmol/galaxy/database/tmp\n\nNote: /media/mol/hdmol is the path to the other disk volume. And all sub-directories from galaxy were copied to that location. Hence, I as using the exact same sub-directories, and changing only the volume (to /media/mol/hdmol/).\nAny help is very much appreciated.\nThank you!"}, {"role": "system", "text": "Galaxy uses an filesystem-independent \u2018Object Store\u2019 to store files. here is a  very good tutorial on how to configure it so you can store files at different volumes.\nAs of the reference data, here is another tutorial on how to attach our shared reference data repository to your Galaxy instance, so users can use hundreds of references prepared by us.\n\n\n\n dodausp:\n\nI believe the galaxy.ini file has been replaced by galaxy.yml file. Is that correct?\n\n\nThis is correct.\nPlease ask if you have more questions."}, {"role": "user", "text": "Thanks a lot, @marten!\nAnd thank you for the references! I have read them through, but one thing puzzled me. Right at the beginning it mentions\n\nThis tutorial assumes you have done the \u201cAnsible for installing Galaxy\u201d\n\nI have installed galaxy on our machine by following the directions from the Use Galaxy webpage, rather than Ansible. (to be honest, I am not familiar at all with Ansible). I believe it is based on their git repository.\nSo, either I could not find the object_store_config_file and galaxy_config_files, or they do not exist. Moreover, those lines described on the config/object_store_conf.xml file are also absent on the file I have here (but it is the exact same name).\nIn any case, I did change all the path lines on the object_store_conf.xml by the directory I was aiming for (it is, /media/mol/hdmol/galaxy/), and it still didn\u2019t work. It kills the sudo sh run.sh without initializing the program.\nAny other suggestions? I am very lost.\nAnd thanks again for the help!"}, {"role": "system", "text": "The document you linked is the Galaxy startup 101 i.e. starting Galaxy for the first time for exploration/development.\nFor anything more serious, with actual users and resources we heavily recommend (and build our tutorials on) Ansible - which is a software for configuration and deployment of applications. That is the best practice for running an actual production Galaxy.\nThere are training that will teach you how to use Ansible in the admin section of the training materials: https://training.galaxyproject.org/training-material/topics/admin/\n\nNow to get your thing working:\nThe sample configuration of object store is distributed with Galaxy at https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/object_store_conf.xml.sample\nWould you mind sharing your object_store_conf.xml so I can have a look?"}, {"role": "user", "text": "Thanks again, @marten!\nI read the documentation and those 10 tips before installing it, and thought that just installing a local instance, as that explained on the tutorial would be enough. Ok, so if you allow me to take a small turn here, before moving on with the original topic: my need for now is to install Galaxy and some other tools in a local computer (good fairly enough to run genomics data in a small scale), so just a couple of users can use it.\nThat being said, would you still think it is a good idea to move on to install a production Galaxy?\nNow, back to subject, here is how the object_store_conf.xml file looks like with the editing:\n<object_store type=\"hierarchical\">\n    <backends>\n        <object_store type=\"distributed\" id=\"primary\" order=\"0\">\n            <backends>\n                <backend id=\"files1\" type=\"disk\" weight=\"1\">\n                    <files_dir path=\"/media/mol/hdmol/galaxy/database/files1\"/>\n                    <extra_dir type=\"temp\" path=\"/media/mol/hdmol/galaxy/database/tmp1\"/>\n                    <extra_dir type=\"job_work\" path=\"/media/mol/hdmol/galaxy/database/job_working_directory1\"/>\n                </backend>\n                <backend id=\"files2\" type=\"disk\" weight=\"1\">\n                    <files_dir path=\"/media/mol/hdmol/galaxy/database/files2\"/>\n                    <extra_dir type=\"temp\" path=\"/media/mol/hdmol/galaxy/database/tmp2\"/>\n                    <extra_dir type=\"job_work\" path=\"/media/mol/hdmol/galaxy/database/job_working_directory2\"/>\n                </backend>\n            </backends>\n        </object_store>\n        <object_store type=\"disk\" id=\"secondary\" order=\"1\">\n            <files_dir path=\"/media/mol/hdmol/galaxy/database/files3\"/>\n            <extra_dir type=\"temp\" path=\"/media/mol/hdmol/galaxy/database/tmp3\"/>\n            <extra_dir type=\"job_work\" path=\"/media/mol/hdmol/galaxy/database/job_working_directory3\"/>\n        </object_store>\n\nNote: as mentioned on the original post, I pasted all the directories and sub-directories of Galaxy to the volume I would like to have the data on. Hence, I just added /media/mol/hdmol/galaxy/."}, {"role": "system", "text": "@gbbio No worries. We made that mindset switch recently and not all of our website reflect it. That said what you linked is a code documentation and the low level details - and I believe they are correct.\nStill the best practice for production instance is to use Ansible to configure these detail and Galaxy at large, and we provide so called \u2018ansible playbooks\u2019 that help with many of these details, because they do them for you."}, {"role": "system", "text": "First, that is not a valid XML (it misses two closing tags).\nSecond, I think you do not want to use hierarchical or distributed OS, so something like this should suffice:\n <?xml version=\"1.0\"?>\n <object_store type=\"disk\">\n    <files_dir path=\"/media/mol/hdmol/galaxy/database/files1\"/>\n    <extra_dir type=\"temp\" path=\"/media/mol/hdmol/galaxy/database/tmp1\"/>\n    <extra_dir type=\"job_work\" path=\"/media/mol/hdmol/galaxy/database/job_working_directory1\"/>\n </object_store>\n\nThird, make sure you tell Galaxy about this store configuration by putting it in the galaxy.yml\nobject_store_config_file: config/object_store_conf.xml\n"}], [{"role": "user", "text": "Hi all,\nI created a bash script that runs a tool and then it takes some of the output files put them in  a directory that I\u2019ve created for this tool and copy them into some bash variables:\ncp ${output_dir}/file1.txt $2\ncp ${output_dir}/file2.txt $3\nTherefore in the XML tool definition file that runs this bash script I specify this outputs:\nsh -e $__tool_directory__/tool.sh\n      $input \n      $file1\n      $file2\n\n  <outputs>\n            <data name=\"file1\" format=\"txt\" label=\"file 1\"/>\n            <data name=\"file2\" format=\"txt\" label=\"file 2\"/>\n    </outputs>\n\nSo I force Galaxy to put this scripts in a directory created by me. Is this wrong? Or should I let Galaxy put the output files in the default directory?"}, {"role": "system", "text": "Hi Patri,\ncan you explain better what your tool should do? can you paste the tool.sh script?"}, {"role": "user", "text": "This is a wrapper in bash that runs my python tool (mytool.py) that takes one input and produces two outputs. Then it puts this outputs in a folder created for this tool. And I force Galaxy to put the outputs in this folder. In fact after I run this tool in Galaxy I have all the files in this folder \u201c/results/output-tools/MYTOOL/sessions/${session_dir}\u201d and not in the default folder in which Galaxy stores the outputs one /database/files. Is this ok or it may cause some sort of problem since I am running a Galaxy instance with a lot of tools that I integrated in this way on a server?\n#!/bin/sh\n\n# run program\nrun_output=$(python mytool.py --input $1)\n\n# get session name\nsession_dir=$(echo \"${run_output}\" | grep \"Session:\" | sed \"s/^Session:\\ //\")\n\n# folder with the session name\noutput_dir=`\"/results/output-tools/MYTOOL/sessions/${session_dir}\"`\n\n# check if the output files exist\nfor f in \"file1\" \"file2\"; do\n        if [ ! -e ${output_dir}/$f ]; then\n                exit 1\n        fi\ndone\n\ncp ${output_dir}/file1.txt $2\ncp ${output_dir}file2.txt $3"}, {"role": "system", "text": "It looks complicated. I personally create a temp directory like:\ntempfolder=$(mktemp -d /galaxy/galaxy/database/files/XXXXXX)\nAnd then execute python like:\npython mytool.py --input $1 --outputdir $tempfolder\nAnd then move the files like:\ncp ${tempfolder}/file1.txt $2\nDon\u2019t know if it is preferred, it is just to give you inspiration. Btw, don\u2019t forget to remove your\noutput_dir when you are done.\nEDIT:\nJust read the second post better. You do have the output double now so in your own folder and in a galaxy folder."}, {"role": "user", "text": "But if I remove the folder, the output files will still show up in Galaxy?\nThe fact is that my tool put all the outputs in a specific dir and i need Galaxy to retrieve them in that specific folder. But as you said I have the the data in the galaxy folder and in my own folder. Maybe I should just put all the files where the script runs by moving them out of the session folder I created and let galaxy deal with them."}, {"role": "system", "text": "Yes, but you remove the folder after you moved or copied it to galaxy. So it will be the very last line of the bash script. $2 and $3 are basically paths."}], [{"role": "user", "text": "Hi i am using paired end data and downloaded the sra file from ncbi  and uploaded it on galaxy server. and can i use this data directly for assembly or first i have to split the files then use it"}, {"role": "system", "text": "Hi @NITISH_DAVE\nIt depends on which assembly tool you are using, but some do require that interleaved reads are split into forward/reverse first. None accept an sra archive directly (that I am aware of). QA/QC is also strongly recommended (FastQC, fastp, Trimmomatic).\nPlease see these FAQs, in particular, the last one:\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nReformatting fastq data loaded with NCBI SRA\n\nAlso, there two primary tools to extract fastq data from NCBI. The first will output data as it is formatted from NCBI. The second will split any pairs and place all data into Dataset Collections and can extract fastq sequences from sra data that has been Uploaded (probably the option you need).\n\n\nDownload and Extract Reads in FASTA/Q  format from NCBI SRA\n\nFaster Download and Extract Reads in FASTQ  format from NCBI SRA\n\nThanks!"}, {"role": "user", "text": "\n\n\n jennaj:\n\nFaster Download and Extract Reads in FASTQ format from NCBI SRA\n\n\nthank you for reply i used Faster Download and Extract Reads in FASTQ  format from NCBI SRA and it yielded 4 files however according to metadata it contains data from two different tissue but here i only observed pair end data with separate forward and reverse file. where can i find separate data as the data is presented in single SRA file at ncbi."}, {"role": "system", "text": "\n\n\n NITISH_DAVE:\n\naccording to metadata it contains data from two different tissue but here i only observed pair end data with separate forward and reverse file. where can i find separate data as the data is presented in single SRA file at ncbi.\n\n\nI\u2019m sure that I understand.\n\nDid you enter a distinct accession for each distinct sample? The form accepts individual or lists of accessions.\nOr, did you extract data from the SRA archive that you had already loaded? Which archive (URL)?\n\nThanks?"}, {"role": "user", "text": "I used one accession which i extracted using Faster Download and Extract Reads in FASTQ format from NCBI SRA. does that single accession contains both tissue data? Below is metadata for reference.\nRun, Assay Type, AvgSpotLen, Bases, BioProject, BioSample, BioSampleModel, Bytes, Center Name, Collection_Date, Consent, DATASTORE filetype, DATASTORE provider, DATASTORE region, dev_stage, Experiment, geo_loc_name_country, geo_loc_name_country_continent, geo_loc_name, Instrument, Isolate, Isolation_source, Library Name, LibraryLayout, LibrarySelection, LibrarySource, Organism, Platform, ReleaseDate, Sample Name, sample_type, SRA Study, tissue\nSRR4299636, CLONE, 298, 9557003942, PRJNA344442, SAMN05823576, Plant, 3924705078, TIANJIN UNIVERSITY OF SCIENCE AND TECHNOLOGY, 2015-11-27, public, sra, \"ncbi,gs,s3\", \"gs.US,s3.us-east-1,ncbi.public\", Young leaf, SRX2194189, China, Asia, China, NextSeq 500, Leaves, soil, PRJNA344442SAMN05823576, PAIRED, cDNA, TRANSCRIPTOMIC, Azadirachta indica, ILLUMINA,2017-04-26T00:00:00Z, Neem advewntitioius root, Tropic plant, SRP090539, Neem adventitious root in MS medium\n\nadmin edit: text reformat"}, {"role": "system", "text": "\n\n\n NITISH_DAVE:\n\nSRR4299636\n\n\nThe RUN accession represents a single sequencing event. In this case, it is paired-end fastq data. At the highest level, the BioProject accession only includes one sequencing run. Same for the SRA Study accession. There is no information indicating that more than one tissue was sequenced (just Neem adventitious root in MS medium).\n\nhttps://www.ncbi.nlm.nih.gov/bioproject/PRJNA344442\nhttps://www.ncbi.nlm.nih.gov/search/all/?term=SRP090539\n"}, {"role": "user", "text": "Thanks again for reply, in their research manuscript they have mentioned about neem leaf and neem adventitious root transcriptome so, i thought  that they done sequencing for  both leaf and root sample. I have attached the image please have a look. can you suggest any way to compare the differential gene expression between these two tissue.\n\ndata597\u00d7549 54.2 KB\n  "}, {"role": "system", "text": "Publications should clearly reference all data.\nThe SRR accession appears to be a match for the AR reads, and are the only reads submitted for the BioProject.\nChoices:\n\nContact the author(s) for clarification.\nConsider the publication, and associated data, flawed and avoid. The SRR submission has some issues: Library Name is oddly annotated. Sample Name has a typo. ReleaseDate is after the paper\u2019s publication date.\n"}, {"role": "system", "text": "For your Trinity questions:\n\nTrinity requires that paired-end inputs are \u201cmatched pairs\u201d. Meaning, both ends of the same read are input.\nIf one of the ends fails QA/QC (Trimmomatic, Fastp, others), then the other associated end cannot be used, even if it happens to passes QA.\nWhen extracting from SRR, the original data will be paired.\nWhen running through QA tools, the data can become un-paired. That said, the  output from QA tools, for example Trimmomatic, the data will be sorted into four datasets.\n\nPaired forward + Paired reverse = use these for assembly inputs\nSingle forward + Single reverse = do not use these for assembly inputs. One end of the original pair did not pass QA, and the assembly will fail if input.\n\n\n\nPlease be aware of a few current factors that can impact assembly success/failures when using the public Galaxy Main https://usegalaxy.org server right now. There is a banner on the server explaining. More details:\n\n\nTrinity and Unicycler are running with reduced memory allocation at this time.\nMake sure to use the most current version of all tools, or unexpected problems can occur. The most current version of any tool\u2019s form will load from the Tool Panel.\nIf your job fails, confirm that you are using the most current tool version.\n\n\nIf not, rerun using the updated version.\nIf yes, then the failure may be due to the reduced memory resources. Try one rerun. If that fails again, there may be some other problem with your inputs. How to check for common input problems is discussed in the topic below.\nMy inputs are Ok \u2013 How to work-around the reduced memory allocation? a) Consider using an alternative public Galaxy server b) Decide if down/sub-sampling your inputs will meet your goals (see Seqtk tools).\n\n\n  \n    \n    \n    Troubleshooting resources for errors or unexpected results usegalaxy.org support\n  \n  \n    Check to see if there is a known usegalaxy.* server issue. \n\nServer Status: https://status.galaxyproject.org/\n\n\nIf the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\n  \n\n\nHope that helps!"}], [{"role": "user", "text": "Hello,\nDoes any one else encountered a \u201cInternal Server Error (500)\u201d when trying to upload files? Does anyone know how to resolve this?\nBest"}, {"role": "system", "text": "Hi @roland\nplease add a server tag to your posts or let us know what Galaxy do you use.\nIt can be a transient error. Upload a small test file or in upload menu switch to Paste/Fetch Data tab, type something in the window (testing only is a good one) and click Start. Do you still see error 500?\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor\nSorry for my late reply.\nI use this ribogalaxy here https://ribogalaxy.genomicsdatascience.ie/. I tried your solutions but I still get the 500 error.\nBest,\nRoland"}, {"role": "system", "text": "Hi @roland\ncheck forum and help  links for ribogalaxy on this page:\n  \n      \n\n      galaxyproject.org\n  \n\n  \n    \n\nRiboGalaxy - Galaxy Community Hub\n\n  All about Galaxy and its community.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nMaybe contact the server support directly.\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you very much!"}, {"role": "system", "text": "Hello @roland,\nI have the same problem, trying to reproduce the test example from the hands-on tutorial from Ribogalaxy home page (hands-on-tutorial-for-RiboGalaxy-v2-1.pdf).\nHowever I get an Internal Server Error (500) like you:\n\nimage901\u00d7548 28.7 KB\n\nHave you found any solution to your problem ? I do not find a support contact. The Robogalaxy forum seems to be under maintenance for quite some time now (https://gwips.ucc.ie/Forum/)\nBest,\nPaul"}, {"role": "system", "text": "At this point I\u2019d email to ribogalaxy@gmail.com and ask them for help directly."}, {"role": "user", "text": "Hello!\nI reported the bug to ribogalaxy@gmail.com and they fixed it back then. It seems that the problem reoccured. My samples are also stuck in the workflow. No idea when it will work again.\nBest"}, {"role": "system", "text": "Hello,\nthanks for your replies @roland and @marten !\nTo avoid being stuck, I did a local installation of galaxy in the end (Get Galaxy - Galaxy Community Hub), and installed the ribogalaxy packages and tools from the Tool Shed as admin.\nBest,\nPaul"}], [{"role": "user", "text": "Hello!\nIn my galaxy there are 3 users with 474, 312 and 0 gigabytes of used space as galaxy says. However the database actually uses 1.2 TB of disk space. I ran cleanup scripts Cleaning up Dataset Objects but they freed exactly 0 bytes.\nWhat else my database contains that is not shown and can not be deleted by scripts? Can i delete it in some other way?\nThanks in advance."}, {"role": "system", "text": "Hi @wormball, are you running Postgresql as the database? If so then you may need to run VACUUM FULL. You will need to stop Galaxy for this work though. PostgreSQL: Documentation: 12: VACUUM"}, {"role": "user", "text": "I think i use the default option (sqlite) but i do not know how to check this."}, {"role": "system", "text": "I think he means the folder galaxy/database. You could check how big the folder galaxy/database/tmp is. And check if it matches up with the 1.2TB."}, {"role": "user", "text": "root@transgen-1:/data/galaxy/scripts/cleanup_datasets# du -d 1 /data/galaxy/database/\n1284\t/data/galaxy/database/tool_search_index\n27516\t/data/galaxy/database/shed_tools\n32\t/data/galaxy/database/mulled\n32\t/data/galaxy/database/citations\n849294740\t/data/galaxy/database/objects\n4\t/data/galaxy/database/object_store_cache\n3679008\t/data/galaxy/database/dependencies\n2192\t/data/galaxy/database/tool_cache\n5132\t/data/galaxy/database/jobs_directory\n398252968\t/data/galaxy/database/tmp\n516\t/data/galaxy/database/compiled_templates\n12\t/data/galaxy/database/container_cache\n1251305624\t/data/galaxy/database/"}, {"role": "system", "text": "\n\n\n wormball:\n\n252968\n\n\nSo the difference between 1.2 and what you see as Galaxy datasets is exactly the size of the /tmp folder. You can clean it up periodically with something like tmpreaper. Thanks @ggbio for identifying this."}, {"role": "system", "text": "@wormball - What Marten said.  I have to regularly empty the tmp file. I normally use something like tmpreaper too."}, {"role": "user", "text": "Thanks all!"}, {"role": "user", "text": "I reviewed our workflow, and it turned out that it were poorly written custom tool xmls that left tons of garbage behind them. Now i had fixed them, and galaxy is innocent."}], [{"role": "user", "text": "Hello,\nI am trying to import a history from another Galaxy server.  When I download the history from the other server, I receive a URL, which I can click to download the tar.gz file.  I have tried to copy that URL into the Users > Import interface on usegalaxy.org, but it ends in an error, screen capture below.  I also tried to upload the local file (both as tar.gz and the extracted file).  No action happens when I click on \u201cImport History\u201d.  (I read a post dated approximately 10 months ago with a similar issue, and I have made the history accessible by link to try the export again, but I am still getting the error.) Please let me know what I am doing wrong.  Thank you!\nimage746\u00d7754 29.2 KB\nn"}, {"role": "system", "text": "Hi @jclaudio\nit seems some restrictions were introduced to data sharing recently. On \u201csource\u201d Galaxy server go to history menu (small triangle icon at the top right corner of the history panel) > Share or publish > make history accessible. This should address the import history error. You can you the old link.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nThanks for the response.  My link was \u201caccessible\u201d on the sending side, and I also tried toggling on the Public share, then creating a new link.  Those still didn\u2019t work for an import by URL, but I did find I was able to download the file then upload it afterward."}, {"role": "system", "text": "Hi @jclaudio\nI am glad you\u2019ve found the solution.\nI reproduced the history import issue. It was resolved by making the history accessible. What do you mean by \u201clink was accessible\u201d? You can copy the link or your history status(?) was changed to \u201caccessible\u201d?\nKind regards,\nIgor"}, {"role": "user", "text": "Hello Igor,\nYes, I clicked on this option (screenshot) to make the history accessible, then used the Export to File function to generate the URL for the archive download.\nimage803\u00d7195 10.1 KB"}, {"role": "system", "text": "Hi @jclaudio\nI am sorry for being too pedantic, but the description says the history is restricted to you and users listed below. Have you clicked at Make History Accessible via Link?\nIt seems you use an old version of Galaxy. A similar page on Galaxy Australia looks somewhat different:\n\nI have not included the link in the screenshot. Note that the description says it is available to anyone with link.\nKind regards,\nIgor"}, {"role": "user", "text": "Hello @igor,\nI apologize \u2013 in my screenshot, I show the buttons prior to actually clicking on the submit button, so you are correct in that the description in my screenshot says \u201crestricted to you and users listed below\u201d, but I did use the link created in order to generate the archive file. (Sorry about that misclarity on my part!)\nI have a related question.  Most of my histories are able to be imported by using the file upload, but not all.  Do you know why this might be the case?\nJennifer"}, {"role": "system", "text": "Hi @jclaudio\nIs the server where the history archive was generated exposed publicly? Meaning, any URL needs to be publicly accessible for a Galaxy server to read it. Servers are not \u201caware\u201d of accounts at other servers by default (or ever) unless some special configuration was involved \u2013 and that wouldn\u2019t be available at many public servers, including UseGalaxy.org.\nIf the server is not hosted publicly, then other servers outside of your local network (or maybe just your computer?) cannot read in data via URL. Instead, you\u2019ll need to download the archive, then upload it to other servers. Transfer entire histories from one Galaxy server to another\nMaybe I\u2019m misunderstanding and you can clarify more?\n\nWhat is the URL of the \u201cfrom\u201d server?\nAnd please confirm that the \u201cto\u201d server is UseGalaxy.org?\n"}, {"role": "system", "text": "Hi @jclaudio\nhere is URL for a tiny history on Galaxy Australia, if you want to try history import:\nhttps://usegalaxy.org.au/api/histories/8d9830910a17f854/exports/dd411d68aa684fae\nI imported it to Europe and ORG.\nHard to say. Some file systems do not support big files, at least it was the case in the past. In this situation big files might be truncated, creating a broken archive. Personally, I found history export very slow, at least on Galaxy Australia, and rarely use it for big histories. I transfer big files individually. New version of Galaxy provides link to files (chain icon). Copy and paste it into target Galaxy upload menu, Get/Fetch data section. It works like a charm between usegalaxy.* servers (with small number of files).\nKind regards,\nIgor"}], [{"role": "user", "text": "Local Galaxy instance.\nI upgraded my local galaxy instance to 19.09 from 19.04.  No initial issues were observed. No errors.\nWhen I restart Galaxy, all installed \u201cshed_tools\u201d disappear from my gui tools menu.   Interesting \u2026\nIf I delete the intergrated_tool_panel.xml file, the shed_tools reappear in the gui menu when I restart.\nRestarting again, and the tools disappear.\nIt would appear that while the shed_tools are added to the GUI menu correctly, they do not appear to be added to the integrated_tool_panel.xml file ( The contents of tools_config.xml is added to the file, but the shed_tools are never added to this file).  If you restart, galaxy reads from the integrated_tools_panel.xml file and therefore no shed_tools.  Delete the file and it works until restart.\nAny ideas,  this one is beyond me\u2026\nRegards,\nMat"}, {"role": "system", "text": "I am a bit confused about your report. Could you please clarify what sequence of actions make the tools appear? Also check the startup logs for whether the specific tools load or not, every tool has a line there."}, {"role": "user", "text": "Hi Martin,  thanks for the fast reply.\nThe only way I have found to make the tools reappear is to delete the intergrated_tool_panel.xml file.\nOnce you do this, when you restart, the shed_tools are in the gui panel, and Galaxy creates a new integrated_tool_panel.xml file.\nIf you restart without deleting this file, the menu does not contain the shed_tools.\nTo directly answer your question, yes I can see the tools being loaded in the log on startup.\nI also noted that  the contents of the integrated_tool_panel.xml file never contains the parsed shed_tools.xml file. It only ever contains the parsed contents of the tool_panel.xml file. Hence when you restart, Galaxy appears to load this file directly, instead of creating the menu from tool_panel, and shed_tools_panel etc.\nI hope that makes sense\u2026\nKind regards,\nMat"}, {"role": "system", "text": "Thanks for elaborating, it indeed sounds tricky. Any errors in the log you can see? Is the location of your configuration files special in any way? How many tool configuration files do you use?"}, {"role": "system", "text": "that\u2019s an interesting bug \u2026 what are the values of tool_config_file, integrated_tool_panel_config and shed_tool_config_file in your galaxy.yml file? Is the restart behavior fixed if you set shed_tool_config_file to your shed_tool_conf.xml file?"}, {"role": "user", "text": "Yep, I\u2019m dangerous enough to solve most simple issues\u2026\nI have some home-grown tools in the tools_config.xml file, but I swapped this file out for the .sample file.  The issue persisted.\nI am now checking the shed_tools.config file to see if there is an xml format error of some sort\u2026\nI\u2019ll let you know.\nMat"}, {"role": "user", "text": "Hi Marius,\nThanks for the reply.\nValues are default.   I run a pretty standard config, so when I update I merge the new galaxy.yml file with the old file. I don\u2019t believe I have changed those values ever.\nWhen I get a chance I\u2019ll go look for the shed_tool_config_file and explicitly set it in galaxy.\nThanks for replying on a Saturday, I appreciate it, but surely you guys have better things to do \nP.S, this Galaxy instance has been running for a number of years, and upgraded several times.  Undoubtedly I\u2019ve done something stupid, probably a while ago,  I just can\u2019t find the where \nRegards,\nMat"}, {"role": "system", "text": "We did change the way default values for shed_tool_config_file are treated in 19.09 while preparing for pip-installable Galaxy, so it is well possible that you didn\u2019t do anything wrong."}, {"role": "user", "text": "\n\n\n Mathew_Moore:\n\nHi Marius, Marten,\n\n\nYes, uncommenting the shed_tool_config line in galaxy.yml resolved the issue.\n#tool_config_file: config/tool_conf.xml\nshed_tool_config_file: config/shed_tool_conf.xml\nOnce I removed the \u201c#\u201d, it worked.   Interestingly, I did not need to uncomment the line  tool_config.xml in galaxy.yml.\nThank you all!!\nRegards,\nMat"}], [{"role": "user", "text": "I am following along with a tutorial given on the coursera genomic data science course. It involved first joining two datasets on a column column to produce another data set. Then it involves grouping the resultant dataset on a column. In the video it shows that there is a drop down list of column names from the dataset being grouped to pick from. No such drop down appears. When I tried typing the column name in from the dataset it seems to have ignored it when executing the task with an error indicating that no column was specified. Any suggestions?"}, {"role": "system", "text": "Welcome, @claudiofr!\nAre you working at Galaxy Main https://usegalaxy.org?\nIf yes, this tool is currently problematic (and we expect to have it fixed very soon): Join two Datasets side by side on a specified field (Galaxy Version 2.1.3)\nMeanwhile, an alternative tool to use is: Join two files (Galaxy Version 1.1.1).\nBoth tools have been upgraded a bit since this particular class was published. There will not be a drop-down list of columns. Instead, enter the column number into the form. Example: For Column 4, enter just: 4\nOther text manipulation tools may require that columns are specified with different nomenclature and the form will note in the help section when it differs. Example for some other tools: For Column 4, enter: c4\nIf you are not sure, feel free to ask.\nFor questions or help with troubleshooting, it also helps if you specify the following in posts:\n\nNote where you are working. Examples: Your own Galaxy (what release and where it was sourced) or if a public Galaxy, include the server\u2019s URL in the post.\nCopy the full tool name and version from the top of the tool form and paste that into your question.\nCapture the exact error message and also paste that into your question. Sometimes just expanding the dataset and copying the comments is enough. Othertimes, try clicking on the bug icon for the red dataset and capture those remarks as well. And sometimes reviewing those remarks will help you to solve the problem on your own.\n\nYou don\u2019t need to actually submit the bug report unless you think it is a true server-side issue. Getting usage help or finding out about known issues here is usually much faster. Should you decide to do both \u2013 including the Galaxy Help post link directly in the bug report comments is very helpful, then come back and update your post to state that you submitted the bug report.\nThe more information you can provide, linked together, the better and quicker we can help. Your question now happens to be a known issue, if I am understanding correctly, but please clarify as needed.\nThanks!"}, {"role": "system", "text": "Hi,\nI am also having similar problems  - however neither join data sets, nor join files work for me - any further suggestions please?"}, {"role": "user", "text": "I am working at Galaxy Main https://usegalaxy.org\nThe tool I am using is:\nGroup  data by a column and perform aggregate operation on\nother columns. (Galaxy Version 2.1.4)\nSpecifying the column to group  by as a number only, i.e. 4 resolved my problem.\nThe help text associated with the tool did not indicate the format to use for specifying columns.\nThank you"}, {"role": "system", "text": "@claudiofr\nThanks for clarifying. Yes, the Group tool is also impacted. We are working on a correction. Update: Group was NOT impacted or related to the other known dependency issues. Problems with Group are likely usage related. See more in follow-up posts below.\nTicket: https://github.com/galaxyproject/usegalaxy-playbook/issues/288 https://github.com/galaxyproject/usegalaxy-playbook/issues/289\nThanks!"}, {"role": "system", "text": "@KeeleyB\nThe alternative Join tool is working at Galaxy Main https://usegalaxy.org.\n\nJoin two files (Galaxy Version 1.1.1)\n\nIt is possible that the tool is not working at some other public Galaxy server, or that there is a usage problem.\nThe Group tool does not have an alternative. Update: And is working as expected.\nText Manipulation functions impacted by dependency issues are a priority correction. The ticket will close out once fixed.\nThanks to all for your patience! Galaxy is undergoing some significant changes at this time but we expect the server-side issues with tool dependencies to be resolved very soon."}, {"role": "system", "text": "Thanks - I used Join and have got everything I needed to done - thanks very much!"}, {"role": "system", "text": "@claudiofr\nAh, I reread your reply more carefully. And Group was determined this morning to NOT be involved with the other tools waiting for the dependency correction.\nIn general, free-text tool form entry is moving away from the \u201cc4\u201d nomenclature (is a Python derived way of naming data columns) to the more end-user friendly \u201c4\u201d nomenclature (creates a better UX experience for those not familiar with coding).\nWhen it doubt, use the simple nomenclature of just the actual column number. Example: \u201c4\u201d (as you have done).\nTools that still require a special nomenclature do state that on the tool form for all cases I have examined. This is an example tool: Compute an expression on every row (Galaxy Version 1.2.0). The tool form explains expected column nomenclature (\u201cc4\u201d). Since the possible expressions that can be entered can become quite complicated with this tool, and that expression is processed directly as entered, this is how the tool \u201cworks\u201d (at least for now!).\nStandardizing the nomenclature across all tools would be ideal, of course \n\n\nMore about Galaxy development\nGalaxy is a popular, evolving, open-source project. Translating feedback from end-users into UX improvements is one of our core missions: Making Galaxy accessible (easy to use) for all.\nWe rely on community participation and contributions, both feedback and development. Perhaps someone from the community interested in a new development project with a clearly defined scope will take this on.\nShould anyone reading this post, now or later, be interested in tackling this, or other UX improvements, or really anything else (!), this is how to get involved: https://galaxyproject.org/develop/\n\nThanks!"}, {"role": "system", "text": "Update\nResolved, along with similar tools that had the same dependency issue."}], [{"role": "user", "text": "Hi everybody,\nfinally I managed to load my custom tool into Galaxy. What I need is give as input the ID of a gene and retrieve some data to be stored/displayed in a table.\nNow my python code is storing this table in a tab file but I\u2019m not able to display it in Galaxy.\nThat\u2019s my xml file for the tool:\n<tool id=\"get_pheno\" name=\"Get phenotypes\" version=\"0.1.0\">\n  <description>for a gene from IMPC</description>\n  <requirements>\n    <requirement type=\"package\" version=\"3.8\">python</requirement>\n\t<requirement type=\"package\" version=\"2.18.4\">requests</requirement>\n\t<requirement type=\"package\" version=\"0.8.9\">tabulate</requirement>\n\t<requirement type=\"package\" version='8.0.1'>IPython</requirement>\n  </requirements>\n  <command><![CDATA[python3 '${__tool_directory__}/get_pheno.py' '$input' '$output']]></command>\n  <inputs>\n\t  <param name=\"input\" type='text' label='Input gene:'/>\n  </inputs>\t  \n  <outputs>\n    <data format=\"tabular\" name=\"output\" label='${tool.name} on ${input}' />\n  </outputs>\n\n  <help>\nThis tool retrieves a list of phenotypes related to a gene entered by the user. The input must be an MGI id.\n  </help>\n\n</tool>\n\nI tried substituting \u201cdata\u201d with \u201coutput\u201d but it\u2019s not workning.\nHow can I get the output as a table?\nI already tried to read some documentation but I didn\u2019t find anything useful for me.\nThank you for the help"}, {"role": "system", "text": "Hi @Andrea_Furlani\nThe GTN tool development tutorials would be a good place to review/learn: Galaxy Training!"}, {"role": "user", "text": "I already checked those slides but the output part it\u2019s quite incomplete respect to the input part:\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Tool development and integration into Galaxy\n\n  Galaxy is an open-source project. Everyone can contribute...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you look the slides about output I already used both the options they indicated and in my code they are not working"}, {"role": "system", "text": "Try using Planemo https://planemo.readthedocs.io/.\nThe current XML has a few problems it could help uncover. One simple example: single quotes instead of double around a requirement version.\nAs a quick solution, maybe try using the ToolFactory tool instead? You could use it directly (only) or create the wrapper that way and then compare it to what you have now.\nToolFactory is itself a tool \u2013 one that creates other tools, and it makes use of Planeno. See the tutorials for the how-to. It is available to install from the Main Toolshed: https://toolshed.g2.bx.psu.edu/. It isn\u2019t hosted on public Galaxy servers for security reasons but it would be fine to install/use in your own private Galaxy server that isn\u2019t exposed to the internet publically."}, {"role": "user", "text": "Thank you but I would prefer to not use external tools to program mine, also because this script it\u2019s only a test, later my tool will be way more complex so I\u2019m not sure I can use Planemo or ToolFactory.\nIs there any way to better understand where is the error? Because watching other tools I can\u2019t understand where is the difference\u2026 I setted the output in the xml file as the other tools and my python code it\u2019s both printing the result (so I can actually see that I have the correct one in the info tab) and saving it in a tab file but I\u2019m not able in any way to have it visible in the \u201cview data\u201d tab"}, {"role": "system", "text": "How do you know that the tool even works? Here we established that you already have version conflicts:Galaxy local python Module Not Found Error\nAnd because the tool will be executed in the background in a conda environment you need to execute python instead of python3.\nTry to make a simple working \u201ctest\u201d tool first. Your XML could be something like:\n<tool id=\"my_first_test\" name=\"test tool\">\n  <requirements>\n    <requirement type=\"package\" version=\"3.8\">python</requirement>\n  </requirements>\n\n  <command>\n<![CDATA[\n python '${__tool_directory__}/test.py' -i $input -o $output\n]]>\n</command>\n  <inputs>\n\t  <param name=\"input\" type=\"text\" label=\"Test input\" />\n  </inputs>\t  \n  <outputs>\n    <data format=\"tabular\" name=\"output\" label=\"test output\" />\n  </outputs>\n\n  <help>\n  </help>\n</tool>\n\nAnd the python code (of top of my head):\nimport argparse\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='')\nparser.add_argument('-i', dest='input', type=str, help='', required=True)\nparser.add_argument('-o', dest='output', type=str, help='', required=True)\nargs = parser.parse_args()\n\nwith open(args.output, \"a\") as out:\n    out.write(args.input)\n\nAnd let\u2019s go from there\n\nIs there any way to better understand where is the error?\n\nIs the ouput at this moment green or red?"}, {"role": "user", "text": "I managed to solve the error and now the tool is running fine (at the end of the job it is green).\nI know that it is working in the correct way because if I print the result I can see the list in the \u201cedit attributes\u201d tab.\nHere\u2019s the code of my python script:\nimport sys\nimport requests\nimport tabulate\nfrom IPython.display import HTML, display\n\nimpc_api_url = \"https://www.gentar.org/impc-dev-api/\"\nimpc_api_search_url = f\"{impc_api_url}/genes\"\nimpc_api_gene_bundle_url = f\"{impc_api_url}/geneBundles\"\n\n# 1-Given a gene id, retrieve all the phenotypes related to it (id and name)\ndef stop_err(msg):\n    sys.exit(msg)\n\n\ndef main():\n\n    try:\n\n        mgi_accession_id = str(sys.argv[1])\n\n        gene_url = f\"{impc_api_search_url}/{mgi_accession_id}\"\n        gene_data = requests.get(gene_url).json()\n\n        p_list = gene_data['significantMpTermNames']\n        textfile = open(\"output.tab\", \"w+\")\n        for element in p_list:\n            textfile.write(element + \"\\t\")\n            print(element)\n        textfile.close()\n\n    except Exception as ex:\n        stop_err('Error running get_pheno.py\\n' + str(ex))\n\n\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n\n"}, {"role": "system", "text": "\n\n\n Andrea_Furlani:\n\ntextfile = open(\"output.tab\", \"w+\")\n\n\nYou are not giving galaxy the output file, it should be:\ntextfile = open(sys.argv[2], \"w+\")\n\nOr! you do this:\n<data format=\"tabular\" name=\"output\" from_work_dir=\"output.tab\" label='${tool.name} on ${input}' />\n"}], [{"role": "user", "text": "Dear Galaxy Team,\nI would like to perform paired-end RNA seq analysis for 21 samples with all of them having a biological replicate. I would like to know how should I build a dataset list for such a cohort. should one data list have all R1 and the other datalist have all R2 to be able to run commands on all sample at once? And how should I account the biological replicates?\nThank you very much in advance for your help!\nBest,\nShweta"}, {"role": "system", "text": "Hi @s.godbole,\ncould you provide me with some additional information about which type of analysis do you want to perform?\nRegards"}, {"role": "user", "text": "Hey @gallardoalba ,\nThanks for your reply! So I have paired-end raw rna-seq data for 7 different cell lines each of them having 3 biological replicates and each biological replicate having 2 technical replicates. Out of these 7 cell lines 3 belong to one particular subgroup and other 4 to another. So I would like to perform all steps from QC-trimming-mapping-to differential expression between these two groups. I hope this makes sense.\nSince I have so many files I would like to know how should I build my dataset lists to be able to execute the same command on all samples.\nthanks once again for your reply and I look forward to your response,\nBest,\nShweta"}, {"role": "system", "text": "Hi @s.godbole,\nthere are different options; since you have technical replicates, I suggest you build a list of dataset pairs for each sample and its technical replicate. Assuming that you are going to follow this pipeline:\nCutadapt \u2192 RNASTAR \u2192 featureCounts \u2192 DESeq2\nYou can work with that collections until the DESeq2 stage. Then you need to merge the technical replicates together in order to use them in DESeq2, which can be done by using two tools:\n\n\nColumn join on multiple datasets:  it allows you to collapse the the technical replicates collections into a single count file with three columns (GeneID, counts replicate 1 and counts replicate 2).\n\nTable Compute computes operations on table data: it will allow you to sum up the count of each replicate.\n\nRegards"}], [{"role": "user", "text": "Hi. This is my first time using galaxy and I got stuck straight away so it might be a basic error that I\u2019ve done but I have been following step by step tutorials and can\u2019t figure out what is wrong. I could not find any help from previous messages.\nI have 2 kinds of errors when trying to run an assembly with Unicycler.\nFirst I have an issue when I try to select the reads I want to use. When I scroll down, I do not see every reads, even though they are loaded properly. When I use the \u201csearch\u201d instead and choose the right read, it turns from green to white and \u201cunavailable\u201d appears before the name of the read like in the screen shot below:\n\nScreen Shot 2019-12-05 at 2.58.51 PM2790\u00d71008 379 KB\n\nI tried to use the very few reads that appear in the scroll down menu and it\u2019s working! But I can\u2019t access all the reads (anything before entry \u201c52\u201d is unavailable, which is most of my data).\nWhen I try to run an assembly anyway with the reads saying \u201cunavailable\u201d, I get the error message:\ntool error\nAn error occurred with this dataset:\nFailed to communicate with remote job server.\nCan anybody help?\nShould I reload all my reads data?\nThanks\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nI suspect that the assigned datatype for the fastq data is just fastq.gz, assigned by you in the Upload tool. Expand one of the \u201cunavailable\u201d datasets to check.\nIf the fastq data is really in compressed fastqsanger format, the datatype fastqsanger.gz should be assigned. The datatype fastq.gz is not enough, and if the quality score encoding is not actually scaled/encoded as Sanger Phred+33 (fastqsanger), or is labeled as compressed (with a \u201c.gz\u201d added) when it is really uncompressed, tools will fail, so you need to check (tool: FastQC). It is a good practice to run FastQC on reads as the initial analysis step to confirm format and content \u2013 unless you did QA/QC prior to loading it into Galaxy or a tutorial states that the data is in that format.\nIf you want Galaxy to do the datatype checks/assignments when using the Upload tool, do not directly assign a datatype but use the \u201cautodetect datatype\u201d option (the default). Compressed fastq data will uncompress with this method and if the quality score encoding matches will be assigned the datatype fastqsanger (no \".gz at the end). If you get a different datatype, then you\u2019ll need to do some manipulations.\nUnicycler is a tool that uses uncompressed fastqsanger anyway. If you input compressed fastqsanger.gz, Galaxy will create a hidden uncompressed version of the data to submit to this tool. Most tools accept compressed fastqsanger.gz data, but this one does not, and you\u2019ll save quota space in your account by not duplicating the data in a compressed + uncompressed format.\nNote: The \u201cname\u201d of a dataset has nothing to do with the datatype. The name is just a label. The assigned datatype is the metadata tools use to ensure that inputs are in an appropriate format.\nThese FAQs explain with more details, including how to check if your data really does have fastqsanger (Sanger Phred+33) scaled quality scores: https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\n\nPlease review and see if this helps!"}, {"role": "user", "text": "Hi @jennaj\nThanks for the information. I checked on the format for reads that worked or not:\n\nfor 55 (which worked) \u201cuploaded fastqsanger.gz file\u201d\nfor 44 (which did not work) \u201cuploaded fastq.gz file\u201d\nSo one is fastq and one is fastqsanger but both seems to be compressed, as per .gz\nMy understanding is that on Galaxy, an uncompressed file was created for entry 55 using the fastqsanger. gz file, but that could not be done using a fastq.gz file?\n\nI\u2019m not too sure to understand your comment on FastQC. I actually ran FastQC on all my data as well as multiQC, which seemed to show good results, and then I ran Trimmomatic an all reads. The trimmimg seems to also have worked, and I tried to run Unicycler first using the trimmed reads instead of raw data, but this did not work.\nSummary results from FastQC for 44 is:\n##FastQC\t0.11.8\n\n\nBasic Statistics\tpass\n#Measure\tValue\nFilename\t6461x11601MD-SKQ_S1_L001_R1_001_fastq_gz.gz\nFile type\tConventional base calls\nEncoding\tSanger / Illumina 1.9\nTotal Sequences\t856011\nSequences flagged as poor quality\t0\nSequence length\t35-301\n%GC\t31\n\n\nEND_MODULE\nSo I do not see any problem here? How does this relate to my problem?\n\n\nSo if I summarize, your advice is to unzip the .gz files before uploading them in order to save some space? And then, when uploading them, use the \u201cauto\u201d for the file format attribution?\nOne last thing: I should probably delete everything and upload everything again so that I start clean. If I click on the little cross on the right panel in the history, will that actually clear the space in my account? I would not want to have data in the background still using some memory.\nThanks for help\nVanina"}, {"role": "system", "text": "\n\n\n vanina.guernier:\n\nfor 44 (which did not work) \u201cuploaded fastq.gz file\u201d\n\n\nYour data is in fastqsanger.gz format. There is no need to reload, just change the datatype for these datasets. Do this for each dataset by clicking on the pencil icon to reach the Edit Attribute forms, on the tab for \u201cDatatypes\u201d. These FAQs explain why the tool is not recognizing the fastq.gz inputs and how-to adjust metadata:\n\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\n\nNext time, you can set the datatype when loading the data in the Upload tool to avoid the extra steps.\nIn most cases, you\u2019ll want to preserve the compressed format.\nYou could also reload the data and use \u201cautodetect\u201d for the datatype. It will uncompress.\nEither option is fine \u2013 it depends on which tools you plan to use and how large your data is. Since you are running tutorials, the data is small. Later on when working with large data, or start to fill up your account quota space, adjusting how you load data for particular tools will be something you\u2019ll want to consider, to avoid duplicated data/using up quota space for no practical purpose.\nExample: If you are starting an analysis that will go through QA steps then mapping, using compressed fastq data all the way through would be fine, and save space.\nTool\u2019s state what the exact expected input formats are when there are no datasets in the history that meet that criteria. So you might want to keep an extra empty history around for that purpose (switch to that history, load the tool form, and review the expected datatypes for each input \u201cselect\u201d option).\nTwo examples with different input requirements:\nUnicycler:\n\nunicycler-no-inputs1446\u00d7808 129 KB\n\nHisat2:\n\nhisat2-no-inputs1444\u00d7770 97.5 KB\n\n\n\n\n vanina.guernier:\n\nIf I click on the little cross on the right panel in the history, will that actually clear the space in my account? I would not want to have data in the background still using some memory.\n\n\nBy clicking on the X icon, datasets will be deleted but still recoverable. The same is true when deleting entire histories. There is an extra step to permanently delete (purge) data (datasets or histories) so that it no longer counts toward the quota. See these FAQs for the how-to:\n\nThe account usage quota seems incorrect\nChecking for active vs deleted vs permanently deleted (purged) datasets and histories\n"}, {"role": "user", "text": "Thanks. It seems to be working for most of my data but not all.\nFor some of them when I try to edit I have this message:\nEdit dataset attributes\nAttributes updated, but metadata could not be changed because this dataset is currently being used as input or output. You must cancel or wait for these jobs to complete before changing metadata.\nThe odd thing is I do not have any job running at the moment\u2026"}, {"role": "user", "text": "In the end I managed to edit them all, and it seems that the assemblies are up and running.\nThanks for helping @jennaj"}, {"role": "system", "text": "@vanina.guernier\nGlad things are working now! "}, {"role": "system", "text": "Hi @jennaj, I\u2019m having the same issue but it seems like the long reads were imported as fastq files, so I\u2019m confused. I have multiple Nanopore files which I have combined into one file (I was trying to import multiple and it wouldn\u2019t let me). Any ideas or help would be appreciated!\nThank you!"}, {"role": "system", "text": "Hi @Esther_Miller\nNanopore reads can be in either fasta or fastqsanger format. When uploading, allowing Galaxy to assign the datatype is usually best but you can change this after upload if Galaxy guessed incorrectly.\nWhy multiple files failed upload \u2013 this could be due to a few reasons, including a slower internet connection. Most residential wifi is optimized for data downloads, not uploads, and many are still working from home right now. Maybe try using FTP to load the data instead of browsing local files? If any transfer fails, it can be resumed.\nI added a few tags to this post that will point to details about using FTP for upload.\nAlso, if you are ever not sure about which datatypes a tool is expecting for input, try this: Create a new empty history, then load up the tool form. Be aware that if data is compressed (example: fastqsanger.gz), Galaxy will uncompress the data at runtime (as additional hidden datasets) for tools that require uncompressed input. Sometimes it is better to uncompress yourself first (pencil icon > Convert), then purge the original compressed, to avoid too much data duplication/quota usage. A side effect is that whatever tool you are running might run better since it doesn\u2019t have to do the \u201cuncompress\u201d step as a pre-processing step for the primary tool. For larger work, that can often make a difference between a successful or failed job (if fails due to resource reasons).\nExample:\n\nunicycler2140\u00d7762 77.3 KB\n\nHope that gives some options that will work for you, if you haven\u2019t found a solution yet."}], [{"role": "user", "text": "I\u2019ve initialized a Galaxy instance on aws and am an admin on there.\nI am wondering how to parallelize the use of some tools on there. For example tophat2 has an option -p where on the command line it allows for parallel processing but that option is not one of the options that can be configured in the User Interface tool. Is there a way around this?\nThere is a notification on the homepage of Cloudman UI that says this version of Galaxy has configured some tools to run on 4 cores in the file \u201cjob_conf.xml.cloud\u201d, but is it possible to edit this file to use more processors?\nThanks"}, {"role": "system", "text": "Yes, you can edit the file. You will need to create a new destination with the number of cores you want set. Something like:\n        <destination id=\"slurm_cluster_cpu16\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=16</param>\n        </destination>\n\nThe default can be seen here:\n\n  \n      github.com\n  \n  \n    galaxyproject/ansible-cloudman-galaxy-setup/blob/master/files/galaxy/job_conf.xml.cloud\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n            <param id=\"drmaa_library_path\">/mnt/galaxy/slurm-drmaa/lib/libdrmaa.so</param>\n        </plugin>\n        <plugin id=\"pulsar_rest\" type=\"runner\" load=\"galaxy.jobs.runners.pulsar:PulsarRESTJobRunner\" />\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\" />\n    </plugins>\n\n    <handlers default=\"main\">\n        <handler id=\"main\" tags=\"handlers\">\n            <plugin id=\"slurm\" />\n        </handler>\n    </handlers>\n\n    <destinations default=\"slurm_cluster\">\n        <destination id=\"slurm_cluster\" runner=\"slurm\"/>\n        <destination id=\"slurm_cluster_cpu4\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=4</param>\n\n\n  This file has been truncated. show original\n\n  \n  \n    \n    \n  \n  \n\n\n@enis, if this file is edited, how should it be persisted?"}], [{"role": "user", "text": "I have paired end fastq files from   illumina Novaseq using whole transcriptome mRNA-seq profiling. My RNA STAR result looks OK (using hg38 gtf file from ucsc table browser).\n                      Number of input reads |\t38164847\n                  Average input read length |\t201\n                                UNIQUE READS:\n               Uniquely mapped reads number |\t30007228\n                    Uniquely mapped reads % |\t78.63%\n                      Average mapped length |\t200.97\n                   Number of splices: Total |\t18124401\n        Number of splices: Annotated (sjdb) |\t17921546\n                   Number of splices: GT/AG |\t17970447\n                   Number of splices: GC/AG |\t117674\n                   Number of splices: AT/AC |\t16850\n           Number of splices: Non-canonical |\t19430\n                  Mismatch rate per base, % |\t0.19%\n                     Deletion rate per base |\t0.01%\n                    Deletion average length |\t1.73\n                    Insertion rate per base |\t0.01%\n                   Insertion average length |\t1.47\n                         MULTI-MAPPING READS:\n    Number of reads mapped to multiple loci |\t7150744\n         % of reads mapped to multiple loci |\t18.74%\n    Number of reads mapped to too many loci |\t62501\n         % of reads mapped to too many loci |\t0.16%\n                              UNMAPPED READS:\n   % of reads unmapped: too many mismatches |\t0.00%\n             % of reads unmapped: too short |\t2.44%\n                 % of reads unmapped: other |\t0.03%\n                              CHIMERIC READS:\n                   Number of chimeric reads |\t0\n                        % of chimeric reads |\t0.00%\n\nHowever, when I use featureCounts I get very few assigned reads, most of the reads are in Unassigned Multimapped category. (I use reverse stranded option as indicated by \u2018infer experiment\u2019)\n\n\n\n\nAssigned\n7712538\n\n\n\n\nUnassigned_Unmapped\n0\n\n\nUnassigned_MappingQuality\n0\n\n\nUnassigned_Chimera\n0\n\n\nUnassigned_FragmentLength\n0\n\n\nUnassigned_Duplicate\n0\n\n\nUnassigned_MultiMapping\n37553849\n\n\nUnassigned_Secondary\n0\n\n\nUnassigned_NonSplit\n0\n\n\nUnassigned_NoFeatures\n4138912\n\n\nUnassigned_Overlapping_Length\n0\n\n\nUnassigned_Ambiguity\n18155778\n\n\n\nWhat could be the reason behind it? Is there a way to improve on that? I am losing a lot of reads due to multimapping.\nI also checked the read distribution. It looks OK to me too.\n\nrseqc_read_distribution_plot1200\u00d7800 37 KB\n"}, {"role": "system", "text": "Welcome @srashid\nAvoid the UCSC reference GTFs from their Table Browser. These often end up truncated, plus there is a serious data content concern. Why is covered in this FAQ in more detail:\n\nExtended Help for Differential Expression Analysis Tools\n\nGood sources for hg38 GTF reference annotation are described in this prior Q&A (and are included in the FAQ above as well):\n\n\n\nRNA-STAR and hg38 GTF reference annotation\n\n\nThe GTF should be based on the UCSC \u201chg38\u201d genome build. Some choices:\n\nFor Gencode , copy the link to the GTF and paste it into the Upload tool. Hg38 data is here https://www.gencodegenes.org/ . After it is loaded, remove the headers (lines that start with a \u201c#\u201d) with the Select tool using the options \u201cNOT Matching\u201d with the regular expression ^# . Once the formatting is fixed, change the datatype to be gft under Edit Attributes (pencil icon). The data will be given the datatype gff by default, which works fine with some tools and but not with others. Avoid the gff3 version of this particular data (contains duplicated IDs and several RNA-seq tools do not work with annotation in that format anyway).\nFor iGenomes , the archive corresponding to the target genome/build needs to be locally downloaded, the tar archive unpacked, and then just the genes.gtf data uploaded to Galaxy (browse the local file, or use FTP). Find all available genome/builds here: https://support.illumina.com/sequencing/sequencing_software/igenome.html\n\n\n\n\nGive one or both of those a try and see if your \u201cUnassigned_Ambiguity\u201d and \u201cUnassigned_MultiMapping\u201d counts reduce \u2013 they should (\u201cgene_id\u201d and \u201ctranscript_id\u201d will no longer be the same value).\nYou may even get fewer \u201cUnassigned_NoFeatures\u201d if the UCSC data was truncated when extracted from the Table Browser."}, {"role": "user", "text": "Thank you! While waiting for your reply, I actually tried igenomes gtf file, it definitely reduces ambiguity , but the multimapping issue still remains\nThis is the output of featureCounts now\n\n\n\n\nAssigned\n23399833\n\n\n\n\nUnassigned_Unmapped\n0\n\n\nUnassigned_MappingQuality\n0\n\n\nUnassigned_Chimera\n0\n\n\nUnassigned_FragmentLength\n0\n\n\nUnassigned_Duplicate\n0\n\n\nUnassigned_MultiMapping\n37557436\n\n\nUnassigned_Secondary\n0\n\n\nUnassigned_NonSplit\n0\n\n\nUnassigned_NoFeatures\n6348801\n\n\nUnassigned_Overlapping_Length\n0\n\n\nUnassigned_Ambiguity\n259191\n\n\n\n"}, {"role": "system", "text": "Hi @srashid\nFeatureCounts only reports unique matches with default settings.\nYour reads are likely hitting more than one \u201cexon\u201d, which leads to \u201cmultimapping\u201d counts when summarized at the Gene level.\nReview the \u201cAdvanced Options\u201d. In particular, pay attention to these parameters, but also review others and see what results. There isn\u2019t a single right answer for everyone. It depends on how you want these counted up, if at all.\n\n\u201cAllow read to contribute to multiple features\u201d (default=no)\n\u201cLargest overlap\u201d (default=no)\n\u201cCount multi-mapping reads/fragments\u201d (default=disabled) and the sub-option (when enabled) \u201cAssign fractions to multimapping reads\u201d\n\nThanks!"}], [{"role": "user", "text": "Hi, I am comparing an assembled transcript file (GTF) with a reference annotation file (GFF) using the GFFCompare tool in Galaxy. In parameters, the only criteria am selecting is \u201cdiscard \u2018duplicate\u2019 query transfrags within a single sample (-D)\u201d. The transcript accuracy report is looking like this-\ngffcompare v0.11.2 | Command line was:\n#gffcompare -r ref_annotation -D -e 100 -d 100 -p TCONS gffread_on_data_171__gtf\n\n#= Summary for dataset: gffread_on_data_171__gtf\nQuery mRNAs :   74051 in   18336 loci  (60300 multi-exon transcripts)\n(9152 multi-transcript loci, ~4.0 transcripts per locus)\nReference mRNAs :   46683 in   36960 loci  (37011 multi-exon)\nSuper-loci w/ reference transcripts:        0\n#-----------------| Sensitivity | Precision  |\nBase level:     0.0     |     0.0    |\nExon level:     0.0     |     0.0    |\nIntron level:     0.0     |     0.0    |\nIntron chain level:     0.0     |     0.0    |\nTranscript level:     0.0     |     0.0    |\nLocus level:     0.0     |     0.0    |\n Matching intron chains:       0\n   Matching transcripts:       0\n          Matching loci:       0\n\n      Missed exons:  206513/206513\t(100.0%)\n       Novel exons:  204613/204613\t(100.0%)\n    Missed introns:  168155/168155\t(100.0%)\n     Novel introns:  115436/115436\t(100.0%)\n       Missed loci:   36960/36960\t(100.0%)\n        Novel loci:   18336/18336\t(100.0%)\n\nTotal union super-loci across all input datasets: 18336\n74051 out of 74051 consensus transcripts written in gffcmp.combined.gtf (0 discarded as redundant)\nThis is not making any sense to me. Why the precision and sensitivity are of 0 value. Thank you so much for your input."}, {"role": "user", "text": "Hi all, can anyone help me with this? I just taking an assembled GTF transcript file and comparing it to an annotated NCBI GFF file (reference file). And I am getting the above transcript report. What wrong am I doing?\nWhen I am comparing the same NCBI GFF file as input with the annotated NCBI GFF file (reference file), as expected the sensitivity and precision is 100%.\nPlease help. Thanks\u2026"}, {"role": "system", "text": "Hi @Sandip_De,\none of the causes may be that the version of the reference genome used to generate the assembly and the one corresponding to the reference annotation file are different. Could you please provide me with detailed information about how the gffread_on_data_171__gtf file was generated?\nRegards"}, {"role": "user", "text": "Hi gallardoalba, you are spot on. I have generated the GFF/GTF file from a new genome assembly and the reference annotation is from the previous genome assembly. My aim is to identify new isoforms/transcripts and genes from this new assembly.\nJust wondering, is there a way to compare the new GTF from the new genome assembly with the old GTF from the old genome assembly? Thanks for your help."}, {"role": "system", "text": "Hi @Sandip_De,  I guess that there is not a tool for that, but it would be possible to write a script. What is your goal for comparing both files?"}, {"role": "user", "text": "Comparing two GFF files should identify new genes, transcripts, alternative transcription start, and end sites etc. I found a software developed for this purpose but not available in Galaxy- ParsEval- parallel comparison and analysis of gene structure annotations (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-187). Is there a way to add that to the Galaxy tool shed?"}, {"role": "system", "text": "Yes, there is a tutorial. Let me know if you consider that you can integrate it. Otherwise I can do it for you.\nRegards."}, {"role": "user", "text": "Can you please integrate the software into Galaxy? Honestly, I browsed through the tutorial link, and did not make much sense to me.\nThanks for your help."}, {"role": "user", "text": "I really appreciate your help.   "}, {"role": "system", "text": "Sure, I will notify you when the tool is available, probably in a couple of weeks.\nRegards."}], [{"role": "user", "text": "Hello,\nI am currently trying to analyse some methylation sequencing data using Bismark. When I try to extract the methylation status using the Bismark Meth. Extractor, I am expecting to also get a .cov coverage file. However, this does not appear to be being generated.\nThe tool output gives the following:\nWriting bedGraph to file: dataset_e734b08e-b355-49c5-94cc-61ef7b831d1e.datbedGraph.gz.\nAlso writing out a coverage file including counts methylated and unmethylated residues to file: dataset_e734b08e-b355-49c5-94cc-61ef7b831d1e.datbismark.cov.gz\nHowever, I do not see the cov.gz in the resulting result archive. Am I using an incorrect setting (I had asked it to give me the genome wide cytosine report) or is this something that it currently does not output?\nAny help would be greatly appreicated!\nMany thanks,\nLewis"}, {"role": "system", "text": "Welcome @Lewis!\nI think your cov file is supposed to be the\n\nMethylation proportion report for each possible position in the read (Mbias Report)\nThis report shows the methylation proportion across each possible position in the read (described in further detail in:Hansen et al., Genome Biology, 2012, 13:R83). The data for the M-bias plot is also written into a text file and is in the following format:\n\n\nread position | count methylated | count unmethylated | % methylation | total coverage\n\nCan you check if this is in the output datasets?"}, {"role": "user", "text": "Hi David,\nMany thanks for your reply! The M-bias report is not quite what I am after. This report is to check whether there is any bias in methylation across the read. As I did 150 PE sequencing, this outputs a file for each one of the 150 positions. What I am after is a coverage file that has counts (both methylated and unmethylated) and their genomic position. The methylation extractor should give this as a .cov file, but this doesn\u2019t appear to be being saved. Here is a screenshot from the Bismark manual of what I am after:\n\nUntitled1032\u00d7399 27.8 KB\n\nKind regards,\nLewis"}, {"role": "system", "text": "Ok, @Lewis ,\nI have some guesses:\n\nThe *cov.gz file declared at the output log is only created as a temp file in order to build other outputs;\n\n\nWriting bedGraph to file: dataset_f5b173bc-5683-4e3a-aae9-bbfab0223a5b.datbedGraph.gz\nAlso writing out a coverage file including counts methylated and unmethylated residues to file: dataset_f5b173bc-5683-4e3a-aae9-bbfab0223a5b.datbismark.cov.gz\n\n(\u2026)\n\nOutput will be written into the directory: /data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmp7xnxvb6j/\nSummary of parameters for genome-wide cytosine report:\nCoverage infile:\t\t\t\tdataset_f5b173bc-5683-4e3a-aae9-bbfab0223a5b.datbismark.cov.gz\nOutput directory:\t\t\t\t>/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmp7xnxvb6j/<\nParent directory:\t\t\t\t>/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmp7xnxvb6j/<\nGenome directory:\t\t\t\t>/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmpeuc9ajoz/<\nCX context:\t\t\t\t\tyes\nGenome coordinates used:\t\t\t1-based (default)\nGZIP compression:\t\t\t\tno\nSplit by chromosome:\t\t\t\tno\n.\n\n\nThe coverage files are only output separately with the optional parameter --bedgraph ((IV) Bismark methylation extractor):\n\n\nAlternatively, the output of the methylation extractor can be transformed into a bedGraph and coverage file using the option --bedGraph (see also --counts ).;\n\n\nwhich I can\u2019t see in the command line of my tests:\n\nMethylation extractor run with: 'bismark_methylation_extractor --no_header -o /data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmp7xnxvb6j --single-end --comprehensive --report --bedGraph --CX_context --cytosine_report --CX_context --genome_folder /data/dnb03/galaxy_db/job_working_directory/022/607/22607745/tmp/tmpeuc9ajoz /data/dnb03/galaxy_db/files/f/5/b/dataset_f5b173bc-5683-4e3a-aae9-bbfab0223a5b.dat'\n\nPrevisously, this error was solved and related to another optional parameter, buffer size (--buffer), in standalone bismark_methylation_extractor v0.20.0; But I can\u2019t test this now.\n"}, {"role": "user", "text": "Hi David,\nI totally agree with you that it seems the .cov file is not being saved, and instead is being used only as a temporary file to generate the genome-wide cytosine coverage report. The --bedGraph option is definitely there in the command line code you copied above,  so I don\u2019t think that this is the issue. The bedgraph file (.datbedGraph.gz) is present in the results archive generated when asking for the genome-wide cytosine report, which is why I found it odd that the .cov file is not there as well. It would definitely be interesting to see if the --buffer option is the cause of this file not being present, however if it can\u2019t be fixed then I think I\u2019ll most likely have to find another way of running the bismark extractor! Many thanks for your help so far!"}, {"role": "system", "text": "Sorry! I showed the text from the log file, but I meant the text in the Commmand Line field, from the Job Information\n\npython \u2018/opt/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/bgruening/bismark/ff6ee551b153/bismark/bismark_methylation_extractor.py\u2019 --multicore \u201c${GALAXY_SLOTS:-4}\u201d --infile \u2018/data/dnb03/galaxy_db/files/f/5/b/dataset_f5b173bc-5683-4e3a-aae9-bbfab0223a5b.dat\u2019 --single-end --splitting_report \u2018/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/outputs/galaxy_dataset_d5147783-6aac-4b1f-ab32-0634ac21f788.dat\u2019 --mbias_report \u2018/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/outputs/galaxy_dataset_9855a442-64fa-4f61-9305-d4f581661ad8.dat\u2019 --cytosine_report \u2018/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/outputs/galaxy_dataset_5a7de963-e281-40a2-af49-df099dde7ebb.dat\u2019 --genome_file \u2018/data/db/data_managers/h7n7_360722/seq/h7n7_360722.fa\u2019 --cx_context --comprehensive --compress\n\u2018/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/outputs/galaxy_dataset_d1e0feca-7641-4b02-84e6-7dff0081a634.dat\u2019 --log_report \u2018/data/dnb03/galaxy_db/job_working_directory/022/607/22607745/outputs/galaxy_dataset_61ded233-440e-4bf5-81b8-9214fa10b5d8.dat\u2019\n"}, {"role": "user", "text": "Ah I see, that definitely could be it! Is there any way of testing this theory out? I have no idea who to contact to see if the methylation extractor could be changed or modified on Galaxy to be able to produce the .cov file, or even if this is possible!"}, {"role": "system", "text": "It can be updated by someone from Galaxy team, but, unfortunately, they are very busy.\nI can try to run Bismark standalone and test --buffer, but I can\u2019t say when I\u2019ll be able to do it.\nAnother option is try the bismark2bedGraph script mentioned in the manual, which may be also quicker."}, {"role": "system", "text": "Hi @Lewis,\nI have updated  Bismark extractor in order to provide the coverage file: Bismark methylation extractor: include coverage output by gallardoalba \u00b7 Pull Request #1131 \u00b7 bgruening/galaxytools \u00b7 GitHub. It will be available soon.\nHowever, I recommend you to use Methyldakel instead of Bismark.\nRegards"}], [{"role": "user", "text": "Dear All,\nBriefly, I would like to download a species genome (CBVK0000000000.1 Picea abies :: NCBI) into Galaxy.\nHowever, the fasta of the full genome of the species in many pieces (32). I have tried Getdata/Collection/Paste-Fetch data. There I pasted all the 32 URLs(links) to the files.\nAfter, there was an option to build a list. I created the list. Now I have no idea what to do. How I can reconstruct the full file or one file with all sequences\u2026I mean how I will get the full genome of the species?\nI\u2019m new to Galaxy and I\u2019m not a programmer.\nPlease advice.\nBest, Thend"}, {"role": "system", "text": "\n\n\n Thend:\n\nCBVK0000000000.1 Picea abies :: NCBI\n\n\nThis is where the genome fasta and reference annotation is located: ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/067/695/GCA_900067695.1_Pabies01"}], [{"role": "user", "text": "Hi, I have run out of space in the usegalaxy.eu server and am trying to download files to delete them from my history to make space on my account. However, I am experiencing an issue when trying to download files from the usegalaxy.eu history, it fails exactly at 1.08Gb. I have tried\nwget -O pool5.tgz https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX \nand\ncurl -o pool5 https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX \nand each time I try to check the files of the content with tar -zvft , it starts reading the file names until it gets to the halfway and then I get the following error\ntar: Truncated input file (needed 101140480 bytes, only 0 available)\ntar: Error exit delayed from previous errors.\nI have tried in a Mac and a Windows laptop, and different files and with one month of difference. I\u2019d appreciate some help with this.\nMany thanks,\nRamiro"}, {"role": "system", "text": "Hello @Ramiro\nThis command works on Mac/Linux: tar -xvzf <file.tgz>. You may need to quote the file name if it contains spaces.\nThe issue could involve the dataset permissions in Galaxy or possibly your internet connection. Updating curl might also help. Resuming a partial/aborted transfer is also possible but that doesn\u2019t seem to be your issue.\nFAQ: https://galaxyproject.org/support/download-data/\nPrior Q&A: Downloading a collection from usegalaxy.org fails to complete\nHope that helps!"}, {"role": "user", "text": "Hello @jennaj,\nThanks for your comments. I have updated curl but still the download does not work. I have also tried restarting the partial downloads with wget and curl following the commands on the FAQ page but it does not work either. I am working from home (due to covid-19) and have tried through wireless and connecting to the router. I have also tried from my work server (logging through a vpn) and the files do not download either.\nI have been looking at some other comments on this topic and this is my case too and it wasn\u2019t solved Downloading Large Files - 1Gb Limit (Galaxy Main | wget | curl | API Key).\nAny other suggestion? Many thanks,\nRamiro"}, {"role": "system", "text": "Thanks for the update.\nThe transfer stopping around 1 GB was a known issue with the curl utility (unrelated to Galaxy) but it is difficult to know if that is what the problem is here.\nOne potential workaround is to copy just the datasets you want to download into a new history, set that history to be in a shared state (by link, under User > Histories > \u201cShare or Publish\u201d per history pull-down menu). Then under the History menu (gear icon), Export that subset History to an archive. These take time to compress \u2013 click on the link to refresh the status. Once ready, the message will update and the History archive can be downloaded. Your datasets will be inside.\nLet\u2019s also get some input from the developers/admins and see if they have other ideas about what might be going wrong with collection downloads. I started a Gitter chat here: https://gitter.im/galaxyproject/Lobby?at=5f36bb4060892e0c69702c32\nFeel free to comment at Gitter, too."}, {"role": "system", "text": "Update: More feedback in the Gitter thread.\nTry the history archive workaround. The data is pre-compressed with that method and avoids \u201ctemporary data caching\u201d limitations.\nCopies of your own dataset do not consume any additional quota space, so you can reorganize anyway you want. But once downloaded, you\u2019ll need to purge all copies to free up space. Be sure to check the downloaded data first (before purging!) so you don\u2019t lose anything important.\nWe\u2019ll probably be looking into better solutions but this should get you moving forward for now."}, {"role": "user", "text": "Great, thank you for the help. Will try the work arounds suggested and let you know here.\nThank you!"}, {"role": "user", "text": "Dear @jennaj,\nAn update on the subject. The workaround you propose of making a history archive and downloading it has worked. And there does not appear to be any error on the downloaded data.\nThanks for your help."}], [{"role": "user", "text": "Dear All,\nI deployed Galaxy locally using sh run.sh on Ubuntu 16.04 LTS. My tool was dockerized and set to run locally through Galaxy interface. The tool ran ok as I saw files generated. But Galaxy interface couldn\u2019t list files in the webpage. Here is my Galaxy logs copied from my terminal (at  galaxy logs)\nMy job_config.xml is like\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"1\"/>\n    </plugins>\n    <destinations default=\"local\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"docker_local\" runner=\"local\">\n          <param id=\"docker_enabled\">true</param>\n        </destination>\n\n        <destination id=\"metaboflow-container\" runner=\"local\">\n           <param id=\"docker_enabled\">true</param>\n           <param id=\"docker_volumes\">$defaults,/opt/galaxy_data:rw</param> -->\n           <param id=\"docker_sudo\">false</param>\n           <param id=\"docker_auto_rm\">false</param>\n        </destination>\n\n    </destinations>\n    <tools>\n\n     <tool id=\"metaboflow\" destination=\"metaboflow-container\"/>\n    </tools>\n</job_conf>\n\n\nmy tool xml is\n<tool id=\"metaboflow\" name=\"Metabo Flow (in docker)\">\n    <description>Metabo Flow</description>\n    <requirements>\n      <container type=\"docker\">jianlianggao/metaboflow_pre:20203</container>\n    </requirements>\n    <command><![CDATA[\n      /usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o \\$PWD\n     ]]>\n    </command>\n    <inputs>\n       <param name=\"testdata_input\"  type=\"data\" label=\"Test Dataset\" help=\"Demo data set (Dataset_Metaboflow_Ionomic_Workflow.csv) can be downloaded from the link below.\" />\n        <param name=\"pre_define_sd\"  type=\"data\"  label=\"Pre-defined standard deviation matrix\" />\n    </inputs>\n    <outputs>\n      <!--  <collection name=\"metaboflow_output_plots\" type=\"list\" label=\"MetaboFlow output plots from ${on_string}\">\n\n             <discover_datasets pattern=\"(?P&lt;name&gt;)\\.pdf$\" ext=\"pdf\" />\n\n                 </collection> -->\n         <data name=\"data_long\" format=\"csv\" /> <!--  from_work_dir=\"data_long.csv\" label=\"data_long.csv\"/>\n          <data name=\"output2\" file=\"/opt/galaxy_data/data_wide.csv\" label=\"data.wide.csv\"/>\n         <data name=\"output3\" file=\"/opt/galaxy_data/data_wide_Symb.csv\" label=\"data.wide_Symb.csv\"/>\n         <data name=\"data4\" file=\"/opt/galaxy_data/stats.median_batch_corrected_data.txt\" label=\"stats.median_batch_corrected_data.txt\"/>\n         <data name=\"data5\" file=\"/opt/galaxy_data/stats.outliers.txt\" label=\"stats.outliers.txt\"/>\n         <data name=\"data6\" file=\"/opt/galaxy_data/stats.raw_data.txt\" label=\"stats.raw_data.txt\"/>\n         <data name=\"data7\" file=\"/opt/galaxy_data/stats.standardised_data.txt\" label=\"stats.standardised_data.txt\"/>\n         <data name=\"data8\" file=\"/opt/galaxy_data/plot.logConcentration_by_batch.pdf\" label=\"plot.logConcentration_by_batch.pdf\"/>\n         <data name=\"data9\" file=\"/opt/galaxy_data/plot.logConcentration_z_scores.pdf\" label=\"plot.logConcentration_z_scores.pdf\"/> -->\n    </outputs>\n    <tests>\n      <test>\n         <param name=\"testdata_input\" value=\"Dataset_Metaboflow_Ionomic_Workflow.csv\"/>\n         <param name=\"pre_define_sd\" value=\"pre_defined_sd.txt\"/>\n         <output name=\"data_long\" file=\"data_long.csv\"/>\n      </test>\n    </tests>\n    <help>\n    </help>\n</tool>\n\nLook forward to getting your comments to help me sort out my issue. Thank you very much.\nBest regards,\nJianliang"}, {"role": "system", "text": "Besides I can not give a full answer and only some small hints now, it does not look oke to me.\nThis line for example:\n<data name=\"data_long\" format=\"csv\" />\n\nYou can see this line as \u201cthe output path\u201d. The variable $data_long contains a path and filename where galaxy is expecting the output. But I dont see that you give this variable to the\nrunPreProc.R script. So how can your script know how to give the output to galaxy?\nI think the answer is or use from_work_dir what you commented out now. Or change and execute the script like:\n/usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o \\$PWD -outputfile $data_long\n\nTo use from_work_dir I am guessing your code should look like something like this:\n/usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o \"output\"\n<data name=\"data_long\" format=\"csv\"  from_work_dir=\"output/data_long.csv\" label=\"data_long\"/>\n\nWhere your runPreProc.R script writes all the output files to a folder named \u201coutput\u201d. You need to create this folder first.\nIt can be possible that something like:\n/usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o \"\"\n<data name=\"data_long\" format=\"csv\"  from_work_dir=\"data_long.csv\" label=\"data_long\"/>\n\nalso works but I don\u2019t know how you script looks like."}, {"role": "user", "text": "Hi Marten,\nThank you very much for your comments.\nMy script only takes 3 parameters, they are immediately after the switches -d -p -o. The files are generated in the R script. It doesn\u2019t take $data_long (from output name). It doesn\u2019t make sense for me to have the parameter in command line. As I commented out in the tool xml file, I will have more files to collect. Should I put all the variable names in the command line?\nBest regards,\nJianliang"}, {"role": "system", "text": "I see it are quite a lot of files and I would use the from_work_dir method.\nSo\n\nShould I put all the variable names in the command line?\n\nNo\nYou can reply if it is still not clear, then me or some else can explain it better or in more detail. I understand that my first answer is a bit cryptic at the moment =)"}, {"role": "user", "text": "Hi Marten,\nThank you very much. I still got errors as my R code couldn\u2019t recognize the subfolder \u201coutput\u201d. Should I create the subfolder in my tool xml or my job_conf.xml before R script is run?\nBest regards,\nJianliang"}, {"role": "system", "text": "My first question would be, did you tried it without that extra output folder. So doing something like -o \"\" when you execute the script.\nIf you need or want that folder I think in your case you could do.\n<![CDATA[\n      mkdir output && /usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o output\n     ]]>\n\nSo in your tool xml or in the R scripts itself"}], [{"role": "user", "text": "I\u2019m following the Scaling and Load Balancing \u2014 Galaxy Project 22.09.dev0 documentation and cannot get gunicorn to bind to the socket file.\n\u2013 gunicorn-galaxy.socket \u2013\n[Unit]\n\nDescription=gunicorn socket for galaxy\n\n[Socket]\n\nListenStream=/run/gunicorn-galaxy.sock\n\n[Install]\n\nWantedBy=sockets.target\n\n\u2013 galaxy.yml gunicorn section \u2013\n  gunicorn:\n    enable: true\n    bind: 'unix:/run/gunicorn-galaxy.sock'\n    workers: 2\n    timeout: 300\n    extra_args: '--forwarded-allow-ips=\"*\"'\n    preload: true\n\n\u2013 gunicorn-galaxy.service \u2013\n[Unit]\n\nDescription=gunicorn daemon for galaxy\n\nAfter=network.target\n\nAfter=time-sync.target\n\n[Service]\n\nPermissionsStartOnly=true\n\nType=simple\n\nUser=galaxy\n\nGroup=www-data\n\nRestart=on-abort\n\nWorkingDirectory=/home/galaxy/galaxy\n\nTimeoutStartSec=10\n\nExecStart=/home/galaxy/galaxy/.venv/bin/galaxy \\\n\n--state-dir /home/galaxy/galaxy/database/gravity\n\nEnvironment=VIRTUAL_ENV=/home/galaxy/galaxy/.venv PATH=/home/galaxy/galaxy/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n[Install]\n\nWantedBy=multi-user.target\n\nThis is the serviced error:\n$ sudo systemctl status gunicorn-galaxy.service\n\u25cf gunicorn-galaxy.service - gunicorn daemon for galaxy\nLoaded: loaded (/etc/systemd/system/gunicorn-galaxy.service; enabled; vendor preset: enabled)\nActive: active (running) since Mon 2022-10-17 21:20:23 UTC; 14min ago\nTriggeredBy: \u25cf gunicorn-galaxy.socket\nMain PID: 5053 (galaxy)\nTasks: 9 (limit: 18962)\nMemory: 577.3M\nCPU: 7min 57.313s\nCGroup: /system.slice/gunicorn-galaxy.service\n\u251c\u25005053 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/galaxy --state-dir /home/galaxy/galaxy/database/gravity\n\u251c\u25005055 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/supervisord -c /home/galaxy/galaxy/database/gravity/supervisor/supervisord.conf --nodaemon\n\u251c\u25005058 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery worker --concurrency 2 --loglevel DEBUG --pool threads --queues celery,galaxy.internal,galaxy.external\n\u251c\u25005059 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery beat --loglevel DEBUG --schedule /home/galaxy/galaxy/database/gravity/celery-beat-schedule\n\u251c\u25005063 /usr/bin/tail -f /home/galaxy/galaxy/database/gravity/log/gunicorn.log /home/galaxy/galaxy/database/gravity/log/celery.log /home/galaxy/galaxy/database/gravity/log/celery-beat.log\n\u251c\u25005083 /home/galaxy/galaxy/.venv/bin/python -c \u201cfrom multiprocessing.semaphore_tracker import main;main(7)\u201d\n\u2514\u25005323 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/gunicorn \u201cgalaxy.webapps.galaxy.fast_factory:factory()\u201d --timeout 300 --pythonpath lib -k galaxy.webapps.galaxy.workers.Worker -b unix:/run/gunicorn-galaxy.sock --workers=2 --config python:galaxy.web_stack.gunicorn_config --preload \u201c\u2013forwarded-allow-ips=*\u201d\nOct 17 21:34:07 ip-10-0-1-180 galaxy[5063]: [2022-10-17 21:34:07 +0000] [5310] [ERROR] Can\u2019t connect to /run/gunicorn-galaxy.sock\nThe file exists and is rw to all:\n$ ls -l /run/gunicorn-galaxy.sock\nsrw-rw-rw- 1 root root 0 Oct 17 20:55 /run/gunicorn-galaxy.sock"}, {"role": "system", "text": "Hi @isaphan\nI\u2019ve cross posted your question to our Admin chat to bring in some expert help. They may reply here or there, and feel free to join the chat: Gitter galaxyproject/admins - Gitter or at Matrix You're invited to talk on Matrix\nTutorials https://training.galaxyproject.org/training-material/topics/admin/tutorials/"}, {"role": "user", "text": "Hello @jennaj I\u2019m getting no response on the chat and the links that you provided redirected me to an error page: Oops!\n\nWe can\u2019t seem to find the page you\u2019re looking for.\nPerhaps you are here due to a mis-typed URL"}, {"role": "user", "text": "For future reference: admin suggested downgrading to galaxy version 22.01.\nI am getting exactly the same error as with version 22.05.\nSocket is up and listening:\n$ sudo systemctl status gunicorn-galaxy.socket\n\u25cf gunicorn-galaxy.socket - gunicorn socket for galaxy\n     Loaded: loaded (/etc/systemd/system/gunicorn-galaxy.socket; enabled; vendor preset: enabled)\n     Active: active (running) since Thu 2022-10-20 18:31:51 UTC; 19s ago\n   Triggers: \u25cf gunicorn-galaxy.service\n     Listen: /run/gunicorn-galaxy.sock (Stream)\n     CGroup: /system.slice/gunicorn-galaxy.socket\n\nOct 20 18:31:51 ip-10-0-1-180 systemd[1]: Listening on gunicorn socket for galaxy.\n\nSystemd service file (Group changed to \u2018galaxy\u2019 as recommended in release_22.01 docs):\n[Unit]\n\nDescription=gunicorn daemon for galaxy\n\nRequires=gunicorn-galaxy.socket\n\nAfter=network.target\n\nAfter=time-sync.target\n\n[Service]\n\nPermissionsStartOnly=true\n\nType=simple\n\nUser=galaxy\n\nGroup=galaxy\n\nWorkingDirectory=/home/galaxy/galaxy\n\nTimeoutStartSec=60\n\nExecStart=/home/galaxy/galaxy/.venv/bin/galaxy \\\n\n--state-dir /home/galaxy/galaxy/database/gravity\n\nEnvironment=VIRTUAL_ENV=/home/galaxy/galaxy/.venv PATH=/home/galaxy/galaxy/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n[Install]\n\nWantedBy=multi-user.target\n\nSystemd fails to run gunicorn:\n$ sudo systemctl status gunicorn-galaxy\n\u25cf gunicorn-galaxy.service - gunicorn daemon for galaxy\n     Loaded: loaded (/etc/systemd/system/gunicorn-galaxy.service; disabled; vendor preset: enabled)\n     Active: active (running) since Thu 2022-10-20 18:31:58 UTC; 1min 16s ago\nTriggeredBy: \u25cf gunicorn-galaxy.socket\n   Main PID: 68088 (galaxy)\n      Tasks: 11 (limit: 18962)\n     Memory: 316.9M\n        CPU: 55.129s\n     CGroup: /system.slice/gunicorn-galaxy.service\n             \u251c\u250068088 /home/galaxy/galaxy/.venv/bin/python3.7 /home/galaxy/galaxy/.venv/bin/galaxy --state-dir /home/galaxy/galaxy/database/gravity\n             \u251c\u250068090 /home/galaxy/galaxy/.venv/bin/python3.7 /home/galaxy/galaxy/.venv/bin/supervisord -c /home/galaxy/galaxy/database/gravity/supervisor/supervisord.conf --nodaemon\n             \u251c\u250068093 /home/galaxy/galaxy/.venv/bin/python3.7 /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery worker --concurrency 2 --loglevel DEBUG --pool threads --queues celery,galaxy.internal,galaxy.external\n             \u251c\u250068094 /home/galaxy/galaxy/.venv/bin/python3.7 /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery beat --loglevel DEBUG --schedule /home/galaxy/galaxy/database/gravity/celery-beat-schedule\n             \u2514\u250068096 /usr/bin/tail -f /home/galaxy/galaxy/database/gravity/log/gunicorn.log /home/galaxy/galaxy/database/gravity/log/celery.log /home/galaxy/galaxy/database/gravity/log/celery-beat.log\n\nOct 20 18:33:05 ip-10-0-1-180 galaxy[68096]: galaxy.webapps.galaxy.buildapp DEBUG 2022-10-20 18:33:05,661 [pN:main,p:68136,tN:MainThread] Prior to webapp return, Galaxy thread <_MainThread(MainThread, started 140058733199872)> is alive.\nOct 20 18:33:06 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:06 +0000] [68136] [INFO] Starting gunicorn 20.1.0\nOct 20 18:33:06 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:06 +0000] [68136] [ERROR] Retrying in 1 second.\nOct 20 18:33:07 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:07 +0000] [68136] [ERROR] Retrying in 1 second.\nOct 20 18:33:08 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:08 +0000] [68136] [ERROR] Retrying in 1 second.\nOct 20 18:33:09 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:09 +0000] [68136] [ERROR] Retrying in 1 second.\nOct 20 18:33:10 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:10 +0000] [68136] [ERROR] Retrying in 1 second.\nOct 20 18:33:12 ip-10-0-1-180 galaxy[68090]: 2022-10-20 18:33:12,000 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\nOct 20 18:33:12 ip-10-0-1-180 galaxy[68096]: [2022-10-20 18:33:12 +0000] [68136] [ERROR] Can't connect to /run/gunicorn-galaxy.sock\nOct 20 18:33:14 ip-10-0-1-180 galaxy[68090]: 2022-10-20 18:33:14,546 INFO exited: gunicorn (exit status 1; not expected)\n"}, {"role": "user", "text": "Fixed: moved sock file from /run  to <galaxy_project_location>/var\nPerhaps it should be stated in the docs that unix socket files must be located inside the project root?"}, {"role": "system", "text": "Glad you discovered the problem and that you posted that back here (thank you!).\nGoing further and adding a note to the docs sounds like a great idea. Please suggest the edit at Github in a PR."}, {"role": "user", "text": "The solution allows me to start galaxy\u2026 but it is unusable as I cannot upload any files. I am now fighting with tusd and not getting anywhere\u2026 I\u2019m not a sysadmin, yet  I must have installed galaxy a dozen times since 2007 and this is by far the very worst experience I\u2019ve had with it. Very disappointing."}, {"role": "system", "text": "Hi @isaphan\nI\u2019m sorry that this has been difficult.\nDo you think the instructions here are not working correctly? It was updated recently, about a month ago, as part of the overall update of the admin docs/tutorials that is mostly done by now. Some tutorials were updated as recently as today.\nIf you want to start up a new topic with the details, we can get some more community help again."}, {"role": "user", "text": "Hello @jennaj ,\nThank you for the instructions. If I had the help of a unix sysadmin, I would certainly ask them to look into ansible.\nWe finally got galaxy to run with a working file upload on our system:\n\n\ntusd settings for external tusd server: we could not figure it out, due to the \u201cMixed Content\u201d error that masked tusd configuration issues (see below). We are now using the tusd that comes already shipped with galaxy. I was very confused by the fact that the tusd config has to be disabled in galaxy.yml in order for tusd to work by default ?!\n\n\nScaling and Load Balancing \u2014 Galaxy Project 22.05.1 documentation : on our system, the group has to be set to www-data and not galaxy, even though the galaxy user is in the galaxy group.\n\n\nthe NGINX config statement in the docs: \u201cproxy_set_header X-Forwarded-Proto $scheme;\u201d causes uploads to fail on Chrome and Firefox with error: \u201cMixed Content: The page was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint\u201d. Replacing $scheme with https fixed this.\n\n\nI searched the project tickets in GitHub and since nobody reported the same issues, I assume the problems are due to our setup, where an external load balancer translates http requests to https -an institutional constraint over which we have no control."}], [{"role": "user", "text": "Hi,\nI run into problems with mummerplot, could you please help me?\nI have submitted a reference genome and an assembly (both in fasta format), for comparison, and I got the following message:\nReading delta file /data/dnb02/galaxy_db/files/008/660/dataset_8660337.dat\nWriting plot files out.fplot, out.rplot, out.hplot\nWriting gnuplot script out.gp\nRendering plot out.png\nWARNING: Unable to run \u2018false out.gp\u2019, Inappropriate ioctl for device\nDid I miss something or is there a pb with the tool?\nthanks"}, {"role": "system", "text": "I have the same problem."}, {"role": "system", "text": "ping @hxr @bjoern.gruening"}, {"role": "system", "text": "Hi @1114 and @friedanne,\nsorry for my late reply. This should be fixed now.\nThanks,\nBjoern"}], [{"role": "user", "text": "I\u2019m using a usegalaxy.org instance to analyze illumina data (paired end, trimmomatic trimmed) to a reference fungal genome (one I downloaded from Ensembl). I am using BWA-MEM in paired-end mode, but I think that it is failing in the indexing.\nEvery time I run it, I get this error:\n[E::idx_find_and_load] Could not retrieve index file for \u2018/galaxy-repl/main/files/042/990/dataset_42990690.dat\u2019\n[E::idx_find_and_load] Could not retrieve index file for \u2018/galaxy-repl/main/files/042/990/dataset_42990690.dat\u2019\nAnyone have any suggestions?"}, {"role": "system", "text": "Hi Rachel, are your datasets green ? This message is emitted by pysam before we index the bam file. If it is green there is nothing to worry about."}, {"role": "user", "text": "Thanks for the quick reply! The job is red indicating a failure. And this is the last thing it says before it fails. I was able to run the fastq files and genome through bwa mem using the standalone programs on the same files though. I ended up using the command line software to generate my bam files and moved those back to Galaxy for the rest of the analysis. Still no idea why it failed on the BWA-MEM step, but I was able to work around."}, {"role": "system", "text": "Could you send a bug report for the dataset that failed ? You can do that with the little bug icon that says \u201cview or report this error\u201d"}, {"role": "system", "text": "Hi @Rachel\nWe\u2019ve uncovered the root cause for the error. It was a server-side issue that will be corrected when this change is incorporated (within a few days) https://github.com/galaxyproject/pulsar/pull/230\nUpdate 1 fix pending as of 9/2/20, see post below for a workaround.\nUpdate 2 9/15/20: dedicated ticket for BWA + BWA-MEM issues https://github.com/galaxyproject/usegalaxy-playbook/issues/307\nThanks for sending in the bug report \u2013 very helpful!"}, {"role": "user", "text": "Thanks for the update!"}, {"role": "system", "text": "I believe I am still experiencing this problem (BWA-MEM terminates in red error state and does not create a BAM index) on the public Galaxy site."}, {"role": "system", "text": "Hi @Geoffrey_Smith\nThanks for sending in the bug report. Yes, your job appears to related. There were other changes made at usegalaxy.org, but the full resolution is not implemented yet.\nKnown impacted tools are: Stringtie, BWA, and BWA-MEM\nMeanwhile, you can try directing jobs to the alternative cluster as described here: Error running Cufflinks\ncc @nate"}, {"role": "system", "text": "This was fixed as of September 22, sorry for not updating here. Please let us know if you\u2019re still encountering issues."}], [{"role": "user", "text": "Hi, I have tried running some RNAseq datasets on Trimmomatic but they keep failing. The error I get when clicking on the bug icon is the following:\nFatal error:\nWhen checking the information section, I get \u201cempty\u201d for \u201cTool standard error\u201d, and \u201cTool standard output\u201d is as below:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/046/864/46864652/_job_tmp -Xmx28g -Xms256m\nTrimmomaticSE: Started with arguments:\n-threads 6 fastq_in.fastqsanger.gz fastq_out.fastqsanger.gz ILLUMINACLIP:/corral4/main/jobs/046/864/46864652/configs/tmptcibpyu4:2:30:10 LEADING:30 TRAILING:30\nUsing Long Clipping Sequence: \u2018ATCGGAAGAGCACACGTCTGAACTCCAGTCACGGTGAACCATCTCGTATG\u2019\nUsing Long Clipping Sequence: \u2018GATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTTCCAACGCGTGTAGATCT\u2019\nUsing Long Clipping Sequence: \u2018ATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTTCCAACGCGTGTAGATCTC\u2019\nUsing Long Clipping Sequence: \u2018GATCGGAAGAGCACACGTCTGAACTCCAGTCACGGTGAACCATCTCGTAT\u2019\nILLUMINACLIP: Using 0 prefix pairs, 4 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nQuality encoding detected as phred33\njava.io.EOFException: Unexpected end of ZLIB input stream\nat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\nat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\nat java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)\nat org.usadellab.trimmomatic.util.ConcatGZIPInputStream.read(ConcatGZIPInputStream.java:73)\nat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\nat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\nat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\nat java.io.InputStreamReader.read(InputStreamReader.java:184)\nat java.io.BufferedReader.fill(BufferedReader.java:161)\nat java.io.BufferedReader.readLine(BufferedReader.java:324)\nat java.io.BufferedReader.readLine(BufferedReader.java:389)\nat org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:71)\nat org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)\nat java.lang.Thread.run(Thread.java:745)\nException in thread \u201cThread-0\u201d java.lang.RuntimeException: java.io.EOFException: Unexpected end of ZLIB input stream\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:56)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\nat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\nat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\nat java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)\nat org.usadellab.trimmomatic.util.ConcatGZIPInputStream.read(ConcatGZIPInputStream.java:73)\nat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\nat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\nat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\nat java.io.InputStreamReader.read(InputStreamReader.java:184)\nat java.io.BufferedReader.fill(BufferedReader.java:161)\nat java.io.BufferedReader.readLine(BufferedReader.java:324)\nat java.io.BufferedReader.readLine(BufferedReader.java:389)\nat org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:71)\nat org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)\n\u2026 1 more\nInput Reads: 17849000 Surviving: 17595136 (98.58%) Dropped: 253864 (1.42%)\nTrimmomaticSE: Completed successfully\nWhat I was trying to do, was to run a fastq.gz file and trim the beginning and end bases below a certain quality score (using LEAD and TRAIL commands). I was also trying to trim the adapter sequences shown as overrpresented in FastQC report, entering the specific FASTA sequences shown in the report.\nTried other trimming tools (Trim Galore! and Cutadapt) but they also result in errors (\u201cFatal error: Exit code 1 ()\u201d and \u201cFatal error: Exit code 2 ()\u201d respectively). FastQC works fine.\nIf you could help discovering what went wrong, would really appreciate it - I\u2019m a total newbie and this is something I have been struggling with for the last couple of days."}, {"role": "system", "text": "I think this mostly has to do with corrupted files or a wrong fastq format. If you look up \u201cTrimmomatic java.io.EOFException: Unexpected end of ZLIB input stream\u201d you get some results that might help you. On galaxy there is also a tool called FASTQ info, maybe this can also help. (I am not familiar with this tool)"}, {"role": "user", "text": "A possibility. I\u2019ve had issues with the original file downloads from NCBI SRA (some files would end up truncated, so I had to re-download them). But then FastQC worked. What method do you usually use to download the datasets and check their integrity?"}, {"role": "system", "text": "\n\n\n gbbio:\n\nOn galaxy there is also a tool called FASTQ info\n\n\n\n\n\n Egle:\n\nBut then FastQC worked. What method do you usually use to download the datasets and check their integrity?\n\n\nEither or both of those tools can be run to detect truncated datasets. tutorials/quality-control\nYou didn\u2019t state how you were loading the data \u2013 but SRA data from NCBI is best retrieved by accession ID (one or a list) using this tool Faster Download and Extract Reads in FASTQ. The result will be sorted into collections with the proper format/datatype.\n\nDirect link to that tool at UseGalaxy.org == fasterq_dump\n\nAt other servers, please search the tool panel.\n"}, {"role": "user", "text": "An update.\nSo I tried the FASTQ info tool as per gbbio\u2019s suggestion. It indeed showed that some of the datasets I was not able to trim were truncated or otherwise affected (something about sequence and quality not having the same length), despite FastQC reports looking alright. Many of the FASTQ info reports also give \u201cRead name provided with no suffix\u201d message. Some got partial reports with messages about running out of memory.\nSo I assume these datasets are somehow damaged even if they looked alright in FastQC reports? \n@jennaj I downloaded the datasets using Google Chrome, going to NCBI SRA Run selector, clicking on individual SRR numbers and then \u201cFASTA/FASTQ download\u201d. And then uploaded them manually one by one from my computer onto Galaxy using FastQC tool. Which was extremely inefficient time-wise.\nI have now tried running Faster Download and Extract Reads in FASTQ for one of the datasets. I see that it generated paired-end data files. Which is interesting - the previous download method resulted in fastq.gz files that I was able to run only as single-end reads on Trimmomatic or alternative tools despite them labelled as \u201cpaired\u201d in SRA database.\nI guess my questions are:\n\nWould you recommend to redownload the datasets of interest using this Faster Download and Extract Reads in FASTQ tool and re-rerun the FastQC analyses/trimming?\nWould you recommend running additional quality validation via FastQ info?\nIs Faster Download and Extract Reads in FASTQ something that would ensure the best speed and reliability ratio (in terms of dataset integrity) for downloads of large datasets?\n\nI\u2019ve been trying to investigate alternative download/upload methods but I understand FTP downloads/uploads are not supported by Galaxy any more?\nWould highly appreciate your advice."}, {"role": "system", "text": "\n\n\n Egle:\n\nI guess my questions are:\n\nWould you recommend to redownload the datasets of interest using this Faster Download and Extract Reads in FASTQ tool and re-rerun the FastQC analyses/trimming?\nWould you recommend running additional quality validation via FastQ info?\nIs Faster Download and Extract Reads in FASTQ something that would ensure the best speed and reliability ratio (in terms of dataset integrity) for downloads of large datasets?\n\n\n\nYes to all of these.\nFastQC only reads in a subset of total reads, and I guess it could miss truncated data. That seems new and unexpected if the input was compressed. If you want to send in an example as a bug report directly from an error, I\u2019d be interested in reviewing it.\nFastQ info checks everything.\nFaster Download and Extract Reads in FASTQ is a dedicated tool intended for batch data transfers both on Galaxy\u2019s end and NCBI\u2019s.\nWhen navigating NCBI for links directly and pasting those into the Upload tool, some of the metadata is lost and/or the link might be to the \u201csubmitted\u201d data version versus the \u201cprocessed\u201d version. The differences are technical but most want the \u201cprocessed\u201d and that is what the Faster tool retrieves.\nPartial data load should be rare-ish with any method but no one ever regrets running a bit more QA and avoiding problems downstream  That said, NCBI does have super busy times when any data retrieval or sometimes specific types of data retrieval will fail. The solution is to wait a bit then try again.\nFor large amounts of data, it can help to get things organized at the Upload step. These specific tutorials cover batch concepts and methods.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "\nYes to all of these.\n\nWonderful.\n\nFastQC only reads in a subset of total reads, and I guess it could miss truncated data. That seems new and unexpected if the input was compressed. If you want to send in an example as a bug report directly from an error, I\u2019d be interested in reviewing it.\nFastQ info checks everything.\n\nThank you, that explains a lot.\nAs for the bug reports - so the below is the tool standard error for the FastQC output of the dataset:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/046/559/46559587/_job_tmp -Xmx28g -Xms256m\njava.io.EOFException: Unexpected end of ZLIB input stream\nat java.base/java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:245)\nat java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:159)\nat java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)\nat uk.ac.babraham.FastQC.Utilities.MultiMemberGZIPInputStream.read(MultiMemberGZIPInputStream.java:68)\nat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\nat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\nat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\nat java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)\nat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\nat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\nat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\nat uk.ac.babraham.FastQC.Sequence.FastQFile.readNext(FastQFile.java:138)\nat uk.ac.babraham.FastQC.Sequence.FastQFile.next(FastQFile.java:125)\nat uk.ac.babraham.FastQC.Analysis.AnalysisRunner.run(AnalysisRunner.java:77)\nat java.base/java.lang.Thread.run(Thread.java:834)\nI see now that it has the same \u201cUnexpected end of ZLIB input stream\u201d issue. But the FastQC output reports came up green, without any bug icons flagged up (both the Webpage and RawData) or anything else that would have suggested truncation. Just to verify - if I ran Trimmomatic (or something else) on this dataset, this report remains as it was until I run FastQC on the trimmed dataset, is that correct?\nThis was my first run that worked, so I trusted green until the trimming step \nThe FastQ info report also came green, without bugs. The tool standard error was empty, but this is what the output looked like:\nfastq_utils 0.25.1\nDEFAULT_HASHSIZE=39000001\nScanning and indexing all reads from /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat\nRead name provided with no suffix\n10000020000030000040000050000060000070000080000090000010000001100000120000013000001400000150000016000001700000180000019000002000000210000022000002300000240000025000002600000270000028000002900000300000031000003200000330000034000003500000360000037000003800000390000040000004100000420000043000004400000450000046000004700000480000049000005000000510000052000005300000540000055000005600000570000058000005900000600000061000006200000630000064000006500000660000067000006800000690000070000007100000720000073000007400000750000076000007700000780000079000008000000810000082000008300000840000085000008600000870000088000008900000900000091000009200000930000094000009500000960000097000009800000990000010000000101000001020000010300000104000001050000010600000107000001080000010900000110000001110000011200000113000001140000011500000116000001170000011800000119000001200000012100000122000001230000012400000125000001260000012700000128000001290000013000000131000001320000013300000134000001350000013600000137000001380000013900000140000001410000014200000143000001440000014500000146000001470000014800000149000001500000015100000152000001530000015400000155000001560000015700000158000001590000016000000161000001620000016300000164000001650000016600000167000001680000016900000170000001710000017200000173000001740000017500000176000001770000017800000\nERROR: Error in file /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat: line 71398384: file truncated\nThe numbers did not come up as a block in the report, but were separated out by series of squared question marks (15 in between each number).\n"}, {"role": "system", "text": "Thanks for following up! I reviewed carefully and think everything is working how it is expected to.\n\nFastQC\n\n\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: ID line didn\u2019t start with \u2018@\u2019\n\nInput dataset 8 \u2013 datatype/format sra_manifest.tabular\nOutput dataset 9 \u2013 \u201cred\u201d errored result\nThe tool is expecting fastq reads and filters for those in the input drop down menu of potential datasets. Dataset 8 is not in the last listing. I\u2019m guessing that you dragged-and-dropped the dataset instead (?) That can lead to errors since it is an \u201coverride\u201d function that bypasses initial format checks.\nIf a dataset is not in the listing for a tool\u2019s input area (any) \u2013 either the datatype/format needs to be modified or the content isn\u2019t a match for what that tool can manipulate. #tool-doesnt-recognize-input-datasets\nSeveral of the early FastQC runs in your history are similar. The input wasn\u2019t a content match, and at least one input was in an error state.\n\nFastQC\n\n\njava.io.EOFException: Unexpected end of ZLIB input stream\n\nInput dataset 45 \u2013 datatype/format sra_manifest.tabular\nOutput dataset 46 \u2013 \u201cgreen\u201d (putatively) successful result\nThe output was purged so I can\u2019t see all of it, just a bit from the \u201craw\u201d output (none of the HTML). The input does appear to be in a fastq format. The datatype was compressed \u201cfastqsanger.gz\u201d. I can\u2019t check the actual contents except for the first few lines. Those look to be a hybrid between \u201csubmitted\u201d and \u201cprocessed\u201d SRA reads that NCBI hosts.\nIf you really want to investigate, this older FAQ #ncbi-sra-sourced-fastq-data has a list of all the manipulations we knew about to standardize the various read formats that NCBI hosts. Why so much variation from NCBI in the past? Changes over time in sequencing protocols and the SRA submission process itself and in how NCBI creates/standardizes the \u201cprocessed\u201d read data then versus now, and probably some other historical things I don\u2019t know or can\u2019t remember \nConfusing but shouldn\u2019t matter now, as format issues will be avoided with the Faster tool, and you are using that now.\n\nFastq info\n\nThis is different. It is the output from the tool, and it happens to also be in a job log.\n\nfastq_utils 0.25.1\nDEFAULT_HASHSIZE=39000001\nScanning and indexing all reads from /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat\nRead name provided with no suffix\n> 10000020000030000040000050000060000070000080000090000010000001100000120000013000001400000150000016000001700000180000019000002000000210000022000002300000240000025000002600000270000028000002900000300000031000003200000330000034000003500000360000037000003800000390000040000004100000420000043000004400000450000046000004700000480000049000005000000510000052000005300000540000055000005600000570000058000005900000600000061000006200000630000064000006500000660000067000006800000690000070000007100000720000073000007400000750000076000007700000780000079000008000000810000082000008300000840000085000008600000870000088000008900000900000091000009200000930000094000009500000960000097000009800000990000010000000101000001020000010300000104000001050000010600000107000001080000010900000110000001110000011200000113000001140000011500000116000001170000011800000119000001200000012100000122000001230000012400000125000001260000012700000128000001290000013000000131000001320000013300000134000001350000013600000137000001380000013900000140000001410000014200000143000001440000014500000146000001470000014800000149000001500000015100000152000001530000015400000155000001560000015700000158000001590000016000000161000001620000016300000164000001650000016600000167000001680000016900000170000001710000017200000173000001740000017500000176000001770000017800000\nERROR: Error in file /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat: line 71398384: file truncated\n\nInput dataset 32 \u2013 datatype/format fastqsanger.gz\nOutput dataset 253 \u2013 \u201cgreen\u201d successful result\nHow to interpret this: the Fastq info tool ran successfully (didn\u2019t fail itself) and it reported back a problem it found in the input. This test history I created a few months ago has an example that matches your use case (artificially created). I\u2019ll leave this shared as a reference. Galaxy | Accessible History | test fastq_info\nHow to catch these: These are text files so are simple to parse/filter/combine. Even if all the data is in a collection, you\u2019ll be able to tell which datasets any errors are from. I just did this two different ways in that shared history as examples and there are likely 100+ ways to do this. Anything \u201ctext manipulation\u201d you can do on the command line can usually be done in Galaxy, with the bonus of being able to add that to workflows, see data-manipulation-olympics/tutorial.html#cheatsheet\nThanks again and hope that helps!"}], [{"role": "user", "text": "Hi everyone,\nI\u2019m trying to use an RNA-seq pipeline to analyze my own data for C. elegans.\nEvery time I try to upload the gtf file download from UCSF, wormbase, or ensemble, I get the following error message \u201cAn error occurred setting the metadata for this dataset\u201d.\nAfter one day trying different options on solving the problem, I have tried to upload the same dataset on the galaxy.eu in which I did not receive any error. This made me to think that there is no problem with the file itself.\nMay I ask if there is a bug on the galaxy.org my local Galaxy server and if it\u2019s possible to fix it?\nThank you so much!\nIni"}, {"role": "system", "text": "Hi @inivasDL,\ncould you share your history with me? I\u2019ll have a look at it.\nRegards"}, {"role": "user", "text": "Hi Gallardo,\nthank you so much for replying to me and to have a look at it.\nHere is the link for the history.\nhttp://localhost:8080/u/marziawanglab/h/21663771788138242n2rna-seqanalysis\nPlease let me know if it\u2019s not working as it is my first time sharing it.\nLine 169 is the metadata error, and in line 168 I tried to use the auto detection to correct it, but it\u2019s running since yesterday.\nAny advice you would have, I will really appreciate it.\nThank you so much!\nIni"}, {"role": "system", "text": "Hi @inivasDL,\nit seems that you are using a local Galaxy instance. Could you run the analysis in usegalaxy.eu? In that way, I could have access to your history.\nRegards"}, {"role": "user", "text": "Yes, I can.\nEven though in the galaxy.eu I didn\u2019t have any error.\nGalaxy | Europe | Accessible History | rna-seq\nIt is line1. I did as a trial to know whether the problem was associated with the file. I\u2019m not sure if this information can be useful. If there is an additional way I can try to run the analysis without using the local Galaxy.\nThank you,\nIni"}, {"role": "user", "text": "I just found out that if I upload the same gtf file on the galaxy without using the local server (using .org or .eu), there is no metadata error detected.\nHow could I fix it when using the local one?\nDid you or anybody encounter a similar problem?\nBest,\nIni"}, {"role": "system", "text": "Hi @inivasDL\nData upload problems are usually the first indicator of some configuration problem after a new Galaxy is first started up.\nThat said, sometimes odd errors, including this one, are produced when a file is not completely uploaded (new server or not). There could be many reasons for that with a local Galaxy server. A local Galaxy needs some admin configuration to be used without problems coming up. Some of this we can help with and some of this will be specific to your computer/network that we can\u2019t help with (since it is private) but we do have documentation that covers the basic setup and use cases.\n\n\n\nI can't open http://localhost:8080 in my browser\n\n\nGalaxy resources\n\nQuick start: Get Galaxy - Galaxy Community Hub\n\nTutorials: Galaxy Training!\n\nDocs: Galaxy Documentation \u2014 Galaxy Project 21.09.1.dev0 documentation\n\n\n\n\nThere will be a training event in March that includes administrative topics. Many of the tutorials linked above will be included. Anyone can join. The event is global, asynchronous, and many more people from the community will be available to help. GTN Sm\u00f6rg\u00e5sbord 2: 14-18 March | Gallantries\nYou can also go through those tutorials on your own now and try to uncover the problem, or at least be able to provide more context about your configuration so the community can assist here. Examples of information that will help: Galaxy version, computer OS/version, dependency management choice(s), what admin configurations you have done, plus how some tests worked out. Have you installed tools and run tool tests? The goal is to find out things like: Do other datasets load properly? This same file type fails (always/sometimes? from different sources or just one particular source?). Do other data upload successfully \u2013 or do all uploads fail? Has any upload ever worked? Have any tools ever worked? If the analysis was successful before, have you made any changes since things worked?\n\nWhat may be best if you do not want to go through all the administrative setup/tests, and this is a new server that you haven\u2019t heavily invested in already:\n Try an alternative deployment choice. Docker Galaxy comes with more of the administrative details pre-configured. Details here: Galaxy Docker - Galaxy Community Hub"}], [{"role": "user", "text": "I have four R1 and four R2 fasq.gz files of a bacterial genome sequence. Do I need to concatenate each of four R1 or each of four R2 files directly?"}, {"role": "system", "text": "Hello, the titel of your post and the text contain a different question.\n\nDo I need to concatenate each of four R1 or each of four R2 files directly?\n\nMostly not, what do you want to do with the files? What kind of analyses do you want to perform?\nThis can also be a good place to start:\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello,\nI have a fresh new installation of Galaxy (version 19.09 on Ubuntu) hooked into PostgreSQL.  After users install a version of bwa in Galaxy (revision: 01ac0a5fedc3) from the \u201cInstall new tools (Beta)\u201d link under the \u201cAdmin\u201d section, the new bwa shows missing dependencies.  Conversely, when I install an older version  (revision: c71dd035971e) using the \u201cInstall New Tools\u201d link (NOT Beta), the dependencies are installed along with bwa.  I do not know how to resolve/debug these missing dependencies given my 3 weeks experience with Galaxy.  Both bwa and samtools are installed in Ubuntu and included in the PATH variable for operating system user Galaxy.  This seems like it should be an obvious thing so I\u2019m seeking guidance on a good approach to debug this.  Should I expect all tools under \u201cInstall New Tools (Beta)\u201d to work?   I wonder whether the PATH is not being properly passed to Galaxy so it can\u2019t locate the operating system installations of bwa and samtools.  In the Galaxy app under \u201cManage Dependencies\u201d I tried selecting \u201cInstall checked dependencies using Conda\u201d but it generates errors in the galaxy.log.  Sorry, not sure how to include the galaxy.log without going over the 30K limit.\nBackground Info:\nI verified that bwa and samtools are both in the operating system PATH for user galaxy (see below).\nI used \u201capt-get\u201d instead of conda to install bwa on Ubuntu:  \u201c(sudo apt-get update -y;   sudo apt-get install -y bwa)\u201d.\nI\u2019m launching galaxy as \u201csh /apps/galaxy/galaxy_app/run.sh start > /apps/galaxy/galaxy_app/galaxy_start.log 2>&1\u201d.\nBWA operating system verification\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ which bwa\n/usr/bin/bwa\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ bwa\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li lh3@sanger.ac.uk\nSAMTOOLS operating system verification\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ which samtools\n/usr/bin/samtools\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ samtools\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.6 (using htslib 1.6)\nBWA tool information in Galaxy after installation\nTool shed:    toolshed.g2.bx.psu.edu\nName:          bwa\nDescription:\nRevision:       01ac0a5fedc3\nOwner:          devteam\nLocation:       /apps/galaxy/galaxy_app/database/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/bwa/01ac0a5fedc3/bwa\nDeleted:        False\nDependencies:\nbwa       version:0.7.17  Resolver: none  Exact Version: True  Installation Status:  not installed\nsamtools  version:1.6     Resolver: none  Exact Version: True  Installation Status:  not installed"}, {"role": "system", "text": "Can you paste the relevant parts of the error log? If not, you can use a service such as gist.github.com to paste the log contents, or put the full log file somewhere that it can be downloaded and paste the link.\nGalaxy manages its dependencies, by default using Conda, so the system versions are generally irrelevant to the Galaxy tool (although it will use things it finds on $PATH as a last resort)."}, {"role": "user", "text": "Here is a partial cut of the galaxy.log.  If you need more log info I\u2019ll work to get a more complete log file to you.  I deleted TONS of dependency statements at the end like these\u2026:\nDependency biom-format not found.\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,705 [p:111907,w:1,m:0] [uWSGIWorker1Core1]\n\n\u2014 this is part of my galaxy.log file showing the conda errors from clicking on the\n\u2014    \u201cInstall checked dependencies using Conda\u201d link in the \u201cManage Dependencies\u201d section.\n\u2014 DEBUG is turned on:  but I do not think the long list of DEBUG statements at the end\n\u2014    is related since other tools seem to install fine.\n\n...  most stuff deleted  ...\n\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,596 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name mulled-v1-a1698655ddc683fd2767d127c7c583056e87322876f94e167ba3900da02c1fc4 bwa=0.7.17 samtools=1.6\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,616 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name mulled-v1-a1698655ddc683fd2767d127c7c583056e87322876f94e167ba3900da02c1fc4 bwa=0.7.17 samtools=1.6\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,616 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of [CondaTarget[bwa,version=0.7.17], CondaTarget[samtools,version=1.6]]\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,616 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __bwa@0.7.17 bwa=0.7.17\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,634 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __bwa@0.7.17 bwa=0.7.17\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,635 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of bwa, version '0.7.17'\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,635 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __samtools@1.6 samtools=1.6\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,650 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __samtools@1.6 samtools=1.6\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,651 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of samtools, version '1.6'\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,651 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name mulled-v1-9057fa4fae7a852d56c94c9b9ec7d41fbc58918a1c8f7f74859454940ac7d851 bwa samtools\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,668 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name mulled-v1-9057fa4fae7a852d56c94c9b9ec7d41fbc58918a1c8f7f74859454940ac7d851 bwa samtools\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,668 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of [CondaTarget[bwa,unversioned], CondaTarget[samtools,unversioned]]\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,668 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __bwa@_uv_ bwa\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,683 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __bwa@_uv_ bwa\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,683 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of bwa, version 'None'\ngalaxy.tools.deps.conda_util DEBUG 2020-01-23 10:47:55,683 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Executing command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __samtools@_uv_ samtools\ngalaxy.tools.deps.conda_util ERROR 2020-01-23 10:47:55,700 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Failed to execute command: /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda create -y --quiet --override-channels --channel iuc --channel conda-forge --channel bioconda --channel defaults --name __samtools@_uv_ samtools\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/deps/conda_util.py\", line 235, in exec_command\n    return self.shell_exec(cmd, env=env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 58, in shell\n    p = shell_process(cmds, env, **kwds)\n  File \"lib/galaxy/tools/deps/commands.py\", line 88, in shell_process\n    p = subprocess.Popen(cmds, **popen_kwds)\n  File \"/usr/lib/python2.7/subprocess.py\", line 394, in __init__\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1047, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\ngalaxy.tools.deps.resolvers.conda DEBUG 2020-01-23 10:47:55,700 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Removing failed conda install of samtools, version 'None'\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,704 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Dependency biom-format not found.\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,705 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Dependency biom-format not found.\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,705 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Dependency biom-format not found.\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,705 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Dependency biom-format not found.\ngalaxy.tools.deps DEBUG 2020-01-23 10:47:55,705 [p:111907,w:1,m:0] [uWSGIWorker1Core1] Dependency\n"}, {"role": "system", "text": "This error suggests that Conda is not installed. The run.sh script  should have done this for you before Galaxy started and placed it in /apps/galaxy/galaxy_app/database/dependencies/_conda. You may notice an error of this at the beginning of your startup log, or you can try the process manually  with:\n$ cd /apps/galaxy/galaxy_app\n$ . galaxy_app/.venv/bin/activate\n$ python ./scripts/manage_tool_dependencies.py init_if_needed\n"}, {"role": "user", "text": "@Nate:  thanks.  I will try this out in the morning, however I did verify conda is installed at the location you mention.\ngalaxy@PB-DEV-01:~$ ls -altr /apps/galaxy/galaxy_app/database/dependencies/_conda\ntotal 32\n-rw-r--r--  1 galaxy galaxy 4134 Apr 17  2019 LICENSE.txt\ndrwxr-xr-x  2 galaxy galaxy   42 Aug 23 16:07 compiler_compat\ndrwxr-xr-x  3 galaxy galaxy   29 Aug 23 16:07 x86_64-conda_cos6-linux-gnu\ndrwxr-xr-x 10 galaxy galaxy  152 Aug 23 16:07 share\ndrwxr-xr-x  9 galaxy galaxy 4096 Aug 23 16:07 include\ndrwxr-xr-x  3 galaxy galaxy   30 Aug 23 16:07 shell\ndrwxr-xr-x  4 galaxy galaxy   47 Aug 23 16:07 etc\ndrwxr-xr-x  2 galaxy galaxy   10 Aug 23 16:07 envs\ndrwxr-xr-x  3 galaxy galaxy  178 Aug 23 16:07 ssl\ndrwxr-xr-x 39 galaxy galaxy 4096 Aug 23 16:07 pkgs\ndrwxr-xr-x 15 galaxy galaxy 4096 Aug 23 16:07 lib\ndrwxr-xr-x  2 galaxy galaxy 4096 Aug 23 16:07 conda-meta\ndrwxr-xr-x  2 galaxy galaxy   27 Aug 23 16:07 condabin\ndrwxr-xr-x  2 galaxy galaxy 4096 Aug 23 16:07 bin\ndrwxr-xr-x 15 galaxy galaxy  286 Aug 23 16:07 .\ndrwxr-xr-x  7 galaxy galaxy   98 Jan 23 14:20 ..\n\n\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app$ find . -type f -name conda.sh -print\n./database/dependencies/_conda/etc/profile.d/conda.sh\n./database/dependencies/_conda/lib/python3.7/site-packages/conda/shell/etc/profile.d/conda.sh\n./database/dependencies/_conda/pkgs/conda-4.6.14-py37_0/etc/profile.d/conda.sh\n./database/dependencies/_conda/pkgs/conda-4.6.14-py37_0/lib/python3.7/site-packages/conda/shell/etc/profile.d/conda.sh"}, {"role": "system", "text": "Ok, please also verify that /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda is present, and check the output of /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda info."}], [{"role": "user", "text": "Hi there,\nI\u2019m trying to cut barcodes out of my reads and append them to the read name. Cutadapt clearly shows how to do this using the -u --rename function and then indicating barcode = {cut_prefix} (suffix, in my case). The cutadapt documentation is here Cutadapt Documentation.\nUnfortunately, this functionality seems to be lost in the Galaxy wrapper. You are allowed to add a prefix or suffix to the name, but it only seems to put 1 text value in as the prefix. Does anyone know of a workaround or maybe I\u2019m doing something wrong?\nBarcode splitter seems to separate every barcode into different files. I have a random 9bp sequence set, so there would be 10\u2019s thousands of files with 1 sequence in them each.\nThanks,\nRick"}, {"role": "system", "text": "Welcome!\nWhich galaxy server are you using?\nMaybe you can concatenate the data from the info file to reconstruct the reads and play with the adapter names:\n\nThe info file contains information about the found adapters. The output is a tab-separated text file. Each line corresponds to one read of the input file.\nColumns contain the following data:\n1st: Read name\n2nd: Number of errors\n3rd: 0-based start coordinate of the adapter match\n4th: 0-based end coordinate of the adapter match\n5th: Sequence of the read to the left of the adapter match (can be empty)\n6th: Sequence of the read that was matched to the adapter\n7th: Sequence of the read to the right of the adapter match (can be empty)\n8th: Name of the found adapter\n9th: Quality values corresponding to sequence left of the adapter match (can be empty)\n10th: Quality values corresponding to sequence matched to the adapter (can be empty)\n11th: Quality values corresponding to sequence to the right of the adapter (can be empty)\nThe concatenation of columns 5-7 yields the full read sequence. Column 8 identifies the found adapter. Adapters without a name are numbered starting from 1. Fields 9-11 are empty if quality values are not available. Concatenating them yields the full sequence of quality values.\nIf no adapter was found, the format is as follows:\nRead name\nThe value -1\nThe read sequence\nQuality values\n"}, {"role": "user", "text": "\n\n\n David:\n\nWelcome!\nWhich galaxy server are you using?\nMaybe you can concatenate the data from the info file to reconstruct the reads and play with the adapter names:\n\n\nI am using EU and US servers (mostly EU during the day time). They have versions 1.16.5 and 1.16.6 of the tool, respectively \u2026 but neither have --rename functionality.\nWhere do I find the info file? Can I feed it back into Galaxy? Or do I have to process it with another platform?\nSorry, I\u2019m a chemist, so this is all very new to me!\nThanks,\nRick"}, {"role": "system", "text": "Rick, you can find a button to enable the info file as output of cutadapt in both Galaxy servers you\u2019re using.\nCheers"}, {"role": "user", "text": "Perfect! There is also a wildcard output file! I put in my 3\u2019 adapter as ADAPTERNNNNNNNNN. The wildcard output is the Barcode sequences found for N\u2019s along with the readnames. Now, I just need to figure out how to merge this with the trimmed sequences.\nThank you!"}, {"role": "system", "text": "You\u2019re welcome. There are tools in Galaxy to work with, like, merge columns, cut columns, awk, etc."}, {"role": "system", "text": "Hi @Rickpatbrown,\nI\u2019m currently updating cutadapt in order to include the --rename option. It will be available in a few days.\nRegards"}, {"role": "system", "text": "Hi @Rickpatbrown, we have updated cutadapt recently. The last version will be available in usegalaxy.eu in a few days.\nRegards"}], [{"role": "user", "text": "Hello, I am trying to provide a list of input coordinates so that I can use this tool to stitch together existing MAF blocks, but it is surprisingly complicated. I don\u2019t understand why galaxy is insisting that the \u201cchoosing intervals\u201d input is a file (shouldn\u2019t it just be intervals, such as (1,6)?\nI\u2019m also not sure what to put for MAF type. Any help would be immensely appreciated, thank you!\n\nimage1658\u00d71144 83.5 KB\n"}, {"role": "system", "text": "Welcome, @danielrebib\nThis publication I helped to author from 2012 covers the usage of this tool suite in great detail. It is open source. You\u2019ll notice that the UI has been updated but the underlying logic and data formats are still the same, and the original example data is still available in a shared history. Please give that a review, as I think it will address all of your questions, but you can ask if anything is not clear.\n\n  \n      \n\n      PubMed Central (PMC)\n  \n\n  \n    \n\nUsing Galaxy to Perform Large-Scale Interactive Data Analyses\n\n  Innovations in biomedical research technologies continue to provide experimental biologists with novel and increasingly large genomic and high-throughput data resources to be analyzed. As creating and obtaining data has become easier, the key...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi everyone!\nI started a unicycler run two days ago (07/08/2023) on a set of 87 paired reads. Everything went fine (80 ok, 4 with errors) until yesterday morning when I noticed two jobs were running and 1 was waiting for quite a long time. Past issues solved themselves with time and the problem was me being impatient so I decided to wait, today these two jobs are still running and the other one is still waiting, so I suspect something\u2019s off. Should I keep on waiting?\nThanks a lot!"}, {"role": "system", "text": "\n\n\n Alan:\n\ntoday these two jobs are still running and the other one is still waiting, so I suspect something\u2019s off. Should I keep on waiting?\n\n\nIf the jobs are yellow in color (executing) it is usually best to allow those to complete to get the full job logs at the end.\nFor gray datasets, when running larger batches of work at the public servers, it isn\u2019t unusual for some of those jobs to queue. Some of your jobs run, then some of other people\u2019s jobs, then more of yours, repeat until completed. Try not to rerun since rerunning always puts jobs back at the end of the queue.\nAnd \u2026 it has been several hours now since you wrote in\u2026 is the progress better? If not, would you please confirm that you working at UseGalaxy.org? Or, describe if somewhere else."}, {"role": "user", "text": "Hi @jennaj,\nAs usual, thanks for the quick reply. I\u2019m aware that by using a public server I\u2019ll have to share the resources with everyone, I\u2019m perfectly fine with it  What I found curious is that the Unicycler call started quite fast and the 80 assemblies were done in the expected amount of time but then the two jobs I mention started executing (yellow) and they remained this way since.\nThe progress is the same as yesterday and yes, I\u2019m working at Usegalaxy.org\nThanks for your help!"}, {"role": "system", "text": "That does seem very odd. The jobs should time out after about 48 hours in the executing mode (yellow) at UseGalaxy.org. That doesn\u2019t include the queue time (gray). So something weird is going on.\nDo you want to share back a link to the history? I\u2019ll take a look, and you can unshare after. Sharing your History"}, {"role": "user", "text": "Hi @jennaj\nYes, sure, here\u2019s the link url: https://usegalaxy.org/u/alan.elena/h/acras-c3g4 "}, {"role": "system", "text": "Hi @Alan\nThe history shows the two executing (yellow) jobs were started today. So, I\u2019m guessing that you decided to rerun. Let those jobs run, or maybe better, start over with at least addressing the first tip below.\nTwo tips, both important.\n\nWhen creating a collection, try to not include characters like a Dot in the collection element identifiers. Elements are the files inside collection folders.\n\n\nWhy this is important is complicated and can\u2019t be worked around and may not impact every tool but avoid it anyway. Especially if things go wrong.\nUse the function to strip off file extensions when creating the collection or use Relabel collection identifiers if the data is already loaded. There are a few more ways to do this but it looks like you already understand how collections generally work! For others reading, please see Search GTN Materials\nDashes and Underscores are Ok and your base file names otherwise looks fine.\nThe assigned datatype is what matters i.e. telling the tool the data format. Only one tool wrapped in Galaxy that I know of actually interprets the dataset_name.extension directly (Prokka, due to the way the original was written). If you plan to use that tool later, search this forum for the how-to.\n\n\nConsider running some QA. At the start, or after weird behavior. Assembly is sensitive to read content, and most of those tools are also super picky in other ways (require intact pairs, etc).\n\n\nFastQC and Fastq info are good choices and check different things (read quality and intact pairs, respectively).\nThat QA catches most data content/format problems at the start, or at least gives some context for assemblies that may fail or need parameter tuneups, or maybe more trimming.\nRunning these before and after trimming informs about what that trimming step did (if anything!).\nYour current jobs are not failing for quality reasons but is something to know about.\n\nHope this helps!"}, {"role": "user", "text": "Hey @jennaj !\nThanks for the reply and for checking my history. I don\u2019t know if they were started on Friday or not but I didn\u2019t do anything since the original Unicycler call (By the way, things are still unchanged). I usually rename file names to avoid spaces or dots but I didn\u2019t know the extension was also read as a part of the name, I\u2019ll remove the file extensions for the next run.\nI normally do QC before and after trimming as you suggest so I also think it doesn\u2019t have to do with quality.\nI didn\u2019t try it yet but would it be possible to download the assembled reads even if the collection hasn\u2019t been processed as a whole? If so, I might do this and retry the assembly with the affected reads.\nThanks again!"}, {"role": "system", "text": "\n\n\n Alan:\n\nBy the way, things are still unchanged\n\n\nThe jobs look failed now, so you could try rerunning.\n\n\n\n Alan:\n\nI didn\u2019t try it yet but would it be possible to download the assembled reads even if the collection hasn\u2019t been processed as a whole? If so, I might do this and retry the assembly with the affected reads.\n\n\nYes, you can filter a collection to exclude empty or failed datasets: see tools in the group Collection Operations.\n\n\n\n Alan:\n\nIf so, I might do this and retry the assembly with the affected reads.\n\n\nDo you mean create a new collection with just those datasets that didn\u2019t process with the batch? If yes, you can go into the hidden datasets tab in the history, unhide the inputs, create a new collection out of them, then run those. Later on if that works, you could combined the two collections together again (original results + rerun results).\nThat said, the usual way is to rerun those elements again while in the collection making sure to check the box to replace back into the original output collection.\nAnd, you could get fancy with filtering by element identifiers but with so few datasets and no workflow considerations, just doing it directly might be faster.\nSo that\u2019s a few ways to go forward. Hope one works out for you!"}], [{"role": "user", "text": "ampvis2 is a great tool, and I see it is available in the toolshed. Will it make it to Galaxy EU?"}, {"role": "system", "text": "Hi @vebaev,\nthe tools will be installed over the weekend.\nCheers,\nBjoern"}, {"role": "user", "text": "That\u2019s great!"}, {"role": "user", "text": "Hi @bjoern.gruening do we have the tools in eu, cant locate them?"}, {"role": "system", "text": "Its called ampvis2 you can search for it. This is one tool out of many: https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/ampvis2_venn/ampvis2_venn/2.7.22+galaxy0"}], [{"role": "user", "text": "Hi,\nI am trying to use Generate gene to transcript map for Trinity assembly (Galaxy Version 2.8.4) tool. I end up with this warning.\n\nCan anyone say what is the problem and how to rectify.\nThank you"}, {"role": "system", "text": "\n\n\n YKV:\n\nGenerate gene to transcript map\n\n\nThe tool is specifically having trouble parsing the \u201c>\u201d fasta assembly title lines.\nThis is an example of the type of \u201c>\u201d title lines this particular tool is designed to parse: tools-iuc/Trinity.fasta at master \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\nIf you ran Trinity successfully in Galaxy the format will be Ok by default. If you ran Trinity someplace else and uploaded the result, compare your title lines (transcript IDs) with the example.\nAssemblies generated by other methods would not be appropriate inputs for this tool. That said, it could still be possible to parse out transcript identifiers and genes using other tools (general Text Manipulation tools) \u2013 it depends on the content of your data.\nNote: This tool wasn\u2019t always available in Galaxy. This is prior Q&A from about a year ago when this specific tool\u2019s functionality was recreated with a simple workflow. It could be used now as an example for custom parsing (IF your title lines have transcript nomenclature that encodes some type of transcript-to-gene relationship/grouping):\n\n  \n    \n    \n    Is it possible to download 'Trinity.fasta.gene_trans_map' from Galaxy Trinity? usegalaxy.org support\n  \n  \n    Dear all, \nAfter I executed Trinity, I obtained 2 files as assembly transcripts.fasta and log.txt files. \nI would like to know that is it possible to download \u2018Trinity.fasta.gene_trans_map\u2019 from Galaxy Trinity. It is important for my downstream analysis. \nThank you so much, \nKamoltip\n  \n\n\nIf you still need help after reviewing:\nPost back 10-20 of your assembly \u201c>\u201d title lines for some help in figuring out how to parse the data or some help in determining if this type of parsing is even possible with your given data\u2019s content. Please don\u2019t include the sequences, just the \u201c>\u201d title lines \u2013 and enough lines that the data is representative of the whole.\nThe Select tool can be used to isolate title lines from large fasta datasets. Use the option \u201cMatching\u201d with the regular expression: ^>\nThe regular expression means:\n\n\n^ the start of a line\n\n> a \u201cgreater than\u201d symbol (how fasta title lines are designed)\n\nFasta format FAQ: Datatypes - Galaxy Community Hub\nSmall disclaimer: How Trinity formats title lines changed a bit between releases, so that may be where your problem is. But the help here should help even in that case \u2013 the workflow could definitely be tuned for any Trinity-based format."}], [{"role": "user", "text": "Can I request Genome scaffolder on galaxy?"}, {"role": "system", "text": "Hello @Rahul_Shelke\nNew wrapped tool requests can be made to the IUC by creating a ticket here: https://github.com/galaxyproject/tools-iuc/\nOr, you can vet the request first at their Gitter channel here. Someone may already be working on wrapping this tool, or a related tool: https://gitter.im/galaxy-iuc/iuc\nExisting wrapped tools can be searched for at the ToolShed https://usegalaxy.org. You can check to see which servers have any of these tools installed already by reviewing/searching the tool panel of that server (keyword: \u201cassembly\u201d will almost always work best).\nThanks!"}], [{"role": "user", "text": "Hi there,\nI was using galaxy earlier (was able to log on and install tools) but when I was uploading data from my computer to the local galaxy (link copy) I got an error message and it logged me out of galaxy. When I tried to log back into galaxy, I am unable to, the web log page does not respond and then times out. I have tried clearing my history and deleted stored passwords to galaxy but I am still unable to log in.\nThe last line in my terminal is:\ngalaxy.webapps.galaxy.controllers.user DEBUG 2020-04-21 15:04:28,746 [p:8320,w:1,m:0] [uWSGIWorker1Core3] trans.app.config.auth_config_file: /Volumes/My_Book/20.01/galaxy/config/auth_conf.xml\nI have no idea how to resolve this. Any help would be hugely appreciated.\nadditional when run sh run.sh, I am not given a http link to galaxy but was able to access the web browser page because I had the address copy and available to paste from earlier.\nadditional details:\nUsing Mac OS Catalina, galaxy 20.01(new clone re-installed yesterday)"}, {"role": "system", "text": "Hi @Lauren\nOdd. As a first pass solution to try, check to see if Galaxy is running in multiple processes, and shut all down. Then start things up again and see if that resolves the problems. Let us know how that works out and we can troubleshoot more as needed.\nFAQ: https://galaxyproject.org/admin/get-galaxy/#shutting-down-galaxy\n\nAlso, if you are loading local data up to your local Galaxy (on the same computer), is there a specific reason why you are using URL Upload? The URL would need to be in a publically accessible location for that to work, and there are alternative ways to load data. Or, maybe I am misinterpreting what you mean by \u201c(link copy)\u201d?\nLocal file browsing is one method to Upload data but is slow. Loading the data directly into a Data Library is much faster, if you are an admin, and avoids routing data through a web browser unnecessarily.\nFAQ: https://galaxyproject.org/data-libraries/\nWhy using Data Libraries can be a good choice:\n\nData does not consume more storage space when it is just an exact dataset copied from a Data Library into a History (for everyone who has access/permission to that Library).\nThe same is true for datasets copied from one History to another History (but only when both Histories are associated with the same account).\nIf you are the only user of your own Galaxy server and only use one account, the above won\u2019t matter, but it seemed worth mentioning.\nAnd \u2026 sometimes it is convenient to use a script to move data into Galaxy. This is commonly from a location where data is routinely stored, or set up as a dedicated location to write production outputs (example: sequencing runs). That script could be run at regular intervals, to off-load running routine admin tasks e.g. set it up to run/load data whenever \u201cnew data\u201d is available.\nData Libraries, created by any method, can have permissions set up to control access (if or when needed\u2026 and again, is a feature more relevant for multi-user Galaxy servers).\n"}, {"role": "user", "text": "thanks for getting back to me @jennaj\nI have tried those suggestions fg, the response was no jobs running and the command jobs brought up no response in the terminal.\nSorry I wasn\u2019t clear, I do add data through the data library system, and I tick the option of link copy the files.\nWhat I have also noticed is, the original galaxy.yml file seems to go funny after making a copy of it. So when I first download galaxy, I make a duplication of the galaxy.yml.sample file, I edit the copy with text editor but it does not let me change the name, so I then save a copy to make desktop, make me an admin rename to create galaxy.yml and drag that to the config folder. but when trying to open the original galaxy.yml.sample file (which I have not opened at all before only duplicated and opened the copy) it says \"operation can\u2019t be completed because the original item \u201cgalaxy.yml.sample\u201d can not be found.\nCould this have something to do with it? Although I\u2019ve had galaxy up and running after making me as an admin, but it won\u2019t let me log into galaxy now."}, {"role": "user", "text": "Update: I have now been able to log into galaxy, this was resolved by clearing my web browser history again. I\u2019m not sure why this had an effect.\nBut I am now getting errors through the galaxy web interface and the terminal when trying to run a tool. When I tried to go into admin to go to the error log, It had logged me out so was unable to access the admin tab and then went I try to log back in, I get the same problem again of not being able to log in. The web browser is unresponsive and the last line in the terminal is the same as before:\ngalaxy.webapps.galaxy.controllers.user DEBUG 2020-04-22 10:35:36,368 [p:877,w:1,m:0] [uWSGIWorker1Core1] trans.app.config.auth_config_file: /Volumes/My_Book/20.01/galaxy/config/auth_conf.xml"}, {"role": "system", "text": "@Lauren\nYou might want to start over with a fresh checkout.\n\nStart Galaxy up the first time, without making any changes. Then shut it down and make administrative adjustments as needed.\nNote that the galaxy.yml.sample file does not really exist under the /your-galaxy-path/config directory directly anymore, that is now just a symbolic link to where the file actually resides: /your-galaxy-path/lib/galaxy/config/sample/galaxy.yml.sample\n\n\nCompare and you\u2019ll see the symlink, either at Github or in your own galaxy directory.\nSymlink for galaxy.yml.sample is in here: https://github.com/galaxyproject/galaxy/tree/release_20.01/config\nversus\nWhere that galaxy.yml.sample symlink points to: https://github.com/galaxyproject/galaxy/blob/release_20.01/lib/galaxy/config/sample/galaxy.yml.sample\nGive that a try and see if it resolves the problems. We can troubleshoot more from there."}, {"role": "system", "text": "Are you using python 3.7?  This is a bug I have a fix for in https://github.com/galaxyproject/galaxy/pull/9642.  You can pull that manually if you\u2019d like, or, in the interim downgrade to python 3.6, and you should be good to go.  Technically, we only support 3.5 and 3.6, but this code will fix login issues with 3.7."}], [{"role": "user", "text": "Hi!\nOur lab uses Trinity for most of our assembly work, but lately, I\u2019ve been unable to get the tool to run correctly.\nWhat I am attempting to assemble is a set of paired-end Illumina reads, with a file size of about 360MB for each strand. This created a near-instantaneous error report when the job transitioned from \u2018pending\u2019 to \u2018running\u2019. The job API ID is bbd44e69cb8906b567f21c7489016a08, and the error code is \u201c[Errno 122] Disk quota exceeded\u201d. My understanding of that error code is that it is a memory issue, but we\u2019ve assembled larger datasets than that in the last without issue. Any clarification you could give would be appreciated."}, {"role": "system", "text": "I had the same issue today. Must be a problem on the server. I sent an error report."}, {"role": "system", "text": "Hi @BirdNerd\nThis error was due to a cluster problem at UseGalaxy.org that should be resolved right now [Errno 122] Disk quota exceeded. If you are still getting that error from jobs that were started this week it would be unexpected. Please send in a bug report that includes a link to this topic thread in the comments.\nThat said, reviewing your account at this server, the most recent Trinity runs have a different error. The complaint from the tool stdout is that the fastq inputs were not formatted corrrectly. Common reasons include: data not fully uploaded, gz compression didn\u2019t load correctly, or the data is truncated before uploading to Galaxy. Check the data locally to make sure it is intact, then upload uncompressed fastq and run some QA/QC checks. As you probably know, Trinity expects that both ends of a pair are input. I added a few tags to your post that link to prior Q&A that helps with troubleshooting that kind of problem.\nThanks!"}, {"role": "system", "text": "Hi @R_R_R \u2013 Thanks for sending in the bug report. That issue should be fixed but \u2026 some data migrations are still going on. We\u2019ll review and post back an update."}, {"role": "user", "text": "Okay, thanks for helping with that. I\u2019m less concerned about the second dataset (although I had noticed that it had a different error code), as I just wanted to see if I could replicate the error independent of the Pluvialis data. Thanks for the information, and I\u2019ll try and run it again."}, {"role": "system", "text": "Thanks Jen.\nI am trying again now with a small dataset (one I know should work) to see if it happens again. If it does, I\u2019ll make sure to send another bug report."}, {"role": "system", "text": "@R_R_R\nFor your run sent in, the job did hit a cluster problem that should now be resolved (was present for a short time today).\n@BirdNerd\nAlso please try a rerun (with intact fastq).\nAbout 20 GB per end (compressed) is the largest I\u2019ve seen assemble at a public Galaxy. Larger work can be moved to a custom Galaxy.\nThanks to you both for reporting the problem!"}], [{"role": "user", "text": "Hello,\nI\u2019m going through the following tutorial on usegalaxy.org. I got stuck at the point where I should click D. melanogaster link to see mapped reads on IGV after Mapping with TopHat2.\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Reference-based RNAseq data analysis (long)\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWhen I try it on Chrome, it suddenly closes the window that opens in milliseconds.\nWhen I try it on Firefox, it asks me to \u201csave/open with\u201d a file (igv.jnlp) 2.0KB.\nI cannot get the view of the reads that are mapped on the genome as shown in the tutorial. What am I supposed to do?\nThank you"}, {"role": "system", "text": "Welcome, @ysrbrs!\nHave you used IGV before in your current machine?\nCheck Chrome downloads; you may have the *jnlp file automatically downloaded in chrome, that\u2019s why it closes quickly; but Firefox first asks what to do with it;\nThe *.jnlp file is opened by IGV (JWS, Java Web Start); e.g. IGV 2.3 | Integrative Genomics Viewer :\n\n(1) Java Web Start (JWS). Use the buttons below to launch IGV directly from our web site.\n\n\nClicking on one of the buttons will download a .jnlp file and then execute the file using Java Web Start (JWS).\n\n\nSome web browsers will only download the .jnlp file and not automatically launch IGV. For these browsers, locate the file in your folder designated for browser downloads, and double-click on the file to launch IGV.\n\n\n\nYou\u2019ll find more info in the IGV User Guide | Integrative Genomics Viewer"}], [{"role": "user", "text": "Hi, I am getting this error when trying to run RNA-STAR, how can I resolve it? Thank you.\nFatal error: Matched on FATAL ERROR\nTranscriptome.cpp:48:Transcriptome: exiting because of INPUT FILE error: could not open input file /cvmfs/data.galaxyproject.org/managed/rnastar_index2/hg38/dataset_950901_files/exonGeTrInfo.tab\nSolution: check that the file exists and you have read permission for this file\nSOLUTION: utilize --sjdbGTFfile /path/to/annotantions.gtf option at the genome generation step or mapping step\nMar 06 00:51:59 \u2026 FATAL ERROR, exiting"}, {"role": "system", "text": "Hello,\nYou need to supply a reference annotation GFT dataset from the history at runtime.\nThe GTF should be based on the UCSC \u201chg38\u201d genome build. Some choices:\n\nFor  Gencode, copy the link to the GTF and paste it into the  Upload  tool. Hg38 data is here https://www.gencodegenes.org/. After it is loaded, remove the headers (lines that start with a \u201c#\u201d) with the  Select  tool using the options \u201cNOT Matching\u201d with the regular expression  ^# . Once the formatting is fixed, change the datatype to be  gft  under Edit Attributes (pencil icon). The data will be given the datatype  gff  by default, which works fine with some tools and but not with others. Avoid the  gff3  version of this particular data (contains duplicated IDs and several RNA-seq tools do not work with annotation in that format anyway).\nFor  iGenomes, the archive corresponding to the target genome/build needs to be locally downloaded, the tar archive unpacked, and then just the  genes.gtf  data uploaded to Galaxy (browse the local file, or use FTP). Find all available genome/builds here: https://support.illumina.com/sequencing/sequencing_software/igenome.html\n\n"}], [{"role": "user", "text": "Hello,\nI am developing a new galaxy tool using R as main programming language.\nWhen I upload a number of txt files with the Upload tool in Get Data, I noticed that it stores the files in galaxy/database/files/000  directory and changes the file names from the originalname.txt  into dataset_number .dat. How can I set up Upload tool in order to store the file with the originalname.txt format.\nThank you very much in advance for your help."}, {"role": "system", "text": "This isn\u2019t possible, Galaxy may store files in object stores that don\u2019t resemble classical file systems.\nThe correct way is to reference files in your tool using $file.element_identifier."}], [{"role": "user", "text": "Heyya,\nI am trying to adapt the HISAT2 options to my samples and I encountered this problem:\n\nCapture2.PNG941\u00d7181 5.71 KB\n\nI know what I want: I don\u2019t want HISAT2 to try and align individual mates.\nSo I should add the command --no mixed behaviour.\n\nCapture.PNG1325\u00d792 6.56 KB\n\nBut what this means in Galaxy? Yes or No?\nIt basically says: HISAT2 does this. By default is False (no-mixed). Do you want for No-mixed behaviour to be disabled? Y/N.\nIt sounds like I am disabling the button that disables? Is Galaxy using the default settings for HISAT2 (and other programs)? Or it comes with its own recommendations?\nI am not sure what Yes and No mean in this context. Am I reading too much into this?"}, {"role": "system", "text": "\n\n\n Sammy:\n\nI know what I want: I don\u2019t want HISAT2 to try and align individual mates.\n\n\nThen leave the option as the tool form default (set to \u201cNo\u201d).\nI agree this is a bit confusing \u2026 underlying tool defaults versus tool form defaults \u2026 but hope that helps!"}], [{"role": "user", "text": "I am trying to follow this tutorial here: https://galaxyproject.github.io/training-material/topics/visualisation/tutorials/jbrowse/tutorial.html\nI am using a local installation of Galaxy v.19.01 and JBrowse 1.16.5+galaxy5. I whitelisted JBrowse and imported the datasets via the links from the tutorial to my history.\nJBrows itself seems to run fine, but when I want to look at the results, as soon as I open a track I get the following error messages:\n\nAn error was encountered when displaying this track.\nError: HTTP 200 when fetching http://XXXgalaxy/datasets/f0e0bb0fb5(...)/display/data/raw/6aa7a84(...)_0.gff.gz bytes 0-262143\n\ngalaxy_errors1913\u00d7395 106 KB\nI\u2019d be happy for any tips why this happens and how to solve this.\nthanks,"}, {"role": "system", "text": "Hi,\ngot the exact same problem on a fresh installed galaxy server.\ndoes anybody know how to fix it ?\nregards,\nSeb"}, {"role": "system", "text": "Same problem here on my galaxy server. This was working for me in Feb and I just noticed the problem now. Galaxy 20.01 and I just upgraded to JBrowse 1.16.9+galaxy0 to see if it would fix the problem."}, {"role": "system", "text": "Hi @galaxyForumAcc, @Seb, @rbatorsky!\nI\u2019m sorry for the delayed response here, I didn\u2019t notice this issue previously.  I\u2019m one of the two devs and maintainers of that tool.\nJBrowse can generate this error when range requests do not work properly. JBrowse makes a request to your galaxy for the datasets, but it only requests the portion that you\u2019re viewing with a header like Range: bytes 0-135. Given that it is making a range request, JBrowse expects to see a 206 Partial Content http return code, but instead it sees 200 OK, as your webserver does not support range requests, and thus returns the whole file.\nIf you\u2019re using Nginx or Apache, please consider turning on the sendfile mechanism:\n\nnginx\napache\n\nUnfortunately this means JBrowse does not always work nicely with development servers, and works best under a proper webserver.\nI\u2019ve tested the honour-range option from uwsgi but it seems to have some issues so I have to recommend nginx or apache with sendfile.\nSorry for not seeing this sooner!"}], [{"role": "user", "text": "I have tried to assemble paired reads (Illumina) with nanopore reads but I always obtain the same error:\nAn error occurred with this dataset:\nformat fasta database [?]\ncannot find \u2018lr\u2019\nCould anyone help me with this?\nThanks in advance"}, {"role": "system", "text": "Hi @mlopezm,\nthe dataset W1550.R1.fastq.gz seems to be truncated. I suggest you check the original files in order to confirm that both the forward and reverse datasets have similar sizes (differences usually indicates truncated datasets).\nRegards"}, {"role": "user", "text": "Thanks @gallardoalba ,\nHowever, I have tried with another fasta files and I still obtain always the same error. I also tried with the fastaq (isteas of .fq.gz) and stiil the same error:\nAn error occurred with this dataset:\nformato gfa1 base de datos ?\ncannot find \u2018lr\u2019\nand:\nAn error occurred with this dataset:\nformato fasta base de datos ?\ncannot find \u2018lr\u2019\nI would appreciate any help with this,\nRegards!"}, {"role": "system", "text": "\n\n\n mlopezm:\n\nI also tried with the fastaq (isteas of .fq.gz) and stiil the same error:\n\n\nHi @mlopezm \u2013 someone else had a problem with Unicycler recently, and while the problem was slightly different (reads were in a collection), simplifying the data labels fixed the issues. Maybe give it a try?\nSpecifically, remove any dot characters. Underscores/dashes are fine.\nmyfile.fq.gz -> myfile\n\nmyfile.fastq -> myfile\n\nmyfile_R1.fq.gz -> myfile_R1\nmyfile_R2.fq.gz -> myfile_R2\n\nDisclaimer  May not work but worth a try."}, {"role": "user", "text": "Thanks @jennaj ,\nI have tried simplifying the names, but still the same problem\u2026 I tried with the dataset of the tutorial and it works."}, {"role": "system", "text": "Hi @mlopezm,\nas I mentioned to you by email, please try to reproduce the error in usegalaxy.eu and share the history with me.\nRegards"}, {"role": "user", "text": "Dear Cristobal,\nFirst of all, I am very grateful for your help. After been trying different options (I even tried to assemble the reads that were included in the tutorial and it works), I thought that the problem was in my fastaq.gz files. Another person suggested simplifying the name, but this did not work either. Finally, after reading this tutorial: https://www.melbournebioinformatics.org.au/tutorials/tutorials/hybrid_assembly/nanopore_assembly/, it was indicated that:\nSelect long reads - nanopore_reads.fastq\n(if nanopore_reads.fastq does not appear in the dropdown, its datatype needs to be changed - click then pencil icon next to nanopore_reads.fastq in the history panel \u2192 \u2018Datatypes\u2019 tab \u2192 \u2018New Type\u2019 - fastqsanger)\nI tried this, and, at least, it is processing (other times it failed in the first minutes), so I think this could be the answer.\nNevertheless, I\u00b4ll keep you posted about the result of the assembly.\nThanks again,\nRegards\nMar\u00eda\n"}, {"role": "system", "text": "Updates, please see run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210"}], [{"role": "user", "text": "Hi all, I have created paired dataset for my samples. Trying to download, but it keeps failing. Is it because of the size of the dataset? How can I work around this?"}, {"role": "system", "text": "Hello @chi1989\nIf the datasets are inside a collection, downloading that collection directly is currently problematic at UseGalaxy.org.\nA workaround using the \u201chistory archive\u201d method is in this Galaxy Help topic: Problems in downloading collections using both browser and curl - #4 by jennaj\nBut if it really is just a single paired collection (two datasets), you could also go into the hidden datasets tab for the history, and download the two datasets from there.\nHope one of those work out for you!"}, {"role": "user", "text": "Hi, I uploaded the reference files from one of the tutorials : Analyses of metagenomics data - The global picture (the files that are mentioned in classify.seq). When reading these files failed, I used files directly downloaded from mothur (v18) and I have been getting the same error."}, {"role": "system", "text": "Hi @chi1989\nMaybe I misunderstood what you are doing? Please explain a bit more.\n\nAre you attempting to download datasets from your Galaxy history?\nOr, are you attempting to upload datasets into your Galaxy history?\n"}, {"role": "user", "text": "I have uploaded the dataset to my history, no issues there. But the error message pops up while using the files to run command.\nThe files I downloaded are:\nhttps://zenodo.org/record/815875/files/trainset16_022016.pds.fasta\nhttps://zenodo.org/record/815875/files/trainset16_022016.pds.tax\n\nI have both these files in my history, I am trying to run classify.seq using these files, and I get error"}, {"role": "system", "text": "Thanks, that helps.\nThe stderr/stdout from these tools will usually state the exact problem. Backing up and reviewing upstream job is usually the way to correct problems. The data are complicated and easy to mix up.\nAnother way to troubleshoot (any tutorial) is to run the workflow associated with that tutorial. That output could be used to find prior mixups when running through the process manually.\nHow to troubleshoot error messages: Troubleshooting errors\n\n\n\n chi1989:\n\nI am trying to run classify.seq using these files, and I get error\n\n\nWhat is that error message?"}, {"role": "system", "text": "Update\nTo learn which Galaxy servers are appropriate per tutorial, see the pull-down menu in the top information box labeled \u201cAvailable on these Galaxies\u201d.\nThe two tutorials involved are not support at UseGalaxy.org for now. If that changes, the menu will automatically update.\nPlease retry at a Galaxy server that supports the tools/data included in the tutorial."}], [{"role": "user", "text": "I am using Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy4) and it is giving fatal error since the 15th, after your maintenance.\nI am using usegalaxy.org\nUse of uninitialized value in subroutine entry at /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-dd38f3b35796eaa94d0b5d1d9c1428ec987dcd7f39e81a08df8df138203a8004/bin/\u2026/lib/Circos/Configuration.pm line 781.\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-dd38f3b35796eaa94d0b5d1d9c1428ec987dcd7f39e81a08df8df138203a8004/bin/\u2026/lib/Circos/Colors.pm line 348,  line 6."}, {"role": "system", "text": "Hi @jaiswalp, I\u2019m one of the tool authors.\nThe earlier versions of the wrapper had some significant bugs in the color functions, those should all be fixed in the newer version. Please try 0.69.8+galaxy7 if it is available on .org.\nIt should be available on UseGalaxy.eu."}], [{"role": "user", "text": "Hi,\nI want to get gene ids from the results of SAlmon. Which GTF/CSV should be used? Do I need to use tximport. Please help"}, {"role": "system", "text": "Hi @Pragathi_Sneha\nFor counts \u201cby gene\u201d, you\u2019ll need to incorporate a gene-transcript mapping file (tabular, two columns) or a reference annotation dataset (GTF/GFF). The transcript names should exactly match how your transcripts are labeled in the input transcript fasta.\nInput the annotation data under this option on the Salmon tool form: File containing a mapping of transcripts to genes\n\nsalmon-counts-by-gene1130\u00d7246 38.8 KB\n\nThere is no special way that you need to Upload the data in most cases. Use default settings and if you don\u2019t get the expected datatype, try a search at this forum with it as a keyword and you\u2019ll find much Q&A about formats, datatypes, tips to standardize content, reassign a type if Galaxy guess\u2019s wrong (unlikely with the formats expected for this particular input). If you cannot solve it, send back a picture of the full expanded fasta and annotation datasets (both) \u2013 with the datatype and the \u201cpeek view\u201d showing and we can troubleshoot more from there.\nHope that helps!"}, {"role": "user", "text": "Thank you @jennaj . I want to know if we have salmon output with transcript ids .I mean is there a way to map and convert them to gene IDs"}, {"role": "user", "text": "I have provided tabular file with transcript ID in first column and gene ID in the second column. I am getting the  NM_transcript IDs in salmon output."}, {"role": "system", "text": "Did you successfully get both outputs after incorporating the mapping file? The second represents counts \u201cby gene\u201d. Quote from the tool form:\n\nSalmon will output both quant.sf and quant.genes.sf files, where the latter contains aggregated gene-level abundance estimates.\n\nThis will require that the transcript identifiers in the fasta exactly match the transcript identifiers in the mapping file. Maybe check that \u2013 from your description, it seems like one of these things might be going on:\n\n\nFirst, confirm that you incorporated gene identifiers. If the transcript identifier is the same as the gene identifier on rows in the mapping file, then you are still counting \u201cby transcript\u201d. So, you need to add in the gene identifiers if you want to group/count \u201cby gene\u201d. Or, you can use a GTF file (faster). UCSC hosts those in their Downloads area. You\u2019ll want the RefGene annotation track. This FAQ lists out a few exact links, but the navigation path is the same for any genome they host that has an annotation track: Help for Differential Expression Analysis - Galaxy Community Hub\n\n\nDo the transcript identifiers exactly match between the fasta and mapping file? This means that any content on the \u201c>\u201d title line is only a single transcript identifier. If you need to remove description content from the title lines, try the tool NormalizeFasta using the option to remove anything after the first whitespace. FAQ: Datatypes - Galaxy Community Hub\n\n\nNotes: Converting transcript identifiers to gene identifiers is not a one-to-one relationship. There are often multiple transcripts per gene. A count file with rows of duplicated gene identifiers won\u2019t work with differential expression tools that are based on \u201cgene\u201d. Have Salmon summarize counts across all transcripts associated with a particular gene. Use the same transcript fasta file and the same mapping file for all samples. In the end, you\u2019ll want output that has: one gene per row, no duplicated genes within any particular file, and the same exact genes in the same exact order in each of the count files input to Deseq2.\nIf you need more help, please do post some screenshots of your data, and we might be able to spot the problem."}, {"role": "user", "text": "\ngalaxyhelp-fig1035\u00d7565 200 KB\n\nI am using UCSC Known gene fasta file. Is it right?"}, {"role": "system", "text": "Hi @Pragathi_Sneha\nI see a few problems with these fasta title lines (the \u201c>\u201d lines). These need to be just transcript identifiers in the same exact format as included in your transcript-to-gene mapping file. Item 1 below definitely needs to be addressed. Items 2 and/or 3 might need to be changed.\nThis formatting problem will definitely cause a mismatch and needs to be fixed before that fasta file is used with any tool. It is the minimal change required and the tool NormalizeFasta can be used (explained in the prior post).\n\nextra content after the first whitespace \u2013 \u201crange\u201d and everything after should be removed as it is not part of the transcript identifier portion of the fasta title lines\n\nThese two might cause problems, it depends on how those same identifiers are formatted in your mapping file.\n\n\nextra content before the first whitespace, where the \u201cidentifier\u201d is located \u2014 the \u201chg38_knownGene_\u201d portion might need to be removed\n\n\nthe transcript identifiers include the version (the extra .N where N is a number). If your mapping file includes the version, then it is a match. If not, then it won\u2019t match.\n\n\nThe mapping file wasn\u2019t posted, but you can compare yourself to the fasta identifiers.\n\nThis ENST00000456328\n\nIs different than ENST00000456328.2 or hg38_knownGene_ENST00000456328.2\n\nAnd none of those will match up with the full title \u201c>\u201d line as shown in your screenshot.\n\nIdentifiers cannot include any whitespace (spaces, tabs). And as long as the identifier used matches in all your files, and doesn\u2019t include any whitespace, this tool will work.\nHope you solved the problem already, but if not yet, the above should help you to find and resolve the mismatches. \nFAQ for fasta: Datatypes - Galaxy Community Hub"}], [{"role": "user", "text": "Hi All,\nI\u2019ve been working and exploring Galaxy quite intensely this last year, and many histories are of no added value because they are mainly small attempts to find out the possibilities of the Galaxy platform . They do, however, obscure those histories that contain interesting workflows and that I like to have a look at every now and then. Is it possible to completely remove these obsolete histories (they are purged and do not take up any memory) from my account/the server?"}, {"role": "system", "text": "If they are already purged you should not see them anymore, isn\u2019t that the case?"}, {"role": "user", "text": "Dear Bjoern,\nThanks for your reaction. Indeed that\u2019s the case. However, if I want to have a look at histories that are of interest but which I also purged due to lack of memory space to run new a new history, I select include deleted histories in the search bar. Then all deleted and purged histories are shown of which quite a number are obsolete and obscure the purged histories that I\u2019d like to have another look at.\nKind regards,\nHenk\n"}, {"role": "system", "text": "I see. We can not remove these histories. We keep all of them for reproducibility reasons. Can you maybe tag your histories and search/sort for tags to get rid of your junk histories?"}], [{"role": "user", "text": "Hi,\nThe download links for bam output files (13G) from MarkDuplicate and AddOrReplaceReadGroups are downloading only few Mbs of html file rather bam. I tried using \u201cwget\u201d, \u201ccurl\u201d both and also tried to transfer these files to useglaxy.org server but it failed.\nPlease tell me how to download appropriate files or what is the problem with copy link option.\nThanks"}, {"role": "system", "text": "Hi,\nhow do you copy the download link for bam file? BAM datasets in Galaxy have links to both BAM alignment and index file. To get link to BAM alignment, click on Save icon for BAM dataset (left mouse click). You\u2019ll see two links, Download dataset and Download index. Copy link to the dataset using rigt mouse click > Copy URL.\nHope this helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi,\nI did the exactly same as you explained. The downloaded file is of 6317b only, however the file size on details section shows 13GB. I don\u2019t understand what\u2019s wrong here.\nNeed help!\nThanks,\nguptapa"}, {"role": "system", "text": "Hi guptapa,\nI reproduced the download using link issue with Galaxy Europe. I cannot copy a file using URL from Galaxy Europe to Galaxy Australia but have no issue with download of the file to a local computer.\nNo problem with copying data using URL from Galaxy Australia to Galaxy Europe.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi igor,\nI\u2019m sharing the screenshot to show from where I\u2019m coping the downloadable link and link as well. Could you please share the link which allowed you to download correct file?\nI used wget with --no-check-certificate to download the file.\nDownload dataset\n\nScreen Shot 2022-05-02 at 10.45.45 AM.png349\u00d7640 104 KB\n\nCan I transfer the data from Galaxy Europe to USA (usegalaxy.org) with correct download link?\nThanks,\nguptapa"}, {"role": "system", "text": "Hi Guptapa,\nI can download (small test) files from Galaxy Europe by clicking on Save icon (left mouse click). I have not tried a big file.\nDownload from Galaxy Europe using URL does not work for me. I used a link for a file on Galaxy Europe. I hope admins of Galaxy Europe can comment on it, but it seems the issue is withing Galaxy Europe: I can copy files from Galaxy Australia to Galaxy Europe using a link from Galaxy Australia.\nKind regards,\nIgor"}, {"role": "system", "text": "Hi @guptapa and @igor\nGood detective work. The how-to is a bit different at EU. Maybe someone already discovered how this works, but in case not:\nUseGalaxy.eu is set up to have all data private and not publically accessible by URL. If you are trying to move data between servers, the accounts are distinct, so the sharing state needs to be set at EU to be more permissive to transfer data by URL to anywhere else (other Galaxy servers, external cloud storage, etc).\nTry this:\n\nGo to the history that contains the dataset\nPull down the history  menu and click into Share or Publish.\nSet the first option to share the history by link, and make sure that the dataset you want to share is included. You don\u2019t need to publish.\nGo back to your history with the dataset\nThen copy/paste the dataset link other places (other Galaxy servers Upload tool).\nUnshare the history or leave it in a shared state.\n\nAdjusting privacy settings for your account:\nAccounts at any of the UseGalaxy.* servers have the options to adjust the default sharing status for what is most convenient for you. It would be very unlikely for anyone to guess what a random link is, let alone understand what that data represents, but any access at all has to be an explicit opt-in at the EU server for GDPR reasons.\nTo see how it works \u2013 try doing the reverse between the ORG server to the EU server without any changes. The data will transfer unless you set up the account preferences to be different from the default.\nAnother way to see how it works is to toggle the first of these two global settings at EU in User > Preferences, create a new history, add a new dataset, and what happens when transferring that dataset by URL. Data created before the toggle will not be impacted, data created after will be.\nThe second setting makes more permanent changes to all current data (everything has the state changed to private) but know that this cannot be undone with one change. Instead, you\u2019d need to directly reshare any histories (and the datasets contained) / workflows / etc that were previously shared if that matters to you.\nSet Dataset Permissions for New Histories\n\nGrant others default access to newly created histories. Changes made here will only affect histories created after these settings have been stored.\n\nand\nMake All Data Private\n\nClick here to make all data private.\n\nDownloading to a local desktop from the  works at EU with any sharing state since when you are logged into your own account, your account is assumed to be private and only directly accessible by you, and only while logged in. I\u2019m not sure if the API key is needed for wget  at the EU server. The EU team can advise, or someone can test that out and post back what works and doesn\u2019t at the different servers. We could make an FAQ to capture the information in one place, and attribute your account here, or better, your github account.\nThe March 24th webinar here has more details. We went over the account preferences options then ran through a collaborate-by-sharing demonstration: Galaxy-ELIXIR webinars series: Advanced Features | ELIXIR.\nHope that helps!"}], [{"role": "user", "text": "Hi,\nIm currently in the middle of my uni project and I\u2019m trying to follow an RNA-Seq HiSat pipeline to analyse reads obtained from NCBI Gene Expression Omnibus (GEO). I\u2019ve got the data, and tried to put it through HiSat. Didn\u2019t work and came back with error messages, realised that I probably should have put it through FastQC/Trimmomatic first.\nTried however, it didn\u2019t accept or recognise any fastq files, couldn\u2019t understand why til I looked at the data files. It seems that I had \u201cfastq.gzReadsPerGene.out.tab.gz\u201d instead of normal fastq files. Apparently from what i\u2019ve been told, I\u2019ve got from GEO are mixed messages. The first part \u201cfastq.gz\u201d would represent raw NGS reads, which should either be in fastqsanger format or can be converted to fastqsanger.\nIf FastQC doesn\u2019t run and won\u2019t accept the files then it is unlikely they are in a fastq format. The second part  \u201cReadsPerGene.out.tab.gz\u201d suggests these are aligned sequences with gene counts.\nSo i\u2019ve now been trying to determine whether these files are in fastq format or not. Tried to convert the files using FastQ Groomer, didn\u2019t accept the files. FastQC didn\u2019t accept them so I\u2019m assuming they\u2019re not in fastq format. Tried running through HiSat again and I just encounter the same errors as before. I\u2019ve looked almost everywhere to see if I can fix this problem and I\u2019ve emailed again to ask what to do as I\u2019m relatively new to using this tool. Does anyone know how I resolve this issue?\nThis was all done on my uni\u2019s galaxy server."}, {"role": "system", "text": "The filename suggest it is something else then a fastq file to me. How did you got the files? Would it be possible for you to open the files on your local computer? So download them first, unzip (gunzip) them and just open it with a text editor?"}], [{"role": "user", "text": "Hello,\nI was using QIIME 2 on the Galaxy server and I got an error message saying;\n[Errno 28] No space left on device. I tried deleting histories but it didn\u2019t work. How can I fix this? I\u2019ve used only 3.5GB of storage.\nThank you in Advance.\nBrigitta"}, {"role": "system", "text": "Hi @Brigitta,\nthis error does not refer to your user quota, but to space left on the hard disk of the machine your job has been running on.\nCan you please file a bug report (through that little bug icon on your failed dataset)? That will give us all information we need to debug this via an email to the Galaxy Europe team.\nThanks,\nWolfgang"}, {"role": "user", "text": "Hi @wm75 ,\n\n\n\n wm75:\n\nCan you please file a bug report (through that little bug icon on your failed dataset)? That will give us all information we need to debug this via an email to the Galaxy Europe team.\n\n\nThere was an error when I tried to report it. The error I got was as follows;\n\u201cAn error occurred sending the report by email: Error reporting has been disabled for this Galaxy instance\u201d\nStated below are the details of the activities that lead me to the \u201cErrno 28\u201d error\nI have been using the Galaxy server to analyze a data set using QIIME2. I got the above error (Errno 28 No space left on device) while I was trying to use the qiime2 feature-classifier classify-sklearn plugin to assign taxonomy to my data set using a database called silva-138-99-515-806-nb-classifier but it works fine when I use the greengenes database (gg-13-8-99-515-806).\nThank you In advance,\nBrigitta"}, {"role": "system", "text": "\n\n\n Brigitta:\n\n\u201cAn error occurred sending the report by email: Error reporting has been disabled for this Galaxy instance\u201d\n\n\nSo you are not working on usegalaxy.eu then?"}, {"role": "user", "text": "\n\n\n wm75:\n\nSo you are not working on usegalaxy.eu then?\n\n\nNo, I am working on cancer.usegalaxy.org"}, {"role": "system", "text": "Ok, then you need to contact the admins of that server. As this is an internal error in their compute environment, there is not much anyone else can help you with.\nI\u2019ve tried to inform people behind your server about your issue and, hopefully, they\u2019ll chime in here later today (it\u2019s around midnight in their timezone currently)."}, {"role": "user", "text": "\n\n\n wm75:\n\nI\u2019ve tried to inform people behind your server about your issue and, hopefully, they\u2019ll chime in here later today (it\u2019s around midnight in their timezone currently).\n\n\nThank you so much!"}, {"role": "system", "text": "\n\n\n Brigitta:\n\nNo, I am working on cancer.usegalaxy.org \n\n\n\n\n\n wm75:\n\nI\u2019ve tried to inform people behind your server about your issue and, hopefully, they\u2019ll chime in here late\n\n\nHi @Brigitta\nWe were able to contact the administrators for this server, and the issue should be resolved by now. Would you please try a rerun?\ncc @sargentl"}, {"role": "system", "text": "Heya, thanks for the report, and sorry it was cumbersome to file it.\nRe: your main issue: We\u2019ve adjusted resource allocations to ease the pressure on the file system and memory when running that particular tool. I was able to replicate the error, and confirm that the changes made resolved it, so you should be good to go! It\u2019s possible, however, that other, more complex/large datasets might exceed this new allocation, so I\u2019ll be keeping an eye on future executions more closely to see if further adjustments are needed.\nRe: your issue issuing issues: we are working on making it easier to report errors directly to us instead of driving you to go through intermediaries (thanks @wm75 and @jennaj !); those changes should be live soon.\nThanks again for the report, very valuable and actionable info!"}], [{"role": "user", "text": "SPADes does not seem to be running at all. No error, just highlighted grey to show that the job is queued.\nPlease advice on what to do, or enlighten us on whether this is normal.\nThanks."}, {"role": "system", "text": "Not sure but maybe this can help? Some info about galaxy is given here.\n  \n    \n    \n    RNA STAR not running! \n  \n  \n    Hello Team, \nI would like to bring to your notice that the command RNA STAR is not running and the status says \u201cThis job is waiting to run\u201d for almost 18 hours by now. How can I fix this? \nThanks\n  \n\n"}, {"role": "system", "text": "Hi @Rito\nYou are using multiple accounts at UseGalaxy.org. Please resolve this problem yourself, otherwise you\u2019ll end up in a situation similar to this person. Instructions about how and why to fix this are in that post.\nOnce that is done, please write back and let us know which account at UseGalaxy.org has the job delays and we can take a look. You don\u2019t need to post back the registered email address \u2013 your public name (username) is enough.\nThe server is busy but more might be going on. The person that @gbbio linked to had a technical problem plus the same delays as everyone else. Their solution required an input adjustment then a rerun of all. Waiting for several days for a job to queue and execute, only to have those jobs fail, can be frustrating. We can help.\nThanks!"}, {"role": "user", "text": "Hi Jennifer,\nThanks for alerting me about the multiple accounts issue. I have deleted the additional account. My registered account is now @mikhari.\nI have also just restarted running SPAdes, terminating the previous job (hopefully).\nThanks.\nKind regards,\n@Mikhari\n"}, {"role": "system", "text": "Hi @Rito\nThe SPAdes job is queued and will run. The inputs look appropriate. Our clusters are all very busy \u2013 so please give it some time to complete.\nThanks for clearing up the other account. "}], [{"role": "user", "text": "in processing some RAT RNA-seq data I recieved the following error:\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nEXITING because of FATAL ERROR: could not open genome file /cvmfs/data.galaxyproject.org/managed/rnastar/2.7.4a/rn6/rn6/dataset_f3c6453e-d0d8-4981-acbc-d8ed812ad69e_files//genomeParameters.txt\nSOLUTION: check that the path to genome files, specified in --genomeDir is correct and the files are present, and have user read permsissions\nSep 11 20:53:44 \u2026 FATAL ERROR, exiting\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038405.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038405.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038406.dat\u201d : No such file or directory\nIs this a problem with the GTF file and if so can someone point me towards the correctly formatted file type?"}, {"role": "system", "text": "\n\n\n Max_Boeck:\n\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\n\n\n^^ This part of the error can be ignored \u2013 it is just a log message produced by the remote cluster\n\n\n\n Max_Boeck:\n\nEXITING because of FATAL ERROR: could not open genome file /cvmfs/data.galaxyproject.org/managed/rnastar/2.7.4a/rn6/rn6/dataset_f3c6453e-d0d8-4981-acbc-d8ed812ad69e_files//genomeParameters.txt\n\n\n^^ This part describes a tool/server/resource path problem \u2013 specifically, the double slash //.\nWe\u2019d like to take a closer look. Would you please send in a bug report from the red error dataset? Leave the inputs/outputs undeleted, include a link to this topic in the comments, and post back here once sent in so we know when to look for it.\nThe indexes were recently reorganized for RNA-Star (older versions) plus new indexes were created for the latest version as available at usegalaxy.org. Other tests passed, so we are curious about what exact inputs/settings/parameters led to this type of error.\nIt doesn\u2019t appear to be a problem with a reference annotation (gtf) input, but we can check that at the same time and provide feedback if there is some secondary issue. If the tool itself ran correctly, problems with gtf content would create a different type of error, or possibly just odd yet putatively successful results.\nThere is much prior Q&A about troubleshooting issues with reference annotation at this forum. Search with a keyword like \u201cgtf\u201d to find those. Help is also in this FAQ: https://galaxyproject.org/support/diff-expression/\nThanks!\ncc @nate @dave"}, {"role": "system", "text": "Thanks for sending the bug report in \u2013 reviewing.\nUpdate 1: Other tests still running to address the path problem, but the gtf is also problem. The chromosome identifiers are a mismatch with UCSC\u2019s rn6 genome chromosomes. The FAQ I linked before will help to get that corrected. UCSC has a version (linked in FAQ), as does Gencode for GRCm38/rn6 here: GENCODE - Mouse Release M27. If you get the Gencode version, you\u2019ll need to remove the header lines. Many topics cover it, this is a good one (refers to human but the same instructions apply to mouse): Wrong! Update 9/15/20: For rn6 UCSC does not have GTF in the right format (why explained here: Help for Differential Expression Analysis - Galaxy Community Hub) and Gencode does not have it at all (only human/mouse are supported).\nBut iGenomes does \u2013 scroll down in the topic below for instructions about how to get the data into Galaxy from that data provider. Or, you may be able to convert the identifiers in the current GTF with this tool Replace column by values which are defined in a convert file (Galaxy Version 0.2). One source for \u201cconvert files\u201d is linked on that tool form, scroll down into the help. You probably want this file Rnor_6.0_ensembl2UCSC.txt \u2013 and the \u201craw\u201d URL for that data here should be good to paste into the Upload tool without any manipulations re format/datatype: https://raw.githubusercontent.com/dpryan79/ChromosomeMappings/b2862c4897cbe43f731e2cd6a2fdd2588b4e49b0/Rnor_6.0_ensembl2UCSC.txt. You can test either method out and use the resulting GTF with HISAT2 to see if it is complete/correct.\n\n  \n    \n    \n    RNA-STAR and hg38 GTF reference annotation usegalaxy.org support\n  \n  \n    Hello, \nYou need to supply a reference annotation GFT dataset from the history at runtime. \nThe GTF should be based on the UCSC \u201chg38\u201d genome build. Some choices: \n\nFor  Gencode, copy the link to the GTF and paste it into the  Upload  tool. Hg38 data is here https://www.gencodegenes.org/. After it is loaded, remove the headers (lines that start with a \u201c#\u201d) with the  Select  tool using the options \u201cNOT Matching\u201d with the regular expression  ^# . Once the formatting is fixed, change the datatype t\u2026\n  \n\n\nUpdate 2: Execution issues with RNA-Star are confirmed. An alternative tool is HISAT2. Issue ticket (will close out once fixed): Jetstream: Fails RNA-STAR 2.7.5b \u00b7 Issue #306 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub"}, {"role": "user", "text": "My data is not of the greatest quality, but I wanted to create a work flow for other data I have coming soon"}, {"role": "user", "text": "Thanks for working on this, just to clarify I\u2019m working with the rat genome, not mouse"}, {"role": "system", "text": "\n\n\n Max_Boeck:\n\njust to clarify I\u2019m working with the rat genome, not mouse\n\n\nThe test cases included in the ticket were chosen for specific purposes related to the technical nature of the presenting problems (there is more than one factor involved).\nSo \u2013 another way of stating the goal is to get the most commonly used model genomes indexed, functional at the target clusters, and in the correct format for use with the latest version of the RNA-Star wrapper. Rat rn6 will be part of the priority genome set \u2013 I\u2019ll add a note to the ticket to make that clearer.\nUpdate 9/15/20 \u2013 Sorry, missed what you were referring to re mouse/rat. I updated the above help for better options: 1) the iGenomes source for a GTF based on rn6 UCSC identifiers and 2) how to possibly convert your existing GTF from Ensembl-to-UCSC chromosome identifiers. You\u2019ll need to try those and see if it works or not (should). Maybe do both and compare, you may simply prefer the annotation content from one versus the other. "}, {"role": "system", "text": "Update:\n\n\nrn6 is now indexed for RNA-Star\nmore other genomes are also indexed\nas new genomes are completed, these will become available in the tool form\n"}], [{"role": "user", "text": "Hi all, I have uploaded my WGS fq.gz files yesterday but they are still waiting to run\u2026\nAnyone else with similar problems or with any suggestion? Thanks for any help."}, {"role": "system", "text": "Same problem here.  Local file uploads and URL imports get stuck in waiting mode\u2026"}, {"role": "user", "text": "Thanks Michael,\nany update?\nMy files are still stuck in waiting mode\u2026"}, {"role": "system", "text": "Hey @christkr08 still no change.  My files are also stuck in waiting mode\u2026"}, {"role": "system", "text": "@christkr08 uploads seem to be working now\u2026"}, {"role": "system", "text": "I had the same problem. Jobs were stuck in waiting to run for several hours, but it\u2019s finally worked."}, {"role": "system", "text": "Same here. I have had problems with uploading files since last week, uploaded files got stuck in the orange processing stage, now they won\u2019t even process just stays in cue waiting. A general problem at the moment or something on my end?"}, {"role": "system", "text": "I tested upload (Upload menu > paste/fetch and drag-and-drop into upload window)  on Galaxy Europe with small  text files. Waiting time was under one minute, and uploads have been completed within one minute. The files were in Australia. I don\u2019t see any issue with upload in Europe.\nHow do you upload the data? Do you use specialized tools, such as download and extract reads? Many servers limit number of concurrent jobs, but I don\u2019t know settings for Galaxy Europe.\nKind regards,\nIgor"}], [{"role": "user", "text": "I have seen this issue indicated several times here but I couldn\u2019t find any clear solution.  Hope anyone can help me.\nI\u2019m following the ITS DADA2 Pipeline Workflow to analyze my data: 36 paired fastqs files obtained using Illumina MiSeq. The input samples were fungal ITS amplicons. I followed the tutorial first in Galaxy and then in Rstudio and I run into the same problem. When assigning taxonomy this happens:\n#Assign Taxonomy\npath \u2190 file.path(\u201cUNITE_public_10.05.2021.fasta.gz\u201d, \u201cUNITE_public_10.05.2021.fasta\u201d)\ntaxa \u2190 assignTaxonomy(seqtab.nochim, path, multithread = TRUE, tryRC = TRUE,\ntaxLevels = c(\u201cKingdom\u201d, \u201cPhylum\u201d, \u201cClass\u201d, \u201cOrder\u201d, \u201cFamily\u201d, \u201cGenus\u201d, \u201cSpecies\u201d),\nverbose = FALSE)\nError in C_assign_taxonomy2(seqs, rc(seqs), refs, ref.to.genus, tax.mat.int,  :\nMemory allocation failed.\nIn addition: Warning message:\nIn .Call2(\u201cfasta_index\u201d, filexp_list, nrec, skip, seek.first.rec,  :\nreading FASTA file UNITE_public_10.05.2021.fasta.gz/UNITE_public_10.05.2021.fasta: ignored 8 invalid one-letter sequence codes\nI tried to increase the memory limit and this is what I have\n\nmemory.size()\n[1] 11921.24\nmemory.limit()\n[1] 31975\n\nThese are the characteristics of my laptop:\nProcessor\tAMD Ryzen 7 PRO 4750U with Radeon Graphics   1.70 GHz\nRAM \t32.0 GB (31.2 GB utilizable)\nSystem\tSistema operativo de 64 bits, procesador x64\nIf necessary this is the step before the taxonomy assignment:\n\n#Track reads through the pipeline\ngetN \u2190 function(x) sum(getUniques(x))\ntrack \u2190 cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\ncolnames(track) \u2190 c(\u201cinput\u201d, \u201cfiltered\u201d, \u201cdenoisedF\u201d, \u201cdenoisedR\u201d, \u201cmerged\u201d, \u201cnonchim\u201d)\nrownames(track) \u2190 sample.names\n\n\nhead(track)\ninput filtered denoisedF\n1-01A_S63_L001_R1_001.fastq 39949    39663     37575\n1-01B_S64_L001_R1_001.fastq 51059    50727     47766\n1-01C_S65_L001_R1_001.fastq 71229    70897     68033\n1-04A_S78_L001_R1_001.fastq 48126    47701     44850\n1-04B_S79_L001_R1_001.fastq 78663    78350     75736\n1-04C_S80_L001_R1_001.fastq 67420    67102     64241\ndenoisedR merged nonchim\n1-01A_S63_L001_R1_001.fastq     34082   3900    3831\n1-01B_S64_L001_R1_001.fastq     41556   4241    4139\n1-01C_S65_L001_R1_001.fastq     54961  16499   16157\n1-04A_S78_L001_R1_001.fastq     41881  13130   12954\n1-04B_S79_L001_R1_001.fastq     69691  11161   11070\n1-04C_S80_L001_R1_001.fastq     59966  25060   24773\n\nThank you very much in advance!"}, {"role": "system", "text": "Hi @Thais_Guillen\nMemory errors can happen with any tool, in Galaxy or not, for three primary reasons. These are all broad cases, and why there isn\u2019t a single/clear answer.\n\nThere is a problem with the inputs\nThe work actually exceeds computational resources\nThere is some problem with the command string versus what the tool is expecting \u2013 not all tools will report a specific error for all unexpected inputs/usage, some will just try to execute until they run out of resources.\n\n\n\n\n Thais_Guillen:\n\npath \u2190 file.path(\u201cUNITE_public_10.05.2021.fasta.gz\u201d, \u201cUNITE_public_10.05.2021.fasta\u201d)\n\n\nI usually start with troubleshooting the inputs when a problem comes up in the primary Galaxy applications (since the command string is automatically generated by tools). For your case, it looks as if the reference fasta is being input twice. Maybe start checking there yourself?\nThese are large data, so it is also possible that the tool is actually running out of resources. Or, maybe the fasta wasn\u2019t downloaded completely or has some formatting problem (obtained from here? if not, maybe try this version instead? Taxonomic reference data).\nYou might also try comparing how this tool and your data process at a large public server versus your laptop. Wrapped versions are available at the usegalaxy.* servers Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub if you want to try that way.\nHope that helps a little bit!"}, {"role": "user", "text": "Hi jennaj\nThank you very much for your help! I actually used the version of UNITE indicated in Taxonomic reference data. The reference data is extracted in a file with the same name within the R directory, that\u2019s why it looks like a double input.  I downloaded it 3 times, thinking of possible errors but I obtained the same result.\nYour suggestions make total sense, thank you very much, I will try to process my data on a large server and see how it goes. Have a nice day!"}, {"role": "user", "text": "UseGalaxy.org (Main) - Galaxy Community Hub gives the same error"}, {"role": "system", "text": "Please send in the error as a bug report. We can try to see if there is some input/format problem or if the tool is actually running out of resources. Please include the URL for this topic in the comments to help associate the two. How-to: Galaxy Training!\nAfter doing that, try running at UseGalaxy.eu \u2013 they have some very large clusters available, so jobs that fail for true resource/memory reasons at one server may work there."}, {"role": "system", "text": "Hi @Thais_Guillen\nThanks for sending in the report. The reference fasta has title lines (\">\" lines) that Dada2 doesn\u2019t know how to interpret. Help for the expected format is near the bottom of the tool form in Galaxy.\nTry this:\n Reformat the fasta title lines with the tool Text transformation with sed using the following two lines of script for the \u201cSed Program\u201d. Note that using Sed is just an example \u2013 you can use any tool/method you want for this, as long as the end result is the same.\ns/(^>)(.+\\|)(.+)(\\|.+)/\\1\\3/\ns/[a-z]{1}__//g\n\nThat will convert fasta title lines that originally are formatted like this:\n\n>UDB016649|k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Thelephorales;f__Thelephoraceae;g__Thelephora;s__Thelephora_albomarginata|SH1502188.08FU\n\nTo this format:\n\n>Fungi;Basidiomycota;Agaricomycetes;Thelephorales;Thelephoraceae;Thelephora;Thelephora_albomarginata\n\n Rerun using that reformatted reference fasta input. Be sure to enter the taxonomy levels \u2013 it looks like you included that content in your Rscript but it wasn\u2019t included with the job sent in for the bug report.\nThe option is right under where the reference fasta is input on the tool form, labeled as \u201cNames of the taxonomic levels in the data set\u201d [comma separated list (taxLevels)].\n\nKingdom,Phylum,Class,Order,Family,Genus,Species\n\n"}], [{"role": "user", "text": "after installing galaxy locally using git when i execute  the script run.sh it\u2019s hanged after showing the server address localhost:8080 when i try to call server address localhost:8080 through browser it\u2019s showing a blank page. I have galaxy 18.0 in the same system its working fine.please help"}, {"role": "system", "text": "Welcome, @smritiseb!\nIs the older release completely shut down when the new one is brought up? Are you sure? How to check.\nIt is not possible to run two Galaxy instances on the same computer using the default configuration."}, {"role": "system", "text": "You are trying to access the web interface from the same computer, do you?\nIt\u2019s possible your old version was configured to serve on the local network, but that is not the default for a new server."}, {"role": "user", "text": "In terminal where run.sh is invoked both version shows same ip.(serving on http://127.0.0.1:8080). if i type http://127.0.0.1:8080/ in the browser after invoking run.sh(new version) blank page is displayed whereas older version displays the galaxy page. Is this error has to do something with conda python?"}, {"role": "user", "text": "Please help.I am stuck with it. I tried editing the ip in config/galaxy.yml to my system ip then also its not\nworking. the title of the browser page is showing galaxy. but content is blank. Is there any log files to check the error?And when I checked the the config file of galaxy 18 config/galaxy.ini\nits extension is .ini instead of .yml Is this error has any relation with the extension of config file?\nThanks in advance."}, {"role": "system", "text": "\n\n\n smritiseb:\n\nIs this error has to do something with conda python?\n\n\nAh, that may be an important bit of info. Galaxy is intended to be started with regular Python being available through the command python. If your python command invokes conda python instead, things can go wrong. Since you\u2019re also saying that your browser is connecting to something, just nothing is showing up, my guess would be that something went wrong with Galaxy\u2019s attempt to install JavaScript client code.\nYou may have gotten an error message about this, when you first started Galaxy, but maybe you didn\u2019t notice.\nYou could attempt to fix the situation, but it\u2019s probably simpler to just start over: clone Galaxy once more, but this time make sure you have python in your terminal point to standard Python 2.7 before you execute sh run.sh for the first time."}, {"role": "user", "text": "i tried re-installing it.but still that error persists. The following lines are seen in terminal where galaxy is invoked\nStarting server in PID 7777.\nserving on http://127.0.0.1:8080\n/home/cmfri/galaxy/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:603: SAWarning: Dialect sqlite+pysqlite does not support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\n\u2018storage.\u2019 % (dialect.name, dialect.driver))\n127.0.0.1 - - [26/Mar/2019:14:29:43 +0600] \u201cGET / HTTP/1.1\u201d 200 - \u201c-\u201d \u201cMozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0\u201d\n[pid: 7777|app: 0|req: 1/1] 127.0.0.1 () {36 vars in 630 bytes} [Tue Mar 26 14:29:43 2019] GET / => generated 95396 bytes in 99 msecs (HTTP/1.1 200) 3 headers in 266 bytes (1 switches on core 0)\n[uwsgi-fileserve] security error: /home/cmfri/galaxy/static/scripts/bundled/base.css is not under /home/cmfri/galaxy/static/style/blue or a safe path\n[pid: 7777|app: -1|req: -1/2] 127.0.0.1 () {38 vars in 742 bytes} [Tue Mar 26 14:29:43 2019] GET /static/style/jquery-ui/smoothness/jquery-ui.css?v=1553590714 => generated 20147 bytes in 0"}, {"role": "system", "text": "\n\n\n smritiseb:\n\n127.0.0.1 - - [26/Mar/2019:14:29:43 +0600] \u201cGET / HTTP/1.1\u201d 200 - \u201c-\u201d \u201cMozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0\u201d\n\n\nOk, one more attempt. Is Firefox 31 really what you are using? If so, that is really quite outdated. Have you verified that things aren\u2019t working with a different browser?\nEverything else you\u2019ve pasted looks rather innocuous to me."}], [{"role": "user", "text": "Hello, I am having an issue running STAR on some paired end data. I am running galaxy through AWS and cloudman. My data is one set of paired end illumina fastq data. around 2.3 MB each. I also have uploaded the GR38 .gtf file from ensemble. WHen I try to run star with the options\npaired-end (as individual datasets,)\nuse a built-in index\nuse genome reference without builtin gene-model\nhg38 as the reference genome\nGR38.gtf (that I uploaded) as the gene model\n49 as the length of genomic sequence around annotated junctions (read length was 50 from my QC)\nThe error I get is\n\u201cEXITING: fatal error trying to allocate genome arrays, exception thrown: std::bad_alloc\nPossible cause 1: not enough RAM. Check if you have enough RAM 32002674832 bytes\nPossible cause 2: not enough virtual memory allowed with ulimit. SOLUTION: run ulimit -v 32002674832\u201d\nI was wondering how I could go about fixing this issue? Since I am on AWS using a large cluster I don\u2019t see how I could not have enough RAM.\nThanks"}, {"role": "system", "text": "Hi @jgoldst7\nJobs can error for all sort of odd reasons (including memory reasons) when there are problems with the inputs.\nIn your case, I suspect the reference annotation is a mismatch for the reference genome. The built-in \u201chg38\u201d human genome is sourced from UCSC.  Please see this prior Q&A for more help and details: RNA-STAR and hg38 GTF reference annotation\nFAQ: https://galaxyproject.org/support/\n\nMismatched Chromosome identifiers (and how to avoid them)\n\nThanks!"}, {"role": "user", "text": "Thank you for the reply, my refer nice annotation is from Ensemble, so that is most likely the reason"}, {"role": "user", "text": "Hi again,\nI downloaded the USCS version of the genome annotation and got the same error. Are there other common problems that could be causing this error?"}, {"role": "system", "text": "The job is probably actually running out of memory during execution if you are certain that the inputs are a match.\nBTW \u2013 Using UCSC\u2019s annotation if often not a good choice. Why?\n\nThe GTF\u2019s extracted from the Table Browser can be truncated (only about 100k lines are downloaded, whether to Galaxy or any other destination, including local file download).\nMany GTFs can be large, human and mouse in particular. If the download was truncated, it could result in a tool error. Check the end of the file to see if this happened \u2013 you\u2019ll see a message. That message can create an out-of-specification data with poor formatting (leading to tool errors) and is an incomplete dataset anyway (not useful). Tool: Select last lines from a dataset (tail) (Galaxy Version 1.1.0)\n\nGTF data from this source, even when completely transferred, will have the same value populated in the 9th column attributes field for gene_id and transcript_id. Both will be the transcript name. This effectively means that all counts or other summaries that are putatively \u201cby gene\u201d are actually \u201cby transcript\u201d (a scientific content problem). Use one of the other sources linked above instead for best results. Gencode or iGenomes are both good choices for human/mouse, with iGenomes supporting even more genomes.\n"}, {"role": "user", "text": "I\u2019m sorry, I misspoke about the gene annotation. I meant I got one based on USCS, to match the reference genome for STAR. I downloaded the comprehensive genome annotation from GENCODE. After the 5 header files starting with #, the file is 2.9 million lines and first line looks like this\nchr1\tHAVANA\tgene\t11869\t14409\t.\t+\t.\tgene_id \u201cENSG00000223972.5\u201d; gene_type \u201ctranscribed_unprocessed_pseudogene\u201d; gene_name \u201cDDX11L1\u201d; level 2; havana_gene \u201cOTTHUMG00000000961.2\u201d;\nSo if that is my gene annotation file and I am selecting HG38 at the STAR reference genome, then the only thing causing my error would be actually running out of memory? I am using an m5.xlarge VM from AWS (16 GB OF RAM) with an added 100 GB of persistant storage volume and trying to align a single set of paired end reads that are ~2.5 GB each while in the fastq.gz format. It seems like should be enough to run STAR, as 16 GB is about an average laptop would have."}, {"role": "system", "text": "Thanks for the extra info.\nYes, the Gencode GTF looks to be not the problem.\nRNA-STAR is a compute-intensive tool. 16 GB is not sufficient for human mapping. The memory needed to run the tool line-command is the same as when running it within Galaxy. See: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4631051/\nQuote from that publication:\n\nNecessary Resources\nHardware\n\nA computer with Unix, Linux or Mac OS X operating systems.\nRAM requirements: at least 10 x GenomeSize bytes. For instance, human genome of ~3 GigaBases will require ~30 GigaBytes of RAM. 32GB is recommended for human genome alignments.\n\n\nSufficient free disk space (>100 GigaBytes) for storing output files.\n\n\nAlternative ways to run your own Galaxy server, including Cloud choices. Since you are already using Cloudman, you probably just need to bump up the VM choice to one with more resources: https://galaxyproject.org/use/.\nPlease also see this recent Galaxy Blog post. This is an alternative for using a local Galaxy with on-demand cloud resources incorporated: https://galaxyproject.org/blog/ > Enabling cloud bursting for Galaxy\nThanks!"}], [{"role": "user", "text": "Dear Galaxy users,\nI am trying to analyse some 16S rRNA data following the Galaxy tutorial. When i run the unique.seqs call i get the following message:\nAn error occured while running the tool toolshed.g2.bx.psu.edu/repos/iuc/mothur_unique_seqs/mothur_unique_seqs/1.39.5.0.\nTool execution generated the following messages:\nFatal error: Exit code 137 ()\n/home/nick/galaxy/database/jobs_directory/002/2067/tool_script.sh: line 25: 31810 Done                    echo \u2018unique.seqs( fasta=fasta.dat, format=name )\u2019\n31811                       | sed \u2018s/ //g\u2019\n31812 Killed                  | mothur\n31813                       | tee mothur.out.log\nDo you know why this keeps happening? The previous call was the screen.seqs() and was successful.\nKind regards\nNick"}, {"role": "system", "text": "It looks like the tool is running out of memory during the job processing.\nThis could be because of mixed-up inputs or using inputs that are too large to execute on your Galaxy server.\nHas the tool worked before or is the first time using it? You might want to double check that the Mothur tool suite is installed correctly and all dependencies are intact.\nServer administrator help is in a few places:\n\nhttps://docs.galaxyproject.org\nhttps://galaxyproject.org/admin/\nhttps://github.com/galaxyproject/dagobah-training\n"}, {"role": "user", "text": "Dear jennaj,\nMany thanks for your reply. I have run before this command without any problems. Do you mean that my RAM is running out?\nShall i try to break down the dataset to smaller datasets and then merge the data? Is that possible?\nThanks a lot for your kind help\nBest\nN\n"}, {"role": "system", "text": "There could be a format/content problem with the input or the job is really running out of memory during execution.\nBreaking the dataset up into smaller chunks would not work well with this tool.\nWhat happens if you run the same job at a public Galaxy server? You can try at Galaxy Main https://usegalaxy.org or Galaxy EU https://usegalaxy.eu. If the tool errors, a share link to the history can be generated and sent in for feedback publically by posting back the link and the dataset numbers involved. Or, ask for feedback privately by sending in a bug report from the red error dataset \u2013 please include a link to this post in the comments to associate the two support questions with context."}, {"role": "user", "text": "Many thanks for your reply again. I might not be able to run the same job online as my dataset is large and i might exceed the available space. I am running again the same calls with a smaller dataset. Is there any way to increase the memory?\n[UPDATE]\nI finished running the same calls with a smaller dataset and commands were successful. It might be then that i am running out of space.\nKind regards\nNick"}, {"role": "system", "text": "Ok, that does help to confirm that memory is the root problem.\nPlease see the Galaxy administrator help for how to increase memory allocation for tools. This tuning is usually most appropriate for those that are running jobs on a cluster, which might not be your case.\nBy default, when running Galaxy locally, job memory is limited by whatever hardware you have natively available on your computer. 16 GB is considered the minimum, but many tools and certainly large datasets can need more.\nYou might want to consider moving to a cloud version of Galaxy if you plan to process large data. See:\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n"}], [{"role": "user", "text": "I have been trying to use a genome build for sheep (OviAri4 UCSC gold fasta) for mapping single end RNAseq data with bowtie2. No matter what I try, downstream use of the mapped BAM output with featurecounts fails give give any counts. I have tried all the options/advice described under the help section, but to no avail.\nAny possibility that the sheep genome (and if possible the goat genome) could be added as built-ins for bowtie2?"}, {"role": "system", "text": "Hi @Phil,\nI informed the sysadmin in order to include both genomes; also, could you share your Galaxy history with me? I will have a look at the problem. My email is .\nRegards"}, {"role": "user", "text": "Sure\u2026\nThanks. I am curious to understand the issue I am having, so any insights are greatly wlecome.\nPhil"}, {"role": "system", "text": "Hi @Phil,\ncould you provide me with some details about the version of the goat genome that you need?\nRegards"}, {"role": "user", "text": "Thanks for your response.\nThe ARS1 Aug 2016 (San Clemente) should do the trick thanks.\nPhil"}, {"role": "system", "text": "Hi @Phil,\nthe genomes are now available.\nRegards"}], [{"role": "user", "text": "Hi everybody.\nI\u2019m developing a tool that allows to choose between different options for a query.\nNow my code is inside a python file where for each type of query I have a different function.\nFollowing this guide I created the XML file and everything is running correctly without errors when I start Galaxy.\nThe problem is that when I run the tool (with every option) I get this error:\nJob submission failed.\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\nIn the terminal I get this:\ngalaxy.tools ERROR 2022-02-28 11:43:36,932 [pN:main.web.1,p:13472,w:1,m:0,tN:uWSGIWorker1Core0] Exception caught while attempting to execute tool with id 'impc_test':\nTraceback (most recent call last):\n  File \"lib/galaxy/util/template.py\", line 80, in fill_template\n    return unicodify(t, log_exception=False)\n  File \"lib/galaxy/util/__init__.py\", line 1060, in unicodify\n    value = str(value)\n  File \"/Users/andrea/Desktop/galaxy/.venv/lib/python3.6/site-packages/Cheetah/Template.py\", line 1053, in __unicode__\n    return getattr(self, mainMethName)()\n  File \"cheetah_DynamicallyCompiledCheetahTemplate_1646045016_93172_79883.py\", line 91, in respond\nEOFError: EOF when reading a line\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/__init__.py\", line 1770, in handle_single_execution\n    flush_job=flush_job,\n  File \"lib/galaxy/tools/__init__.py\", line 1858, in execute\n    return self.tool_action.execute(self, trans, incoming=incoming, set_output_hid=set_output_hid, history=history, **kwargs)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 529, in execute\n    handle_output(name, output)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 455, in handle_output\n    data.name = self.get_output_name(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 784, in get_output_name\n    return fill_template(output.label, context=params, python_template_version=tool.python_template_version)\n  File \"lib/galaxy/util/template.py\", line 124, in fill_template\n    python_template_version=python_template_version,\n  File \"lib/galaxy/util/template.py\", line 126, in fill_template\n    raise first_exception or e\n  File \"lib/galaxy/util/template.py\", line 80, in fill_template\n    return unicodify(t, log_exception=False)\n  File \"lib/galaxy/util/__init__.py\", line 1060, in unicodify\n    value = str(value)\n  File \"/Users/andrea/Desktop/galaxy/.venv/lib/python3.6/site-packages/Cheetah/Template.py\", line 1053, in __unicode__\n    return getattr(self, mainMethName)()\n  File \"DynamicallyCompiledCheetahTemplate.py\", line 89, in respond\nEOFError: EOF when reading a line\ngalaxy.tools.execute WARNING 2022-02-28 11:43:36,932 [pN:main.web.1,p:13472,w:1,m:0,tN:uWSGIWorker1Core0] There was a failure executing a job for tool [impc_test] - Error executing tool with id 'impc_test': EOF when reading a line\n\nHere\u2019s the .py code that I\u2019m using as test and its XML file:\ntest.py (here I don\u2019t have functions since I\u2019m making tests on the input which is the part that, in my opinion, is raising the error)\nimport sys\nimport argparse\nimport requests\nimport pandas as pd\nimport numpy as np\n\ndef main():\n    \n    # define options\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"-k\", \"--option\", type=str, help=\"Option selected\")\n\n    # parse\n    args = parser.parse_args()\n    option = str(args.option)\n    \n    if option == 'q1':\n        print(\"Hello 1! Input: \" + sys.argv[1])\n        textfile = open(sys.argv[2], \"w+\")\n        textfile.write( \"Here's the input of first query: \" + sys.argv[1] + \"\\n\")\n        textfile.close()\n    elif option == 'q2':\n        print(\"Hello 2! Input: \" + sys.argv[1])\n        textfile = open(sys.argv[2], \"w+\")\n        textfile.write( \"Here's the input of second query: \" + sys.argv[1] + \"\\n\")\n        textfile.close()\n    else:\n        sys.exit(\"Error!\")    \n    \nif __name__ == \"__main__\":\n    main()        \n\ntest.xml\n<tool id=\"impc_test\" name=\"Test \" version=\"0.1.0\">\n\t<description>for impc query selection</description>\n    <requirements>\n      <requirement type=\"package\">requests</requirement>\n      <requirement type=\"package\">pandas</requirement>\n      <requirement type=\"package\">numpy</requirement>\n    </requirements>\n    <command>\n            python3 \"{$__tool_directory__/test.py}\" '$query_type.input' '$output' #set $option = str($selector) --option $option\n\t</command>\n\t<inputs>\n\t\t\t<conditional name=\"query_type\">\n\t\t\t\t<param name=\"selector\" type=\"select\" labels=\"Select a query:\">\n\t\t\t\t\t<option value=\"q1\">First</option>\n\t\t\t\t\t<option value=\"q2\">Second</option>\n\t\t\t\t</param>\n\t\t\t\t<when value=\"q1\">\n\t\t\t\t\t<param name=\"input\" type=\"text\" label=\"Input 1:\"/>\n\t\t\t\t</when>\n\t\t\t\t<when value=\"q2\">\n\t\t\t\t\t<param name=\"input\" type=\"text\" label=\"Input 2:\"/>\n\t\t\t\t</when>\n\t\t\t</conditional>\n\t</inputs>\n    <outputs>\n            <data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n    </outputs>\n\t<help>\n\t\tJust a test for IMPC queries.\n\t</help>\n</tool>\n\nEssentially, I\u2019m trying to set \u2018option\u2019 on different values so inside the code I can use it to call different functions.\nThank you all for the help."}, {"role": "system", "text": "I thought I understand your question but your code does not look like the guide link that you are sharing. You are pointing out slide 22. So not sure why you dont just use that example but maybe I understand your question wrong. See code below for inspiriation, not sure if it is a working example.\n<tool id=\"impc_test\" name=\"Test \" version=\"0.1.0\">\n\t<description>for impc query selection</description>\n    <requirements>\n      <requirement type=\"package\">requests</requirement>\n      <requirement type=\"package\">pandas</requirement>\n      <requirement type=\"package\">numpy</requirement>\n    </requirements>\n    <command>\n            python3 \"{$__tool_directory__/test.py}\" '$query_type.input' '$output'\n            #if $query_type.selector == 'q1':\n            --option $query_type.input_q1\n            #else:\n            --option $query_type.input_q2\n             #end if\n\t</command>\n\t<inputs>\n\t\t\t<conditional name=\"query_type\">\n\t\t\t\t<param name=\"selector\" type=\"select\" labels=\"Select a query:\">\n\t\t\t\t\t<option value=\"q1\">First</option>\n\t\t\t\t\t<option value=\"q2\">Second</option>\n\t\t\t\t</param>\n\t\t\t\t<when value=\"q1\">\n\t\t\t\t\t<param name=\"input_q1\" type=\"text\" label=\"Input 1:\"/>\n\t\t\t\t</when>\n\t\t\t\t<when value=\"q2\">\n\t\t\t\t\t<param name=\"input_q2\" type=\"text\" label=\"Input 2:\"/>\n\t\t\t\t</when>\n\t\t\t</conditional>\n\t</inputs>\n    <outputs>\n            <data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n    </outputs>\n\t<help>\n\t\tJust a test for IMPC queries.\n\t</help>\n</tool>\n"}, {"role": "user", "text": "Yes it\u2019s a bit different from the slide 22 because I already tried to use a similar code and I had the exact same error that I have with the new code.\nEssentially I would like to do something like:\nif selector == 'q1'\n    python3 test.py_function1 (not correct syntax, just a way to say that I would like to call that function)\nelse\n    python3 test.py_function2\n\nIt seems that it\u2019s not possible to call directly the function, so I\u2019m trying to set a third variable to be used inside the python script."}, {"role": "user", "text": "To make the things more clear, I\u2019m adding here an easier example that is giving me the exact same error:\ntest.xml\n<tool id=\"test_impc\" name=\"Test \" version=\"0.1.0\">\n\t<description>for impc.</description>\n\t<command><![CDATA[\npython3 '{$__tool_directory__/test.py}' '$output' $selection\n#if $operation.selection == 'op1':\n\t'$operation.input1'\n#else:\n\t'$operation.input2'\n]]></command>\n\t<inputs>\n\t\t<conditional name=\"operation\">\n\t\t\t<param name=\"selection\" type=\"select\" labels=\"Select an option:\">\n\t\t\t\t<option value=\"op1\">First</option>\n\t\t\t\t<option value=\"op2\">Second</option>\n\t\t\t</param>\n            <when value=\"op1\">\n                    <param name=\"input1\" type=\"text\" label=\"Input first:\"/>\n            </when>\n            <when value=\"op2\">\n                    <param name=\"input2\" type=\"text\" label=\"Input second:\"/>\n            </when>\n\t\t</conditional>\t\n\t</inputs>\n\t<outputs>\n\t\t<data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n\t</outputs>\n\t<help>A simple test for impc, with options!</help>\n</tool>\n\ntest.py\nimport sys\n\ndef main(): \n    \n    opt = str(sys.argv[1])\n    if opt == 'op1':\n        first()\n    else:\n        second()\n            \n    sys.exit(0)    \n\ndef first():\n    inpt = sys.argv[3]\n    out = sys.argv[2]\n    opt = str(sys.argv[1])\n    \n    txt = open(out, \"w+\")\n    txt.write(\"You selected the first option (\" + opt + \") with input string \" + inpt + \"\\n\")\n    txt.close()\n    \ndef second():\n       inpt = sys.argv[3]\n       out = sys.argv[2]\n       opt = str(sys.argv[1])\n    \n       txt = open(out, \"w+\")\n       txt.write(\"You selected the second option (\" + opt + \") with input string \" + inpt + \"\\n\")\n       txt.close()\n    \nif __name__ == \"__main__\":\n    main()\n\nThe syntax of both files seems to be correct, I cannot really understand what is causing the error."}, {"role": "system", "text": "\n\n\n Andrea_Furlani:\n\n#if operation.selection == 'op1':\n\t'$operation.input1'\n#else:\n\t'$operation.input2'\n\n\n\nFor starters you are already missing the $ character.\n#if $operation.selection == 'op1':"}, {"role": "user", "text": "Sorry it was a typo, in my code the $ is present"}, {"role": "system", "text": "You are also missing ${input}:\n<data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n\nYou use it in your output but to what is it pointing? Try to just use some text first and check if it works. Something like:\n<data format=\"tabular\" name=\"output\" label=\"testing output\"/>\n\nAnd on top of my head, once you do have added $input also try this:\n<data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input.display_name}\"/>"}], [{"role": "user", "text": "HI,\nWhen I try to run galaxy local, I get the message \u201cconda installation requested and failed\u201d.\nError: did not recognise option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\nI have gone into the galaxy.yml file and amended it as so:\nconda_prefix: <tool_dependency_dir>/_conda\ntool_dependency_dir: dependencies\nconda_auto_init: true\nI cannot understand what the issue is. Despite this, it allows me to open galaxy on the web and download tools but the tools do not work.\nany help would be much appreciated!!"}, {"role": "system", "text": "These are the default setting, you don\u2019t need to change them.\nIf you do, you need to replace <tool_dependency_dir>/ with a path, like conda_prefix: /Book/Galaxy/galaxy/database/dependencies/_conda/ , but again that isn\u2019t necessary.\nCan you be more specific about what you do to see these error messages. Are they in the user interface, or in the logs ?\nI would also suggest reading https://docs.galaxyproject.org/en/master/admin/conda_faq.html if you haven\u2019t, that might clear things up for you."}, {"role": "user", "text": "@mvdbeek I run galaxy in the terminal\nThen it says installing Conda, then it says Conda installation requested and failed (in the terminal). This happened before I even configured the galaxy.yml file. After failing to install, it repeats the process of trying to install Conda and fails again\nI have read that page you suggested and from that made the changes to the galaxy.yml file. I am new to this, so have very little experience and I am struggling to understand some of the material."}, {"role": "system", "text": "Hi @Lauren\nThe advice from @mvdbeek is very good. Unless an advanced admin configuration is needed for some reason, using all defaults can be a simple method to get a single-user local Galaxy instance running.\nThese instructions in prior Q&A explain the first steps when setting up a local under OSX. You might choose to start over, following the help, to see if that works for you. Example step-by-step for a basic local Galaxy install with an admin account created\nThe most current version of Galaxy is always listed here: https://docs.galaxyproject.org/en/master/releases/index.html\nFor even more administrative help, including tutorials and training resources, please see: Local Galaxy: Basic Question -- Installing more tools"}, {"role": "user", "text": "thank you @jennaj\nBut I am still having issues and am unable to use local galaxy.\nI have deleted and reinstalled it many times. I have followed the instructions from get galaxy and left everything as the default settings but when I run galaxy in the terminal using sh run.sh, it activates the virtual environment and then it gets to the point where galaxy wants to install conda but fails to do so, this cycle continues. Why might it be failing?\nActivating virtualenv at .venv\ngalaxy.tool_util.deps WARNING 2020-03-31 12:11:48,004 Path \u2018database/dependencies\u2019 does not exist, ignoring\ngalaxy.tool_util.deps WARNING 2020-03-31 12:11:48,005 Path \u2018database/dependencies\u2019 is not directory, ignoring\ngalaxy.tool_util.deps.conda_util INFO 2020-03-31 12:11:48,183 Installing conda, this may take several minutes.\nERROR: did not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\ngalaxy.tool_util.deps.installable WARNING 2020-03-31 12:14:40,066 Conda installation requested and failed.\ngalaxy.tool_util.deps.conda_util INFO 2020-03-31 12:14:40,067 Installing conda, this may take several minutes.\nI still get these error messages also:\ngalaxy.tools.toolbox.base ERROR 2020-03-31 12:17:57,604 [p:2741,w:0,m:0] [MainThread] Error reading tool configuration file from path \u2018evolution/codingSnps.xml\u2019: [Errno 2] No such file or directory: \u2018/Volumes/My Book/Galaxy/galaxy/tool-data/codingSnps.loc\u2019\ngalaxy.tools.toolbox.base ERROR 2020-03-31 12:17:57,606 [p:2741,w:0,m:0] [MainThread] Error reading tool configuration file from path \u2018evolution/add_scores.xml\u2019: [Errno 2] No such file or directory: \u2018/Volumes/My Book/Galaxy/galaxy/tool-data/add_scores.loc\u2019\ngalaxy.tools.toolbox.base ERROR 2020-03-31 12:17:57,645 [p:2741,w:0,m:0] [MainThread] Error reading tool configuration file from path \u2018phenotype_association/sift.xml\u2019: [Errno 2] No such file or directory: \u2018/Volumes/My Book/Galaxy/galaxy/tool-data/sift_db.loc\u2019\n\u2026Eventually it seems to give up after many DEBUG lines and I am provide with the http address but this takes a while to load in the web browser and the tools do not work.\nI have python 3.7.7 installed on my Mac, this has now been set as the default and I have also recently updated my MacOS to Catalina.\nIts getting very frustrating!"}, {"role": "system", "text": "As far as I know you should not get this error: database/dependencies\u2019 does not exist, ignoring if you really start over from the beginning and not change anything. If you re-install but use the same config.yml you are still doing the same.\nMaybe it helps if you post the exact lines here again from galaxy/config/galaxy.yml that has to do with conda.\nAlso, try to do it step by step. With the information that you are giving now it is hard to help you.\nSo first download a fresh install, don\u2019t change the config and start galaxy. Does this work? Can you start galaxy without errors and can you open the webinterface?\nSecond, change only the following lines to:\n  conda_auto_install: true\n  conda_auto_init: true\nTry to start galaxy again, does it still work?\nAs last, I think this has not to do with you problem but check if there are no old hidden .conda folders. You can check it with the ls -la command. In your case a good thing to check is ls -la Book/Galaxy/\nYou could also install conda outside galaxy and tell galaxy where to find conda. But this can cause some permission/rights issues. Those issues are not hard to solve but you need some \u201ccommandline\u201d experience."}, {"role": "user", "text": "Thank you @gbbio\nI have completely reset my Mac (now using Catalina OS), installed Xcode, home-brew and python3 and then used the terminal to clone galaxy onto my external hard drive and I ran galaxy using sh run.sh but I still get the same outputs.\nActivating virtualenv at .venv\nRequirement already satisfied: pip>=8.1 in ./.venv/lib/python3.7/site-packages (20.0.2)\nIgnoring bz2file: markers \u2018python_version < \u201c3.3\u201d\u2019 don\u2019t match your environment\nIgnoring configparser: markers \u2018python_version < \u201c3.2\u201d\u2019 don\u2019t match your environment\nIgnoring contextlib2: markers \u2018python_version < \u201c3.5\u201d\u2019 don\u2019t match your environment\nIgnoring enum34: markers \u2018python_version < \u201c3.4\u201d\u2019 don\u2019t match your environment\nIgnoring funcsigs: markers \u2018python_version < \u201c3.3\u201d\u2019 don\u2019t match your environment\nIgnoring functools32: markers \u2018python_version < \u201c3.2\u201d\u2019 don\u2019t match your environment\nIgnoring futures: markers \u2018python_version == \u201c2.6\u201d or python_version == \u201c2.7\u201d\u2019 don\u2019t match your environment\nIgnoring ipaddress: markers \u2018python_version < \u201c3.3\u201d\u2019 don\u2019t match your environment\nIgnoring pathlib2: markers \u2018python_version < \u201c3.6\u201d\u2019 don\u2019t match your environment\nIgnoring pyinotify: markers \u2018sys_platform != \u201cwin32\u201d and sys_platform != \u201cdarwin\u201d and sys_platform != \u201csunos5\u201d\u2019 don\u2019t match your environment\nIgnoring python-openid: markers \u2018python_version < \u201c3.0\u201d\u2019 don\u2019t match your environment\nIgnoring ruamel.ordereddict: markers \u2018platform_python_implementation == \u201cCPython\u201d and python_version <= \u201c2.7\u201d\u2019 don\u2019t match your environment\nIgnoring scandir: markers \u2018python_version < \u201c3.5\u201d\u2019 don\u2019t match your environment\nIgnoring subprocess32: markers \u2018python_version < \u201c3.0\u201d\u2019 don\u2019t match your environment\nIgnoring typing: markers \u2018python_version < \u201c3.5\u201d\u2019 don\u2019t match your environment\nIgnoring unicodecsv: markers \u2018python_version < \u201c3.0\u201d\u2019 don\u2019t match your environment\nLooking in indexes: https://wheels.galaxyproject.org/simple, https://pypi.python.org/simple\nThe Galaxy client has not yet been built and will be built now.\nyarn install v1.22.4\n[1/4]   Resolving packages\u2026\n[2/4]   Fetching packages\u2026\n[3/4]   Linking dependencies\u2026\nwarning \" > @handsontable/vue@2.0.0\" has incorrect peer dependency \u201chandsontable@^4.0.0\u201d.\nwarning \" > eslint-plugin-vue@5.2.3\" has incorrect peer dependency \u201ceslint@^5.0.0\u201d.\nwarning \u201ceslint-plugin-vue > vue-eslint-parser@5.0.0\u201d has incorrect peer dependency \u201ceslint@^5.0.0\u201d.\n[4/4]   Building fresh packages\u2026\n  Done in 1688.75s.\nyarn run v1.22.4\n NODE_ENV=production gulp && yarn run webpack-production-maps && yarn run save-build-hash\n[14:04:59] Using gulpfile /Volumes/My Book/Galaxy/galaxy/client/gulpfile.js\n[14:04:59] Starting 'default'...\n[14:04:59] Starting 'fonts'...\n[14:04:59] Starting 'stageLibs'...\n[14:04:59] Starting 'cleanPlugins'...\n[14:04:59] Finished 'stageLibs' after 61 ms\n[14:04:59] Finished 'cleanPlugins' after 86 ms\n[14:04:59] Starting 'buildPlugins'...\n[14:04:59] Finished 'buildPlugins' after 1.14 ms\n[14:04:59] Starting 'stagePlugins'...\nInstalling Dependencies for /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/annotate_image/\ninfo No lockfile found.\n[1/4] \ud83d\udd0d  Resolving packages...\nwarning babel-preset-env > babel-plugin-check-es2015-constants > babel-runtime > core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning parcel-bundler > core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning parcel-bundler > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > @parcel/fs > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > htmlnano > svgo > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > htmlnano > uncss > request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\nwarning parcel-bundler > htmlnano > uncss > jsdom > request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\nwarning parcel-bundler > @parcel/watcher > chokidar > fsevents > node-pre-gyp > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > micromatch > snapdragon > source-map-resolve > resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated\nwarning parcel-bundler > micromatch > snapdragon > source-map-resolve > urix@0.1.0: Please see https://github.com/lydell/urix#deprecated\nwarning parcel-bundler > @parcel/watcher > chokidar > fsevents > node-pre-gyp > tar > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\n[2/4] \ud83d\ude9a  Fetching packages...\n[3/4] \ud83d\udd17  Linking dependencies...\n[4/4] \ud83d\udd28  Building fresh packages...\nsuccess Saved lockfile.\nBuilding  /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/annotate_image/\n parcel build src/script.js -d static --no-source-maps\n  Built in 23.13s.\nstatic/script.js    354.8 KB    18.80s\nInstalling Dependencies for /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/\ninfo No lockfile found.\n[1/4]   Resolving packages\u2026\nwarning hyphy-vision > extract-text-webpack-plugin@2.1.2: Deprecated. Please use https://github.com/webpack-contrib/mini-css-extract-plugin\nwarning hyphy-vision > popper.js@1.16.1: You can find the new Popper v2 at @popperjs/core, this package is dedicated to the legacy v1\nwarning hyphy-vision > alignment.js > popper.js@1.16.1: You can find the new Popper v2 at @popperjs/core, this package is dedicated to the legacy v1\nwarning hyphy-vision > alignment.js > phylotree > rollup-plugin-node-resolve@3.4.0: This package has been deprecated and is no longer maintained. Please use @rollup/plugin-node-resolve.\nwarning hyphy-vision > create-react-class > fbjs > core-js@1.2.7: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning hyphy-vision > alignment.js > @gmod/bam > @babel/runtime-corejs2 > core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning webpack > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning webpack > terser-webpack-plugin > cacache > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning webpack > terser-webpack-plugin > cacache > move-concurrently > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning webpack > micromatch > snapdragon > source-map-resolve > resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated\nwarning webpack > micromatch > snapdragon > source-map-resolve > urix@0.1.0: Please see https://github.com/lydell/urix#deprecated\nwarning webpack > terser-webpack-plugin > cacache > move-concurrently > copy-concurrently > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning webpack > watchpack > chokidar > fsevents > node-pre-gyp > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning webpack > watchpack > chokidar > fsevents > node-pre-gyp > tar > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\n[2/4]   Fetching packages\u2026\n[3/4]   Linking dependencies\u2026\nwarning \u201chyphy-vision > extract-text-webpack-plugin@2.1.2\u201d has incorrect peer dependency \u201cwebpack@^2.2.0\u201d.\nwarning \u201chyphy-vision > react-scrollchor@4.2.1\u201d has unmet peer dependency \u201cfbjs@*\u201d.\n[4/4]   Building fresh packages\u2026\nsuccess Saved lockfile.\nBuilding  /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/\n$ webpack --mode production\nHash: cfc7880b36d01b3069ce\nVersion: webpack 4.42.1\nTime: 24968ms\nBuilt at: 04/01/2020 2:25:49 PM\nAsset      Size  Chunks                    Chunk Names\nmain.css   272 KiB       0  [emitted]  [big]  main\nscript.js  2.65 MiB       0  [emitted]  [big]  main\nEntrypoint main [big] = main.css script.js\n[5] (webpack)/buildin/global.js 472 bytes {0} [built]\n[10] ./node_modules/underscore/modules/index-all.js + 1 modules 326 bytes {0} [built]\n|    2 modules\n[14] ./src/index.js 215 bytes {0} [built]\n[42] (webpack)/buildin/amd-define.js 85 bytes {0} [built]\n[43] (webpack)/buildin/amd-options.js 80 bytes {0} [built]\n[65] ./node_modules/react-router-dom/es/index.js + 29 modules 76.8 KiB {0} [built]\n|    30 modules\n+ 64 hidden modules\nWARNING in asset size limit: The following asset(s) exceed the recommended size limit (244 KiB).\nThis can impact web performance.\nAssets:\nmain.css (272 KiB)\nscript.js (2.65 MiB)\nWARNING in entrypoint size limit: The following entrypoint(s) combined asset size exceeds the recommended limit (244 KiB). This can impact web performance.\nEntrypoints:\nmain (2.92 MiB)\nmain.css\nscript.js\nWARNING in webpack performance recommendations:\nYou can limit the size of your bundles by using import() or require.ensure to lazy load some parts of your application.\nFor more info visit https://webpack.js.org/guides/code-splitting/\nChild mini-css-extract-plugin \u2026/\u2026/\u2026/\u2026/\u2026/\u2026/\u2026/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/node_modules/css-loader/dist/cjs.js!../\u2026/\u2026/\u2026/\u2026/\u2026/\u2026/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/node_modules/hyphy-vision/dist/hyphyvision.css:\nEntrypoint mini-css-extract-plugin = *\n2 modules\nChild mini-css-extract-plugin \u2026/\u2026/\u2026/\u2026/\u2026/\u2026/\u2026/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/node_modules/css-loader/dist/cjs.js!../\u2026/\u2026/\u2026/\u2026/\u2026/\u2026/My Book/Galaxy/galaxy/config/plugins/visualizations/hyphyvision/node_modules/phylotree/build/phylotree.css:\nEntrypoint mini-css-extract-plugin = *\n2 modules\nInstalling Dependencies for /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/openlayers/\ninfo No lockfile found.\n[1/4]   Resolving packages\u2026\nwarning babel-preset-env > babel-plugin-check-es2015-constants > babel-runtime > core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning parcel-bundler > core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.\nwarning parcel-bundler > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > @parcel/fs > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > htmlnano > svgo > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > htmlnano > uncss > request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\nwarning parcel-bundler > htmlnano > uncss > jsdom > request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\nwarning parcel-bundler > @parcel/watcher > chokidar > fsevents > node-pre-gyp > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\nwarning parcel-bundler > micromatch > snapdragon > source-map-resolve > resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated\nwarning parcel-bundler > micromatch > snapdragon > source-map-resolve > urix@0.1.0: Please see https://github.com/lydell/urix#deprecated\nwarning parcel-bundler > @parcel/watcher > chokidar > fsevents > node-pre-gyp > tar > mkdirp@0.5.4: Legacy versions of mkdirp are no longer supported. Please update to mkdirp 1.x. (Note that the API surface has changed to use Promises in 1.x.)\n[2/4]   Fetching packages\u2026\n[3/4]   Linking dependencies\u2026\n[4/4]   Building fresh packages\u2026\nsuccess Saved lockfile.\nBuilding  /Volumes/My Book/Galaxy/galaxy/config/plugins/visualizations/openlayers/\n$ parcel build src/script.js -d static --no-source-maps\nnpm WARN deprecated babel-preset-es2015@6.24.1:   Thanks for using Babel: we recommend using babel-preset-env now: please read https://babeljs.io/env to updat\u2728  Built in 162.58s.\nstatic/script.js      1.25 MB    158.39s\n[14:37:20] Finished \u2018fonts\u2019 after 32 min\n[14:37:38] Finished \u2018stagePlugins\u2019 after 33 min\n[14:37:38] Finished \u2018default\u2019 after 33 min\n$ GXY_BUILD_SOURCEMAPS=1 webpack -p\nBrowserslist: caniuse-lite is outdated. Please run next command yarn upgrade caniuse-lite browserslist\nBrowserslist: caniuse-lite is outdated. Please run next command yarn upgrade\nBrowserslist: caniuse-lite is outdated. Please run next command yarn upgrade\n[BABEL] Note: The code generator has deoptimised the styling of /Volumes/My Book/Galaxy/galaxy/client/node_modules/handsontable/dist/handsontable.js as it exceeds the max of 500KB.\nHash: 0cabcf3cda4b1e2c46cc\nVersion: webpack 4.29.6\nTime: 203671ms\nBuilt at: 04/01/2020 2:41:33 PM\nAsset       Size  Chunks                    Chunk Names\nadmin.bundled.js   1.35 MiB       2  [emitted]  [big]  admin\nadmin.bundled.js.map   4.52 MiB       2  [emitted]         admin\nanalysis.bundled.js   1.44 MiB       3  [emitted]  [big]  analysis\nanalysis.bundled.js.map   4.75 MiB       3  [emitted]         analysis\nassets/065988da56cecfb3b0656bff85f5aca8.png  488 bytes          [emitted]\nassets/09387d05e6ddc1ef8f075125a2381afb.png  378 bytes          [emitted]\nassets/17a69f29b21e2aa9b8a7a0fdb3ff81d1.png   1.03 KiB          [emitted]\nassets/19c1b34012559d0591b070a08039fd87.png    1.1 KiB          [emitted]\nassets/1f075735090412ed7eb8077d819b19c6.png   1.84 KiB          [emitted]\nassets/23029e3456c59e9d6bcecc5aa4bd5b43.png   1.31 KiB          [emitted]\nassets/27664fc7eebd4eee0ab33ac5dfadbcb9.png   1.17 KiB          [emitted]\nassets/27aade6683b945167764ab7574aaa3a8.png  566 bytes          [emitted]\nassets/28ebb58fc523bca914caec25e480350a.png  532 bytes          [emitted]\nassets/2ca61b76e22053571dd8611e5aac4900.png  613 bytes          [emitted]\nassets/2d4616fb4c853c612b921c060a8816b7.png  596 bytes          [emitted]\nassets/2e2754da2967e360bba3a1bd8eeb4ace.png  689 bytes          [emitted]\nassets/2f14cb3a743ea736c1929642c5598c9e.png  721 bytes          [emitted]\nassets/3b7a845229f4041f5d8a18399f4c7357.gif  815 bytes          [emitted]\nassets/3b826fc6b1952a6124edd1ff72b8570b.png  613 bytes          [emitted]\nassets/418d5485d914e6236f8666d69ab3cd6e.png  722 bytes          [emitted]\nassets/43e1045b71da619b1d7e53fb3217be93.gif  752 bytes          [emitted]\nassets/46bd59d0472597ded9f5a15dc3f573ce.gif  867 bytes          [emitted]\nassets/49e3f006018662f60f1db2aec0b2cca9.png  845 bytes          [emitted]\nassets/4bee8922fea44f857d65744f7052c36f.png   1.32 KiB          [emitted]\nassets/4d286d58f72d47ed5dfcb74b7c6f7b41.png  516 bytes          [emitted]\nassets/4e1e4881535880c858540c03e97afd69.png  331 bytes          [emitted]\nassets/4eae101f8d2e9470fc8912125fd5fa26.png  403 bytes          [emitted]\nassets/5648b8df09221214723d59eb54fbaaa2.png  682 bytes          [emitted]\nassets/61a4250152fabad0a6a9cd83d40df651.png   1.26 KiB          [emitted]\nassets/63912368b751c160129e6c42b8824821.png   1.21 KiB          [emitted]\nassets/674f50d287a8c48dc19ba404d20fe713.eot    162 KiB          [emitted]\nassets/67ebb9f28d2604faa6acfa8fa1157e41.gif   4.36 KiB          [emitted]\nassets/68f6ead9b879fa12c09aa5ea225584f0.png   87 bytes          [emitted]\nassets/6d1eda748d2e466db16d38bf9e7583c3.png  103 bytes          [emitted]\nassets/6ea6da3e8d696b686043bd111f84c8f5.png  624 bytes          [emitted]\nassets/70d1aacb47c9039b487dc012cd1a88e5.gif  836 bytes          [emitted]\nassets/79bf2c5b0236eda94ba8b67b8bf2a35f.png  736 bytes          [emitted]\nassets/79ee7f3ae726dc325c2507bb4fd2920b.png  130 bytes          [emitted]\nassets/7b93116a225b6113d2ae55932862340f.png   1.39 KiB          [emitted]\nassets/7b9776076d5fceef4993b55c9383dedd.gif   1.81 KiB          [emitted]\nassets/8c7b4ca75bd836682f1a585503237f63.png  119 bytes          [emitted]\nassets/8dc260a6da267c5e8f9899dc93c1ef02.png   1.36 KiB          [emitted]\nassets/90221f73d1320fd0d2d45734df212fe2.png   1.24 KiB          [emitted]\nassets/912ec66d7572ff821749319396470bde.svg    434 KiB          [emitted]  [big]\nassets/9a63495bf1449dfa480feb44e93068fe.png  520 bytes          [emitted]\nassets/9ed4669f524bec38319be63a2ee4ba26.gif   1.68 KiB          [emitted]\nassets/9f6745862ec59fbb7996b08372557212.png  721 bytes          [emitted]\nassets/a0e8a19a97f92a19079f88923b0dd226.png   1.13 KiB          [emitted]\nassets/a50b48a558d13ae2d72c691697330efc.svg  785 bytes          [emitted]\nassets/a51c5608d01acf32df728f299767f82b.gif   3.13 KiB          [emitted]\nassets/a5dc8319bff018855ee0441e4a3b54b9.png  343 bytes          [emitted]\nassets/a8404382a2467b54bb973b9565a2567d.png  522 bytes          [emitted]\nassets/ad3ca81465da6334b9573b197f2e08f4.png  448 bytes          [emitted]\nassets/af7ae505a9eed503f8b8e6982036873e.woff2   75.4 KiB          [emitted]\nassets/b06871f281fee6b241d60582ae9369b9.ttf    162 KiB          [emitted]\nassets/c20270c7806ef19bd902cb5143d55e1c.png  849 bytes          [emitted]\nassets/c5081b04fc1f75da3d815725ebd61c8d.png   1.24 KiB          [emitted]\nassets/c57d26e66ee81c9750a35266329fb15a.png  657 bytes          [emitted]\nassets/ca50d4c073ace660f0c2a812bf269c7a.png   1.54 KiB          [emitted]\nassets/ca817b7e7903ed90baf47c7ad75e280a.png  594 bytes          [emitted]\nassets/cc158654dad038e5648aea8a49078cfa.png   1.02 KiB          [emitted]\nassets/cdb2e2d5f9a8663335cf047791970f9f.png   1.24 KiB          [emitted]\nassets/cf784a27568fb2349c19b2b94c2752e2.png  826 bytes          [emitted]\nassets/cfce5a66786eb61eb171449e06db22b9.png  447 bytes          [emitted]\nassets/d728100efa2c410296dd74e37364b25e.png  575 bytes          [emitted]\nassets/db49c8de4f267eede40a9a8843efcdec.png   3.05 KiB          [emitted]\nassets/dea1324aada21c5fc2f20e95f9fd075f.png   22.6 KiB          [emitted]\nassets/e33cb5c3aef19ce1d7b34e269ac0dc64.png   1.27 KiB          [emitted]\nassets/e417938def579af5c4d237a1cc340445.png  416 bytes          [emitted]\nassets/e75095752dde848823757312f127ebcd.png  454 bytes          [emitted]\nassets/ec107d97171bb75a32bba466ec1f6df4.png  758 bytes          [emitted]\nassets/ece5a3abe6e80a27c06da70414c94e42.png  689 bytes          [emitted]\nassets/eee1cc051bd3c37b21fa1836df3dbfcb.png  770 bytes          [emitted]\nassets/ef571a1808168de9278d5f1a326156a1.png  664 bytes          [emitted]\nassets/fee66e712a8a08eef5805a46892932ad.woff   95.7 KiB          [emitted]\nbase.chunk.js  100 bytes       0  [emitted]         base\nbase.chunk.js.map   90 bytes       0  [emitted]         base\nbase.css    523 KiB       0  [emitted]  [big]  base\nbase.css.map   1.59 MiB          [emitted]\ngeneric.bundled.js   1020 KiB       4  [emitted]  [big]  generic\ngeneric.bundled.js.map   3.31 MiB       4  [emitted]         generic\nlibs.chunk.js   1.97 MiB       1  [emitted]  [big]  libs\nlibs.chunk.js.map   7.56 MiB       1  [emitted]         libs\nlogin.bundled.js   1.01 MiB       5  [emitted]  [big]  login\nlogin.bundled.js.map   3.37 MiB       5  [emitted]         login\nruleCollectionBuilder.chunk.js   68.2 KiB       6  [emitted]         ruleCollectionBuilder\nruleCollectionBuilder.chunk.js.map    215 KiB       6  [emitted]         ruleCollectionBuilder\nvendors~ruleCollectionBuilder.chunk.js   1.03 MiB       7  [emitted]  [big]  vendors~ruleCollectionBuilder\nvendors~ruleCollectionBuilder.chunk.js.map   3.36 MiB       7  [emitted]         vendors~ruleCollectionBuilder\nEntrypoint login [big] = base.css base.chunk.js base.chunk.js.map libs.chunk.js libs.chunk.js.map login.bundled.js login.bundled.js.map\nEntrypoint analysis [big] = base.css base.chunk.js base.chunk.js.map libs.chunk.js libs.chunk.js.map analysis.bundled.js analysis.bundled.js.map\nEntrypoint admin [big] = base.css base.chunk.js base.chunk.js.map libs.chunk.js libs.chunk.js.map admin.bundled.js admin.bundled.js.map\nEntrypoint generic [big] = base.css base.chunk.js base.chunk.js.map libs.chunk.js libs.chunk.js.map generic.bundled.js generic.bundled.js.map\n[7] ./galaxy/scripts/utils/utils.js 10.4 KiB {2} {3} {4} {5} [built]\n[40] ./galaxy/scripts/onload/index.js + 13 modules 15.5 KiB {2} {3} {4} {5} [built]\n| ./galaxy/scripts/onload/index.js 728 bytes [built]\n| ./galaxy/scripts/onload/standardInit.js 3.3 KiB [built]\n| ./galaxy/config/production.js 48 bytes [built]\n| ./galaxy/scripts/onload/defaultAppFactory.js 679 bytes [built]\n| ./galaxy/scripts/onload/globalInits/index.js 1.22 KiB [built]\n| ./galaxy/scripts/utils/installMonitor.js 4.54 KiB [built]\n| ./galaxy/scripts/onload/globalInits/initSentry.js 617 bytes [built]\n| ./galaxy/scripts/onload/globalInits/initTooltips.js 494 bytes [built]\n| ./galaxy/scripts/onload/globalInits/iframesAreTerrible.js 1.11 KiB [built]\n| ./galaxy/scripts/onload/globalInits/onloadWebhooks.js 497 bytes [built]\n| ./galaxy/scripts/onload/globalInits/replace_big_select_inputs.js 1.57 KiB [built]\n| ./galaxy/scripts/onload/globalInits/initTours.js 182 bytes [built]\n| ./galaxy/scripts/onload/globalInits/initModals.js 225 bytes [built]\n| ./galaxy/scripts/utils/mock.js 348 bytes [built]\n[94] (webpack)/buildin/global.js 472 bytes {1} [built]\n[192] ./galaxy/scripts/layout/page.js 6.75 KiB {2} {3} {5} [built]\n[205] ./galaxy/scripts/polyfills.js 2.95 KiB {2} {3} {4} {5} [built]\n[216] ./galaxy/scripts/bundleEntries.js-exposed 75 bytes {2} {3} {4} {5} [built]\n[232] ./galaxy/scripts/bundleEntries.js + 92 modules 693 KiB {2} {3} {4} {5} [built]\n| ./galaxy/scripts/bundleEntries.js 3.99 KiB [built]\n| ./galaxy/scripts/viz/circster.js 35.9 KiB [built]\n| ./galaxy/scripts/layout/modal.js 3.97 KiB [built]\n| ./galaxy/scripts/viz/phyloviz.js 30.5 KiB [built]\n| ./galaxy/scripts/viz/sweepster.js 29.4 KiB [built]\n| ./galaxy/scripts/ui/toast.js 216 bytes [built]\n| ./galaxy/scripts/galaxy.library.js 9.37 KiB [built]\n| ./galaxy/scripts/galaxy.pages.js 1010 bytes [built]\n| ./galaxy/scripts/mvc/history/multi-panel.js 41.2 KiB [built]\n| ./galaxy/scripts/mvc/history/history-view-annotated.js 3.85 KiB [built]\n| ./galaxy/scripts/legacy/grid/grid-view.js 20.2 KiB [built]\n| ./galaxy/scripts/reports/run_stats.js 16.2 KiB [built]\n| ./galaxy/scripts/toolshed/toolshed.groups.js 1.14 KiB [built]\n| ./galaxy/scripts/galaxy.interactive_environments.js 9.07 KiB [built]\n| ./galaxy/scripts/mvc/visualization/chart/chart-client.js 1.94 KiB [built]\n|     + 78 hidden modules\n[533] multi polyfills bundleEntries entry/login 52 bytes {5} [built]\n[855] multi polyfills bundleEntries entry/analysis 52 bytes {3} [built]\n[866] multi polyfills bundleEntries entry/admin 52 bytes {2} [built]\n[873] multi polyfills bundleEntries entry/generic 52 bytes {4} [built]\n[874] ./galaxy/scripts/entry/generic.js 122 bytes {4} [built]\n[936] ./galaxy/scripts/entry/admin/index.js + 248 modules 254 KiB {2} [built]\n| ./galaxy/scripts/entry/admin/index.js 692 bytes [built]\n| ./galaxy/scripts/entry/admin/AdminRouter.js 8.04 KiB [built]\n| ./galaxy/scripts/entry/panels/admin-panel.js 4.73 KiB [built]\n|     + 246 hidden modules\n[937] ./galaxy/scripts/entry/analysis/index.js + 188 modules 407 KiB {3} [built]\n| ./galaxy/scripts/entry/analysis/index.js 855 bytes [built]\n| ./galaxy/scripts/entry/analysis/AnalysisRouter.js 15 KiB [built]\n| ./galaxy/scripts/entry/panels/tool-panel.js 7.84 KiB [built]\n| ./galaxy/scripts/entry/panels/history-panel.js 3.64 KiB [built]\n| ./galaxy/scripts/mvc/tool/tool-form.js 17 KiB [built]\n| ./galaxy/scripts/mvc/entrypoints/view.js 1.04 KiB [built]\n| ./galaxy/scripts/mvc/grid/grid-shared.js 2.2 KiB [built]\n| ./galaxy/scripts/mvc/history/history-list.js 4.26 KiB [built]\n| ./galaxy/scripts/mvc/tool/tool-form-composite.js 26.8 KiB [built]\n| ./galaxy/scripts/mvc/dataset/dataset-error.js 9.44 KiB [built]\n| ./galaxy/scripts/mvc/dataset/dataset-edit-attributes.js 6.44 KiB [built]\n| ./galaxy/scripts/mvc/upload/upload-view.js 6.04 KiB [built]\n| ./galaxy/scripts/mvc/history/options-menu.js 5.95 KiB [built]\n| ./galaxy/scripts/mvc/history/history-view-edit-current.js 18.5 KiB [built]\n| ./galaxy/scripts/mvc/entrypoints/poll.js 820 bytes [built]\n|     + 174 hidden modules\n[938] ./galaxy/scripts/entry/login/index.js + 15 modules 14.6 KiB {5} [built]\n| ./galaxy/scripts/entry/login/index.js 1.07 KiB [built]\n|     + 15 hidden modules\n+ 1233 hidden modules\nWARNING in moment\nMultiple versions of moment found:\n2.20.1 ./~/moment\n2.22.2 ./~/pikaday/~/moment\nWARNING in popper.js\nMultiple versions of popper.js found:\n1.14.7 ./~/popper.js\n1.15.0 ./~/bootstrap-vue/~/popper.js\nCheck how you can resolve duplicate packages:\n\n  \n      \n      GitHub\n  \n  \n    \n\ndarrenscerri/duplicate-package-checker-webpack-plugin\n\n\ud83d\udd75\ufe0f Webpack plugin that warns you when a build contains multiple versions of the same package - darrenscerri/duplicate-package-checker-webpack-plugin\n\n\n  \n  \n    \n    \n  \n  \n\n\nWARNING in asset size limit: The following asset(s) exceed the recommended size limit (244 KiB).\nThis can impact web performance.\nAssets:\nassets/912ec66d7572ff821749319396470bde.svg (434 KiB)\nbase.css (523 KiB)\nlibs.chunk.js (1.97 MiB)\nadmin.bundled.js (1.35 MiB)\nanalysis.bundled.js (1.44 MiB)\ngeneric.bundled.js (1020 KiB)\nlogin.bundled.js (1.01 MiB)\nvendors~ruleCollectionBuilder.chunk.js (1.03 MiB)\nWARNING in entrypoint size limit: The following entrypoint(s) combined asset size exceeds the recommended limit (244 KiB). This can impact web performance.\nEntrypoints:\nlogin (3.49 MiB)\nbase.css\nbase.chunk.js\nlibs.chunk.js\nlogin.bundled.js\nanalysis (3.92 MiB)\nbase.css\nbase.chunk.js\nlibs.chunk.js\nanalysis.bundled.js\nadmin (3.83 MiB)\nbase.css\nbase.chunk.js\nlibs.chunk.js\nadmin.bundled.js\ngeneric (3.47 MiB)\nbase.css\nbase.chunk.js\nlibs.chunk.js\ngeneric.bundled.js\n/Volumes/My Book/Galaxy/galaxy\nActivating virtualenv at .venv\ngalaxy.tool_util.deps WARNING 2020-04-01 14:42:04,343 Path \u2018database/dependencies\u2019 does not exist, ignoring\ngalaxy.tool_util.deps WARNING 2020-04-01 14:42:04,343 Path \u2018database/dependencies\u2019 is not directory, ignoring\ngalaxy.tool_util.deps.conda_util INFO 2020-04-01 14:42:04,480 Installing conda, this may take several minutes.\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100 44.0M  100 44.0M    0     0   258k      0  0:02:54  0:02:54 --:\u2013:--  282k\nERROR: did not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\ngalaxy.tool_util.deps.installable WARNING 2020-04-01 14:44:58,988 Conda installation requested and failed.\ngalaxy.tool_util.deps.conda_util INFO 2020-04-01 14:44:58,989 Installing conda, this may take several minutes.\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100 44.0M  100 44.0M    0     0   279k      0  0:02:41  0:02:41 --:\u2013:--  283k\nERROR: did not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\ngalaxy.tool_util.deps.installable WARNING 2020-04-01 14:47:40,314 Conda installation requested and failed.\nI am eventually given a http link but I am assuming the tool still won\u2019t work because the tools require Conda and Conda has not installed.\nIf I do this\u2026\n\" only  the following lines to:\nconda_auto_install: true\nconda_auto_init: true\"\nI still get:\nActivating virtualenv at .venv\ngalaxy.tool_util.deps.conda_util INFO 2020-04-02 12:08:27,618 Installing conda, this may take several minutes.\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100 44.0M  100 44.0M    0     0   278k      0  0:02:41  0:02:41 --:\u2013:--  284k\nERROR: did not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\ngalaxy.tool_util.deps.installable WARNING 2020-04-02 12:11:09,670 Conda installation requested and failed.\nHow do I go about doing this\u2026.\u201cYou could also install conda outside galaxy and tell galaxy where to find conda. But this can cause some permission/rights issues. Those issues are not hard to solve but you need some \u201ccommandline\u201d experience.\u201d\nwould it be possible to install Conda where galaxy expects it to be i.e database/dependencies\nthank you,"}, {"role": "system", "text": "Ah! I think you have problems because of the space in your path.\nLook at the error:\ndid not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\nAnd look at the path that it should be:\n/Volumes/My Book/Galaxy/galaxy/database/dependencies/_conda\nTry to change /Volumes/My Book to something like  /Volumes/My_Book. Besides your problem now you should not have spaces in folder or file names. Good luck and let us know if this solved it."}], [{"role": "user", "text": "Hello!\nI updated galaxy from 20.09 to 21.05 and tried to invoke my workflow. But i got a message like this:\nAn error occurred\nAn error occurred while updating information with the server. Please contact a Galaxy administrator if the problem persists.\n\ufffcOk \ufffcDetails\nThe following information can assist the developers in finding the source of the error:\n\n{\n  \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"21.05\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"{\\\"err_msg\\\": \\\"Uncaught exception in exposed API method:\\\", \\\"err_code\\\": 0}\",\n    \"responseJSON\": {\n      \"err_msg\": \"Uncaught exception in exposed API method:\",\n      \"err_code\": 0\n    },\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"1970-01-01T00:00:00.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n.....\n.......\n......\n\nand was logged out of the server, and some tasks were not scheduled. When i tried to save the workflow in the workflow editor, i got similar error and also was logged out.\nGalaxy.log says something like this:\ngalaxy.jobs.runners.local DEBUG 2021-10-04 15:23:37,549 [pN:main.web.1,p:514441,w:1,m:0,tN:LocalRunner.work_thread-8] (149) executing job script: /media/transgeneprep/DataStore/galaxy/database/jobs_directory/000/149/galaxy_149.sh\ngalaxy.web_stack.handlers INFO 2021-10-04 15:23:38,179 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] (Job[id=153,tool_id=star_fusion]) Handler '_default_' assigned using 'HANDLER_ASSIGNMENT_METHODS.DB_SKIP_LOCKED' assignment method\ngalaxy.tools.execute DEBUG 2021-10-04 15:23:38,432 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Executed 1 job(s) for tool star_fusion request (1138.948 ms)\ngalaxy.workflow.run DEBUG 2021-10-04 15:23:39,276 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Workflow step 106 of invocation 5 invoked (7473.255 ms)\ngalaxy.web.framework.decorators ERROR 2021-10-04 15:23:51,970 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core3] Uncaught exception in exposed API method:\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n  File \"lib/galaxy/web/framework/decorators.py\", line 184, in decorator\n    rval = func(self, trans, *args, **kwargs)\n  File \"lib/galaxy/webapps/galaxy/api/datasets.py\", line 157, in show\n    return self.hda_serializer.serialize_to_view(dataset,\n  File \"lib/galaxy/managers/base.py\", line 690, in serialize_to_view\n    return self.serialize(item, all_keys, **context)\n  File \"lib/galaxy/managers/hdas.py\", line 436, in serialize\n    return super().serialize(hda, keys, user=user, **context)\n  File \"lib/galaxy/managers/datasets.py\", line 606, in serialize\n    serialized = super().serialize(dataset_assoc, keys, **context)\n  File \"lib/galaxy/managers/base.py\", line 603, in serialize\n    returned[key] = self.serializers[key](item, key, **context)\n  File \"lib/galaxy/managers/hdas.py\", line 403, in <lambda>\n    'resubmitted': lambda i, k, **c: self.hda_manager.has_been_resubmitted(i),\n  File \"lib/galaxy/managers/hdas.py\", line 174, in has_been_resubmitted\n    return self.app.model.context.query(query.exists()).scalar()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3523, in scalar\n    ret = self.one()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3490, in one\n    ret = self.one_or_none()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3459, in one_or_none\n    ret = list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3535, in __iter__\n    return self._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT EXISTS (SELECT 1 \nFROM job_to_output_dataset, job_state_history \nWHERE job_to_output_dataset.dataset_id = ? AND job_state_history.job_id = job_to_output_dataset.job_id AND job_state_history.state = ?) AS anon_1]\n[parameters: (182, <states.RESUBMITTED: 'resubmitted'>)]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.workflow.run ERROR 2021-10-04 15:23:52,919 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Failed to schedule Workflow[id=3,name=main.pipeline.PE.updated.24.08.2021 (imported from uploaded file)], problem occurred on WorkflowStep[index=21,type=tool].\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/workflow/run.py\", line 193, in invoke\n    workflow_invocation.steps.append(workflow_invocation_step)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/attributes.py\", line 294, in __get__\n    return self.impl.get(instance_state(instance), dict_)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/attributes.py\", line 730, in get\n    value = self.callable_(state, passive)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/strategies.py\", line 759, in _load_for_state\n    return self._emit_lazyload(\n  File \"<string>\", line 1, in <lambda>\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/strategies.py\", line 900, in _emit_lazyload\n    q(session)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/ext/baked.py\", line 544, in all\n    return list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/ext/baked.py\", line 444, in __iter__\n    return q._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT (SELECT workflow_invocation_to_subworkflow_invocation_association.subworkflow_invocation_id \nFROM workflow_invocation_to_subworkflow_invocation_association \nWHERE workflow_invocation_to_subworkflow_invocation_association.workflow_invocation_id = workflow_invocation_step.workflow_invocation_id AND workflow_invocation_to_subworkflow_invocation_association.workflow_step_id = workflow_invocation_step.workflow_step_id) AS anon_1, workflow_invocation_step.id AS workflow_invocation_step_id, workflow_invocation_step.create_time AS workflow_invocation_step_create_time, workflow_invocation_step.update_time AS workflow_invocation_step_update_time, workflow_invocation_step.workflow_invocation_id AS workflow_invocation_step_workflow_invocation_id, workflow_invocation_step.workflow_step_id AS workflow_invocation_step_workflow_step_id, workflow_invocation_step.state AS workflow_invocation_step_state, workflow_invocation_step.job_id AS workflow_invocation_step_job_id, workflow_invocation_step.implicit_collection_jobs_id AS workflow_invocation_step_implicit_collection_jobs_id, workflow_invocation_step.action AS workflow_invocation_step_action \nFROM workflow_invocation_step \nWHERE ? = workflow_invocation_step.workflow_invocation_id]\n[parameters: (5,)]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.workflow.run ERROR 2021-10-04 15:23:54,931 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Failed to execute scheduled workflow.\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/workflow/run.py\", line 82, in __invoke\n    outputs = invoker.invoke()\n......\n.....\n.......\n\nor this:\ngalaxy.jobs.handler ERROR 2021-10-04 15:41:35,174 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerStopQueue.monitor_thread] Exception in monitor_step\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/handler.py\", line 927, in monitor\n    self.monitor_step()\n  File \"lib/galaxy/jobs/handler.py\", line 958, in monitor_step\n    newly_deleted_jobs = self.sa_session.query(model.Job).enable_eagerloads(False) \\\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3373, in all\n    return list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3535, in __iter__\n    return self._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT job.id AS job_id, job.create_time AS job_create_time, job.update_time AS job_update_time, job.history_id AS job_history_id, job.library_folder_id AS job_library_folder_id, job.tool_id AS job_tool_id, job.tool_version AS job_tool_version, job.galaxy_version AS job_galaxy_version, job.dynamic_tool_id AS job_dynamic_tool_id, job.state AS job_state, job.info AS job_info, job.copied_from_job_id AS job_copied_from_job_id, job.command_line AS job_command_line, job.dependencies AS job_dependencies, job.job_messages AS job_job_messages, job.param_filename AS job_param_filename, job.runner_name AS job_runner_name_1, job.job_stdout AS job_job_stdout, job.job_stderr AS job_job_stderr, job.tool_stdout AS job_tool_stdout, job.tool_stderr AS job_tool_stderr, job.exit_code AS job_exit_code, job.traceback AS job_traceback, job.session_id AS job_session_id, job.user_id AS job_user_id, job.job_runner_name AS job_job_runner_name, job.job_runner_external_id AS job_job_runner_external_id, job.destination_id AS job_destination_id, job.destination_params AS job_destination_params, job.object_store_id AS job_object_store_id, job.imported AS job_imported, job.params AS job_params, job.handler AS job_handler \nFROM job \nWHERE job.state IN (?, ?, ?) AND job.handler = ?]\n[parameters: (<states.DELETED_NEW: 'deleted_new'>, <states.DELETING: 'deleting'>, <states.STOPPING: 'stop'>, 'main.web.1')]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.jobs.handler DEBUG 2021-10-04 15:41:35,268 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Grabbing WorkflowInvocation failed (serialization failures are ok): (sqlite3.OperationalError) database is locked\n[SQL: UPDATE workflow_invocation SET update_time=?, handler=? WHERE workflow_invocation.id IN (SELECT workflow_invocation.id \nFROM workflow_invocation \nWHERE workflow_invocation.handler IN (?, NULL) AND workflow_invocation.state = ? ORDER BY workflow_invocation.id)]\n[parameters: ('2021-10-04 12:41:30.263975', 'main.web.1', '_default_', 'new')]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:45,073 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Exception during reset or similar\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:45,073 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Exception closing connection <sqlite3.Connection object at 0x7f6c64150300>\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n    self._dialect.do_close(connection)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n    dbapi_connection.close()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\ngalaxy.webapps.galaxy.controllers.user DEBUG 2021-10-04 15:41:45,433 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core2] trans.app.config.auth_config_file: /media/transgeneprep/DataStore/galaxy/config/auth_conf.xml\ngalaxy.auth.providers.localdb DEBUG 2021-10-04 15:41:45,466 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core2] User: wormball@gmail.com, LOCALDB: True\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:56,525 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] Exception during reset or similar\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102974043904 and this is thread id 140102437172992.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n    self._dialect.do_close(connection)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n    dbapi_connection.close()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102974043904 and this is thread id 140102437172992.\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.jobs.handler ERROR 2021-10-04 15:42:08,088 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerStopQueue.monitor_thread] Exception in monitor_step\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n.....\n.........\n........\n\nI tried several times but got about the same result (however number of scheduled steps was different every time). However scheduled steps are performed as charm (at least now that i corrected paths and invoked data managers). Similar behavior is also observed when i purge histories (but not when i delete history steps).\nIs there a way to overcome these errors?\nThanks in advance."}, {"role": "system", "text": "Did you restarted galaxy (stopped it completely) between the retries and made sure there were no running processes anymore?"}, {"role": "user", "text": "Several times."}, {"role": "system", "text": "Hmm strange, what if you excute the following command:\nlsof <data_dir>/universe.sqlite the default data_dir is galaxy/database/.\nIf you get some results you could kill that process and try to start galaxy again."}, {"role": "user", "text": "transgeneprep@transgeneprep-System-Product-Name:/media/transgeneprep/DataStore/galaxy/database$ lsof universe.sqlite\nlsof: WARNING: can't stat() fuse file system /root/.cache/doc\n      Output information may be incomplete.\nlsof: WARNING: can't stat() fuse.gvfsd-fuse file system /root/.cache/gvfs\n      Output information may be incomplete.\ntransgeneprep@transgeneprep-System-Product-Name:/media/transgeneprep/DataStore/galaxy/database$ sudo lsof universe.sqlite\nlsof: WARNING: can't stat() fuse.gvfsd-fuse file system /run/user/1000/gvfs\n      Output information may be incomplete.\nlsof: WARNING: can't stat() fuse file system /run/user/1000/doc\n      Output information may be incomplete.\n\nI tried again and got the same error. However now i also got this error running optitype:\ndocker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create: dial unix /var/run/docker.sock: connect: permission denied.\nSee 'docker run --help'.\n\nBut when i logged in and ran optitype again, it worked fine, however it logged me out again some time later without any apparent reason."}, {"role": "system", "text": "It is hard to debug. So in the basics a sqlite database can only be used by one process at the time. (Only one process can write to that database at the time and during writing it will be locked). There is a situation where it can be locked if the database file is located on a NFS-mounted or remote drive but if it worked before this is probably not the case. Here I read that it can also happen if you have unsaved actions Python SQLite: database is locked - Stack Overflow but I would not know how to solve that. Maybe if you dont have anything important in that database you could create a new one."}], [{"role": "user", "text": "I am getting error with the STR DETECTION. This file cannot be used for further analysis.\n\n/corral4/main/jobs/040/701/40701606/galaxy_40701606.sh: line 123: galaxy-set-metadata: command not found"}, {"role": "system", "text": "Hi @libinglin\nDid you already click on the link to retry \u201cauto-detection\u201d? If not, do that first.\nThis looks like a job that failed for a technical reason. It was probably a cluster hiccup or there might be some problem with the format of the dataset. Common reasons include: partially uploaded data or possibly an uneven number of columns per row or the upstream tool failing to fully output the result. If the latter, you should try a rerun of that upstream tool."}, {"role": "user", "text": "Hello\nyeah, I click on the link to retry \u201cauto-detection\u201d. I Add an annotation to this dataset,but the next \u201ccount\u201d missing columns of this database.\n\n00001311\u00d7527 14.4 KB\n\n\n1111309\u00d7606 21.1 KB\n\nIn addition, I found that all the data in this database are right aligned. Is there a problem with the format?\n\n2221130\u00d7652 41.6 KB\n\nThanks"}, {"role": "user", "text": "The figure below\nNo erroneous data was previously analyzed\n\n33331245\u00d7552 58.3 KB\n"}, {"role": "user", "text": "Dear Jennaj\nyeah, I click on the link to retry \u201cauto-detection\u201d. I Add an annotation to this dataset,but the next \u201ccount\u201d missing columns of this database.\nIn addition, I found that all the data in this database are right aligned. Is there a problem with the format?\n  \n    \n    \n    \"An error occurred setting metadata for this data set\"-STR DETECTION \n  \n  \n    Hello \nyeah, I click on the link to retry \u201cauto-detection\u201d. I Add an annotation to this dataset,but the next \u201ccount\u201d missing columns of this database. \n [0000] \n[111] \nIn addition, I found that all the data in this database are right aligned. Is there a problem with the format? \n [222] \nThanks\n  \n\n"}, {"role": "system", "text": "Hi @libinglin\nIt looks like you are using a custom reference fasta for hg38 in some steps, and in other steps, you are assigning the built-in index for hg38.\nMixing up which reference genome is used in the same analysis almost never works out.\nTry using the same exact reference genome for all steps. If you really need to use a custom genome, promote it to a custom build (with a custom \u201cdatabase aka dbkey\u201d that isn\u2019t already in use), then assign that custom database to your data.\nPlease note that using a very large custom genome fasta will cause some tools to run out of memory, including the \u201ccustom build\u201d functionality. \u201cVery large\u201d includes the human genome, most other mammalian genomes, some plant genomes. For these cases, using the built-in index is a better choice.\nI\u2019ve added a tag to your post that links to other Q&A that describes how to format a custom genome plus how to create a custom build from one. You should start with the fasta formatting first, then try to create a build from it (will likely fail but you can try). Or, you can go directly to using the built-in indexed version of that same genome instead."}], [{"role": "user", "text": "Hi\nI just finished deep learning tutorials and still some points are not clarified for me. for example : How do i should determine the number of neurons, batch size and epochs?\nRegards\nAmir"}, {"role": "system", "text": "Hi @amir,\nI informed the responsible for the training; he will probably answer you in the next few days.\nRegards"}, {"role": "user", "text": "Thank you. I think still there is a lot of problems in tools related to deep learning. it would be great if some one consider to solving those problems.\nBest\nAmir"}, {"role": "system", "text": "Could you provide me with some details about additional problems?  Thank you so much for your feedback. Regards!"}, {"role": "user", "text": "Hi\nThank you for your response.\nYou can see the first problem in attached file.\n\nUntitled392\u00d7576 14.2 KB\n\nit seems the deep learning tool still has some main problems, I tried some other options but still no progress.\nalso, this is galaxy training datasets.\nBest\nAmir"}, {"role": "system", "text": "Hi Amir,\nThanks for using the deep learning tutorials. I agree these \u201cjobs\u201d shown in grey did not start because they require GPUs to process and none of them is free at the moment. Once, at least one or more become free, these jobs would start. We are currently working on finding an ideal job scheduling scheme for such tools to maximize the usage of CPUs and GPUs that we currently have."}], [{"role": "user", "text": "Hi guys! I had a Galaxy installed in a local server, which I had used for 2 years. However, the TI guy updated the Linux, as well as change the folders, because he made an HDs upgrade too. After that, I can\u2019t open anymore the old Galaxy version, but I have the folder with all datasets and a lot of data analyzed. We tried to re-install the old version, but we can\u2019t, cause an incompatibility of systems versions as well as the Python version. We tried unsuccessfully a lot of times and in different ways.\nHow can I recover these data in an installed new version of Galaxy?\nthanks for any help.\nLeandro"}, {"role": "system", "text": "Hi @lfdeoliveira\nThe admin docs are here https://docs.galaxyproject.org/ but your situation is a bit tricky. Older tools may not work in the current release. That could impact workflows. If the goal is just to migrate data, that is simpler (comparatively).\nLet\u2019s get some expert help at the admin Gitter chat. They may reply here or there, and feel free to join the chat galaxyproject/admins - Gitter\nMeanwhile, if you could clarify what is working versus not, that would help. The usual advice is to update the original to each newer release, in order, until the database is current. Several releases over the last two years involved significant changes (and others have had problems with larger \u201chops\u201d).\nLet\u2019s start there"}, {"role": "system", "text": "Make sure you backup your database!\nYou can try setting up a fresh copy of the database, and a fresh copy of Galaxy. It will detect the database, and then if everything goes well, it starts migrating up to the latest version of the database schema. Generally database migrations just work, but always backup first in case!\nThat might work. You\u2019ll need to configure your galaxy with the same tools and same path to data as the old galaxy was configured, and that can take some time.\nUsually we recommend going release by release as @jennaj mentions, but sometimes that isn\u2019t an option."}], [{"role": "user", "text": "Hi,\nI have about 5000 variants on one chromosome that I would like to phase in a few families. I have the data from NGS in a tab delimited format. Can this be done with Galaxy tools?\nthanks"}, {"role": "system", "text": "Hi @kingab,\nBeagle seems to the most suitable tool for performing such analysis, but currently is it not available in Galaxy;  If you consider it appropriate (or have any other suggestion) it can be included.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nthanks a lot for your reply. I would find this application very useful, however I am not sure if you need to reach a \u2018critical mass\u2019 of interest before introducing new tools to Galaxy\u2026 thanks for considering!\nRegards,\nKinga"}, {"role": "system", "text": "I\u2019ll work on it, doesn\u2019t seem a very complex tool to implement; I will try to make it available for next week.\nRegards"}, {"role": "user", "text": "Wow, that\u2019s amazing thanks @gallardoalba !"}, {"role": "user", "text": "Hello @gallardoalba, I was wondering if you ware able to implement the Beagle to galaxy\u2026 thank you,\nK"}, {"role": "system", "text": "Hi @kingab,\na PR has been opened in order to include this tool: Pull Request #3760.\nRegards"}], [{"role": "user", "text": "I am trying to assemble a bacterial genome sequenced with long reads (miNION), with Minimap2/Miniasm. The assembly works fine (it produces 1 contig; the same as with Flye and Raven+Racon) but then I need to polish the assembly, and Minipolish is not in the Galaxy toolshed.\nDoes anyone have any suggestions to overcome the absence of Minipolish? I would like to feed the three assemblies to Trycycler/Medaka to get a more robust asembly, but it seems that polishing after Miniasm assembly is essential\u2026"}, {"role": "system", "text": "Hi @iker,\ncurrently, the best alternative would be to use Racon. I\u2019ll work in order to include minipolish for the next week.\nRegards"}, {"role": "user", "text": "Thanks very much for including Minipolish. It will allow us to complete the Minimap2/Miniasm/Minipolish workflow\u2026\nAbout Racon\u2026 That is what I thought in the beginning and I tried it. There must be something I am doing wrong because I always get an error\u2026 I transform the GFA into fasta (using the tool GFA to Fasta). Then I feed Racon with the three files: the raw reads (I\u2019ve tried fastq and fastqsanger), the overlaps (I\u2019ve tried the PAF file obtained with Minimap2 and a SAM created from the BAM obtained with Minimap2 too (-O BAM), and  the draft assembly (in fasta). I always get an error: empty overlap set! But the overlap file is not empty\u2026 Any clues why? I attach a screenshot of the look of the PAF file as seen in Galaxy\n\nimage1235\u00d7434 115 KB\n\nThanks very much again,\niker"}, {"role": "system", "text": "Hi @iker,\ncould you share your Galaxy history with me? Also, minipolish will be available from Monday.\nRegards"}, {"role": "user", "text": "Done!\nThanks very much again!"}, {"role": "system", "text": "Hi @iker,\nas I mentioned to you in the email, the problem is that, in order to generate the overlap file, the reads should be mapped against the reference genome.\nRegards"}], [{"role": "user", "text": "I am trying to transfer a history from usegalaxy.org to usegalaxy.eu, however, the import job fails with a python error. The history has been exported to a URL and published on the .org side.\nHistory:\nGalaxy | Published History | Unnamed history (~6GB)\nEdit: Further testing with a very small history (70 bytes) yields the same error.\nFurther testing to import the same exported history from usegalaxy.org into usegalaxy.no yields the same error. Further testing exporting a history from usegalaxy.eu into usegalaxy.no results in the same error.\nI conclude that this has to be a general problem with these Galaxy instances, or the function has been disabled, in which case it should be removed from the UI, or the documentation is misleading and the functionality is not to be used this way.\nTraceback (most recent call last):\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 92, in <module>\n    main(options, args)\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 73, in main\n    check_archive(archive_file, dest_dir)\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 38, in check_archive\n    with tarfile.open(archive_file, mode=\"r\") as archive_fp:\n  File \"/usr/lib64/python3.8/tarfile.py\", line 1608, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n"}, {"role": "system", "text": "Hi @Michael,\nthank you for reporting the problem. I can confirm the problem. I suggest you download the history to your local computer and upload it to the different server as a workaround. An issue will be opened in order to fix it.\nRegards"}, {"role": "system", "text": "Hi @Michael,\nthe problem is that the history should be publically accessible. Regards"}, {"role": "user", "text": "\nScreen Shot 2022-10-17 at 16.12.34 PM2012\u00d7368 51.1 KB\n\nThe histories I have used for testing are all public and accessible."}, {"role": "system", "text": "Thanks, I\u2019ll report it."}, {"role": "system", "text": "This is working fine for me, can you share the export link ?"}], [{"role": "user", "text": "Hi,\nI want to specify an input parameter that allows selection of one and only one output from the history. I read the XML specifications, but still don\u2019t know how to fulfill this goal.\nThanks for reading."}, {"role": "system", "text": "True, the input selects for tools are designed to allow end users to pick the input as one dataset, multiple datasets (individual), or multiple datasets (collection). To not allow multiple/collections selects will limit the utility of the tool.\nSome tools consume all inputs into the same job but most run in \u201cbatch mode\u201d (one job per input). Maybe batch mode would be meet your goals? It is the default behavior and is how most tools function.\nhttps://docs.galaxyproject.org/en/master/dev/schema.html#tool-inputs-section\nI asked at the Galaxy Gitter channel to make sure this is all correct and there isn\u2019t some other method. Please join in there or just follow along: https://gitter.im/galaxyproject/Lobby?at=5c1012c4f992693c7a5b1fe2"}, {"role": "system", "text": "@SamGG can you give us a use case? type=\"data\" should to exactly this. By default the input is only one file. A use is not able to pass multiple files to one job-instance of this tool."}, {"role": "user", "text": "Hi. I would like that the end-user is not tempted by the middle or the right button (capture below). So, I would like to propose a simple popup menu (aka select) filled with a list of SMILES chemical results. The processes are very simple: one smi input or a set of parameters, one smi output and a HTML report. There is no reason to process many files at once.\nIn the following capture, I would like to get only the menu, or the buttons with only the leftmost activated, or any way to select only one single file from the history.\nHope I am clear.\n"}, {"role": "system", "text": "@SamGG I understand that the additional buttons might be confusing to the user - but its part of the general unified Galaxy UI between tools. And once users learned what it means it would be confusing if special tools allow/disallow these buttons. Just to make sure I understood your usecase - you only want to process one-job-at-a-time. Means you take one SMILES file and e.g. convert it to InChI. That should still work isn\u2019t it? You are just worried about confusion? The additional buttons are actually giving more power to the user, without scarifying the intention of the tool author. E.g in one of my workflow I create 1.000 SMILES files. If I know want to use your tool - I can still use it on all 1000 files at once. Which would not be feasible if we remove those buttons. In my use-case 1000 jobs will be started. So one job still just take one SMILES file."}, {"role": "user", "text": "@bjoern.gruening Hi Bjoern, you get my point. I am worried about the fact that the processing we set up is quite intensive and has no reason to be applied on many smi files at the same time. It would be OK if I can check that the user has selected only one single file. Can this be achieved using a validator statment?\nBest."}, {"role": "system", "text": "We need to distinguish between different layers here. On the tool level - what a tool developer can control in the XML file - you have done a good job. You made sure that only one file can be processed at a time. its not possible to put multiple files into one job. Now you want to avoid that a single user can submit many jobs, aka run your tool on 1000 individual datasets, producing 1000 of different jobs, right? This is to be controlled on the Galaxy Admin level. So the Galaxy admin, that worries about computational load can set this. You can say a user is only allowed to submit 3 concurrent jobs etc \u2026 or you can say this tool get a lower priority on your cluster. Which means a user can still submit 1000 of jobs but they will be processed with a lower priority. There are multiple of these setting to keep the load not to high on our cluster. The important thing here is, that these are different level: 1) tool level - defining an interface, and 2) Admin level keeping the cluster load reasonable.\nPlease let me know if this is unclear, I know my explanations can be bad "}], [{"role": "user", "text": "Hello,\nI want to convert SRA files from GEO into fastq files in order to map them on the genome. I have uploaded SRA files directly on Galaxy but it does not seem to be the right thing to do.\nNow, I\u2019m going through the instructions on the support page of Galaxy: NCBI SRA Fastq\nIt asks to manipulate the file before importing to Galaxy but I cannot see mentioned tools on the NCBI SRA Run Selector page (highlighted above). Is there a more detailed explanation or tutorial to follow because the link I shared seems to be the thing I need but it is not very clear to me how to proceed.\nThank you."}, {"role": "system", "text": "@ysrbrs,\nFirst you isolate the IDs:\n\nOrganizing metadata\nThe \u201cRunInfo Table\u201d provides the experimental condition and replicate structure of all of the samples. Prior to importing the data, we need to parse this file into individual files that contain the sample IDs of the replicates in each condition. This can be achieved by using a combination of the \u2018group\u2019, \u2018compare two datasets\u2019, \u2018filter\u2019, and \u2018cut\u2019 tools to end up with single column lists of sample IDs (SRRxxxxx) corresponding to each condition.\n\nThen you import IDs with Download and Extract Reads in FASTA/Q; which corresponds to NCBI SRA Tools (fastq-dump):\n\nWhat it does?\nThis tool extracts data (in fastq format) from the Short Read Archive (SRA) at the National Center for Biotechnology Information (NCBI). It is based on the fastq-dump utility of the SRA Toolkit.\n"}, {"role": "user", "text": "Thank you for your reply!\nI have done what\u2019s recommended but now I have 18 individual fastqsanger.gz files but I want to group them by condition, replica. I have 6 different conditions with 3 replicates each.\nAny tutorial advice on how to do it? It would be more organized this way before going with trimming and alignment etc. In addition, I have to do the same analysis with a much bigger dataset so I would be happy to learn it."}, {"role": "user", "text": "https://galaxyproject.org/tutorials/collections/\nThis tutorial has not been helpful because my fastqsanger.gz files aren\u2019t directly on the history pane but inside a folder \u201cSingle-end data (fasterq-dump)\u201d. That is why I cannot see check box on upper right as shown on the link."}, {"role": "system", "text": "@ysrbrs, you understand that the datasets are organized by collections?\nCan you please paste a screenshot so we can see how your history looks like?"}, {"role": "system", "text": "Hi @ysrbrs,\nprobably in that case, the faster way would be to upload the datasets in groups, according to your replicates/conditions.\nRegards"}, {"role": "system", "text": "Hi @ysrbrs\nThat FAQ is intended to help resolve issues around format variations \u2013 after the data is in Galaxy. The collections GTN tutorial is an overview of the ways to manipulate data in collections.\n@David is correct about the output of the collection from this tool,  and the advice from @gallardoalba is a good simple way to get your data organized at the start.\nShould you want to explore advanced methods to download data in a structured format from the very start, see the GTN tutorials here under the section Uploading Data \u2013 the two about the \u201cRule Based Uploader\u201d are what you will be doing, and the one above it provides context and the one below it demonstrates usage in the context of a practical example. Many other tutorials cover the Upload/Collection functions \u2013 use the search at that site to find them. Even it is not exactly your use case, can still be helpful.\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Dear all, @David @gallardoalba\nThank you for your responses. I somehow did not see these messages. Sorry for the late reply.\nI figured out that, in Galaxy, it\u2019s easier to merge multiple files into a data collection, than splitting a collection into multiple files.\nSo, I uploaded multiple Acc_List.txt in groups depending on the condition. Each contained only three (replicates) SRA Run accessions for each sample."}], [{"role": "user", "text": "Dear support team at GALAXY,\nI have recently performed a ChIP-seq experiment and I have just got my raw seq files from the seq service. I would like to perform my analysis but each of my samples was sequenced twice; meaning that each sample has two seq files. This is because each sample was sequenced twice to get a workable number of total reads (because of very low concentration of my ChIPed DNA). Thus, I am looking for a tool in GALAXY that will allow me to merge the two seq files of the same samples in to just one, with whom I will then proceed to the mapping with Bowtie2, the Peak calling with MACS2 etc. Could you please propose me a tool that I can properly perform such a task of merging?\nMany thanks in advance,\nGreetings,\nManolis"}, {"role": "system", "text": "Hi @Manolis1\nTry this tool (per sample pair):  Concatenate datasets tail-to-head (cat)"}, {"role": "user", "text": "Hi @ jennaj,\nMany thanks for your fast response.\nI did it but I got this below:\n\nWhen you look into it:\n\nFor info: The format of my 2x files that I was about to merge was:\n\nWhen you look into them file-1 (the 3x first lines):\n\nFile-2 (the 3x first lines):\n\nWhat am I doing wrong?\nGreetings,\nManolis"}, {"role": "system", "text": "Hi @Manolis1\nThis tool tends to work best with uncompressed inputs.\nPlease try uncompressing the inputs, then run the tool. Galaxy does some intermediate steps to transform the data during processing (in this case: fastq > plain text > fastq). When compressed fastq is input, sometimes a compression version isn\u2019t exactly what is expected at a technical level. Uncompressing first will usually avoid problems with any text manipulation tool.\nTo uncompress a fastqsanger.gz dataset:\n\nClick on the pencil icon to reach the Edit Attributes functions\nClick into the Convert tab\nThe drop-down menu will include a choice to produce an uncompressed version of the data (these will be new datasets)\nRun the Concatinate tool on the uncompressed fastqsanger datasets\nAfter everything is completely done and confirmed to be correct, go back and permanently delete (purge) the original compressed data to remove it from your quota usage.\n\nI tested this out last week for another person (same exact tool/problem) and it worked, but please let us know if that doesn\u2019t work for your case and we can troubleshoot more. "}], [{"role": "user", "text": "Hello all,\nI\u2019ve been struggling here for days now on attempting to get multiple factors to work in Galaxy for either EdgeR OR limma. I\u2019ve searched every post about this and each says it\u2019s a header issue. It is NOT a header issue. If I have only one factor and input the 5 separate groups within that one factor, each with it\u2019s own count file it works fine and I can compare between each via contrasts. The count files have headers as GeneID for column1 and the unique sample name in column 2. However, when I attempt to add a separate Factor and include use the same files that worked in the previous step\u2026I always get a duplicate row name error. I\u2019ve tried even doing this via a count matrix file with all of the samples and counts\u2026as well as a Factor info file; for both I followed the EXACT format as described on the package help section displayed if you scroll down. This did not work either.\nCurrently, I have absolutely no way to utilize the platform to peform DE analysis with the appropriate factor levels. Again, this is not a header issue with the counts files. I\u2019ve tried this with both htseq counts (which I manually entered the header) AND featurecounts. These work fine if I only have one Factor level\u2026as soon as I add an additional factor level it fails, despite being the exact same files that worked previously. I REALLY need help. I\u2019d be greatly appreciative as my advisor needs this analysis for a grant application."}, {"role": "system", "text": "@Peter_Jon can you share your history. I will have a look, this sounds super strange. Btw. is deseq2 working?"}, {"role": "user", "text": "I\u2019ve only really tried to do this extensively with EdgeR and limma, but it didn\u2019t work for DESeq2 either. How can I share my history to you?"}, {"role": "user", "text": "Annotation 2020-08-07 1559381445\u00d7909 21.6 KB\n\u201cError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: rownames<- \u2026 row.names<- -> row.names<-.data.frame -> .rowNamesDF<-\nIn addition: Warning message:\nnon-unique values when setting \u2018row.names\u2019:\nExecution halted\u201d\nI could show you that all of my counts files have the correct format based upon the guide."}, {"role": "system", "text": "See here for how to share histories: https://galaxyproject.org/learn/share/\nAlso described here: https://galaxyproject.github.io/training-material/topics/introduction/tutorials/galaxy-intro-101/tutorial.html\nAnd here are many transcriptomics tutorials: https://galaxyproject.github.io/training-material/topics/transcriptomics/\nI will have a look into your history tomorrow.\nCiao,\nBjoern"}, {"role": "user", "text": "https://usegalaxy.org/u/pferran/h/rnaseq-de-5-groups-3-replicates-edger-limma"}, {"role": "system", "text": "I just glanced over it as it is late here, but your files in both factors needs to be the same. Please consider the example of treated vs. un-treated and single vs. paired in this tutorial as example: https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#analysis-of-the-differential-gene-expression for a 2 factor study.\nI hope that helps,\nBjoern"}], [{"role": "user", "text": "Hello,\nPlease I have a question regarding ChIP-Seq data analysis using Galaxy platform! During the trimming step (using Trim Galore) for ChIP-Seq data, I got error message:\nFatal error: Exit code 1 ()\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\n\n\nAUTO-DETECTING ADAPTER TYPE\n===========================\nAttempting to auto-detect adapter type from the first 1 million sequences of the first file (>> input_1.fastq.gz <<)\n\nFound perfect matches for the following adapter sequences:\nAdapter type\tCount\tSequence\tSequences analysed\tPercentage\nIllumina\t207\tAGATCGGAAGAGC\t1000000\t0.02\nsmallRNA\t3\tTGGAATTCTCGG\t1000000\t0.00\nNextera\t2\tCTGTCTCTTATA\t1000000\t0.00\nUsing Illumina adapter for trimming (count: 207). Second best hit was smallRNA (count: 3)\n\n\ngzip: stdout: Broken pipe\nWriting report to './input_1.fastq.gz_trimming_report.txt'\n\nSUMMARISING RUN PARAMETERS\n==========================\nInput filename: input_1.fastq.gz\nTrimming mode: single-end\nTrim Galore version: 0.4.3\nCutadapt version: 1.13\nQuality Phred score cutoff: 15\nQuality encoding type selected: ASCII+33\nAdapter sequence: 'AGATCGGAAGAGC' (Illumina TruSeq, Sanger iPCR; auto-detected)\nMaximum trimming error rate: 0.1 (default)\nMinimum required adapter overlap (stringency): 3 bp\nMinimum required sequence length before a sequence gets removed: 20 bp\nOutput file(s) will be GZIP compressed\n\nWriting final adapter and quality trimmed output to input_1_trimmed.fq.gz\n\n\n  >>> Now performing quality (cutoff 15) and adapter trimming in a single pass for the adapter sequence: 'AGATCGGAAGAGC' from file input_1.fastq.gz <<< \n10000000 sequences processed\n20000000 sequences processed\n30000000 sequences processed\n40000000 sequences processed\n50000000 sequences processed\n60000000 sequences processed\n70000000 sequences processed\nThis is cutadapt 1.13 with Python 3.6.1\nCommand line parameters: -f fastq -e 0.1 -q 15 -O 3 -a AGATCGGAAGAGC input_1.fastq.gz\nTrimming 1 adapter with at most 10.0% errors in single-end mode ...\ncutadapt: error: Line 1 in FASTQ file is expected to start with '@', but found '\\n'\n\n\nCutadapt terminated with exit signal: '256'.\nTerminating Trim Galore run, please check error message(s) to get an idea what went wrong...\n\n\ngzip: stdout: Broken pipe\n\nplease if you can help me solve it!\nThank you very much!\nSham"}, {"role": "system", "text": "Hi!\nIf you look closely in your error message you\u2019ll see the following line:\n\n\n\n shamjdeed:\n\ncutadapt: error: Line 1 in FASTQ file is expected to start with \u2018@\u2019, but found \u2018\\n\u2019\n\n\nSo something is wrong with your FASTQ file and there\u2019s either a blank line inserted, or something else is strange. Hope this helps!"}, {"role": "user", "text": "Hi hxr,\nThank you  for your reply, yes I noticed this but the FASTQ file I am using starts like this: @SRR6128868.1.1 1 length=76 ! as normal!"}, {"role": "system", "text": "HI @shamjdeed \u2013 The problematic formatting may be internal, not at the start. The error is a bit misleading.\n\nLine 1 in FASTQ file\n\nCould be interpreted as:\n\nLine 1 in FASTQ record\n\nThis can be due to:\n\n\nTruncated dataset \u2013 usually occurs because Upload was not complete, or the file was truncated from an earlier data transfer upstream from Galaxy.\n\n\nManipulated dataset \u2013 most often seen when multiple fastq datasets were concatenated together, and one or more contained an empty blank line at the end which ends up somewhere in the middle of the final dataset. This could happen in Galaxy or upstream from Galaxy.\n\n\nTools that can help find the problem:\n\n\nFastq Groomer \u2013 this tool has enhanced data checks.\n\nUse all default settings.\nData that is already in fastqsanger format (and assigned that datatype) will remain unchanged in the new output if there are no problems and could be permanently deleted once the QA is done/passes to reduce quota usage/duplicate data.\nIf there are some formatting problems, the first malformed fastq record will be reported in the error message. Be aware that there could be more problems, but this tool can give some clues about where/what an example problem is, so you can find all occurrences like it.\n\n\n\nSelect last lines of a dataset (tail) \u2013 this is how to review the ends of files to see if they are truncated or contain empty blank trailing lines.\n\nThe last 10 lines (the default) should be sufficient.\n\n\n\nAdvanced methods \u2013 to examine (any) data in more depth, try using tools in these groups:\n\nText Manipulation\nFilter and Sort\nJoin, Subtract and Group\n\n\n\nReference: FAQ for Fastq formatting\n\nCommon datatypes explained\nSee >> https://galaxyproject.org/learn/datatypes/#fastq\n\n\nHope that helps!"}], [{"role": "user", "text": "I am tryng to run a very small tblastn, with only three query sequences.  Both a conventional FASTA file and a BlastDB for the subject are sitting \u201cThis Job is Waiting to Run\u201d\nIs there a problem with the server?"}, {"role": "system", "text": "Can you point what is the actual server you are using (galaxy.eu, galaxy.org, galaxy.org.au., galaxy.be) ?"}, {"role": "system", "text": "I\u2019m also having this problem, using the main server (usegalaxy.org). I have been trying to run a job with the tool Trinity for about 5 days. After the job was in queue for 4 days, I thought maybe I used the wrong input, but the other possibility has now been queued for almost a day.\nBy the way, I\u2019m doing the Galaxy training tutorial \u201cDe novo transcriptome assembly, annotation, and differential expression analysis\u201d and this step is projected to take 2 hours.\nIs there a backlog of jobs in the queue or is something else wrong?"}, {"role": "system", "text": "@tom_abrams_lab @jaredbernard What about these troubleshooting tips, have you guys checked?\nI\u2019m not from galaxy team, btw, I\u2019m just an user trying to help.\nMaybe the guys from usegalaxy can answer properly."}, {"role": "system", "text": "I did look at the troubleshooting tips, but nothing about delayed jobs. Other jobs I\u2019ve submitted in the meantime have run, including a somewhat large job with the Maker tool.  Could there be delays with specific tools (e.g. Trinity or BlastDB)? It would be nice if someone from Galaxy Team could look into it."}, {"role": "system", "text": "Hi all,\nThe usegalaxy.org server is very very busy (and has been for about a week).\nThe best advice is to leave the queued jobs queued and to avoid rerunning, otherwise, the new jobs will end up back at the end of the queue again.\nI\u2019ve also checked in with our administrator @nate, and there is a bit of a backlog. This should clear up \u2013 how long to completely clear up depends on when you originally started the jobs and how many jobs you have queued.\nThe above backlog mostly applies to Trinity, Unicycler, and SPAdes but not BLAST (that last tool runs on a different cluster, also busy, but not as busy.\nFAQ: How Jobs Execute\nHope that helps & thank you @David for helping first!!"}], [{"role": "user", "text": "Hi all,\nI am trying to install galaxy in my own compurter and I got this error when I run common \u201csh run.sh\u201d.\n\u201cFile \u201c/anaconda3/envs/galaxy/lib/python3.6/site-packages/filelock/init.py\u201d, line 8\nfrom future import annotations\n^\nSyntaxError: future feature annotations is not defined\u201d\nI understand the error means that I need a newer version of python, however, I do have a python version 3.9, but it still runs in the python version 3.6. So I am not sure where went wrong.\nPlease help, thank you."}, {"role": "system", "text": "Hi @keee,\ncould you confirm that you are using the last Galaxy release:\ngit clone -b release_21.09 https://github.com/galaxyproject/galaxy.git\n\nRegards"}, {"role": "user", "text": "Hi,\nYes, it is what I have.\nBest,\nKe"}, {"role": "user", "text": "Do I have to creat an environment, python 3.6.10, to be able to run galaxy?\nBest,\nKe"}, {"role": "system", "text": "@keee Galaxy is working with Python 3.6 - 3.10. So that should not be the problem, but somehow you got a newer filelock package.\nCan you remove your Galaxy env and recreate it? If you like you can recreate it with python 3.8 or so.\nconda create -n galaxy python=3.9"}, {"role": "user", "text": "Hi,\nI did this as well, but when i try to run it will still use python3.6 for some reason, does not matter which conda environnment I am in. So, I have to eventually create a virtualenv to run the \u201csh run.sh\u201d commond. It worked for me, I guess the problem is sloved, thanks everyone."}, {"role": "user", "text": "If anyone is on Mac and installed conda, and have same issue with me. You need to:\n\nupdate your python to 3.9 or newer (3.7/8 will also have python errors, I tried\nremove galaxy environment that created automatically since you have conda installed:\n\nconda env remove -n _galaxy_\n\n\ncreated a virtualenv on your computer and then run:\nsh run.sh --no-create-venv\nflag is needed otherwise it will created the galaxy environment again\u2026\n"}, {"role": "system", "text": "\n\n\n keee:\n\nSyntaxError: future feature annotations is not defined\u201d\n\n\nHi @keee,\nthat problem has been recently fixed, but is not included in the 21.09 release. If you get the dev branch, it should work fine:\ngit clone https://github.com/galaxyproject/galaxy.git\n\nRegards"}], [{"role": "user", "text": "Hi I\u2019m studying on a plant that has  no reference genome  and only has  one scaffold assembly  and  one gff3  annotation file. If I want to do  de novo assembly  using the  Galaxy  Can I import the links of 20 fastq files from the RNA-seq of the samples together to construct the Reference Genome with  Trinity ? If this is possible, please help me import the links of these files together into Galaxy and make the reference genome. Maximum number of samples or data can be used for Deno Assembly in Galaxy?\nThanks a lot"}, {"role": "system", "text": "Welcome, @snt\nIn short, yes you can assemble within Galaxy. Trinity is designed to assemble RNA-seq reads into a Transcriptome Assembly (not Genome). Genome assembly from WGS reads works best with smaller genomes (procaryotic) when working at public Galaxy servers due to resources (Unicyler is one tool choice for that purpose).\nThe reads need to be prepared and input properly. The upper limit of what can be assembled is usually based more on the read content and genome size/characteristics, rather than the volume/size of the read data. You won\u2019t know until you try, to see how your data will assemble at any particular server (your own or a public site). The help below covers QA, input format/structures, and advice about downsampling as needed.\nAlso, keep in mind that some plant genomes can be quite complicated to assemble for biological reasons, and can require more resources (especially memory). Some will require employing advanced assembly methods. Much discussion about plant assembly and strategies can be found online with a few searches. Issues around large genomes in general, and sub-features like high repetitive content (example: Lettuce), high ploidy content (example: Strawberry), large chromosomes (example: Wheat) can all be factors to consider. But try the simple approach described below first, certainly can\u2019t hurt.\nRequirements/Best practices for Reference Transcriptome assembly using RNA-seq reads:\n\n\nReads must be in fastqsanger format (Sanger Phred+33 quality scores, what results from Illumina 1.8+ sequencing) with that datatype assigned. If the data is in compressed fastqsanger.gz format, uncompressed versions of the inputs will be created as hidden datasets. This can unexpectedly increase quota usage. Sometimes uploading uncompressed fastq can be a better choice \u2013 it depends on the tools you plan to use \u2013 some work with compressed fastq and some do not (but Galaxy will \u201cmanage\u201d that for you). Just something to be aware of.\n\n\nLoad reads by URL or with FTP. If \u201cautodetect\u201d for datatype assignment is used, fastqsanger will be assigned automatically if the reads are actually in that format. If you get a different autodetected datatype, there is an input problem that needs to be addressed first (examples: fastqillumia== needs quality score rescaling OR just fastq == probably a format issue).\n\n\nNOTE: If you decide to upload and directly assign fastqsanger.gz to preserve compression, you should definitely run the QA steps to verify that datatype is assigned correctly, or expect problems (assembly jobs failing due to unexpected quality score scaling \u2013 will look like any other memory failure, and require that you have to back up to the start, run QA, and fix the reads if that is the actual issue). If the data is actually uncompressed and assigned that datatype, any tool can also fail, and most but not all will report what the problem is. FastQC will guess the quality score scaling. That can be compared with the specific datatypes. See the Support FAQs to learn how to interpret the report and how to convert to fastqsanger if needed (tool: \"Fastq Groomer`).\n\n\nPunch line: It is more satisfying/less frustrating to get the inputs correct at the start, so you don\u2019t need to hunt for basic issues later on, possibly after much other work is already done. No one likes to \u201cstart over\u201d.\n\n\n\n\nRun some QA on your reads. FastQC followed by MultiQC (to summarize the raw FastQC reports). Running FastQC on the individual original datasets, in pairs (R1+R2) will be more informative. This can be run in batch or by using Dataset Collection(s), if wanted.\n\n\nNext, run Trimmomatic if adaptor or low-quality ends are reported by FastQC. You may want to run this anyway to get your reads paired up and catch content issues not found by FastQC (only checks a subset of the data, first 200k reads or so, and can be biased). There will be four outputs per paired-end input. Two will be the reads that are still paired after QA. This is important \u2013 Trinity requires paired-end fastq inputs to be in intact pairs & tends to run better (not fail for resource reasons) with cleaned-up reads.\n\n\nUse Concatenate to merge all R1 (forward) reads into one dataset and all R2 (reverse) reads into another. two distinct runs.\n\n\nIf the Trinity run fails for exceeding resources, then subsampling your reads can help. You may decide to do this per-pair or after concatenating (after QA is done) \u2013 your choice. See the Seqtk tools: seqtk_sample or seqtk_seq are good choices. The first is a specific function, the latter more comprehensive with that function included. In most cases, you\u2019ll need to run the tool twice (once to output R1 forward reads, once to output R2 reverse reads). Be conservative first, to preserve as much of the original data as possible, then move to more restrictive as needed.\n\n\nIf the data will not assemble at all then one of these is going on: a) some input problem is present, b) not enough QA was done, c) the data exceeds processing needs that a public Galaxy server, or your own, has allocated. Public resources are fixed for most tools, including Trinity. A cloud Galaxy may be more appropriate.\n\n\nBe aware that if you combined the data into a Dataset Collection (R1 forward collection + R2 reverse collection, or in an R1+R2 paired-end collection) at the start, you\u2019ll need to use Collection Operations tools to manipulate the read data to fit the tools (example: Collapse Collection will perform the same manipulation as Concatenate). See the Dataset Collection \u201cCollection Operations\u201d tutorials linked below to understand how to work with collections. Help is also on the tool forms. With so many inputs, Collections will be very useful, but also a bit trickier to use if you haven\u2019t tried them yet. That said, these are definitely worth learning how to use, now or later. Often a mix of paired-end and single-end collections, used at different steps, is needed to work through an analysis.\nTutorials & Support FAQs & Related:\n\nGTN tutorials: https://training.galaxyproject.org/ (all)\n\nStart with those in the groups \u201cAssembly\u201d, \u201cTranscriptomics\u201d, and \u201cData Manipulation\u201d (collection manipulations are covered).\n\n\nSupport FAQs: https://galaxyproject.org/support/#troubleshooting\n\nStart with the first few in \u201cUnexpected Results\u201d and \u201cGetting Inputs Right\u201d to understand format, datatypes, error messages (and what to do about each!), etc. See \u201cLoading Data\u201d if you are not sure how to use FTP (usually only needed with slower internet connections, but is fast/simple for batch uploads too).\n\n\n\nFastQC is covered in many tutorials, but reading the tool\u2019s FAQs about specific statistics reported is informative, see: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ (Docs & Example reports)\n\nI also added some tags to your post. Clicking on any may provide more clues about what may be going wrong if you run into trouble.\nThanks!"}], [{"role": "user", "text": "Currently, i am trying to prepare the workflow for the Qiime2. But unfortunately, the file type for the Qiime2 (QZA,QZV and others) are not present in the list provided with the current version of the Galaxy.\nKindly help me how to add another File type as input for the same problem."}, {"role": "system", "text": "Hello,\nI\u2019m not sure what protocol you are following, but the datatypes required by these tools as wrapped for Galaxy by the IUC are all supported by the recent Galaxy releases (including the most current, version 19.01).\nEach tool form will note the format and datatype of the expected input(s).\nThe Qiime tools installed at the public Galaxy EU server https://usegalaxy.org have the tool version 1.9.1.N.\nOr are you running your own Galaxy server? The IUC version 1.9.1 of the tools installed as a suite (https://toolshed.g2.bx.psu.edu/view/iuc/suite_qiime/a3f284fa7215) should also work in a recent Galaxy version.\nAre one of those presenting with a problem? Which tool?"}, {"role": "user", "text": "I am using galaxy Versoin 19.01v.\nTool used to run Qiime is\nhttps://toolshed.g2.bx.psu.edu/view/florianbegusch/qiime2_wrappers/51b9b6b57732\nIt is the wrapper to run Qiime2.\nplease suggest to resolve the problem faced ."}, {"role": "system", "text": "Thanks for clarifying.\nThis is a tool wrapper developed by the larger community. It doesn\u2019t seem to include custom defined datatypes as part of the install instructions/dependencies (and it probably should).\nThe development repository linked from the ToolShed has one issue (now closed) that is about the same issue you are having: https://github.com/florianbegusch/qiime2_wrappers/issues/1\nYou could try what the other admin did, to see if it works. If not, open another issue at the repository and see if the developer can offer more support/help.\nYour other choice is to use the IUC wrapped version of the tool. This development group includes many members of the core Galaxy team and closely related community contributors, and are a bit easier to reach if there are problems (Gitter chat, etc). Please see: https://galaxyproject.org/iuc/\nAs a last resort, any tool author can be sent a message through the ToolShed https://usegalaxy/toolshed interface. Create an account first (anyone can), navigate to the tool, and find the option \u201ccontact owner\u201d in the \u201crepository actions\u201d menu in the top right corner.\nBe aware that whether they reply or not is up to them and the level of support offered. Anyone from the community can contribute tools (on purpose). If there are problems and no support, it is probably a tool you want to avoid."}, {"role": "system", "text": "Hi,\nqza is a binary format so you do not have to define a new datatype. The only thing you have to do is to include in the datatypes_conf.xml :\n<datatype extension=\"qza\" type=\"galaxy.datatypes.binary:CompressedZipArchive\" display_in_upload=\"true\"/>"}], [{"role": "user", "text": "After upgrading to the latest version of FASTQC (0.11.7 to 0.11.8) the HTML files cannot be viewed in browser. Is there a fix available ?"}, {"role": "system", "text": "Hi,\nis this a Galaxy qquestion? The Galaxy Tool wrapper for FastQC is currently at version 0.72+galaxy1 (and wraps FastQC 0.11.8). How exactly did you upgrade FastQC?"}, {"role": "system", "text": "Hi - You will need to set some admin permissions for viewing the HTML output. This is set by tool and version.\nIn the Galaxy UI, go to Admin > Tool Management > Manage whitelist and check off the updated FastQC tool version. I can\u2019t remember if this requires a restart or not, so if it doesn\u2019t seem to work after, restart and all should work.\nUpdate: Does NOT require a server restart"}], [{"role": "user", "text": "Hi!\nI have a sequnce logo of an miRNA, that gives me no output? All the steps are done correctly in Galaxy sequnce logo tool. What can be evaluated on such an output?\nNo conservation?\nThanks!"}, {"role": "system", "text": "Welcome, @A_T!\nIs this the tool being used? Sequence Logo generator for fasta (eg Clustal alignments) (Galaxy Version 3.5.0)\nThe input requirements are very specific. Review the tool help and compare your data. Does your multiple alignment fasta meet the expected formatting?\nIf so, and you are working at a public Galaxy server where a shared history link can be generated (including Galaxy Main https://usegalaxy.org), you can post it back here (publically) or send it to me as a direct message (private), and we can help to troubleshoot.\nShould you be doing something else (different tool, etc), please explain with more details.\nThanks!"}, {"role": "user", "text": "We did the following:\nSelect lines that match an expression --> get flanks --> extract genomic DNA using coordinates from assembled/unassembled genomes and lastly sequence logo generator for fasta\n\nlogo.png923\u00d7417 5.66 KB\n"}, {"role": "system", "text": "Thanks for the extra info.\nThe Sequence Logo tool requires a fasta input that is in an alignment format, not just a regular fasta file. ClustalW can be used to generate the proper fasta input \u2013 use the output option \u201cOutput alignment format\u201d as \u201cfasta\u201d.\nPlease be aware that ClustalW does have some input limitations on the public server, so do not submit too many fasta sequences at once or that tool will fail. How many exactly depends somewhat on the length of the input fasta sequences, so you\u2019ll need to test what works for your data if > 100 sequences or so.\nI\u2019ll also go in and run a test with some sample data to make sure there is not more going on and will write back with that result. I am also going to test MAFFT (potential alternative tool for ClustalW step) \u2013 cannot recall if the aligned fasta produced by that tool works with Sequence Logo or not.\nPlease try ClustalW in the meantime and see if that works for you."}], [{"role": "user", "text": "Hello everyone.\nA few months ago I created for a colleague of mine a Galaxy instance using Docker on a virtual machine on our cloud.\nAfter she was done with her analyses and I explained her how to download her results, I deleted the docker container and the virtual machine. I saved the mapping directory \u201cjust in case\u201d.\nShe recently asked me if I could restart the Galaxy instance since she had a problem when saving her datasets.\nNow I have troubles starting a new container using the same mapping directory.\nI had to go inside the Docker container and manually launch the startup script to get more information.\nI had the following error:\n\nException: Your database has version \u2018134\u2019 but this code expects version \u2018141\u2019. Please backup your database and then migrate the database schema by running \u2018sh manage_db.sh upgrade\u2019.\n\nSo I backuped my database, upgraded postgresql, reimported the backuped database and launched the startup script again.\nNow I can access the Galaxy instance but I cannot log to the admin account (which was used by my colleague to do her analyses. She didn\u2019t bother creating a new account). I have no error message but the \u201cadmin\u201d tag doesn\u2019t appear and when I click on \u201cuser\u201d it says \u201cconnected as\u201d with nothing following.\nI even tried to import the database backup into a \u201cvirgin\u201d Docker container created from the same Galaxy Docker image but there were no datasets in this new instance.\nI know the datasets are there. There are a lot of files in mapping/galaxy-central/database/files/000/ but named dataset_XXX.dat. I don\u2019t know where to find the information about the real datasets names, the histories names\u2026\nIs there a way to get back the datasets with their metadata?\nThank you for your help."}, {"role": "system", "text": "All the information you are seeking should be in Galaxy\u2019s database. What database did you use in your configuration? Do you have it backed up from the time after your colleague\u2019s analysis?"}, {"role": "user", "text": "Hi Marten. Thank you for your message.\nI don\u2019t know how to get the information I need in the database. I would like to give my colleague either an archive with all her datasets (with the correct names and extensions) or (best) create a new Galaxy instance with all her work.\nI didn\u2019t change anything to the database configuration.\nMy Galaxy instance is based on quay.io/shiltemann/galaxy-metagenomics, which is based on bgruening/galaxy-ngs-preprocessing:17.05, and this one is based on bgruening/galaxy-stable:17.05.\nThis is what I did to backup the database:\nsu - postgres\npg_dumpall > pg_backup.bak\nThen I created a new Docker container based on the same image, I copied the pg_backup.bak file in the appropriate directory and I imported it (psql -f pg_backup.bak postgres) but I couldn\u2019t see my datasets in this new instance."}, {"role": "user", "text": "UPDATE\nI realized that I didn\u2019t used the right Docker image\u2026\nSo I just created a new container based on the right Docker image and linked to my mapping directory.\nI connected to the container interactively to see what happens.\nFirst the postgresql service was down. Starting failed because of the following error:\n\n\nError: The cluster is owned by user id 1000 which does not exist any more\n\n\nSo I ran the following commands and then restarted the postgresql service (successfully):\n\napt-get -y --purge remove postgresql*\nsudo rm -Rf /etc/postgresql/\nsudo rm -Rf /etc/postgresql-common\nsudo rm -Rf /var/lib/postgresql\nuserdel -r postgres\ngroupdel postgres\napt-get -y install postgresql-common postgresql-9.3 postgresql-contrib-9.3 postgresql-doc-9.3\n\nThen I got this error:\n\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL:  password authentication failed for user \u201cgalaxy\u201d\nFATAL:  password authentication failed for user \u201cgalaxy\u201d\n\nI ran the following commands to recreate the galaxy user for postgresl:\n\nsu - postgres\ncreateuser -P -s galaxy\n\nBut I still get the same error."}, {"role": "system", "text": "This looks like your database password/connection string is incorrect. Check Galaxy\u2019s configuration file (galaxy.yml)."}, {"role": "user", "text": "I don\u2019t have the galaxy.yml file (my instance is a bit old\u2026), I guess I must check galaxy.ini file.\nThe database_connection line looks like this:\n\n#database_connection = sqlite:///./database/universe.sqlite?isolation_level=IMMEDIATE\n\nBut this is exactly the same line in the config file of another (working) instance of Galaxy Docker. I\u2019m a bit surprised to see sqlite here\u2026\nI\u2019ve replaced it with\n\ndatabase_connection = postgresql://galaxy:galaxy@localhost/galaxy\n\nBut I still can\u2019t start Galaxy. I got this:\n\npostgresql: ERROR (abnormal termination)\n[\u2026]\n==> /home/galaxy/logs/uwsgi.log <==\nreturn self.dbapi.connect(*cargs, **cparams)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/psycopg2/init.py\u201d, line 164, in connect\nconn = _connect(dsn, connection_factory=connection_factory, async=async)\nOperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\nIs the server running on host \u201clocalhost\u201d (127.0.0.1) and accepting\nTCP/IP connections on port 5432?\ncould not connect to server: Cannot assign requested address\nIs the server running on host \u201clocalhost\u201d (::1) and accepting\nTCP/IP connections on port 5432?\n\nWhen doing this\n\ntail /var/log/postgresql/postgresql-9.3-main.log\n\nI got the following line:\n\n2019-07-16 08:56:15 UTC LOG:  could not receive data from client: Connection reset by peer\n\nPostgresql service is on."}, {"role": "system", "text": "\n\n\n i-guigon:\n\nI guess I must check galaxy.ini file.\n\n\ncorrect\n\n\n\n i-guigon:\n\ncould not receive data from client: Connection reset by peer\n\n\nthis does not seem like a postgres problem, this probably means your client (Galaxy) application shut down unexpectedly\n\n\n\n i-guigon:\n\n==> /home/galaxy/logs/uwsgi.log <==\n\n\nGiven you are using galaxy.ini, you should probably be using the Paste server, not uwsgi. I would try that next.\nfrom docs:\n\nif a Galaxy has a  galaxy.ini file configured, it will continue to use Paste by default unless additional steps are taken by the administrator\n\nWhat version of Galaxy are you at?"}, {"role": "user", "text": "I don\u2019t know Paste or uwsgi. I didn\u2019t change anything to Galaxy\u2019s inner architecture or config.\nMy instance is based on Docker image bgruening/galaxy-stable:17.05.\nBut I just found a way to solve my problems. Here is what I did:\n\nCreate a Docker container based on the same Docker image I used previously\nand mapping to the same mapping directory I used previously and which contains\nmy datasets.\n\n\nsudo docker run -d -p XXXX:80 -v /my/old/mapping/dir/:/export/ --name galaxy myGalaxyImage\nsudo docker exec -it galaxy /bin/bash\n\n\nBack-up Postgresql database (inside Docker container).\n\n\nsu - galaxy\npg_dump -U galaxy -W -F t galaxy > dump_galaxy.tar\nexit    # exit from galaxy user\ncp /home/galaxy/dump_galaxy.tar /export/galaxy-central/\n\n\nCreate a new Docker container still based on the same Docker image\nbut mapping to a new, empty mapping directory.\n\n\nsudo docker run -d -p XXXX:80 -v /my/new/mapping/dir/:/export/ --name galaxy_clean myGalaxyImage\n\n\nCopy back-uped database from the \u201cold\u201d mapping directory to the \u201cnew\u201d mapping directory\n\n\nsudo cp /my/old/mapping/dir/galaxy-central/dump_galaxy.tar /my/new/mapping/dir/galaxy-central/\n\n\nConnect to \u201cnew\u201d Docker container\n\n\nsudo docker exec -it galaxy_clean /bin/bash\n\n\nRestore the back-uped Postgresql database (inside \u201cnew\u201d Docker container).\n\n\ncp /export/galaxy-central/dump_galaxy.tar /home/galaxy\nsu - galaxy\npg_restore -d galaxy dump_galaxy.tar -c -U galaxy\nexit    # exit from galaxy user\nexit    # exit from Docker container\n\n\nCopy datasets files from \u201cold\u201d container to \u201cnew\u201d one\n(which are stored in /export/galaxy-central/database/)\n\n\nsudo tar czf /my/old/mapping/dir/galaxy-central/database.tar.gz /my/old/mapping/dir/galaxy-central/database\nsudo cp /my/old/mapping/dir/galaxy-central/database.tar.gz /my/new/mapping/dir/galaxy-central/database.tar.gz\nsudo mv /my/new/mapping/dir/galaxy-central/database /my/new/mapping/dir/galaxy-central/database_bak\nsudo tar xzf /my/new/mapping/dir/galaxy-central/database.tar.gz\n\n\nRestart \u201cnew\u201d Docker container\n\n\nsudo docker restart galaxy_clean\n\nI don\u2019t know if what I did is really clean but at least it allowed me to get back my datasets simply from the mapping directory.\nMaybe this can help other people as well."}], [{"role": "user", "text": "Dear all,  I\u2019m trying to get TPM data from FastQ file(mouse, illumina, pair-end, 150bp, 20-30M), I tried Stringtie, Salmon, Sailfish, Killisto quant, and I did get the results, but I have a couple of questions about the data:\n\n\nStringtie data showed one gene with multiple TPM values, my understanding is these different reads mapped to different sites of the same gene(there is information about the ch site), should I just plus all the TPM together as the final TPM for each gene?\n|Rb1cc1| cov 143.169418| FPKM 8.402312| TPM 17.528645|\n|Rb1cc1| cov 0.007920| FPKM 0.000465| TPM 0.000970|\n|Rb1cc1| cov 0.353770| FPKM 0.020762| TPM 0.043313|\n|Rb1cc1| cov 0.006247| FPKM 0.000367| TPM 0.000765|\n|Rb1cc1| cov 0.608617| FPKM 0.035718| TPM 0.074515|\n|Rb1cc1| cov 1.899041| FPKM 0.111451| TPM 0.232505|\n|Rb1cc1| cov 0.131482| FPKM 0.007716| TPM 0.016098|\n|Rb1cc1| cov 0.025351| FPKM 0.001488| TPM 0.003104|\n|Rb1cc1| cov 0.081178| FPKM 0.004764| TPM 0.009939|\n|Rb1cc1| cov 0.661685| FPKM 0.038833| TPM 0.081012|\n\n\nthe data from Salmon, Sailfish, and Killisto quant are more confusing, the target ids are like what showed as following, which one I should use to count TPM? or it\u2019s the issue of the reference genome I use(Grcm38)?\nENSMUST00000130201.7|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072660.1|Mrpl15-203|Mrpl15|1894|protein_coding|\nENSMUST00000156816.6|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072659.1|Mrpl15-206|Mrpl15|4203|protein_coding|\nENSMUST00000045689.13|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072661.1|Mrpl15-201|Mrpl15|497|nonsense_mediated_decay|\nENSMUST00000115538.4|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072664.1|Mrpl15-202|Mrpl15|910|processed_transcript|\nENSMUST00000192286.1|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000127355.1|Mrpl15-207|Mrpl15|4600|retained_intron|\nENSMUST00000146665.2|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072662.2|Mrpl15-205|Mrpl15|1569|protein_coding|\nENSMUST00000132625.1|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072663.1|Mrpl15-204|Mrpl15|654|retained_intron|\n\n\nhonestly, it\u2019s the first time I work with RNAseq data, so for all the programs, I always use the default setting, based on the sequencing parameter (mouse, illumina, pair-end, 150bp, 20-30M), does anyone can give me some advice about those parameters?\n\n\nthank you so much, everyone!"}, {"role": "system", "text": "Dear @yanjiezhu,\nFor both 1. and 2., what is happening is that your tools count for each individual transcript, e.g., ENSMUST00000130201.7 and ENSMUST00000156816.6 are two different transcripts of your gene Mrpl15. Bit more confusing in (1), Stringtie shows just the gene name, but each row stands probably for one transcript of the gene. Some tools like Salifish can be set to the specific ID they should consider for counting (look into the option The key for aggregating transcripts during gene-level estimates of Salifish). Other tools do not support such an option and in order to count only for the gene you would need to get an annotation file, where you just have one row for each gene.\nHave a nice day and best wishes,\nFlorian"}, {"role": "user", "text": "Hi Florian,\nThank you so much for the kind explanation. You are right, Stringtie shows here each row stands  for one transcript of the same gene, I can see the details like this:\nchr1\tStringTie\ttranscript\t3206523\t3216968\t1000\t-\t.\tgene_id \tMSTRG.2\t transcript_id \u201cENSMUST00000159265.1\u201d\t ref_gene_name \tXkr4\t cov \u201c0.011633\u201d\t FPKM \u201c0.000683\u201d\t TPM \u201c0.001424\u201d\nchr1\tStringTie\ttranscript\t3214482\t3671850\t1000\t-\t.\tgene_id \tMSTRG.2\t transcript_id \u201cENSMUST00000070533.4\u201d\t ref_gene_name \tXkr4\t cov \u201c0.117157\u201d\t FPKM \u201c0.006876\u201d\t TPM \u201c0.014344\u201d\nAnd I checked the option you suggested about Salifish, the default setting is gene_id, I changed it to gene_name and re-run it, the job is still waiting, but I\u2019m not sure what I will get.\nLooks like what I got from these programs is actually each transcript with a TPM? do you have any suggestion if I want to get the TPM for each gene?\nThanks,\nYanjie"}, {"role": "system", "text": "Dear @yanjiezhu,\nThere are different approaches to get the count per gene.\nA simple way is to download (or modify) your annotation so that each row stands for one gene and then you apply stringtie (or another counting tool).\nAnother way is to use the ouput of Salmon or kallisto and pick the transcript with the highest CPM.\nCheers,\nFlorian"}, {"role": "system", "text": "Hi @yanjiezhu,\nin the case of Salmon, if you provide an annotation file, it generates two outputs, a quantification file (which provides information about the transcripts) and a gene quantification file (which provides information at gene level). Probably the last file is the one you are interested in.\nRegards"}], [{"role": "user", "text": "hello, I followed the tutorials: Production, Scaling and Load Balancing and Proxy to create a Production Galaxy Instance.\nI am trying to bind everything to the Gunicorn socket while using a Proxy with Nginx. But I do not know if I am making a mistake in the configuration files. I tried to Google the issue and consulted the Gunicorn official documentation.\nWhat is the path to the Gunicorn supposed to be? I searched in: /etc/, /run/, /var/, /tmp/ and so on. My Galaxy instance is on a different \u201cgalaxy\u201d user. Binding by IP on port 8080 works very well, but I am trying to bind by socket to access the server by a subdomain, like the tutorial \u201cgalaxy.example.org\u201d while also securing with lets encrypt.\nThank you!"}, {"role": "system", "text": "\n\n\n IulianIV:\n\nMy Galaxy instance is on a different \u201cgalaxy\u201d user.\n\n\nCould you explain more about this part? Please also let us know which version of Galaxy you are running.\nAnd, let\u2019s cross post this over to the Admin chat for expert help. They may reply here or there, and feel free to join the chat! You're invited to talk on Matrix"}, {"role": "user", "text": "Thanks for the reply!\nWhat I mean is that my galaxy instance is created on a different user called \u201cgalaxy\u201d, as instructed in the tutorials, to avoid manipulation by root. I am on version 22.05.\nAt the moment I am trying to somehow find or create the socket file, maybe that will solve my issues."}, {"role": "system", "text": "In Galaxy 22.05 this would be controlled by the value of gravity.gunicorn.bind in galaxy.yml:\ngravity:\n  gunicorn:\n    bind: localhost:8080\n\nTo set to a unix domain socket, use:\ngravity:\n  gunicorn:\n    bind: unix:/path/to/gunicorn.sock\n\nYou can set the path to whatever makes sense in your case. In the Installing Galaxy with Ansible training we set this to /srv/galaxy/var/config/gunicorn.sock, but whether this directory makes sense for you depends on how you have deployed Galaxy."}, {"role": "user", "text": "Wait, so I just simply set a path? Doesn\u2019t it have to be a path to where there is a .sock file? I mean, is the socket file supposed to be automatically generated or do I just specify the path? I think this my confusion."}, {"role": "system", "text": "Ah! Yes, you just set a path, gunicorn creates the socket on startup. This will need to match the path you specify in the nginx config, but gunicorn is what will create it."}], [{"role": "user", "text": "Hi, I am trying to upload files to Galaxy for analysis. However, the following error appears: Failed: Internal Server Error (500)"}, {"role": "system", "text": "Hi @Virginia_Perez\nPlease try to load the data again. This sound like a transient server error.\nIf that fails again, please explain more. Examples of information to include: Galaxy Training!"}, {"role": "user", "text": "I am using \u2018Galaxy\u2019. Using the Upload data option I am trying to upload a .txt file to carry out a Lefse analysis. When choosing a local file located on my computer and clicking on Start, the following error appears in status \u201cFailed: Internal Server Error (500)\u201d, preventing the upload of the file.\nI have tried to perform this operation at different times and with different computers and browsers, but I always get the same error.\nThanks!"}, {"role": "system", "text": "Hello\nI have also had the same problem since yesterday. Today I tried again and still I get the same error message when uploading my data \u201cFailed: Internal Server Error (500)\u201d (2 KB).\nHow could we solve this problem??\nThankyou in advance!\nAida"}, {"role": "system", "text": "Hello Galaxy team,\nI have the exact same problem with uploading files and exactly the same error-\nFailed: Please make sure the file is available. (500)\nnote- the file weight is 128kb so I don\u2019t think that is the problem.\nI often use your platform and know how to upload files but I didn\u2019t succeed in loading files since yesterday.\nI also try to upload a good file which already uploaded correctly and analyzed using your lefse platform last week.\nWhat can I do?\nThank you in advance!\nHadar"}, {"role": "system", "text": "Hello!\nI have the same issue; it seems that uploading data doesn\u2019t work for anyone\nFailed: Please make sure the file is available. (500)\nIs there any advice?"}, {"role": "system", "text": "Update 3/15/2022\nThe Biobakery Lab that hosts this Galaxy server wrote back that the Upload functionality at the server is now functional. Please try now. Thanks!\n\nHi all,\nIt sounds like most of you are working at this public Galaxy server: http://huttenhower.sph.harvard.edu/galaxy/\nA different version of Lefse is available as an alternative at: UseGalaxy.eu, UseGalaxy.org, and UseGalaxy.org.au.\nIf you need to work at the Huttenhower public Galaxy server, please know that server\u2019s administrators do not track this forum. We can sometimes help with tool errors and related issues, but it does seem that the Upload tool is non-functional from my testing, and we cannot correct it or give any status here.\n\n2022-03-10-huttenhower.sph.harvard.edu-upload-failure2510\u00d71018 276 KB\n\nThe administrators of that server will need to be contacted directly about the problems with the Upload tool at that server. Contact details are below, and if anyone gets an update, please feel free to post it back to this thread. I didn\u2019t find any notifications on their Galaxy server or at their forum.\nGeneral info:\n\n\n\nProblems with Plot Cladogram -- http://huttenhower.sph.harvard.edu/galaxy/\n\n\nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server.\nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers).\nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).\n\nContact: Huttenhower Lab \n\n\nHelp/tutorials: (also linked from the server\u2019s home page, with example data available):\n\nlefse \u2013 The Huttenhower Lab \nlefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHub \n\nPrior Q&A and other related resources including other versions of this tool and different tools that may be helpful when working with this type of data:\n\nLefse Cladogram prior Q&A at this forum: Search results for \u2018Cladogram\u2019 - Galaxy Community Help\n\nAll Galaxy resources: Searching the Galaxy\n\n\n\n"}], [{"role": "user", "text": "Dear all,\nCould you recommend any alternatives to \u201cGemini load\u201d to work with dog genome (canFam3), since Galaxy \u201cGEMINI load\u201d supports only human genome.\nThanks in advance!"}, {"role": "system", "text": "Well, that depends on what you hope to be able to do. Can you specify?"}, {"role": "user", "text": "I need to find SNPs related to cataract."}, {"role": "system", "text": "Ok, but how (technically) are you planning to do so. Through studying a pedigree of dogs and following which genes/variants are linked to the phenotype?\nWhat did you hope to get from GEMINI? A report of variants that are inline with some sort of inheritance pattern (recessive/autosomal/\u2026)?"}, {"role": "user", "text": "I would like to filter the HSF4 gene from database and find the pathogenic SNPs."}, {"role": "system", "text": "Ah I see. Well, for this it sounds as if GEMINI and a database would have been overkill anyway.\nInstead, things like that are not too hard to do with the original VCF file:\n\nYou can use SnpEff to annotate the variants in your VCF with the genes and transcripts affected by them\nuse the SnpSift Filter tool to filter for variants that affect your gene of interest, etc.\ncould have a look at the MiModD Report Variants tool if you find VCF output tedious to parse\n\nBeyond that, there are lots of other tools to manipulate VCF data. On https://usegalaxy.org you can find them combined nicely in the NGS: VCF Manipulation section of the tools, on https://usegalaxy.eu they are part of the section Variant Calling."}], [{"role": "user", "text": "I\u2019m doing gene enrichment analysis using goseq. The two input files for this were generated by following the instructions from 3: RNA-seq genes to pathways.\nI keep getting the following error and I\u2019m not sure what is the matter:\n\n{ \u201ccode_desc\u201d: \u201cAn undefined error occured, please check your input carefully and contact your administrator.\u201d, \u201cdesc\u201d: \u201cFatal error: An undefined error occured, please check your input carefully and contact your administrator.\u201d, \u201cerror_level\u201d: 3, \u201cmatch\u201d: \u201cError in\u201d, \u201cstream\u201d: \u201cstderr\u201d, \u201ctype\u201d: \u201cregex\u201d }\n\nCould it be that I picked the wrong GeneID format? I picked Entrez ID, as the tutorial suggests, but Ensembl ID was the default option\u2026\nAssistance would be appreciated "}, {"role": "system", "text": "Hi @Kanishk,\ncan you share your history with me? I\u2019ll have a look at it. I sent you my email by private message.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nI have shared this history with you now. Please let me know if there\u2019s something I can fix in my inputs to make the program run.\nThanks!"}, {"role": "system", "text": "Hi,\nI\u2019ll check it today.\nRegards"}, {"role": "system", "text": "Hi @Kanishk,\nas I indicated to you in the email, you need to remove the version suffixes (so, you should use ENSG00000168209 instead of ENSG00000168209.6) and the headers.\nRegards"}], [{"role": "user", "text": "When I display Galaxy   https://usegalaxy.eu/ when processing MS mzml files there is a blank area on right side of tools sidebar that cuts off that side. I can send a snapshoot of the display blank area.\nDoes the problem lay with Galaxy or Firefox?  And how do I fix it. I cannot select what I cannot see.\nkleps@uic.edu"}, {"role": "system", "text": "@robk can you please attach a screenshot. FF and Galaxy should both work. Or is your Firefox really really old?"}, {"role": "user", "text": "FF=86.0\n\nImage 3-5-21 at 10.22 AM1541\u00d7776 269 KB\n\nOn the right side of \"tools: side bar is blank space"}, {"role": "user", "text": "FF 86.0\nRobK\n\nImage 3-5-21 at 10.22 AM.jpeg1541\u00d7776 269 KB\n"}, {"role": "system", "text": "Ok, this is not normal. I also have FF86. Can you look at usegalaxy.eu from the incognito mode in FF? Maybe its one of your Firefox extensions? Is Chrome working for you?"}, {"role": "user", "text": "I found an incognito addon for FF86 , installed and restarted. I went to galaxy.eu and open a TOOL and saw no difference in what was displayed. It looked like my last attachment.\nShould I upload new files as incognito and start with a new history?\nRob"}, {"role": "system", "text": "Hi. The incognito mode is a build in feature. For me it looks like this\n\nIts a private mode in your browser. I\u2019m relatively clueless why this is not working for you so my idea is that one of your extensions is messing with the Galaxy layout, but this is just one guess and the inkocnito mode can help here. Do you have also another browser? For exmaple Chrome?"}, {"role": "user", "text": "Chrome does not have this problem.\nFF*^ with incognito has no effect."}], [{"role": "user", "text": "Hello. I\u2019m new to Galaxy and I noticed that the Cut tool seems to be broken relative to its usual functionality in unix as well as what is described in the galaxy 101 tutorial. Namely, I can keep and discard columns, but they cannot be rearranged.\nFor example, if I try to cut (keep) columns 1, 2, 10, and 3 in that order, they are automatically sorted back to 1, 2, 3, and 10.\nIs there another tool for rearranging? Thanks!"}, {"role": "system", "text": "Welcome, @hannah413 and thanks for reporting the problem.\nThere are two versions of the \u201cCut\u201d tool on Galaxy Main https://usegalaxy.org.\nOne is the original, the other is updated with a bit more built-in functionality.\nWould you please click on the re-run button for the dataset that did not order the columns correctly, then copy/paste the entire tool name back here (the complete name that includes the version number)? This is at the very top of the form.\nI\u2019ll test it out and get back to you early next week, probably Monday. Could be a bug or a usage problem. Let\u2019s see if I can reproduce it, then follow-up from there.\nMeanwhile, if you are in a rush, try using the other version.\nFor reference:\n\nOriginal tool version **Cut** columns from a table (Galaxy Version 1.0.2)\n\nAlternate version Cut columns from a table (cut) (Galaxy Version 1.1.0)\n\n\nThe first (original) version was specifically designed to re-arrange columns. I\u2019m not sure about the other, have not used it since we upgraded to the latest tool version and Galaxy release for this type of purpose, and the help on that tool form does not include column re-arrangement examples.\nI\u2019d be somewhat surprised if the first version doesn\u2019t work correctly \u2013 was one of our original tools (created 12+ years ago) so if buggy would be a priority fix.\nThe second is from the IUC \u2013 if there is a problem with the intended functionality, they will definitely appreciate the feedback and also likely address the usage/result optimization going forward if there is something \u201coff\u201d (I\u2019ll make the report/request part of that as needed).\nIdeally, both should work the same at the most basic level, eg: cutting columns out based on user-specified parameters.\nThanks for reporting the issue!"}, {"role": "system", "text": "\n\n\n hannah413:\n\nCut tool seems to be broken relative to its usual functionality in unix\n\n\nI think you\u2019re misremembering that, the output of cut -f1,2 is the same as cut -f2,1, cut does not reorder columns."}], [{"role": "user", "text": "Hi,\nToday, the link to UCSC genome browser in the visualization window (bigwig file) has disappeared.\nHow can I visualise my file again in this browser that I prefer above IGB or IGV?\nI tried:\n\n\u201cBuild custom track for UCSC genome browser (Galaxy Version 1.0.1)\u201d but this resulted in an error.\n\u201cbigWigToBedGraph Convert from bigWig to bedGraph format (Galaxy Version 0.1.0)\u201d but the visualisation window just addad the Ensembl link\n\nThanks for suggestions."}, {"role": "system", "text": "Hi @justme\nThanks for reporting the problem. Few questions to get oriented:\n\nDoes your dataset contain a database metadata attribute? Changing database/build (dbkey)\n\nIs that database supported by UCSC? That application will name the same attribute as dbkey. The listing of all can be found here: UCSC Genome Browser Downloads\n\nWhere are you using Galaxy? URL of the public server please, or describe if other. Troubleshooting errors\n\n\nYou can post back your details plus a screenshot of the expanded dataset for more help "}, {"role": "user", "text": "Hi,\nThanks for looking into my issue.\nIn fact, I always used the Galaxy Europe (usegalaxy.eu) site, which displayed the option to visualize the bigwig file to UCSC genome browser. The usegalaxy.org site still does:\n\n1704\u00d7673 111 KB\n\n\n1699\u00d7670 100 KB\n"}, {"role": "system", "text": "Hi @justme\nThanks for posting back more details. That dataset should have the UCSC link out based on the datatype and database assignments. My test using hg38 instead of hg19 is also missing the UCSC link out, so the problem is likely server side.\nThe quick solution would be to move the data over to UseGalaxy.org or UseGalaxt.org.au until the functionality is repaired. Thanks for the followup!\nPing for EU support/admin @wm75 @gallardoalba + @dannon in case release related\n\nScreen Shot 2023-05-12 at 10.24.44 AM2516\u00d71166 380 KB\n\n\nUpdate: I decided to post this over to the EU chat. They make reply here or there, and feel free to join. You're invited to talk on Matrix"}, {"role": "system", "text": "I have the same problem, any news?"}, {"role": "system", "text": "Hi @imolineris\nNot sure if this was resolved already or not \u2026 but all of the link outs are showing up for me in that same example history as before. This is the history import link if you want to check it out in more detail, or maybe the screenshot helps by itself. Galaxy | Europe\nucsc_linkout2806\u00d71182 339 KB"}], [{"role": "user", "text": "Good Morning,\nI have been trying to get Trinity running for the last week  I have two fastq files that I have been trying to run.  One is a raw fastq (no trimming) about 17.5 GB (pre and post Fastq Groomer), one is a trimmed fastq about 15.8 GB (pre and post Fastq Groomer).   I am trying to run single strand. I have tried F, R, and Not Set for direction.   I have dumped all the data sets that were up previously to free up space. I have tried changing the data type from fastqsanger to fastq.  I currently have one post Fastq Groomer file up (15.86 GB) and it is still hanging. Nothing I have tried seems to work.\nRNA STAR worked fine, even on the raw data.  However, I am trying to do a de novo assembly, so Trinity looks like the only option available.  Any insights welcome.\nThanks"}, {"role": "system", "text": "Hi @pvos,\nA few questions to clarify:\n\nWhere are you working? URL if a public Galaxy server. If a different server, please explain the type/source/version of Galaxy and Trinity.\nWhat does \u201cstill hanging\u201d mean? Is the dataset gray (waiting to run) or yellow (execution in progress)?\n\n\n\nMore about fastq data in Galaxy\nThe datatype will need to be assigned to fastqsanger (uncompressed fastq) or fastqsanger.gz (compressed fastq) for use with Trinity. When a compressed input is selected, a hidden uncompressed version of the data will be created as a preparatory step included in the execution of Trinity.\nAlso, the tool FastqGroomer does not trim reads. It is used to convert between different quality fastq score scaling. Most data is now already in fastqsanger or fastqsanger.gz format, meaning that a grooming step is not needed. You\u2019ll save more quota space by doing one of these:\n\nallowing Galaxy to autodetect the datatype when using the Upload tool. Compressed fastq data will uncompress, and if the content fits fastqsanger format, that datatype will be assigned.\nassigning the datatype fastqsanger.gz when using the Upload tool. This preserves the compressed format. You might want to double-check that your data actually already has Sanger Phred+33 quality score scaling (\u201cfastqsanger\u201d) with the tool FastQC.\nre-assign or re-detect the datatype after using Upload if you loaded it with some other assigned datatype already. Most tools that work with fastq data expect the datatype fastqsanger or fastqsanger.gz is assigned. Click on the \u201cpencil\u201d icon per dataset to reach the Edit Attribute forms \u2013 datatype is assigned/detected on the \u201cDatatypes\u201d tab.\n\nHow-to help for datatypes is below: https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\n\n\nI also added in some tags about the different fastq datatypes. Click on any to review similar prior Q&A."}, {"role": "system", "text": "Possibly related: Unicycler launched but still queued after 2 days"}, {"role": "system", "text": "Update:\nIf you were working at Galaxy Main https://usegalaxy.org, then the jobs were delayed by a server-side issue that was just now corrected.\nPlease allow queued jobs to remain queued and they will eventually process. This is the fastest way to get the job done. Deleting and restarting will only move your job back to the end of the queue, extending the wait time."}, {"role": "user", "text": "I\u2019m using usegalaxy.org and it\u2019s gray. Waiting to run"}, {"role": "system", "text": "Thanks for clarifying.\nMany jobs are queued up for the particular cluster used to run assemblies (Trinity, Unicycler) and it is different from other jobs and their target clusters. All will process in the order originally submitted.\nAvoid deleting/rerunning or the current job will lose the queue placement \u2013 unless you already know there is a problem (inputs or assembly settings) that needs to be changed."}], [{"role": "user", "text": "Hello,\nI was wondering if anyone has more detailed instructions on installing a local instance of galaxy server (release 23.0) on an Ubuntu server LTS 22.04.\nI have tried the straightforward approach outlined in the Galaxy documentation here, but it has failed several times.  The folder downloads from github without issue, but when I use  sh run.sh the installation takes about 10 - 20 minutes before resulting in an error message.  I\u2019ve received:\n\nerror code exit 1,\nerror code exit 134, and\na repeating attempt to synchronize and wake up \u201ccelery-beat\u201d that never resolves.\n\n==> ../log/celery-beat.log <==\n[2023-06-21 09:49:02,380: DEBUG/MainProcess] beat: Synchronizing schedule\u2026\n[2023-06-21 09:49:02,387: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n\nPerhaps it\u2019s a dependency issue, and it would be very helpful to know which packages for Ubuntu should be pre-installed before attempting the Galaxy build.  For example, I know python is required, but what version? What other packages would be required for a successful build?\nIf someone has built a galaxy server on Ubuntu and wouldn\u2019t mind sharing their steps, I\u2019d be very thankful!\nForgive me if this is a naive question; I\u2019m very new to this.\nThanks in advance!"}, {"role": "system", "text": "Welcome, @pjlombardo\nThese are better resources. Be sure to install Galaxy into a Conda .venv. Why is in the admin docs under Framework Dependencies.\nThat said, if this is just for your own use, the Docker version comes with much preconfigured and is usually easier to apply customizations to. But you can review the choices and decide.\n\n\n\nLocal Galaxy: Flye error during consensus, minimap2 stage\n\nReferences:\n\nAdmin training Galaxy Training!\nAdmin docs https://docs.galaxyproject.org/\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\n"}, {"role": "system", "text": "Hello pjlombardo,\nI have to say that I have yet to find any good documentation on doing a local install of Galaxy.  That said, I\u2019m attaching my own notes on how I installed Galaxy. It has gotten me to a running state but my main issue is not getting Jbrowse to install.  What I learned in my many install attempts is permission, permission. It always bites you in the ass. Just make sure to create a local non-root Galaxy user account. Use that account to install Galaxy. Then if you use Nginx to proxy all SSL connections, make sure Nginx account has access to the Galaxy home folder. Now that I think about it. It might be a better idea to simply install Galaxy as the Nginx account. Might try that next.\nAttaching my notes here, Just jump down to the Ubuntu install, keep in mind my home folder path changed during my install attempts on different OS platforms.  Also, you will use sections 3 and 4 on Galaxy config changes and Nginx config settings.\nLet me know if this helped or made things worse. Hopefully not worse.\nInstall notes"}, {"role": "user", "text": "Hi @cjkeist,\nI couldn\u2019t access the notes you attached.  It said \u201cthis shared folder has been removed or is unavailable to you\u201d when I created a box account.  Is there a different way to access the notes?\nPhil"}, {"role": "user", "text": "Thanks Jennifer, I\u2019ll dive into these today!"}, {"role": "user", "text": "Update:\nBased on the helpful suggestions from @cjkeist and @jennaj, I am now installing through a Conda .venv for a non-root galaxy user.  Unfortunately, it still won\u2019t build.\nEverything seems to be going well until it stalls at:\n$ GXY_BUILD_SOURCEMAPS=1 NODE_ENV=production webpack\nBrowserslist: caniuse-lite is outdated. Please run:\n   npx update-browserslist-db@latest\n\nI have run the suggested command several times, and I\u2019m told that it is up to date. I also tried re-installing yarn and npm to make sure they\u2019re both up to date.  I have even double checked that my current version of caniuse-lite is the most recent.  I\u2019m not sure why it gets stuck at this point.\nThe build essentially stalls at this prompt and then eventually throws an error 134, and some message about javascript heap out of memory?\nAny thoughts? Has anyone else seen this?"}, {"role": "system", "text": "Hello,\nSorry, the link did work.  Didn\u2019t realize it would require a Box account. I set it so anyone with the URL should be able to download it. Annoying this forum doesn\u2019t allow pdf files to upload.  Anyway, try this link.\n\n  \n\n      sites.science.oregonstate.edu\n  \n\n  \n    \n\nServers_galaxy.pdf\n\n  393.92 KB\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello,\nI have my datasets from bulk RNA Sequencing. I tried to process my datasets, it\u2019s mouse, using the workflow from Galaxy tutorial. But after the trimming and going in to the alignment with RNA STAR, it gives me lots of error. Like:\nAug 17 10:25:31 \u2026 started STAR run\nAug 17 10:25:31 \u2026 loading genome\nAug 17 10:33:47 \u2026 started 1st pass mapping\nEXITING because of FATAL ERROR in reads input: quality string length is not equal to sequence length\n@A00643:482:HYYN7DSXY:1:1101:2\nand error on gtf file\nAug 17 11:17:05 \u2026 started STAR run\nAug 17 11:17:05 \u2026 loading genome\nAug 17 11:21:48 \u2026 processing annotations GTF\nFatal INPUT FILE error, no valid exon lines in the GTF file: /data/dnb06/galaxy_db/files/9/3/e/dataset_93e3f860-b7ec-4bec-8cb2-6d\nI\u2019m a noobs in this sequencing analysis stuff. How can I fix the error? I will highly appreciate your help "}, {"role": "system", "text": "Hi @zee_azizah\n\n\n\n zee_azizah:\n\nEXITING because of FATAL ERROR in reads input: quality string length is not equal to sequence length\n@A00643:482:HYYN7DSXY:1:1101:2\n\n\nAt least one of your fastq read datasets is truncated, and that starts with the sequence ID in the message. Try uploading that data again, as Galaxy cannot repair uploaded truncated fastq data.\nNever map reads unless they pass basic QA checks. The tool may not fail \u2013 just produce incomplete results and that are difficult to detect. If the tutorial includes QA/QC steps, this should have been caught during QA and before the alignment step. If the tutorial doesn\u2019t include QA/QC steps, then see this tutorial: Quality Control.\n\n\n\n zee_azizah:\n\nFatal INPUT FILE error, no valid exon lines in the GTF file:\n\n\nThis means that the reference annotation isn\u2019t in a format that the tool can use. See prior Q&A at this forum (via the tag I added to your post), and this FAQ working-with-gff-gft-gtf2-gff3-reference-annotation. GTF will work best with this tool. Make sure any header lines are removed."}, {"role": "user", "text": "@jennaj Thanks a lot for the explanation. That means the trimming result is the culprit, right? Should I skip the trimming and just go straight to alignment?"}, {"role": "user", "text": "@jennaj I tried to run my un-trimmed data on RNA STAR, and fixing the gtf file by removing the header but still getting the same error\nOct 10 14:19:56 \u2026 started STAR run\nOct 10 14:19:56 \u2026 loading genome\nOct 10 14:25:35 \u2026 processing annotations GTF\nFatal INPUT FILE error, no valid exon lines in the GTF file: /data/dnb07/galaxy_db/files/a/a/4/dataset_aa45ad6e-56f2-4303-8e43-a1"}, {"role": "system", "text": "\n\n\n zee_azizah:\n\nFatal INPUT FILE error, no valid exon lines in the GTF file\n\n\nIf the headers are removed, then you can trust the error message more. If the GTF doesn\u2019t contain lines with \u201cexon\u201d in the third column (feature), you\u2019ll need to find a version for your genome species/build that has that annotated data content OR try generating that data with the tool gffread as explained in the FAQ.\nYou are mapping against Mouse, correct? If you cannot generate a GTF with the annotation you have now \u2013 most public Galaxy servers host UCSC\u2019s version of genome assemblies when available for a species and UCSC hosts the matching annotation GTFs. Those represent annotation from one or more \u201cGene and Gene Prediction\u201d tracks. Find the files in their Downloads area (avoid the Table Browser \u2013 the data is usually too large to extract). Make sure the GTF\u2019s genome build \u201cdatabase\u201d is exactly the same as the reference genome \u201cdatabase\u201d that you are mapping against.\nmm10 here Index of /goldenPath/mm10/bigZips/genes\nmm39 here Index of /goldenPath/mm39/bigZips/genes\nThe path is similar for other UCSC genome databases indexed for mapping tools. Copy the URL for the GTF file, paste that into the Upload tool, and leave all settings at default. The GTF will automatically uncompress and be ready to use without other formatting changes.\nhttps://hgdownload.soe.ucsc.edu/goldenPath/<database>/bigZips/genes/\n\nOne of these options should work."}, {"role": "user", "text": "@jennaj the GTF file looks like this after the header was removed\n\nimage2165\u00d7821 166 KB\n"}, {"role": "system", "text": "Hi @zee_azizah\nOk \u2013 that screenshot definitely helps.\nYour annotation GTF has Ensembl chromosome identifiers like 1. The mouse reference genome index you are mapping against in Galaxy (mm10) was sourced from UCSC, and has UCSC chromosome identifiers like chr1.\nQuote from understanding-input-error-messages\n\n\nInput mismatch tips.\n\nDo the chromosome/sequence identifiers exactly match between all inputs?\n\u201cChr1\u201d and \u201cchr1\u201d and \u201c1\u201d do not mean the same thing to a tool.\n\n\n\n\n\nYou have two options. The first is much easier and strongly recommended unless you have a specific reason to use the annotation you already have.\n\n\nGet a GTF reference annotation file from a data provider that uses UCSC identifiers instead.\n\nThis includes the UCSC sources I posted in the prior reply.\nFor example, use this GTF instead mm10.ensGene.gtf.gz\n\n\n\n\nTechnically, you can also try to convert the chromosome identifiers using this tool \u2192 Replace column by values which are defined in a convert file.\n\nThis will involve multiple steps, and might not work well.\nInstructions are on the tool form \u2013 be sure to scroll all the way to the bottom to find where to source a mapping file.\nOther related methods are included in this FAQ. All involve complex manipulations without exact help, and none are recommended except in extreme use-cases. I posting this mostly for reference, and not recommending that you try it. mismatched-chromosome-identifiers-and-how-to-avoid-them\n\n\n\n"}], [{"role": "user", "text": "When using RNA STAR in the past, I\u2019ve been able to select from many built-in genomes as a reference. However, under the \u201cselect reference genome\u201d I only see \u201cNo options available.\u201d This occurs if I select with or without a built-in gene model. If anyone can give me any tips on how to find a way to select a reference genome (I only need mm10 and hg38) it would be much appreciated!"}, {"role": "system", "text": "Hello @hwalter684\nWe were able to confirm the issue.\nA ticket to get this resolved has been created and can be found here: https://github.com/galaxyproject/usegalaxy-playbook/issues/292\nA test to see if an earlier version of the tool will work is in progress. You could also test it out. Using usegalaxy.eu instead is another short-term option.\nThe ticket above has the details for everything mentioned. The ticket will update as more workarounds are confirmed and/or the tool itself is reconfigured to locate the indexes properly.\nThanks for reporting the problem!"}, {"role": "system", "text": "This problem still remains or has it been fixed?\nI am trying the same problem these days."}, {"role": "system", "text": "Hi @Vanessa_Souza\nYes, the tool is still problematic at usegalaxy.org.\nAt usegalaxy.org, the version with indexes to use is 2.6.0b-1 \u2013 and that worked for smaller runs for about 3 weeks (the tool has reduced memory allocation, as noted on the server\u2019s banner).\nWe have been making changes to the configuration this week to again run with higher memory and that is not yet completed. Jobs may startup, but then stay in the execute state indefinitely. Unless you happen to hit the window when it is actually fixed! No estimate for that yet but we are trying for \u201cvery soon\u201d.\nIf you really need to use STAR (HISAT2 is not an option for you), using usegalaxy.eu is an option. There are fewer indexed references genomes for the most current tool version than for earlier tool versions, but mm10 and hg38 are indexed (last time I checked). Using a custom genome is a choice, but only for very small genomes. STAR is a memory-intensive tool.\nUsing usegalaxy.org.au will present with the same issues as usegalaxy.org for the reference genomes (re: none for the most current tool version). I haven\u2019t tested earlier versions that do have indexes at that server but I am also not aware of any issues.\nThe cluster set-up is (mostly) different between all usegalaxy.* servers \u2013 so if there are issues at usegalaxy.eu or usegalaxy.org.au those would be due to a different reason than what is impacting usegalaxy.org right now.\nIn summary, two primary factors:\n\nThe latest tool version requires newly rebuilt indexes (everywhere). Those are not all completed nor synched between all usegalaxy.* servers yet.\nServer configuration is undergoing changes at usegalaxy.org. The impacted tools are in the top banner.\n\nThank you for your patience and for asking for a public update!"}, {"role": "system", "text": "This still appears to be an issue. This time for the mouse reference genome:\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nEXITING because of FATAL ERROR: could not open genome file /cvmfs/data.galaxyproject.org/managed/rnastar/2.7.4a/mm10/mm10/dataset_76d29985-2936-4202-8fba-6658b0e3499e_files//genomeParameters.txt\nSOLUTION: check that the path to genome files, specified in --genomeDir is correct and the files are present, and have user read permsissions\nSep 17 19:45:11 \u2026 FATAL ERROR, exiting\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30670709/outputs/dataset_45225756.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30670709/outputs/dataset_45225756.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30670709/outputs/dataset_45225757.dat\u201d : No such file or directory"}, {"role": "system", "text": "The issues with RNA-Star should be resolved now. Please try a rerun if you have not already.\n\n  \n      github.com/galaxyproject/usegalaxy-playbook\n  \n  \n    \n  \n\t  \n  \n\n  \n    \n      Jetstream: Fails RNA-STAR 2.7.5b\n    \n\n    \n      \n        opened 12:31AM - 15 Sep 20 UTC\n      \n\n        \n          closed 11:02PM - 30 Sep 20 UTC\n        \n\n      \n        \n          \n          jennaj\n        \n      \n    \n  \n\n\n\n  Galaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.5b\nTest history. Includes both mouse and human genome targets. hg19 failed -- and it was thought to have...\n\n\n\n    functionality on Main\n\n\n  \n  \n    \n    \n  \n  \n\n"}], [{"role": "user", "text": "Hi all,\n\nI\u2019m trying to run the LEfSe galaxy, and I made 2 different input files.\nAll the data are exactly same, except for taxonomy name format.\n\n\nimage.png1157\u00d7389 9.99 KB\n .\n\nimage.png1395\u00d7412 20.8 KB\n\nHowever, the results of LEfSe from 2 different input files are totally different.\n\nimage.png827\u00d7444 41.3 KB\n\nand\n\nimage.png434\u00d7685 50.6 KB\n\nI don\u2019t know which make input files make totally different results. Please explain it.\n\nI have one more question about the LEfSe input file.\nWhen I see the input format from the LEfSe website, it is not perfectly matched with relative abundance files\nIt looks like the upper taxonomy category includes all the relative abundance in the lower taxonomy class.\nI want to make the same format input files, so is there anyone to teach me some command line to make LEfSe input file?\n"}, {"role": "system", "text": "\n\n\n Jungmin:\n\nI\u2019m trying to run the LEfSe galaxy, and I made 2 different input files.\nAll the data are exactly same, except for taxonomy name format.\n\u2026\nI don\u2019t know which make input files make totally different results. Please explain it.\n\n\nThe first taxonomy format is preferred. See the tool form help for details (scroll down).\n\n\n\n Jungmin:\n\nI want to make the same format input files, so is there anyone to teach me some command line to make LEfSe input file?\n\n\nYou don\u2019t need to do data manipulation line-command. Review the Text/data manipulation tools in Galaxy. Many are based on line-command tools (and named after them) and include help if you are not familiar with them. Nearly any manipulation/reformatting is possible.\nThanks!"}, {"role": "system", "text": "hey,\nhave you solved it?"}, {"role": "system", "text": "I am not sure if these answers were satisfactory for the Jungmin\u2019s problems. I have exactly the same problem.\n\nLefse creates totally different results for the same input files with different formats.\nInput file format provided in Lefse website is not perfectly matched with relative abundances files. As Jungmin stated, in the website upper taxonomy category includes all the relative abundance in the lower taxonomy class. Is there a way to create such an input file from QIIME2 or is it possible to somehow change the format of the file provided by QIIME2?\n\nThanks!"}, {"role": "system", "text": "Has anyone solved this?"}, {"role": "system", "text": "Hi there,\nI have a similar problem and can\u2019t figure out what\u2019s wrong. When I run  B) LDA Effect Size (LEfSe) on data, I get this result:\n\u201cNumber of significantly discriminative features: 0 ( 40 ) before internal wilcoxon\nNo features with significant differences between the two classes\nNumber of discriminative features with abs LDA score > 1.0 : 0\u201d.\nHowever, from previous analyses, I\u2019m sure there are differences between my samples.\nScreenshot of my data format below:\n\nScreenshot 2022-01-27 at 17.26.031920\u00d71005 356 KB\n\nMany thanks,\nViola"}, {"role": "system", "text": "Confirmed issue with the original tool authors. Might be fixed already. See the way microorganisms are named affect the final results of lefse - #6 by jennaj"}], [{"role": "user", "text": "Hello,\nAfter using RNA STAR on 6 rat RNA-seq samples only about 50-60% of reads were mapped uniquely with 30-40% of reads mapped to multiple loci. Is there a way to improve mapping quality or determine what sort of contamination there might be?\nIs there a possibility that the issue is from the gene annotation file? My first attempt at mapping was using a .gtf annotation file from ensembl but it didn\u2019t work. Instead the one I got from UCSC main worked. My hunch is that it was just a formatting issue.\nEnsembl source - https://useast.ensembl.org/Rattus_norvegicus/Info/Index\nUCSC source -  UCSC Genome Browser Downloads\nHere is the RNA STAR log for one of the samples.\n                             Started job on |\tJan 29 08:14:51\n                         Started mapping on |\tJan 29 08:36:14\n                                Finished on |\tJan 29 12:26:14\n   Mapping speed, Million of reads per hour |\t6.18\n\n                      Number of input reads |\t23700301\n                  Average input read length |\t74\n                                UNIQUE READS:\n               Uniquely mapped reads number |\t13214262\n                    Uniquely mapped reads % |\t55.76%\n                      Average mapped length |\t73.63\n                   Number of splices: Total |\t2475399\n        Number of splices: Annotated (sjdb) |\t2413443\n                   Number of splices: GT/AG |\t2408845\n                   Number of splices: GC/AG |\t22608\n                   Number of splices: AT/AC |\t3324\n           Number of splices: Non-canonical |\t40622\n                  Mismatch rate per base, % |\t0.71%\n                     Deletion rate per base |\t0.02%\n                    Deletion average length |\t1.29\n                    Insertion rate per base |\t0.02%\n                   Insertion average length |\t1.43\n                         MULTI-MAPPING READS:\n    Number of reads mapped to multiple loci |\t9233338\n         % of reads mapped to multiple loci |\t38.96%\n    Number of reads mapped to too many loci |\t146819\n         % of reads mapped to too many loci |\t0.62%\n                              UNMAPPED READS:\n\nNumber of reads unmapped: too many mismatches |\t0\n% of reads unmapped: too many mismatches |\t0.00%\nNumber of reads unmapped: too short |\t1075213\n% of reads unmapped: too short |\t4.54%\nNumber of reads unmapped: other |\t30669\n% of reads unmapped: other |\t0.13%\nCHIMERIC READS:\nNumber of chimeric reads |\t0\n% of chimeric reads |\t0.00%"}, {"role": "system", "text": "Dear @ryzhu,\nCheck with FASTQC if there is any adapter contamination still in your data. Furthermore, it might be worth to extract your multi-mapped reads and use featurecounts or blast to see if these come from rRNA (using a gff/gtf for featurecounts or a rRNA sequence file for blast). High ratio of muli-mapped reads in RNA-seq is often a sign of an incomplete depletion of rRNA.\nKind regards,\nFlorian"}], [{"role": "user", "text": "Hi all,\nI am new to using Galaxy and tried to follow the Galaxy 101 tutorial. Unfortuantely. I can\u2019t extract the workflow from history because although my history panel has the cog icon, when I click it, no such option shows up. This is contrary to what is shown in this tutorial: https://galaxyproject.org/tutorials/g101/#creating-and-editing-a-workflow\nPlease help me \nBest,\n4symmetry"}, {"role": "system", "text": "Hi @4symmetry and welcome,\nare you signed in when you\u2019re trying to do this? In general, workflow functionality on Galaxy servers is only available to registered users (cause the workflow has to be associated with you somehow).\nBest,\nWolfgang"}], [{"role": "user", "text": "How to use annotateMyIDs or UniProt ID mapping and retrieval (Galaxy Version 0.2) to covert the refSeq gene ID to gene Name use Galaxy?\nsuch as following  refSeq gene ID:\nNM_001005508.2\nNM_001005510.2\nNM_001005916.2\nNM_001007575.2\nNM_001008501.2\nNM_001008700.3"}, {"role": "user", "text": "Still nobody reply my question, I have settled this problem by my self, I find the gene name of NM_001005508.2 and NM_001005508 are same, so I deleted \u201c.\u201d and the number following it,  than I can use galaxy tool \u201cannotateMyIDs\u201d to get the other information (including gene name) of the gene IDs."}], [{"role": "user", "text": "Is there a command(s) to analyze miseq illumina 16S community profile data for the presence of antibiotic resistance or antibiotic producing genes?\nWe have already completed the 16S Microbial Analysis with mothur (extended) tutorial, but we would like to identify the presence of antibiotic resistance or antibiotic producing genes. If there is a way to run a few more command lines to determine this, we would greatly appreciate the help.\nThank you!"}, {"role": "system", "text": "hi @Melissa_Hayes\n16S data contain sequences from rRNA cluster, not antibiotic resistance genes. It seems you need different type of data, whole metagenome sequencing data.\nOne of the option is a metagenome assembly followed by scan for antibiotic resistance genes. Galaxy Training Network has tutorials for both steps.\nKind regards,\nIgor"}, {"role": "user", "text": "Okay. So, I can\u2019t use my 16S data with these tutorials right? I was hoping to find a way to do something like the protocol in this article https://www.frontiersin.org/articles/10.3389/fmicb.2020.575707/full  in usegalaxy with my dataset.\nCan you send me a link to a tutorial that you would recommend?\nThanks!"}, {"role": "system", "text": "Hi @Melissa_Hayes\nfrom a quick look, the paper you shared describes prediction of ARG from 16S data. Maybe I missed it, but I don\u2019t see any validation of the prediction. The paper relies on Tax4fun. The software was wrapped for Galaxy (=available in the main Galaxy tool shed), but it is not maintained since 2017. The wrapper has no tests, plus there were some other issues with the wrapper, so, it was not installed to Galaxy Australia. I could not find it on Galaxy Europe, either.  If you can find it on a public Galaxy server, you can try the protocol described in the paper. v.2 of Tax4fun was resealed in 2022. Maybe this version will be adapted for Galaxy at some point.\nAs for the tutorials: please check this section Galaxy Training! .\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you so much for your help. I am helping a undergraduate student with a project and we are learning as we go. I greatly appreciate your patience. So, I should be able to use my data to assemble metagenome and then do further analysis using the tutorials to look for args?\nI think I am still confused about whether I can use this data for this process or not. The student would like to look for args so I\u2019m trying to find some way to help them do that with the data that we have."}, {"role": "user", "text": "At this point, I am a little lost. Any help would be appreciate."}, {"role": "system", "text": "Hi @Melissa_Hayes\nFirst, someone with experience in metagenomics might provide a better answer. Second, I have a semantic issue: 16S data does not contain ARGs, hence can be used only for prediction of ARGs, while detection of \u2018ARG presence\u2019 requires whole metagenomic data. I might be wrong here, and people consider prediction or extrapolation as a presence. Maybe this approach provides a good approximation, but it seems there is no validation or confirmation of this in the paper.\nMetagenome cannot be assembled from 16S.\nKind regards,\nIgor"}], [{"role": "user", "text": "I ended up with this error message after using Maker in the Main Galaxy instance: \u201cPossible unintended interpolation of @2 in string \u2026\u201d. My attempts to autodetect the metadata to correct the problem did not work.\nI saw that others had this error with other tools (e.g. Trinity) within the last couple years. In one case, @jennaj said the error meant insufficient memory was allotted to the job or that the inputs were wrong. Hoping this was the case, I checked the input and ran the job again, but got the same result.\nIn another case, Matthias Bernt said it was a miscommunication between Galaxy, which uses the \u201c@\u201d symbol, and the perl interpreter that doesn\u2019t know what to do with it. He suggested adding a \" \\ \" in front of the \u201c@\u201d, although I don\u2019t know how to access the script to do this.\nAny other ideas on how to handle this type of error? I still can\u2019t tell whether it\u2019s possible to salvage my results, which were apparently successfully run aside from the metadata problem that couldn\u2019t be resolved. Or should I simply keep rerunning the job until it works?\nI got this error while doing the genome annotation tutorial, but I want to know how to avoid this issue when I\u2019m using my data.\nThanks!"}, {"role": "system", "text": "Can you post the full error massage, please?"}, {"role": "user", "text": "Thanks, @David. Here\u2019s my error message:\nPossible unintended interpolation of @2 in string at /cvmfs/main.galaxyproject.org/deps/_conda/envs/__maker@2.31.11/bin/\u2026/lib/5.26.2/x86_64-linux-thread-multi/Config_heavy.pl line 260.\nPossible precedence issue with control flow operator at /cvmfs/main.\nBut others have had similar messages, for instance here and here.\nI don\u2019t see this specific issue among the known issues in Galaxy Main (although maybe I missed it). What\u2019s the best way to proceed when confronted with this? Obviously it doesn\u2019t always happen when people use Maker, Trinity, etc., but if there\u2019s a way to go into the script to bypass the \u201c@\u201d problem, I\u2019d like to know."}, {"role": "system", "text": "Hi,\nAFAIK, the only way to correct this is going to the actual code, as it seems is hard-wired. The Galaxy team should take a more detailed look.\nThere are at least 4 other issues related to this specific bug of \u201c@\u201d in paths here at Galaxy Help:\n\n  \n    \n    \n    MAKER failed [qrsh] <defunct> \n  \n  \n    I have an issue with MAKER. \nWhen a launch maker, the jobs never end and use < 1 cpu. \nAfter looking on the node, i see that qrsh failed : [qrsh]  \nAny help would be appreciated. \n  [galaxy@node11 ~]$ ps aux | grep galaxy\n  galaxy   32367  0.0  0.0 107088  1548 ?        Ss   17:51   0:00 /bin/bash /media/vol2/home/galaxy/galaxy/database/jobs_directory/000/56/galaxy_56.sh\n  galaxy   32386  0.1  0.0 106192  1608 ?        S    17:51   0:00 /bin/bash /media/vol2/home/galaxy/galaxy/database/jobs_dire\u2026\n  \n\n this first one is about maker too (V 2.31.10):\nPossible unintended interpolation of @2 in string at /media/vol2/home/galaxy/galaxy/database/dependencies/_conda/envs/__maker@2.31.10/bin/../lib/5.26.2/x86_64-linux-thread-multi/Config_heavy.pl line 260.\n  Possible precedence issue with control flow operator at /media/vol2/home/galaxy/galaxy/database/dependencies/_conda/envs/__maker@2.31.10/lib/site_perl/5.26.2/Bio/DB/IndexedBase.pm line 805.\n  error: executing task of job 1007802 failed: execution daemon on host \"node11\" didn't accept task\n\nThe others :\n  \n    \n    \n    How to view Kraken2 Results in Krona? usegalaxy.org support\n  \n  \n    Hi , I am trying to install kraken2 databases through data manger. Its giving me an URL error to download the databases. Can you please check if the URL provided is correct?. \nhere is the error: \nFatal error: Exit code 1 () \nPossible unintended interpolation of @2 in string at /dep/_conda/envs/__kraken2@2.0.7_beta/bin/kraken2 line 16. \nTraceback (most recent call last): \nFile \u201c/data/shed_tools/testtoolshed.g2.bx.psu.edu/repos/dfornika/data_manager_build_kraken2_database/65cbfb5e6f65/data_manager\u2026\n  \n\n\n\n  \n    \n    \n    Error with Trinity De Novo Assembly in GVL 4.4.0 install usegalaxy.org support\n  \n  \n    We\u2019ve been experiencing persistent error pasted below with our lab\u2019s cloud-based installation of the Galaxy suite via the Genomics Virtual Laboratory 4.4.0.  Specifically, whenever we attempt a Trinity de novo assembly of a collection of Illumina TruSeq stranded mRNA paired end reads, we get the errors below.  We\u2019ve had no problems with these same data sets when using other tools like HiSat2 and StringTie.   We\u2019ve poked around in the \u201cmanage tools\u201d and \u201cmanage dependencies\u201d area of the Galaxy Ad\u2026\n  \n\n\n\n  \n    \n    \n    Kraken2 tool error \n  \n  \n    I see that there is an datamangers for kraken2 to install Minikraken2. Now I am able to install Minikraken2 but it doent contain all the required files to finish the job. \ntaxo.k2d file is missing. \nFatal error: Exit code 2 () \nPossible unintended interpolation of @2 in string at /dep/_conda/envs/__kraken2@2.0.7_beta/bin/kraken2 line 16. \nkraken2: database (\"/tool/tool-data/kraken2_databases/2019-05-07T122927Z_minikraken2_v2_8GB\") does not contain necessary file taxo.k2d \nThanks, \nJayanthi\n  \n\n"}, {"role": "user", "text": "Thanks so much for finding others with the same problem, @David. The fact that this glitch doesn\u2019t happen every time someone uses one of these tool makes me wonder if this problem really is caused by insufficient memory allotment, like @jennaj said.\nIf that\u2019s the case, would it be best to just rerun the job until it works? Seems like a bad use of time/resources.\nOr, if going into the code would be best, could someone show me how to do that? I\u2019m a newb at Galaxy.\nI suppose it\u2019s good to run into these issues on the tutorial before hitting these walls on my data."}, {"role": "system", "text": "Jared,\nI don\u2019t know if rerun can help. I also do not suggest you going to the code, instead, let someone from Galaxy take a more detailed look.\nMaybe you can try other instances of Galaxy and see if you have better luck.\nIndeed, tutorials are very nice to gain experience, good or bad."}, {"role": "user", "text": "Could someone from Team Galaxy help resolve this error when using Maker on Galaxy Main?\n\nPossible unintended interpolation of @2 in string at /cvmfs/main.galaxyproject.org/deps/_conda/envs/__maker@2.31.11/bin/\u2026/lib/5.26.2/x86_64-linux-thread-multi/Config_heavy.pl line 260.\nPossible precedence issue with control flow operator at /cvmfs/main.\n\nAs before, there is a message that says:\n\nRequired metadata values are missing. Some of these values may not be editable by the user. Selecting \u201cAuto-detect\u201d will attempt to fix these values.\n\nBut I\u2019ve tried auto-detecting the metadata, and it doesn\u2019t help.\nI\u2019ve also tried running this on another instance, Galaxy Europe, but got the same interpolation error. (By the way, I\u2019m now using my data rather than the tutorial mentioned above.)\nThanks for any ideas."}, {"role": "user", "text": "It appears to be working now, although I\u2019m still checking the output.\nI still get the @2 interpolation error, so I\u2019m concerned that could cause problems downstream. But now the data sets are usable instead of empty. The difference seems to be that I didn\u2019t select a species for the Dfam database this time.\nTo future users battling this error, check the above links and the responses I got from @bjoern.gruening, @abretaud and @hxr here. Thanks, guys."}], [{"role": "user", "text": "Hello,\nI\u2019m working on adding a couple custom tools to our Galaxy install, and one problem I\u2019m running across is with the names of datasets.\nMore specifically, one tool we use requires all input files to be in a certain \u201cinput\u201d directory. So, first thing my custom tool XML does is copy (using cp) the datasets from the list a user chooses as input (see below) to this folder. However, the datasets all have names such as dataset_9964.dat, dataset_9967.dat, etc. and not their names in the history/original names such as bin1.fasta, bin2.fasta, etc.\nAs such, when running this tool, the output (see second image below) in tabular TSV format things that all my user_genomes which I submitted to the tool are in fact called dataset_9964, dataset_9963, etc. as it uses the input file names to determine row names in output.\nI\u2019m wondering if there\u2019s any way I can retrieve the original files\u2019 names upon job submission.\nOur input section for the tool XML is as follows:\n<inputs>\n    <param name=\"input_files\" type=\"data\" multiple=\"true\" format=\"fasta\" label=\"Input FASTA files\" />\n</inputs>\n\nInput list/collection of FASTA files:\ncollection-input228\u00d7630 15 KB\nFirst column of output TSV, column name is \u201cuser_genome\u201d, and each row should represent a different input file (dataset_9963 = bin1.fasta, etc.):\n\nThanks in advance!"}, {"role": "system", "text": "The ParSNP tool has a similar requirement.\nSee https://github.com/brinkmanlab/galaxy-tools/blob/00868930fc05f48a702fe8357b58f004cf899238/parsnp/ParSNP.xml#L20-L27\nBascally, you don\u2019t need to copy the files, just create symlinks in a working folder.\nThe name of the dataset is accessible as $input_file.element_identifier"}], [{"role": "user", "text": "I am currently doing my project work on Exome sequencing and is using the following tutorial (Exome sequencing data analysis for diagnosing a genetic disease). I have an error in GEMINI load tool. It is showing that the VCF format is unavailable(SNPEff eff). Please look into the problem and help me with it. Thank you\n\nGalaxy - Google Chrome 13-05-2022 20_42_051920\u00d71020 132 KB\n\n\nGalaxy - Google Chrome 13-05-2022 20_41_251920\u00d71020 126 KB\n"}, {"role": "system", "text": "Hi @Thara_Dharanidhar\nnot quite sure what we are seeing in those screenshots.\nApparently, you\u2019ve run the tool already and have produced a gemini.sqlite dataset with it?\nAnyway, the reason why the Vcf dataset is not offered in the select box is likely that you haven\u2019t set a format/dbkey on it. Click on the pencil icon on your dataset and set the dbkey to hg19."}, {"role": "user", "text": "Hi @wm75\nI did exactly what you told but i am not getting the output. I set the dbkey as hg19 but no result.Can u suggest alternative tool instead of GEMINI load.\n\nimage1366\u00d7768 117 KB\n"}, {"role": "system", "text": "I\u2019m sorry, but I don\u2019t understand what the issue is. You have obtained a GEMINI load dataset and its state is green. Everything looks good and you can just continue with the tutorial, can\u2019t you?"}, {"role": "user", "text": "Hi@wm75\nGEMINI load tool is working and its state is also green but I am unable to view the output as some file is getting downloaded every time I click the view option. What kind of data will be obtained from GEMINI load tool as you can see my screenshot showing that (loading chunk 1 , loading chunk 2\u2026) So the data will look like this or in some other format. Please help me out.\n\nimage1920\u00d71080 240 KB\n\nThis file is getting downloaded whenever I click the view data . When I click the file this message appears \u201cfile cannot be viewed\u201d.\n\nimage1920\u00d71080 235 KB\n"}, {"role": "system", "text": "Yes, the GEMINI load tool produces a GEMINI database, which is not intended to be viewed, but queried. Please follow the tutorial, which explains a few ways how you can do that.\nThese tutorials: Identification of somatic and germline variants from tumor and normal sample pairs and Calling variants in diploid systems demonstrate more options.\nThink of the GEMINI database as a computer-friendly version of a VCF. You can\u2019t read it, but queries are very efficient. The GEMINI load tool is essentially the converter that takes VCF and produces the database format."}], [{"role": "user", "text": "I have a problem when trying to get genomic DNA sequences from a list of genomic coordinates. The file contains regions aroung ChIP-seq peaks and has been obtained trough several Galaxy functions (Map with Bowtie, MACS2 callpeak, compute and cut). It is correctly formatted (as .interval, but I have also tried as .bed) with columns corresponding to chr start end name and its genome version is indicated: [format:interval database:mm9].\nWhen I apply the function \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d selecting mm9 from the locally cached genomes I get an empty file with this warning:\n*622 warnings, 1st is: Chromosome by name \u2018chr19\u2019 was not found for build \u2018mm9\u2019. *\nSkipped 622 invalid lines, 1st is #1, \u201cchr19\t3204403\t3204904\tSAM-to-BAM_on_data_7__converted_BAM_peak_1\u201d.\nI get the same when I try to redo the analysis selecting mm10 instead of mm9.\nNote that in https://usegalaxy.org/ there are two instances of \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d, one is version 3.0.3 (the same available in https://usegalaxy.eu/) but there is also a version 2.2.4 which works perfectly fine on the same .interval file above and it returns the correct fasta file with all the sequences.\nMoreover, in https://usegalaxy.org/ the version 3.0.3 of \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d does not even display mm9 among the available cached genomes (even If I do the entire pipeline including mapping).\nI need to perform this step on https://usegalaxy.eu/, can you help me with this?\nBest,\nMattia"}, {"role": "system", "text": "Hi @Mattiaf!\nI noticed a problem myself with a mouse genome build a short time ago at https://usegalaxy.org (this ticket has details). Could be related \u2013 it is not clear which server is presenting with the chr19 not being found \u2013 could you clarify?\nI\u2019ll be looking at this more today and will include figuring out the problems you are seeing with the Extract tool(s). Both servers updated how the tools are organized and it seems some small issues were introduced (duplicated tools + unconnected indexes).\nFor now: Use the version of the tool on the server that is working for you. Make sure the \u201cdatabase\u201d metadata attribute is assigned to your interval/bed input. Either server could be used \u2013 these are distinct servers/accounts, so that might involve copying the data from one server to the other.\nMore feedback soon and thanks for reporting the problem.\nping @hxr \u2013 Let\u2019s coordinate"}, {"role": "user", "text": "Thanks Jennifer.\nI get the \u201cChromosome by name \u2018chr19\u2019 was not found etc.\u201d message when using https://usegalaxy.eu/, with version 3.0.3 of the extract tool.\nRelated to your ticket: when I Choose the source for the reference genome among the locally cached genomes I see two occurrences of mm9 (see picture), but I get the empty file with both.\n\nusegalaxy.eu.png691\u00d7506 14.2 KB\n\nOn https://usegalaxy.org/ I am not able to run the extract tool version 3.0.3 because I cannot see any cached genome.\n\nusegalaxy.org.png738\u00d7509 14.9 KB\n\nOn https://usegalaxy.org/ the extract tool version 2.2.4 works perfectly.\nIn both servers the database metadata attribute is correctly specified."}, {"role": "system", "text": "Thanks for info. Am reviewing with these extra details."}, {"role": "system", "text": "Thanks for the ping @jennaj! Quite strange.\n\n1 warnings, 1st is: Chromosome by name 'chr1' was not found for build 'mm9'. \nSkipped 1 invalid lines, 1st is #1, \"chr1\t1000\t2000\"\n\n\nBut chr1 is in the twobit file, according to faTwoBitInfo\n$ ./twoBitInfo /data/db/reference_genomes/mm9/seq/mm9.2bit out.tab\n$ head out.tab\nchr1    197195432\nchr2    181748087\nchr3    159599783\nchr4    155630120\nchr5    152537259\n\n@Mattiaf can you share the file?"}, {"role": "user", "text": "Dear Helena,\nyou can access the complete history at /u/mforcato/h/historyerrors (I cannot post the entire link).\nDataset 34 contains the regions that I want to give as input to \u201cextract genomic dna\u201d and Dataset 35 is the empty output I get.\nM"}, {"role": "system", "text": "\n\n\n jennaj:\n\nThanks for info. Am reviewing with these extra details.\n\n\nI suspect there is a problem with the updated Extract version 3.0.3 or a problem with the indexes these tools use or both. Since the prior version 2.2.4 was working, and an updated Galaxy release was pending, I had set this aside to focus on other issues, but will go back in and figure this out one.\n@hxr I\u2019ll chat with you direct, or put details in this ticket and ping you there, and you could do the same, or we might need to create an issue ticket against the Extract tool and link everything up if the problem is rooted there\u2026\nOne small note: The Extract tool (at least up to version 2.2.4) was based on a UCSC tool that technically would accept data with a bed OR interval datatype assigned, but the data content had to be formatted as a bed dataset would be (specifically, the column ordering). Not sure yet if/how version 3.0.3 behaves differently. Likely the same but not confirmed. And this shouldn\u2019t impact finding indexes but those will be part of the review."}, {"role": "system", "text": "Hi @Mattiaf,\nthis should now be fixed on usegalaxy.eu.\nCheers,\nBjoern"}], [{"role": "user", "text": "Hello there!\nI am working with WGBS data and I want to extract the DMRs in a particular mutant.\nI used bwameth to align the sequencing data to mm10 genome. After that, I sorted the alignment file and used methyldackel to see the methylation status. I used the metilene tool to extract DMRs but this doesn\u2019t work. I get an error message like this\n\" null device 1 [WARNING] Tue Aug 9, 16:7:8, 2022 Input files need to be SORTED, i.e., use \u201cbedtools sort -i file >file.sorted\u201d. I sorted the methyldackel output files using Sortbed and ran metilene again. Same error message. I read from the manual for Metilene, that the input file has to be in a certain format and the input file can be prepared using bedtools unionbedg. But this tool is not present in usegalaxy.eu or .org. Can anyone help me to prepare the input files for metilene?\nThanks!!\nAbishek"}, {"role": "system", "text": "Hi @srinivasa_abishek,\nI\u2019m trying to reproduce the problem, but I couldn\u2019t. Would you mind to share your history with me? My email is .\nRegards"}, {"role": "system", "text": "I got the same exact error message, while some time back, I ran the same and didn\u2019t get this message\u2026"}, {"role": "system", "text": "Hi @Clem_Lafon,\ndid you solve the problem?\nRegards"}, {"role": "system", "text": "Thanks for asking! Yes I did, by removing the last two columns and first row (description) of the Methyldackel output. It then did the trick and Metilene took them with no problem! The weird thing is that I didn\u2019t have this problem at all a year ago when I used the exact same files and process (using it for teaching a course).\nCl\u00e9ment"}], [{"role": "user", "text": "Dear all,\nI\u2019m discovering the galaxy world and I try to integrate some tools to galaxy.\nI started with recentrifuge a tool for taxonomy visualization but I failed to generate the output.\nIn fact, the planemo test failed to compare because the output of the test is empty.\nThe recentrifuge code don\u2019t explicit an output but I tried with the -o option (give another name to the output) without success.\nSo I don\u2019t know how to deal to solve the no output problem.\nAs a minimal example with only my input file, output files and test code :\n  <command detect_errors=\"exit_code\"><![CDATA[\n        mkdir '$__tool_directory__/output_dir' &&\n        rcf\n        #if $db.db_selector == \"cached\"\n          -n '$db.cached_db.fields.path'\n        #end if\n\n        #if $file_type.filetype == \"centrifuge\"\n          -f '$input_file'\n        #else if $file_type.filetype == \"lmat\"\n          -l '$input_file'\n        #else if $file_type.filetype == \"clark\"\n          -r '$input_file'\n        #else if $file_type.filetype == \"kraken\"\n          -k '$input_file'\n        #end if\n\n        #if '$output_name.modify_name' == \"true\"\n          -o '$__tool_directory__/output_dir/$output_name.file_name'\n        #else\n          -o \"$__tool_directory__/output_dir/rcf.output\"\n        #end if\n\n        -e '$output_type'\n\n    ]]></command>\n<inputs>\n<param name=\"input_file\" type=\"data\" format=\"tabular\" label=\"Output from taxonomy assignation\" help=\"Select taxonomy file tabular formated\"/>\n</inputs>\n  <outputs>\n      <!-- HTML report -->\n      <data name=\"html_report\" format=\"html\" from_work_dir=\"output_dir\"/>\n\n      <data name=\"data_csv\" format=\"csv\" from_work_dir=\"output_dir\">\n         <filter>output_type and \"CSV\" in output_type</filter>\n      </data>\n      <data name=\"data_tsv\" format=\"tsv\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"TSV\" in output_type</filter>\n      </data>\n      <data name=\"stat_tsv\" format=\"tsv\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"TSV\" in output_type</filter>\n      </data>\n      <data name=\"xls_report\" format=\"xlsx\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"XLSX\" in output_type or output_type and \"DYNOMICS\" in output_type </filter>\n      </data>\n        <data name=\"stat_csv\" format=\"csv\" from_work_dir=\"output_dir\">\n          <filter>output_type and \"CSV\" in output_type or output_type and \"MULTICSV\" in output_type </filter>\n      </data>\n    </outputs>\n<tests>\n      <test> <!-- Test 1: Basic test with kraken file-->\n        <conditional name=\"db\">\n          <param name=\"db_selector\" value=\"cached\"/>\n          <param name=\"cached_db\" value=\"test-db-2022\"/>\n        </conditional>\n          <param name=\"filetype\" value=\"kraken\"/>\n          <param name=\"input_file\" value=\"kraken.txt\"/>\n          <param name=\"output_type\" value=\"CSV\"/>\n          <param name=\"scoring\" value=\"KRAKEN\"/>\n          <param name=\"summary\" value=\"avoid\"/>\n\n        <output name=\"data_csv\" file=\"output_dir/rcf.output.data.csv\">\n        </output>\n        <output name=\"stat_csv\" file=\"output_dir/rcf.output.stat.csv\">\n        </output>\n        <output name=\"html_report\" file=\"output_dir/rcf.output\">\n        </output>\n      </test>\n    </tests>\n\nThe planemo report give me the code seem to be ok :\nmkdir '/home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir' && rcf -n '/home/pierre/.planemo/planemo_tmp_a_v6y1k9/test-data/test-db'  -k '/tmp/tmpnivasmvl/files/2/3/3/dataset_2334d82b-ddc1-4dc8-bb41-df026f8c1a34.dat'  -o \"/home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output\"  -e 'CSV'\n\nThe standard ouput from planemo report is okay, the softaware indicate generating of files and give me the output directory\nGenerating final plot /home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output OK!\nGenerating csv extra output /home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output.]*.csv OK!\n"}, {"role": "user", "text": "The link to the git with the code\nrecentrifuge"}, {"role": "system", "text": "Hi @pimarin,\nin the from_work_dir option you should specify the dataset path, not just the folder; e.g:\n<data name=\"data_csv\" format=\"csv\" from_work_dir=\"output_dir/rcf.output\">\n   <filter>output_type and \"CSV\" in output_type</filter>\n</data>\n\nRegards"}, {"role": "user", "text": "Hi @gallardoalba ,\nThank you for you efficiency, but it didn\u2019t work. the -o option is not an obligation, I forced it hoping it solve the problem, but even if I don\u2019t use it I have the same trouble\u2026"}, {"role": "user", "text": "I also tried with multiple dataset :\n<outputs>\n      <data name=\"html_report\" format=\"html\">\n        <discover_datasets pattern=\"__designation__\" directory=\"$__tool_directory__/output_dir\" visible=\"false\" />\n      </data>\n      <data name=\"data_csv\" format=\"csv\">\n        <discover_datasets pattern=\"__designation_and_ext__\" directory=\"$__tool_directory__/output_dir\" visible=\"false\" />\n      </data>\n</outputs>\n<test>\n  <output name=\"data_csv\" >\n          <discovered_dataset designation=\"/home/pierre/Seafile/ABROMICS/galaxy-tools/tools/recentrifuge/test-data/output_dir/data_csv\" ftype=\"csv\">\n             <assert_contents><has_n_lines n=\"10\" /></assert_contents>\n          </discovered_dataset>\n        </output>\n        <output name=\"html_report\" >\n          <discovered_dataset designation=\"/home/pierre/Seafile/ABROMICS/galaxy-tools/tools/recentrifuge/test-data/output_dir/html_report\" ftype=\"html\">\n             <assert_contents><has_text text=\"Enterobacteriaceae\" /></assert_contents>\n          </discovered_dataset>\n        </output>\n</test>\n"}, {"role": "system", "text": "Hi @pimarin,\nI opened a PR; it fixes the problem with the outputs Fix outputs by gallardoalba \u00b7 Pull Request #1 \u00b7 mesocentre-clermont-auvergne/galaxy-tools \u00b7 GitHub\nRegards"}], [{"role": "user", "text": "Hello,\nI installed yesterday Docker on my Windows 8 PC.  I use Norton360 antivirus.\nI then installed Galaxy.\nI start Galaxy as follows:\ndocker run -d -p 8080:80 -p 8021:21 -p 8022:22 bgruening/galaxy-stable\nThe container creates, I wait a few minutes and I can see the logs\nI try to connect to Galaxy using Chrome throught the addresses http://localhost:8080/  and http://127.0.0.1:8080  -  in both cases I get\nThis site can\u2019t be reached\n127.0.0.1 refused to connect.\nERR_CONNECTION_REFUSED\nCan you let me know what do I need to do to be able to access my Galaxy?\nEnclosed the logs.\nThanks !\nGeorge Weingart\n$ docker logs -f awesome_elgamal\nEnable Galaxy reports authentification\nChecking /export\u2026\nDisable Galaxy Interactive Environments. Start with --privileged to enable IE\u2019s.\nStarting postgres\npostgresql: started\nChecking if database is up and running\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: http://sqlalche.me/e/f405)\nWaiting for database\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: http://sqlalche.me/e/f405)\nWaiting for database\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: http://sqlalche.me/e/f405)\nWaiting for database\nDatabase connected\nStarting cron\ncron: started\nStarting ProFTP\nproftpd: started\nStarting Galaxy reports webapp\nreports: started\nStarting nodejs\ngalaxy:galaxy_nodejs_proxy: started\nStarting condor\ncondor: started\nStarting slurmctld\nStarting slurmd\nCreating admin user admin with key fakekey and password password if not existing\n==> /home/galaxy/logs/handler0.log <==\ngalaxy.workflow.scheduling_manager INFO 2020-12-17 18:52:14,638 No workflow schedulers plugin config file defined, using default scheduler.\ngalaxy.workflow.scheduling_manager DEBUG 2020-12-17 18:52:14,638 Starting workflow schedulers\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,688 Binding and starting galaxy control worker for handler0\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,722 Queuing async task rebuild_toolbox_search_index for handler0.\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:52:14,808 handler0 is config watcher\ngalaxy.util.watcher DEBUG 2020-12-17 18:52:14,843 Watching for changes in directory (recursively): /galaxy-central/tool-data\ngalaxy.app INFO 2020-12-17 18:52:14,976 Galaxy app startup finished (21130.909 ms)\ngalaxy.web_stack INFO 2020-12-17 18:52:14,976 Galaxy server instance \u2018handler0\u2019 is running\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,986 Instance \u2018handler0\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.queue_worker DEBUG 2020-12-17 18:52:14,986 App is not a webapp, not building a search index\n==> /home/galaxy/logs/handler1.log <==\ngalaxy.workflow.scheduling_manager INFO 2020-12-17 18:52:24,026 No workflow schedulers plugin config file defined, using default scheduler.\ngalaxy.workflow.scheduling_manager DEBUG 2020-12-17 18:52:24,026 Starting workflow schedulers\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,063 Binding and starting galaxy control worker for handler1\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,082 Queuing async task rebuild_toolbox_search_index for handler1.\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:52:24,148 handler1 is config watcher\ngalaxy.util.watcher DEBUG 2020-12-17 18:52:24,190 Watching for changes in directory (recursively): /galaxy-central/tool-data\ngalaxy.app INFO 2020-12-17 18:52:24,233 Galaxy app startup finished (11560.530 ms)\ngalaxy.web_stack INFO 2020-12-17 18:52:24,234 Galaxy server instance \u2018handler1\u2019 is running\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,263 Instance \u2018handler1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.queue_worker DEBUG 2020-12-17 18:52:24,263 App is not a webapp, not building a search index\n==> /home/galaxy/logs/reports.log <==\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,580 Enabling \u2018history\u2019 controller, class: History\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,590 Enabling \u2018users\u2019 controller, class: Users\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,603 Enabling \u2018tools\u2019 controller, class: Tools\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,609 Enabling \u2018paste.httpexceptions\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,642 Enabling \u2018RecursiveMiddleware\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,661 Enabling \u2018ErrorMiddleware\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,670 Enabling \u2018TransLogger\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,675 Enabling \u2018XForwardedHostMiddleware\u2019 middleware\nStarting server in PID 563.\nserving on http://127.0.0.1:9001\n==> /home/galaxy/logs/slurmctld.log <==\n[2020-12-17T18:52:43.624] No trigger state file (/tmp/slurm/trigger_state.old) to recover\n[2020-12-17T18:52:43.624] _preserve_plugins: backup_controller not specified\n[2020-12-17T18:52:43.624] Reinitializing job accounting state\n[2020-12-17T18:52:43.624] cons_res: select_p_reconfigure\n[2020-12-17T18:52:43.624] cons_res: select_p_node_init\n[2020-12-17T18:52:43.624] cons_res: preparing for 1 partitions\n[2020-12-17T18:52:43.624] Running as primary controller\n[2020-12-17T18:52:43.641] No parameter for mcs plugin, default values set\n[2020-12-17T18:52:43.641] mcs: MCSParameters = (null). ondemand set.\n[2020-12-17T18:52:46.664] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2\n==> /home/galaxy/logs/slurmd.log <==\n[2020-12-17T18:52:44.740] error: Domain socket directory /var/spool/slurmd: No such file or directory\n[2020-12-17T18:52:44.741] Message aggregation disabled\n[2020-12-17T18:52:44.741] CPU frequency setting not configured for this node\n[2020-12-17T18:52:44.839] slurmd version 17.11.2 started\n[2020-12-17T18:52:44.856] slurmd started on Thu, 17 Dec 2020 18:52:44 +0000\n[2020-12-17T18:52:44.859] CPUs=1 Boards=1 Sockets=1 Cores=1 Threads=1 Memory=985 TmpDisk=18275 Uptime=10758 CPUSpecList=(null) FeaturesAvail=(null) FeaturesActive=(null)\n==> /home/galaxy/logs/uwsgi.log <==\ngalaxy.web_stack DEBUG 2020-12-17 18:52:26,867 [p:644,w:1,m:0] [MainThread] Calling postfork function: \ngalaxy.web_stack DEBUG 2020-12-17 18:52:26,870 [p:644,w:1,m:0] [MainThread] Calling postfork function: <function postfork_setup at 0x7fa776f5c2f0>\ngalaxy.web_stack INFO 2020-12-17 18:52:26,871 [p:644,w:1,m:0] [MainThread] Galaxy server instance \u2018main.web.1\u2019 is running\nStarting server in PID 540.\nserving on http://127.0.0.1:8080\nserving on uwsgi://127.0.0.1:4001\ngalaxy.queue_worker INFO 2020-12-17 18:52:26,991 [p:644,w:1,m:0] [Thread-1] Instance \u2018main.web.1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.tools.search DEBUG 2020-12-17 18:52:26,991 [p:644,w:1,m:0] [Thread-1] Starting to build toolbox index.\ngalaxy.tools.search DEBUG 2020-12-17 18:52:27,113 [p:644,w:1,m:0] [Thread-1] Toolbox index finished (122.256 ms)\ngalaxy.tools.search DEBUG 2020-12-17 18:52:27,689 [p:647,w:2,m:0] [Thread-1] Toolbox index finished (836.592 ms)\n==> /home/galaxy/logs/handler0.log <==\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:53:14,860 handler0 is not config watcher\n==> /home/galaxy/logs/handler1.log <==\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:53:24,201 handler1 is not config watcher\n==> /home/galaxy/logs/slurmctld.log <==\n[2020-12-17T18:57:43.858] error: Could not open job state file /tmp/slurm/job_state: No such file or directory\n[2020-12-17T18:57:43.859] error: NOTE: Trying backup state save file. Jobs may be lost!\n[2020-12-17T18:57:43.859] No job state file (/tmp/slurm/job_state.old) found"}, {"role": "system", "text": "I got the same error from a fresh docker run:\ndocker run -d -p 8080:80 -p 8021:21 -p 8022:22 bgruening/galaxy-stable\n\nAlso:\ndocker run -it -p 9080:80 -p 9081:21 -p 9082:22 quay.io/bgruening/galaxy\n\nChecking if database is up and running\nTraceback (most recent call last):\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1284, in _execute_context\n    cursor, statement, parameters, context\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 590, in do_execute\n    cursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \"galaxy_user\" does not exist\nLINE 3: FROM galaxy_user \n             ^\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/check_database.py\", line 25, in <module>\n    query.count()\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3749, in count\n    return self.from_self(col).scalar()\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3469, in scalar\n    ret = self.one()\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3436, in one\n    ret = self.one_or_none()\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3405, in one_or_none\n    ret = list(self)\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3481, in __iter__\n    return self._execute_and_instances(context)\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 3506, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1020, in execute\n    return meth(self, multiparams, params)\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1139, in _execute_clauseelement\n    distilled_params,\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1324, in _execute_context\n    e, statement, parameters, cursor, context\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1518, in _handle_dbapi_exception\n    sqlalchemy_exception, with_traceback=exc_info[2], from_=e\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 178, in raise_\n    raise exception\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1284, in _execute_context\n    cursor, statement, parameters, context\n  File \"/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 590, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \"galaxy_user\" does not exist\nLINE 3: FROM galaxy_user \n             ^\n"}, {"role": "system", "text": "Hi @Diana\nWould you please explain a bit more about your configuration?\n\nWhat version of the Docker are you running? The latest is recommended.\nWhat is your OS + version?\nIs this a new installation or an upgrade? If the second, what was the original version?\nSummary of steps that you have done so far\n\nReferences:\n\nGalaxy Docker - Galaxy Community Hub\ndocker-galaxy-stable/README.md at master \u00b7 bgruening/docker-galaxy-stable \u00b7 GitHub\n\nLet\u2019s start there"}, {"role": "system", "text": "Thanks for your answer.\nAfter 4 hours dealing with this, I was able to get the Galaxy screen.\nI am using Docker version 20.10.7, build 20.10.7-0ubuntu5~20.04.2 and Ubuntu 20 64bits.\nI had to reinstall everything from scratch multiple times and it finally worked.\nNow I am trying to enable the Google login in the container, but I am not getting the login button. (Maybe this should be a different issue).\nThis is an extract of my galaxy.yml, and I have configured the other two xml files:\n...\ngalaxy:\n  enable_oidc: true\n  oidc_config_file: config/oidc_config.xml\n  oidc_backends_config_file: config/oidc_backends_config.xml\n\nWhere can I see a file with errors so I can investigate the problem? The files in /home/galaxy/logs don\u2019t show anything unusual."}, {"role": "system", "text": "\n\n\n Diana:\n\nWhere can I see a file with errors so I can investigate the problem? The files in /home/galaxy/logs don\u2019t show anything unusual.\n\n\nMaybe advanced logging will help? GitHub - bgruening/docker-galaxy-stable: Docker Images tracking the stable Galaxy releases."}], [{"role": "user", "text": "Hi @hxr,\nThanks for your help before\u2013I was able to get those files uploaded!\nNow I am trying to FTP some larger files to usegalaxy.eu and am encountering a similar problem. I have successfully done this in the past to the main usegalaxy.org server, but for usegalaxy.edu, I am getting an error when I try to connect: Connection attempt failed with \u201cETIMEDOUT - Connection attempt timed out\u201d.\nI have used the tutorial on this page: https://galaxyproject.org/ftp-upload/\nand this information (https://galaxyproject.eu/ftp/) specific to usegalaxy.eu for guidance.\nBelow is a screenshot from FileZilla showing the information I input for the FTP (I blocked out my username).\nFrom the Grafana page you sent before, it doesn\u2019t look like now is a particularly \u2018bad\u2019 time to be connecting, so I wonder if I have entered something incorrectly. Do you have any suggestions for solving the issue? I tried increasing my \u201cTimeout in seconds\u201d setting within FileZilla but that didn\u2019t seem to help.\nThanks!\nKristen\n\nScreen Shot 2019-01-09 at 9.58.06 AM.png892\u00d7683 84.4 KB\n"}, {"role": "system", "text": "@krhaynes can you please try ftp://galaxy.uni-freiburg.de as FTP server?"}], [{"role": "user", "text": "Hi, I haven\u2019t been able to get anything to run on Galaxy all morning. I got the following error message that suggests contacting an administrator. Thanks\n\"An error occurred while updating information with the server. Please contact a Galaxy administrator if the problem persists.\n\n\n{\n  \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"22.01\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"\",\n    \"responseJSON\": null,\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"2022-02-08T15:33:46.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n    \"traditional\": true,\n    \"data\": {\n      \"order\": \"hid\",\n      \"v\": \"dev\",\n      \"q\": [\n        \"update_time-ge\",\n        \"deleted\",\n        \"purged\"\n      ],\n      \"qv\": [\n        \"2022-02-08T15:33:46.000Z\",\n        \"False\",\n        \"False\"\n      ]\n    },\n    \"emulateHTTP\": false,\n    \"emulateJSON\": false,\n    \"textStatus\": \"error\",\n    \"errorThrown\": \"Internal Server Error\"\n  },\n  \"url\": \"https://usegalaxy.org/api/histories/eee48555debc811e/contents?order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2022-02-08T15%3A33%3A46.000Z&qv=False&qv=False\",\n  \"model\": [\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"Barcode Splitter on data 101 and data 100: Summary\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 103,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b56d01d266ccbc7ebf\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:07:12.507Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b574968129c4e2f3ec\",\n      \"id\": \"bbd44e69cb8906b574968129c4e2f3ec\",\n      \"extension\": \"tabular\",\n      \"update_time\": \"2022-02-08T20:16:35.347Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b574968129c4e2f3ec\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"download\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b574968129c4e2f3ec&metadata_name=\"\n      }\n    },\n    {\n      \"collection_type\": \"list\",\n      \"deleted\": false,\n      \"history_content_type\": \"dataset_collection\",\n      \"model_class\": \"HistoryDatasetCollectionAssociation\",\n      \"job_source_type\": \"Job\",\n      \"name\": \"Barcode Splitter on data 101 and data 100\",\n      \"element_count\": 97,\n      \"hid\": 102,\n      \"populated_state_message\": null,\n      \"url\": \"/api/histories/eee48555debc811e/contents/dataset_collections/5db9dc0679243c07\",\n      \"history_id\": \"eee48555debc811e\",\n      \"type\": \"collection\",\n      \"type_id\": \"dataset_collection-5db9dc0679243c07\",\n      \"populated_state\": \"ok\",\n      \"update_time\": \"2022-02-08T20:16:35.293Z\",\n      \"tags\": [],\n      \"job_source_id\": \"bbd44e69cb8906b5da1c4e487dc9f4e4\",\n      \"create_time\": \"2022-02-08T20:07:12.488Z\",\n      \"id\": \"5db9dc0679243c07\",\n      \"visible\": true,\n      \"contents_url\": \"/api/dataset_collections/5db9dc0679243c07/contents/234faa589298f537\",\n      \"elements\": []\n    },\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"Non Phi reads Bowtie2 on data 8 and data 5: unaligned reads (L)\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 101,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b58db42f51ccada7ad\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:06:22.860Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b53e081997149ac3c6\",\n      \"id\": \"bbd44e69cb8906b53e081997149ac3c6\",\n      \"extension\": \"fasta\",\n      \"update_time\": \"2022-02-08T20:07:12.503Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b53e081997149ac3c6\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"download\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b53e081997149ac3c6&metadata_name=\"\n      }\n    },\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"IL barcodes.txt\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 100,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b59774e56bdfa98f55\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:06:16.546Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"id\": \"bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"extension\": \"tabular\",\n      \"update_time\": \"2022-02-08T20:07:12.503Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"download\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b5d2eb20229bdea3f3&metadata_name=\"\n      }\n    }\n  ],\n  \"user\": {\n    \"id\": \"e9b380ca8aa5cec4\",\n    \"username\": \"bmaguire\",\n    \"total_disk_usage\": 93489887631,\n    \"nice_total_disk_usage\": \"87.1 GB\",\n    \"quota_percent\": 34,\n    \"is_admin\": false,\n    \"preferences\": {\n      \"dbkeys\": \"{\\\"test\\\": {\\\"name\\\": \\\"hopethisworks.txt\\\", \\\"fasta\\\": 62915310, \\\"len\\\": 62915623, \\\"linecount\\\": 62915624}}\"\n    },\n    \"tags_used\": [\n      \"cpg\",\n      \"exons\"\n    ],\n    \"deleted\": false,\n    \"purged\": false,\n    \"quota\": \"250.0 GB\"\n  }\n}\n"}, {"role": "system", "text": "Hi @HMcBain\nPlease try again. We have been updating UseGalaxy.org over the last few days to prepare for the next software release. A few jobs may run into trouble due to the timing of server restarts. The solution is to rerun.\nIf you end up with a different error, we can follow up from there."}, {"role": "user", "text": "Hi Jenna, Thanks for your reply. I\u2019ve been trying multiple times over the last 9 hours. When I click execute nothing happens. Will let you know if I get another error message- but so far (except that one time) just nothing when I try to execute.\nThanks"}, {"role": "system", "text": "Hi @HMcBain\nIt did run into performance problems today after the original reply: slower UI response timing, occasional pop-ups, or menus not clearing.\nBut I ran a few more tests just now, and jobs are failing for what looks like a server-side issue. I wasn\u2019t able to reproduce the part about jobs or workflows not submitting at all. Everything was submitted but all produced a red error result very quickly. I\u2019ve alerted our admins about it. You may notice that things are working better before I get back to you here due to time differences, but I\u2019ll still post an update either way.\nAnd if you do get an error meanwhile, do send in a bug report for it. One is enough, we\u2019ll see the other errors (if any) that are in the same history. Please include the URL link to this topic in the comments so we can associate the two. Examples of problems are useful.\nKnown issue: The bug report submission form does have a problem like what you stated. In short, the form doesn\u2019t appear to have been submitted when it actually was. So it is OK to click on that submit button one (or a few times but wait a few seconds in between). We\u2019ll notice what is going on. This specific issue was isolated as far as we know and is already on our fix-it shortlist for the upcoming Galaxy release.\nMore updates soon, and that you for the follow-up!"}, {"role": "user", "text": "Hi Jenna. Still can\u2019t get things to execute. I\u2019ve had the odd success- maybe three or four adapter trims per day but that\u2019s all I can get done. Thanks for all you do!"}, {"role": "system", "text": "Hi @HMcBain\nHopefully, you\u2019ve already solved the problem \u2013 but I can let you know that the prior performance issues are resolved at UseGalaxy.org.\nIf the jobs are just queued (grey in color), some wait time is expected when working at public Galaxy servers. These are shared resources: some of your jobs will run, the some of the other people\u2019s jobs, then more of yours again, repeat. Try to not delete queued jobs \u2013 instead, allow jobs to run to completion. When jobs are deleted and rerun, the new job ends up at the back of the queue, extending the wait time. If you do that repeatedly and fast enough, it would be possible to not have many jobs complete at all.\nGetting jobs queued up as soon as possible, and setting an email notification, are helpful strategies. Workflows and Dataset Collections can help to batch an analysis."}, {"role": "user", "text": "Hi Jenna, Thanks for your email. I haven\u2019t been checking recently, as after a few days of trying I got set up with a local instance instead, and that has been working well. Just went back and tried executing a trim on the main Galaxy and it ran OK, so the issue seems fixed. Thanks!"}], [{"role": "user", "text": "The DESEQ results file is empty despite the other files being properly populated (i.e., VST, normalized, rlog, plots). When I used this tool last week, it worked (I am using Galaxy). What am I doing wrong here?"}, {"role": "system", "text": "Sorry, that was a bug in the tool wrapper that has been fixed in https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.6+galaxy1"}, {"role": "system", "text": "I am encountering the same issue now and have tried different versions in usegalaxy.eu (currently the queue in usegalaxy.org is so slow that I have to use the eu one). Here\u2019s my history if it helps Galaxy | Europe | Published History | Unnamed history"}, {"role": "system", "text": "The DESEQ results file is empty despite the other files being properly populated (i.e., VST, normalized, rlog, plots). Please this problem is back again\nIt work fine for me on Sunday can someone help me thanks"}, {"role": "system", "text": "my other files are not being populated\u2026\nI\u2019ve also been having issues running the tool in R."}, {"role": "system", "text": "Dear all,\nI have the same problem in Galaxy and R (download/opening doesn\u2019t work).\nBest and thank you for your help in advance."}, {"role": "system", "text": "Same here. I have been rerunning several jobs from 2 weeks ago, that worked before, with version 2.11.40.6+galaxy2 and version 2.11.40.6+galaxy1 with the same and different settings and keep getting empty results no matter what I change. I also reran Salmon but this didn\u2019t change the outcome."}, {"role": "system", "text": "It works again for me.\nMany thanks to whoever fixed it."}], [{"role": "user", "text": "2 lines\n\nTASK [usegalaxy_eu.certbot : Request certificate] ******************************\nfatal: [galaxyimu]: FAILED! => {\u201cchanged\u201d: true, \u201ccmd\u201d: [\u201c/opt/certbot/bin/certbot\u201d, \u201ccertonly\u201d, \u201c\u2013test-cert\u201d, \u201c\u2013non-interactive\u201d, \u201c\u2013webroot\u201d, \u201c\u2013register-unsafely-without-email\u201d, \u201c\u2013agree-tos\u201d, \u201c-w\u201d, \u201c/srv/nginx/_well-known_root\u201d, \u201c-d\u201d, \u201cgalaxyimu\u201d], \u201cdelta\u201d: \u201c0:00:02.063752\u201d, \u201cend\u201d: \u201c2022-06-27 19:06:29.794965\u201d, \u201cmsg\u201d: \u201cnon-zero return code\u201d, \u201crc\u201d: 1, \u201cstart\u201d: \u201c2022-06-27 19:06:27.731213\u201d, \u201cstderr\u201d: \u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\\nAn unexpected error occurred:\\nError creating new order :: Cannot issue for \"galaxyimu\": Domain name needs at least one dot\\nAsk for help or search for solutions at https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d, \u201cstderr_lines\u201d: [\u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\u201d, \u201cAn unexpected error occurred:\u201d, \u201cError creating new order :: Cannot issue for \"galaxyimu\": Domain name needs at least one dot\u201d, \u201cAsk for help or search for solutions at https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d], \u201cstdout\u201d: \u201cRequesting a certificate for galaxyimu\u201d, \u201cstdout_lines\u201d: [\u201cRequesting a certificate for galaxyimu\u201d]}\n\nTASK [galaxyproject.nginx : Fail due to previous configuration installation or validation errors] ***\nfatal: [galaxyimu]: FAILED! => {\u201cchanged\u201d: false, \u201cmsg\u201d: \u201cThe new nginx configuration failed to install or validate, so the previous configuration has been restored. Please investigate the errors above for more information.\u201d}"}, {"role": "system", "text": "If I figured out properly, you were using the usegalaxy_eu.certbot ansible role to request a SSL certificate for your domain. The domain needs to be a fully qualified domain name like e.g. www.google.com\ngalaxyimu it\u2019s not a f.q.d.n."}, {"role": "user", "text": "TASK [usegalaxy_eu.certbot : Request certificate] ******************************\nfatal: [galaxyimu.utpal.local]: FAILED! => {\u201cchanged\u201d: true, \u201ccmd\u201d: [\u201c/opt/certbot/bin/certbot\u201d, \u201ccertonly\u201d, \u201c\u2013test-cert\u201d, \u201c\u2013non-interactive\u201d, \u201c\u2013webroot\u201d, \u201c\u2013register-unsafely-without-email\u201d, \u201c\u2013agree-tos\u201d, \u201c-w\u201d, \u201c/srv/nginx/_well-known_root\u201d, \u201c-d\u201d, \u201cgalaxyimu.utpal.local\u201d], \u201cdelta\u201d: \u201c0:00:02.088648\u201d, \u201cend\u201d: \u201c2022-06-27 20:21:10.797111\u201d, \u201cmsg\u201d: \u201cnon-zero return code\u201d, \u201crc\u201d: 1, \u201cstart\u201d: \u201c2022-06-27 20:21:08.708463\u201d, \u201cstderr\u201d: \u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\\nAn unexpected error occurred:\\nError creating new order :: Cannot issue for \"galaxyimu.utpal.local\": Domain name does not end with a valid public suffix (TLD)\\nAsk for help or search for solutions at https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d, \u201cstderr_lines\u201d: [\u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\u201d, \u201cAn unexpected error occurred:\u201d, \u201cError creating new order :: Cannot issue for \"galaxyimu.utpal.local\": Domain name does not end with a valid public suffix (TLD)\u201d, \u201cAsk for help or search for solutions at https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d], \u201cstdout\u201d: \u201cRequesting a certificate for galaxyimu.utpal.local\u201d, \u201cstdout_lines\u201d: [\u201cRequesting a certificate for galaxyimu.utpal.local\u201d]}"}, {"role": "system", "text": "\n  \n\n      en.wikipedia.org\n  \n\n  \n    \n\nFully qualified domain name\n\nA fully qualified domain name (FQDN), sometimes also referred to as an absolute domain name, is a domain name that specifies its exact location in the tree hierarchy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone. A fully qualified domain name is distinguished by its lack of ambiguity in terms of DNS zone location in the hierarchy of DNS labels: it can be interpreted only in one way.\n Domain names in DNS are read from right to lef...\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello Community Members\nThe genome that I need to map my reads to is not listed in the reference genome list while using HISAT2. Can anyone share with me the procedure to index my required reference in HISAT2 genome list? Is it alright if I directly upload a fasta file for the reference genome from my PC to the history and use the same as a reference genome?\nFollowing is the NCBI link for the reference genome of the microorganism that I am working with\n  \n      \n      ncbi.nlm.nih.gov\n  \n  \n    \n\nDesulfovibrio alaskensis G20, complete genome - Nucleotide - NCBI\n\n\n\n  \n  \n    \n    \n  \n  \n\n\nThank you for any help"}, {"role": "system", "text": "Hi! I had the same problem and I used an FTP genome that I uploaded from my computer. It worked for HISAT2 so I think it might also work for you.\nHave a good one"}, {"role": "system", "text": "Hi @abhilash719,\nas Stefania stated, you can upload your reference genome via FTP. The following tutorial explains the procedure for uploading files using this protocol: File Upload via FTP. Then you can select it in the HISAT2 tool by selecting the Use a genome from history option.\nRegards"}, {"role": "user", "text": "Thank you, Stefania."}, {"role": "user", "text": "Thank you, Gallardo."}, {"role": "user", "text": "Hi Gallardo\nI am unable to upload the genome via ftp. It\u2019s saying your connection to this site is not private. I think chrome does not support FTP anymore. Is it alright if I just download the genome from NCBI upload it from my PC? The size of the genome is only 4MB."}, {"role": "system", "text": "Hi @abhilash719,\nyou can just copy the Desulfovibio alaskensis reference genome FTP URL from the NCBI FTP server and paste it in the Galaxy uploader (Paste/Fetch data).\nRegards."}], [{"role": "user", "text": "Trinity genome-guided assembly produces empty files despite the job being completed successfully. I obtained the same unexpected results also on galaxy.org.\n\nrun log899\u00d7773 188 KB\n\n\nUnexpected results277\u00d7593 37.6 KB\n"}, {"role": "system", "text": "Hi @Lukasz_Gajda\nAre you still having trouble? If so, it would be helpful for you to provide more information.\nIf possible, please create a share link to the history, paste that back here as a reply, and note which datasets are involved. Please avoiding deleting any of the inputs or outputs please.\nAn alternative to start with could be the Dataset information page associated with one of the empty outputs, and each of the inputs. This includes the tool, input, and parameter details. Click on the  icon with a dataset to find it. Copy the text and paste it back please.\nThanks!"}, {"role": "user", "text": "Dear  @jennaj,\nThank you very much for contacting me. Firstly, I noticed that I apparently used an unsorted bam file in my last attempt, which is bad of course. However, I also obtained empty files in the previous try with a Samtools-sorted bam. De novo assembly works fine for me. I must admit that the genome-guided assembly with Trinity is something new for me.\nLink to the history:\nhttps://usegalaxy.eu/u/ruptawsky/h/unnamed-history\nPrevious attempt:\n6: Trinity on data 4, data 2, and data 1: Assembled Transcripts\nNote: input bam file was prepared with STAR in OmicsBox and sorted with Samtools in Galaxy\nThe latest attempt:\n27: Trinity on data 26, data 2, and data 1: Assembled Transcripts\nNote: input bam file was prepared with STAR in Galaxy\nI am grateful for your support! If you need more details, feel free to ask."}, {"role": "system", "text": "Hi @Lukasz_Gajda\nIt looks like you deleted the empty outputs, so I couldn\u2019t review the exact parameters you used, but I could guess the inputs and reviewed those.\nThe tool is probably having trouble mapping the IDs in the GTF to the custom genome\u2019s IDs. That can lead the removal of assemblies in the output (why? no annotated splices). This can be addressed by cleaning up the fasta and GTF formatting.\n\nfasta-datasets\nreference-annotation\n\nThe custom genome fasta itself is a bit large (numerous sequences), but try the reformatting, then a rerun using those modified inputs. If you get an error or odd results, please leave those results undeleted and we can take another look."}, {"role": "user", "text": "Dear@jennaj,\nI\u2019m not sure, but I think we are talking about different things here. Sorry if I misunderstood you. What tool are you talking about? Do you mean I messed up something with the input bam file i.e., at the RNA-seq alignment with STAR step? In order to run Trinity assembly in genome-guided mode, you must provide read alignments to Trinity as a coordinate-sorted bam file. In that case, your input files for Trinity are: RNA-seq reads (trimmed and filtered) + bam file (RNA-seq reads aligned with reference genome with STAR and sorted with Samtools). I did not delete any Trinity\u2019s results. Trinity produces Assembled transcripts and its own Gene to Transcripts map. These are:\n\n\nfor previous attempts: 7 Trinity on data 4, data 2, and data 1: Gene to transcripts map AND 6 Trinity on data 4, data 2, and data 1: Assembled Transcripts\n\n\nfor the latest try: 28 Trinity on data 26, data 2, and data 1: Gene to transcripts map AND 27 Trinity on data 26, data 2, and data 1: Assembled Transcripts.\n\n\nFor RNA-seq alignment with STAR, I used my Cyprinus carpio RNA-seq reads, while the reference genomic fasta and GFF files were downloaded from GenBank.\nThanks again,"}, {"role": "system", "text": "\n\n\n Lukasz_Gajda:\n\nwhile the reference genomic fasta and GFF files were downloaded from GenBank.\n\n\nThese are the two inputs that may have had issues (technical). By default those files will not be in a strict format from that source. Some tools produce odd results when the identifiers cannot be matched up. I recommended cleaning up data (always) to eliminate easy problems, but not everyone does \nAlso, I reloaded the history and can see the empty Trinity results now. Changing the first item might be enough. If not, consider the other two.\n\nUse a genomic mapping BAM result, not the transcriptome.\nSimplify the reference genome. Maximizing unique hits (scientific result quality) by omitting the unplaced scaffolds could be tested/compared. NCBI reports the BUSCO around 60% duplicated. Seems high, but don\u2019t know if that is expected for this species or not.\nSimplify the reads. If either end is shorter than 25 bases, both ends are sorted out by Trinity. The job logs list many of these. May not matter but could be pre-filtered to shorten the runtime.\n\nThe job logs included this. It means that the transcriptome bam didn\u2019t fully parse with samtools. If the genomic also fails at this step, then definitely consider using primary chromosomes only.\n\nTuesday, September 6, 2022: 08:37:15\tCMD: /usr/local/tools/_conda/envs/mulled-v1-8bfd939b6449b852af28c673a66720972cbc5543a7cc8d50ff9c6e1f04709fb3/opt/trinity-2.9.1/util/support_scripts/ensure_coord_sorted_sam.pl localbam.bam\n-appears to be a coordinate sorted bam file. ok.\nsamtools view: writing to standard output failed: Broken pipe\nsamtools view: error closing standard output: -1\n\nHope that helps!"}, {"role": "user", "text": "Hi @jennaj\nAfter further investigation of the issue, I believed that the current Galaxy version of Trinity has a bug.\nIn order to test Trinity genome-guided mode assembly on Galaxy, I used pre-prepared files from the OmicBox tutorial, i.e.,\nFastq reads (single-end data) from Neisseria gonorrhoeae (SRR3666079) and sorted BAM file (BWA RNA-seq alignment of the SRR3666079 reads with N. gonorrhoeae reference genome - Neisseria_gonorrhoeae_fa_1090.ASM684v1.dna.toplevel.fa). All files were downloaded from:\nhttp://manual.omicsbox.biobam.com/example-datasets/transcriptomics/gene-level-analysis/#Gene-LevelAnalysis-1-RNA-SeqAlignment\nThe resulting Trinity file that should contain assembled transcripts was empty again (with the same warning: \u201csamtools view: error closing standard output: -1\u201d). I also obtained an empty result with another run of my Cyprinus carpio data (with a genomic mapping (sorted) BAM result this time).\nDespite this, in fact, the same bug (\u2018Trinity guided mode output file is not captured\u2019) was reported for Trinity and fixed in version 2.13.2.\nReferences for the bug:\n  \n\n      github.com/trinityrnaseq/trinityrnaseq\n  \n\n  \n    \n  \n    \n  \n\n  \n    \n      critical bugfix for v2.13 genome guided mode so final output is captured\n    \n\n    \n      \n        committed 09:14PM - 02 Sep 21 UTC\n      \n\n      \n        \n          \n          brianjohnhaas\n        \n      \n\n      \n        \n          +5\n          -1\n        \n      \n    \n  \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nReleases \u00b7 trinityrnaseq/trinityrnaseq\n\n  Trinity RNA-Seq de novo transcriptome assembly. Contribute to trinityrnaseq/trinityrnaseq development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n\n      github.com/trinityrnaseq/trinityrnaseq\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      Reference guided mode, no errors, fasta output is empty\n    \n\n    \n      \n        opened 09:54AM - 15 Nov 21 UTC\n      \n\n        \n          closed 07:30PM - 15 Nov 21 UTC\n        \n\n      \n        \n          \n          carolhsb\n        \n      \n    \n\n    \n    \n  \n\n\n  \n    Hi all,\n\nI'm working with a fish species and running Trinity in the reference \u2026guided mode.\nThe run finishes without error, but the output (\"Trinity-GG.fasta\") is empty.\nI've tried to change the parameter --genome_guided_max_intron to 100000 from 10000 and got the same result.\nI'm running Trinity on Docker.\n\nCan you help me?\n\nMy log file: [nohup.out.txt](https://github.com/trinityrnaseq/trinityrnaseq/files/7537576/nohup.out.txt)\n\nThanks in advance,\n\nCarol\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nLink to history:\nhttps://usegalaxy.eu/u/ruptawsky/h/trinity-genome-guided-test\nFeel free to correct me if I missed something.\nI could be wrong in this, but are there any plans to upgrade the Trinity version on Galaxy?\nThank you for your support.\nP.S. Common carp (Cyprinus carpio) is an allotetraploid, so the duplication level is as expected."}, {"role": "system", "text": "Hi @Lukasz_Gajda\nThanks for all of the follow up!\nThe problem when running this in Galaxy is actually the wrapper (not the underlying tool version). Ticket: Trinity: Bug in output collection when using genome-guided mode \u00b7 Issue #3850 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub. And, I\u2019ve asked the Tools WG to take a look again. If you want to help make the changes, please comment on the ticket and reach out to the Tools working group for coordination.\nI thought it was fixed already, but it wasn\u2019t \u2013 sorry! I\u2019ve added your test history to the ticket with a few more manipulations/tests done by me. If you want to purge your copy to recover storage space in your account, that would be Ok now."}], [{"role": "user", "text": "HI Everyone,\nAny job I run on Galaxy gives me the same error, \u2018FATAL:container creation failed\u2019.  Screenshot attached.\nThis happens since last night. Even the jobs I previously run successfully now give this error. Anyone else experiencing the same problems?\nThanks\n\nGalaxyError1001\u00d7467 25.7 KB\n"}, {"role": "system", "text": "Hello @kriskris99\nA few jobs failed due to a problem with our CVMFS dependency last Friday at UseGalaxy.org. That was corrected. Please rerun any jobs that failed. Thanks for reporting the problem!"}], [{"role": "user", "text": "Hey, I encountered an issue: I was working on Galaxy and trying to run lefse but the output files were different when I changed the way microorganisms are named.\nHere is everything that I know:\nThe dataset is green, the job did not fail.\nThe lef_usual.txt is a input file whose first column contained many names of microorganisms, and these names had some special characters like \u201c()\u201d, \u201c-\u201d and \u201c'\u201d, like  \u201cClostridium_sp._DL-VIII\u201d.\nThe lef_usual_result.svg is the output file. There are 5 biomarkers for \u201cmeNorChild\u201d.\nThen, I replaced all the names of microorganisms in the first column in lef_usual.txt with another way of naming, such as s1, s2 and s3. The new input file was named lef_number.txt.\nThe  lef_number_result.svg is the new output file. There are 11 biomarkers for \u201cmeNorChild\u201d.\nThe relevant parameters of the two tasks are the same, only the naming method of the first column of the input file has changed.\nSo, why does just changing the way microorganisms are named affect the final results? What format of naming will give the most correct results?\nThe process screenshots, input files and output files have been uploaded.\nUploading: lef_number.png\u2026\nUploading: lef_number_result.png\u2026\nUploading: lef_usual.png\u2026\nUploading: lef_usual_result.png\u2026\nUploading: process.jpg\u2026\nCould you help me?"}, {"role": "system", "text": "Hi @hlsw2001\nI replied to your email, but for others reading, please follow this formatting advice for any tool / analysis if you get odd results.\nWhy? Tools can be picky about format. Cleaning up data to the most basic of formats, and making sure that all inputs exactly match when a common key (identifier) is involved, can eliminate technical problems so you can focus on scientific results and parameter tuning.\n\nChanging identifiers in the middle of an analysis could cause all sorts of odd problems with any tool.\nIt would be best to adjust identifier names at the very start of processing for any analysis.\nSome tools prefer that labels include only alphanumeric characters A-Z a-z 0-9. Underscores _ are usually Ok. Avoid using a number 0-9 at the very start of an identifier. Avoid including any whitespace space or tab.\nIf you include other characters, especially a dot . or pipe |, that could result in any content after that character being truncated. Or, other weird behavior. It is difficult to predict.\n\nLefse in particular doesn\u2019t handle some special characters well \u2013 whether used in Galaxy or directly. Example Q&A: Problem with LEfSe - LEfSe - The bioBakery help forum\n\nThis is general advice \u2013 some tools do expect that special characters are used. The tool documentation will usually have examples and explanations.\nCleaning up data is one of the most important steps for any technical processing, not just bioinformatics analysis. Try an internet search with the term \u201cdata cleaning\u201d. It is a vast topic!\nFAQ\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nWhat to try for your use case:\n\noriginal\n\nClostridium_sp._DL-VIII\n\n\n\ncleaned up to a basic format (enough for Lefse)\n\nClostridium_sp_DL_VIII\n\n\n\ncleaned up to the most basic of formats\n\nClostridiumspDLVIII\n\n\n\nHope that helps "}, {"role": "user", "text": "Hi\uff0cthanks for your reply. I have replaced all the special characters to underscores since you said that \u201cClostridium_sp_DL_VIII\u201d is enough for Lefse. The result is uploaded.\n\nlef1050\u00d7757 46.1 KB\n\nAnd to test out it, I also cleaned up the data to the most basic of formats like \u201cClostridiumspDLVIII\u201d. The result is uploaded here.\n\nlef_bas1050\u00d7412 18 KB\n\nI found there were still differences between these two results. It seems that changing all the special characters to underscores does not help. But cleaning up to the most basic of formats like \u201cClostridiumspDLVIII\u201d is hard to read and it is also difficult to search \u201cClostridiumspDLVIII\u201d on the Internet. So, can Galaxy solve this problem thoroughly? Can the program of Lefse analysis be optimized?\nMany thanks."}, {"role": "system", "text": "\n\n\n hlsw2001:\n\nCan the program of Lefse analysis be optimized?\n\n\nOk, it does look like Lefse is actually interpreting the underscores, not just masking some characters to an underscore. The original tool authors would have the best advice, and are who would modify the tool. They host a slightly different (custom) version of Lefse at their own server. I\u2019m not sure where you are running the analysis but the usage is known to be different from what is available at other public Galaxy servers.\nAuthor\u2019s public Galaxy server details, including the URL and help resources/forum: Huttenhower Lab - Galaxy Community Hub\nI\u2019ll modify the original reply. I also added a tag that points to prior Q&A involving the Huttenhower server. Most include that same contact information. The authors are the best people to clarify the usage details. If you want to post that back here, along with where you are running the tool (+ version), that would be helpful."}, {"role": "user", "text": "I went to find out about Huttenhower Lab - Galaxy Community Hub  but the forum doesn\u2019t seem to be active. Do you have the  email address of the author?\nThanks."}, {"role": "system", "text": "\n\n\n hlsw2001:\n\nthe forum doesn\u2019t seem to be active\n\n\nThe forum has several Q&A threads from this week, and I see your post. You\u2019ll probably need to be patient for an answer. The way microorganisms are named affect the final results of lefse - LEfSe - The bioBakery help forum\n\n\n\n hlsw2001:\n\nDo you have the email address of the author?\n\n\nThe homepage of their server has contact information for the lab. For Q&A, the forum is better."}], [{"role": "user", "text": "Hello, using galaxy for the first time on AWS through the genomics virtual lab on cloudlaunch. I understand that I cannot use the same username as when I use the public servers but my question is, how do I then use a workflow?\nI want to be able to have a workflow I can run every time I run galaxy on AWS but it seems that it creates a new blank server every time with no way to save a workflow for next time. Is this way of using galaxy only for exploratory analysis?\nThanks"}, {"role": "system", "text": "Workflows can be downloaded and uploaded through the Galaxy interface. Is that what you need?\nLook under the masthead section \u201cWorkflows\u201d. There is an upload/import icon at the top of the page and any workflows that are yours will have \u201cdownload\u201d as an option listed on the per-workflow pull-down menu.\nHope that helps!"}, {"role": "system", "text": "Post that may be of interest (customizing/saving cloud AMI images, etc):\n\n  \n    \n    \n    Concurrent user limits and benchmark usegalaxy.org support\n  \n  \n    Hello, \nI am deploying a new instance of galaxy on my aws account in a m5.xlarge instance. \nIs there any benchmark available to do an approx sizing of the compute capacity I will need to provide my students with the environment for a class? \nIn addition, How many user can access concurrently to my galaxy instance on AWS? Is there any limit at Galaxy (software) level?\n  \n\n"}, {"role": "user", "text": "Thanks for the reply,\nThat is how I would do on the public galaxy server (usegalaxy.org), because I can login into my main user and use workflows I have created. However, while using galaxy on AWS, each time I launch it, it is a \u201cdifferent\u201d galaxy server, so I cannot login with my main username that I use for the usegalaxy.org. and therefore can\u2019t pull workflows from the normal username. Does that clarify my question?"}, {"role": "system", "text": "Try downloading a copy of your workflow from usegalaxy.org. It will be in a file that ends with the extension .ga. That can then be uploaded to the cloud server into your new account.\nThe first time you open the workflow in the editor, warnings will come up if any tools/tool version are a mismatch for what is already installed on the default server. You can update tools in the workflow, install new tools on the server, etc until things match up. Installing new tool indexes is also possible.\nOnce you have customized your cloud server (if you need to), you can save an AMI image for reuse. This will cost money to maintain, so if you decide to do this, keeping the image as small as possible would be a good idea (save a copy that does not include full analysis, just what you need to start up a new analysis \u2013 permanently delete non-reusable account data, etc).\nWorkflow objects themselves use up very little space, so you could keep yours in the AMI. Or, download a copy of the workflow configured to work with the GVL default install or your custom AMI. Saving a backup copy is good idea anyway."}], [{"role": "user", "text": "While mapping with bowtie2, in Galaxy I got the following error messages for each of the paired_end files. I also got alignment result. So, is my alignment result could run completely or they are just a part of the whole thing? Should I solve the problems using fastp and run again? Or I am good to go with the results?\nWarning: skipping mate #1 of read 'SRR14466480.24455260 24455260 length=1' because length (1) <= # seed mismatches (0)\nWarning: skipping mate #1 of read 'SRR14466480.24455260 24455260 length=1' because it was < 2 characters long\nWarning: skipping mate #1 of read 'SRR14466480.24461757 24461757 length=1' because length (1) <= # seed mismatches (0)\nWarning: skipping mate #1 of read 'SRR14466480.24461757 24461757 length=1' because it was < 2 characters long\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of 'int'\n(ERR): bowtie2-align died with signal 6 (ABRT) (core dumped)\n\nAlso got this one-\nScreenshot from 2021-08-30 14-48-231245\u00d7463 39.1 KB\n"}, {"role": "system", "text": "It looks like this is the actual error:\n\nError, fewer reads in file specified with -2 than in file specified with -1\n\nI am not to familiar with bowtie2 errors but I guess that one of the fastq files has more reads than the other which is not expected with paired-end data. You could double check this with FastQC for example. Are you doing a step before bowtie2? And are you performing this step separately on both fastq files?"}, {"role": "user", "text": "I checked the parameter total sequences for each forward and paired files from fastqc. They were same numbers. And as they were in a collection (all the SRA_id under a collection; under the SRA id the forward and reverse read were organized), while mapping I chose paired end collection as the input parameter, but still had these error.\nNow I am doing fastp before mapping again as per some suggestion this could solve the issue. I am not sure whether I did any other mistake."}, {"role": "system", "text": "Could you try to filter out those short reads of only 2 bases? With a tool that is made for paired-end data like cutadapt?\nEDIT:\nThis is also possible with fastp but not default, you need to turn it on under \u201cLenght filtering options\u201d"}], [{"role": "user", "text": "BLAST does not find my Galaxy-hosted and Galaxy-created database\nThere have been previous reports of problems with locally hosted databases, but this is for a Galaxy-hosted database.\nThe error message is\nLibCvmfs version 2.4, revision 25 BLAST Database error: No alias or index file found for nucleotide database [/scratch/03166/xcgalaxy/main/staging//29564501/inputs/dataset_43031652_files/blastdb] in search path [/scratch/03166/xcgalaxy/main/staging/29564\nThis database was created on Galaxy using the \u2018make BLAST database\u2019. I ran the make database program twice, in both cases the database appears to lack an alias/index file.\nIs this a Galaxy software problem?\nAdvice?"}, {"role": "system", "text": "Hi @richard22\nApologies for the delayed reply. This was a server-side problem.\nPlease see this related Q&A. In short, corrections have been made and you can try again if working at UseGalaxy.org.\n\n  \n    \n    \n    Errors with makeblastdb & blastn on very large file sets \n  \n  \n    Hello @aroebuck \nNCBI BLAST+ tools had some corrections today at UseGalaxy.org. Full testing is still in progress but so far have turned out well. Even if your combination of tools/inputs is not marked as \u201cpass\u201d yet in the tracking ticket\u2019s test matrix, consider a rerun anyway. \n\nIssue tracking ticket: https://github.com/galaxyproject/usegalaxy-playbook/issues/318\n\n\nFor any reruns, be sure to use the most current version of all tools in the BLAST+ tool suite (2.10.1+galaxy0), including NCBI BLAS\u2026\n  \n\n"}, {"role": "user", "text": "Dear Jennifer\nI\u2019m afraid the problem has not been fixed.\nI tried to run again, same result.\nRegards\nRichard\nProf. Richard Lathe\nUniversity of Edinburgh Medical School\nFC1FC596DDF54BD38FB8D01703B65ADC.png708\u00d71 82 Bytes"}, {"role": "system", "text": "Hi @jenanaj,\nI was initally able to successfully run makeblastdb (see my previous post), but now I am having the same issue as @richard22 . I run blastn with the database I created with makeblastdb, but it produces the error:\n\"BLAST Database error: No alias or index file found for nucleotide database [pathname] \"\nI checked the files and there is no .pal or .pl files (apparently this is the index file that is missing?)\nIs there possible a way to make my own index file?  This issue doesn\u2019t seem to be resolved unfortunately.\nI tried running makeblastdb again but I get the error:\nBLAST Database creation error: Multi-letters chain PDB id is not supported in v4 BLAST DB"}, {"role": "system", "text": "Hi @richard22\n\n\n\n aroebuck:\n\n\"BLAST Database error: No alias or index file found for nucleotide database [pathname] \"\n\n\n^^ This was part of the prior issue (the database wasn\u2019t created correctly when makeblastdb was run before.\n\n\n\n aroebuck:\n\nI tried running makeblastdb again but I get the error:\nBLAST Database creation error: Multi-letters chain PDB id is not supported in v4 BLAST DB\n\n\n^^ This is an error generated by BLAST indicating a formatting problem with the sequence identifiers in your fasta custom genome input to makeblastdb. BLAST is either actually having trouble parsing the identifiers (content on the \u201c>\u201d title lines) \u2013 OR \u2013 there is some other format problem and this is a fall-back error BLAST produced.\nLet\u2019s check your sequence identifiers first.\n\nWould you please post back ~5 \u201c>\u201d title lines\nAnd post back the first ~3 or so lines from the first sequence (including the title line and sequence lines)\n\n\n\nTool: Select (direct link https://usegalaxy.org/tool_runner?tool_id=Grep1)\n\nOptions:\n\n\nSelect lines from: your_fasta_custom_genome\n\nthat: leave at the default (\u201cMatching\u201d)\n\nthe pattern: ^>\n\n\n\n\nRun Select then copy/paste back the first few title lines in a reply, along with a short sample of a single sequence. If you don\u2019t want to post the content publically for some reason, please reply in a direct message to me here.\nYou could also send in a bug report from your error dataset in case we need to look closer. Please do not delete the inputs/outputs, include a link to this Galaxy Help topic in the comments, and let us know you sent that in when posting back the sequence/title data.\nLet\u2019s start troubleshooting more from there "}, {"role": "system", "text": "Thanks for a bit of direction @jennaj. Please see below the sequence ids and one full sequence from the data set I am trying to BLAST against my database:\n^>BLA_1[54-113]\n^>BLA_2[144-203]\n^>BLA_3[8-229]\n^>BLA_4[242-334]\nTTGACGAGCCCAATCCCTTGGTCGAAGCTGAGACTGAACCCGAAGTCAGGTCACTTGAAGCAAAGCAACCTGTACCTAAAAAAAGCACTCCAC\n^>BLA_5[384-413]\nFor the database, I have only been successful with version 2.7.1 of makeblastdb. Below are a sampling of seqids and a sequence from that file. This was from the nt file downloaded directly from the ncbi ftp site and unzipped on Galaxy (local instance). I tried to run megablast on 2.7.1 to match the version I made the database with, but still having issues.\n^>X17276.1 Giant Panda satellite 1 DNA\nGATCCTCCCCAGGCCCCTACACCCAATGTGGAACCGGGGTCCCGAATGAAAATGCTGCTGTTCCCTGGAGGTGTTTTCCT\nGGACGCTCTGCTTTGTTACCAATGAGAAGGGCGCTGAATCCTCGAAAATCCTGACCCTTTTAATTCATGCTCCCTTACTC\nACGAGAGATGATGATCGTTGATATTTCCCTGGACTGTGTGGGGTCTCAGAGACCACTATGGGGCACTCTCGTCAGGCTTC\nCGCGACCACGTTCCCTCATGTTTCCCTATTAACGAAGGGTGATGATAGTGCTAAGACGGTCCCTGTACGGTGTTGTTTCT\nGACAGACGTGTTTTGGGCCTTTTCGTTCCATTGCCGCCAGCAGTTTTGACAGGATTTCCCCAGGGAGCAAACTTTTCGAT\nGGAAACGGGTTTTGGCCGAATTGTCTTTCTCAGTGCTGTGTTCGTCGTGTTTCACTCACGGTACCAAAACACCTTGATTA\nTTGTTCCACCCTCCATAAGGCCGTCGTGACTTCAAGGGCTTTCCCCTCAAACTTTGTTTCTTGGTTCTACGGGCTG.\n^>X51700.1 Bos taurus mRNA for bone Gla protein\n^>X68321.1 B.taurus mRNA for cyclin A\n^>X55027.1 Bovine mRNA for chromogranin B\n^>Z12029.1 B.indicus gene for alpha-lactalbumin\nI sent in a bug report as your requested with a link to this Help topic.\nThanks again!\nAndrea"}, {"role": "system", "text": "Just closing this out.\nAs communicated via email from the bug report, the fasta dataset had duplicated identifiers.\nAll >indentifer content must have a unique name within the same fasta to generate an index."}], [{"role": "user", "text": "I am repeatedly getting the error message \u201cRemote job server indicated a problem running or monitoring this job.\u201d when trying to run BWA for mapping short reads against a reference genome. I am using single read fastq data and a reference genome from my history.\nThe first time I ran the tool it worked on the first read, but none of the rest (4 others). The second time, it worked on none of the 5. Each time I get the same error message.\nIs there something I should do differently to avoid this?"}, {"role": "system", "text": "Welcome, @katie\n\n\n\n katie:\n\n\u201cRemote job server indicated a problem running or monitoring this job.\u201d\n\n\nThat error message can indicate a server problem (transient, and reruns fix it), but also an input content problem. I\u2019m guessing your case is the latter.\nWhat to do:\n\nConfirm that the fasta is useable, see Reference Genome FAQs\nThen confirm that any reference annotation incorporated is useable, and a match for the fasta, see Working with GFF GFT GTF2 GFF3 reference annotation\nLast, confirm that the read datasets are useable, see Read Quality tutorials\n\nPlease give those a try and let us know if you need more help. Where you are working and more details about the exact tool version and your reads/fasta data would be important. So, a shared history link or more screenshots please, see Troubleshooting errors for where/how to find this info. Please leave data undeleted, including the QA tools run, to make reviewing faster for others (FastQC and Fastq info are two good tool choices that check different data attributes)."}, {"role": "system", "text": "I ran into the same error since yesterday with bowtie2. I kept trying at different time of the day and the run made it through in the evening, therefore I think it has nothing to do with my input content.\nHowever, I am getting \u2018Remote job server indicated a problem running or monitoring this job.\u2019 error since this morning with both bowtie2 and map with bwa-mem. I re-ran the very same job that made it through yesterday but it ran straight to the error. It seems not all tools were affected equal because running Samtools flagstat was fine. Does anyone have any idea on what might have gone wrong?"}, {"role": "system", "text": "Hi @itlwong\nPlease try again now. We had one cluster node that was failing jobs yesterday \u2013 and we\u2019ve removed it from the pool.\nWhen you rerun, avoid setting the job resources destination on the tool form, and instead let Galaxy choose the cluster.\nShould that still fail now, it would be unexpected. You can post back more details and we can take a closer look here. Same for everyone else. But please try the rerun now first since that should solve most problems faster. See Sharing your History\nThanks for reporting the problem!"}, {"role": "system", "text": "Hi @jennaj, thanks for looking into the issue.\nI just tried re-running 2 map with bwa-mem jobs and it still runs into the same error. I left the job resource parameters in default (i.e. use default job resource parameters). I\u2019ve just shared history with you by email (galaxyproject.org).\nPlease let me know if I can provide you with any information you would find helpful. Thanks!"}, {"role": "system", "text": "Hi @itlwong I tend to not review direct messages via email. Too confusing and numerous.\nI\u2019ll start up a chat here so you can share privately if that\u2019s better for you. Public is always better for us since the Q&A becomes an artifact that can help others \u2013 and better for you since more people can help out. But you decide."}, {"role": "system", "text": "Resolved by mapping with BWA-MEM instead."}], [{"role": "user", "text": "Ran Tophat 5X on same data set that worked 2 days ago\u2026 Is Tophat down? I have deadlines to meet\u2026"}, {"role": "system", "text": "hi @rpotens007\nAre you working at Galaxy Main https://usegalaxy.org?\nIf yes, then there are known issues with some mapping jobs. Choosing a specific cluster to run the jobs on will help until the issues are fully resolved.\nTicket about the problem (will close out once fixed): https://github.com/galaxyproject/usegalaxy-playbook/issues/257\nWorkaround with details about how to target the proper cluster is linked from that ticket above, but here is the post directly: HISAT2 Not working\nThis is a priority correction and we expect a full resolution soon."}, {"role": "system", "text": "Update: Fixed\nNote to all end users running mapping:  Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.\nTicket: https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708"}], [{"role": "user", "text": "Hi All,\nI am using usegalaxy to analyze RNAseq for the first time. Is there a tutorial for making Venn Diagram?\nI am using the tool \u201cVenn diagram on list\u201d, but keep failing to make it work.\nThanks"}, {"role": "system", "text": "Dear @Vu_Ngo,\nCan you please state the error that you receive?\nCheers,\nFlorian"}, {"role": "user", "text": "Dear @Flow\nThis is the error I receive \u201cDoing 2-way Venn Diagram\nTotal of 32 IDs\nUnexpected ID ENSMUSG00000025498.15 in tabular file /corral4/main/objects/b/e/5/dataset_be5ed78b-995a-4356-bd3b-eba02ae58e55.dat\u201d\nThis is my first time analyzing RNAseq. And I wanted to generate a Venn diagram for my data sets. I already finished Filter and Annotate DESeq2/DEXSeq output tables steps. But couldn\u2019t find any tutorial for Venn Diagram using usegalaxy.\nThank so much for your help"}, {"role": "system", "text": "Dear @Vu_Ngo,\nI am not super familiar with the tool and just had a quick look at the code. What I supect is a malformatted input file or you have chosen the wrong input files.\nFull dataset (with all identifiers) - Must contain all the identifiers (IDs) from the individual set files you will provide.\nMembers of set - Are the individual set files.\nMake sure the files have in the first column the identifiers. And also make sure the idenfiers of the Members of set are contained in the Full dataset (with all identifiers). I think the program couldn\u2019t find the identifier ENSMUSG00000025498.15.\nCheers,\nFlorian"}], [{"role": "user", "text": "i want upload my data to galaxy.its a little heavy(3Gb). i treid it before and worked ,.but it doesnt now. i waited too long but the uploading  was 0% .where is the problem?"}, {"role": "system", "text": "Hi Amir,\nThe connection might be slower this time for some reason.\nTry FTP instead. If the connection is interrupted, it can be resumed interactively (not possible with a file-browsed upload). FAQ: https://galaxyproject.org/ftp-upload/\nThanks!"}, {"role": "user", "text": "i download the FileZilla FTP. but i dont know what should i put for host name and username and password and port. i did some of note in\n. FAQ: https://galaxyproject.org/ftp-upload/ but it did not connect."}, {"role": "system", "text": "The FTP server is up and working at usegalaxy.org. You use the same login as you use for the Galaxy itself."}, {"role": "system", "text": "Small clarification for what to enter for \u201cusername\u201d (sometimes this is the problem)\u2026\nThe \u201cusername\u201d to enter into Filezilla is your registered Galaxy email address.\nThis is not the same as your custom \u201cusername\u201d (aka \u201cpublic name\u201d) that can be used to log into Galaxy.\nIn short: A registered account\u2019s \u201cemail\u201d or \u201cusername\u201d can be used to log into Galaxy. But you must enter your email address into Filezilla. The registered account must already exist on the same server you are connecting to. Each public Galaxy server is distinct.\nEmail addresses are case-sensitive. So if the email address is not working, check in the Galaxy interface under User > Preferences to see what the formatting of your email address looks like, then use the same in Filezilla.\nFor most people, using all default settings in Filezilla works best. No need to enter a specific port. If you need more custom settings, the FAQ has the details (and a movie).\nThanks!"}, {"role": "user", "text": "i did what you are saying. but it did not connect. Port number isnt important ?"}, {"role": "system", "text": "\n\n\n amir:\n\nPort number isnt important ?\n\n\nFTP clients will know the standard ports\nOne possible issue is that your network admin restricted access to FTP, you can contact them and ask."}], [{"role": "user", "text": "Hi\nI am using Galaxy server for genome assembly using SPAdes. But there is an error occurred:\nCommand line: /cvmfs/main.galaxyproject.org/deps/_conda/envs/__spades@3.12.0/bin/spades.py -o /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working --disable-gzip-output --only-assembler --careful -t 7 -m 288 -k 33,55,91 --cov-cutoff auto --pe1-fr --pe1-1 fastq:/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat --pe1-2 fastq:/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\nSystem information:\nSPAdes version: 3.12.0\nPython version: 3.8.1\nOS: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-glibc2.10\nOutput dir: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working\nMode: ONLY assembling (without read error correction)\nDebug mode is turned OFF\nDataset parameters:\nMulti-cell mode (you should set \u2018\u2013sc\u2019 flag if input data was obtained with MDA (single-cell) technology or --meta flag if processing metagenomic dataset)\nReads:\nLibrary number: 1, library type: paired-end\norientation: fr\nleft reads: [\u2019/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat\u2019]\nright reads: [\u2019/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\u2019]\ninterlaced reads: not specified\nsingle reads: not specified\nmerged reads: not specified\nAssembly parameters:\nk: [33, 55, 91]\nRepeat resolution is enabled\nMismatch careful mode is turned ON\nMismatchCorrector will be used\nCoverage cutoff is turned ON and threshold will be auto-detected\nOther parameters:\nDir for temp files: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/tmp\nThreads: 7\nMemory limit (in Gb): 288\n======= SPAdes pipeline started. Log can be found here: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/spades.log\n===== Assembling started.\n== Running assembler: K33\n0:00:00.000 4M / 4M INFO General (main.cpp : 74) Loaded config from /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/config.info\n0:00:00.001 4M / 4M INFO General (main.cpp : 74) Loaded config from /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/careful_mode.info\n0:00:00.002 4M / 4M INFO General (memory_limit.cpp : 49) Memory limit set to 288 Gb\n0:00:00.002 4M / 4M INFO General (main.cpp : 87) Starting SPAdes, built from N/A, git revision N/A\n0:00:00.002 4M / 4M INFO General (main.cpp : 88) Maximum k-mer length: 128\n0:00:00.002 4M / 4M INFO General (main.cpp : 89) Assembling dataset (/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/dataset.info) with K=33\n0:00:00.003 4M / 4M INFO General (main.cpp : 90) Maximum # of threads to use (adjusted due to OMP capabilities): 7\n0:00:00.003 4M / 4M INFO General (launch.hpp : 51) SPAdes started\n0:00:00.003 4M / 4M INFO General (launch.hpp : 58) Starting from stage: construction\n0:00:00.003 4M / 4M INFO General (launch.hpp : 65) Two-step RR enabled: 0\n0:00:00.003 4M / 4M INFO StageManager (stage.cpp : 132) STAGE == de Bruijn graph construction\n0:00:00.020 4M / 4M INFO General (read_converter.hpp : 77) Converting reads to binary format for library #0 (takes a while)\n0:00:00.020 4M / 4M INFO General (read_converter.hpp : 78) Converting paired reads\n0:00:00.574 100M / 100M INFO General (binary_converter.hpp : 93) 16384 reads processed\n0:00:00.972 116M / 116M INFO General (binary_converter.hpp : 93) 32768 reads processed\n0:00:01.746 144M / 144M INFO General (binary_converter.hpp : 93) 65536 reads processed\n0:00:03.283 204M / 204M INFO General (binary_converter.hpp : 93) 131072 reads processed\n0:00:06.374 320M / 320M INFO General (binary_converter.hpp : 93) 262144 reads processed\n0:00:12.521 548M / 548M INFO General (binary_converter.hpp : 93) 524288 reads processed\n0:00:34.567 908M / 908M INFO General (binary_converter.hpp : 93) 1048576 reads processed\n0:01:09.485 908M / 908M INFO General (binary_converter.hpp : 93) 2097152 reads processed\n0:02:18.535 912M / 912M INFO General (binary_converter.hpp : 93) 4194304 reads processed\n0:04:48.246 920M / 920M INFO General (binary_converter.hpp : 93) 8388608 reads processed\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 58) The number of left read-pairs is larger than the number of right read-pairs\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 60) Unequal number of read-pairs detected in the following files: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\n== Error == system call for: \u201c[\u2019/cvmfs/main.galaxyproject.org/deps/_conda/pkgs/spades-3.12.0-1/share/spades-3.12.0-1/bin/spades-core\u2019, \u2018/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/config.info\u2019, \u2018/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/careful_mode.info\u2019]\u201d finished abnormally, err code: 255\nWhat can I do ? Kindly help me to solve this problem.Thank You"}, {"role": "system", "text": "Did you already checked the input files?\n\nThe number of left read-pairs is larger than the number of right read-pairs\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 60) Unequal number of read-pairs\n\nIf you work with paired-end data the forward and reverse reads need to be the same amount of reads."}], [{"role": "user", "text": "Hello!\nI accidentally tool_conf.xml. However the galaxy still shows all the installed tools. Is there a way to restore it using galaxy? I tried installing some new package but it did not result in creation of tool_conf.xml.\nThanks in advance."}, {"role": "system", "text": "Hi @wormball\nLet\u2019s ask the admins. They may reply here or at Gitter. galaxyproject/admins - Gitter\n(fixed reply, thanks @gmauro !!)"}, {"role": "system", "text": "I think the most appropriate channel is galaxyproject/admins - Gitter"}, {"role": "system", "text": "@wormball The first thing that comes to my mind is to try with bioblend\nhttps://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#bioblend.galaxy.tools.ToolClient.get_tools"}], [{"role": "user", "text": "After using Shovill to assemble paired-end NGS reads for DNAseq data I am hoping to filter my FASTA output file based off the coverage of each assembled contig. The output for Shovill is a FASTA file that contains a unique contig ID and a coverage value. Using \u201cfilter FASTA\u201d on galaxy, I want to output a FASTA file with contigs that have a coverage greater than 1000. Can I use \u201cfilter FASTA\u201d for this and if so what is an example of the regular python expression that would work to filter for coverage?\nThank you very much in advance."}, {"role": "system", "text": "Hi @Gabriella_Quinn\nYes, if the coverage value is somewhere in a line of text data, it can be parsed out and/or filtered on.\nThe tool you mention is a good choice, but it isn\u2019t installed at Galaxy Main https://usegalaxy.org. It is installed at Galaxy EU https://usegalaxy.eu. The post is tagged as usegalaxy.org (??) \u2013 may be a simple mistake? Please clarify the server by URL and that this is the tool you intend to use:\n\n\nFilter FASTA  on the headers and/or the sequences (Galaxy Version 2.1)\n\nIf you share an example of a few of your fasta title lines (the full \u201c>\u201d line for at least three fasta records) we can help you to construct a regular expression. If your contig fasta dataset is large (too large to scroll through to copy/paste out three title lines) you can filter out just the title lines with the tool/expression:\n\n\nSelect lines that match an expression (Galaxy Version 1.0.1)\nusing the regular expression ^>.\n\nThe Select tool could also be used for the full fasta filtering, but that would require converting the data to tabular format first, then back to fasta again after. If you want to do it that way, just state so. The regular expression to use will be a bit different between the two tools.\nShould you decide to post back a few of your fasta title lines, be sure to preserve the formatting by using the \u201cblock quote\u201d format option (\u201cquote\u201d icon at the top of where you write your reply back to this post). That should be enough information, but if not, I\u2019ll ask you to share a history link with me (privately) to review your exact data. Whitespace (spaces, tabs, etc) can look the same with copy/paste functions, and sometimes matter.\nLet\u2019s start there. Thanks!"}, {"role": "user", "text": "Thank you for your response. Yes! This is meant for usegalaxy.eu. My FASTA files are relatively small and the first three lines of one of my samples are posted below. These contigs are pretty short because they are sequences from a viral genome of about 1200bp.\nI am hoping to filter for contigs that have a coverage greater than 500.\nI appreciate your help with this.\n> contig00001 len=187 cov=233.9 corr=0 origname=NODE_1_length_187_cov_233.928571 sw=shovill-spades/1.0.4 date=20190911\nGAAGTTCCTCTTCCTCCTCCTTGTTCAGGCGCTTCCCTCCCGCGCTCAGCTGCTTTCTCTGTTCTCGAGGGCCTTCCTTCGTCGGTGATCCTGCCTCTCCTTGTCGGTGAACCCTCCTTGAGGGGCCTCTTCCTAGGTCCGGAGTCTACTTCCATCTGGTCCGTCCGGGCCTTCTTCGGGGGG\n> contig00002 len=187 cov=24.1 corr=0 origname=NODE_9_length_187_cov_24.107143 sw=shovill-spades/1.0.4 date=20190911\nGGAGTTCCTCTTCCTCCTCCTTGCTCAGGTTCTTCCCTCCCGCGGTCAGCTGCTTTCTCTGTTCTCGAGGGCCTTCCTTCGTCGGTGACCCTGCCTCTCCTTGTCGGTGAACCCTCCTGAGAGGCCTCTTCCTAGGTCCGGTGTCTACTTCCATCTGGTCCGTCCGGGCCCTCTTCGCGGGG\n> contig00003 len=187 cov=62.5 corr=0 origname=NODE_6_length_187_cov_62.511905 sw=shovill-spades/1.0.4 date=20190911\nTGAAGTTCCTCTTCCTCCTCCTTGCTCAGGCGCTTCCCTCCCGCGCTCAGCTGCTTTCTTGTTCTCGAGGGCCTTCCTTCGTCGGTGATCCTGCCTCCCCTTGTCGGTGAACCCTCCTGAGAGGCCTCTTCCTAGGTCCGGAGTCTACTTCCATCTGGTCCGTCCGGGCCTTCTTCGCGGG\n\nnote: admin reformatted"}, {"role": "system", "text": "Hi @Gabriella_Quinn\nJust to make sure I have the formatting correct, is there really a space in between the > and contigNNNN content in your dataset?\nI did change the way the formatting was displayed here (forgot that \u201cblock quote\u201d doesn\u2019t handle lines that start with a > very well). Adding in \u201cfour spaces\u201d before each line worked better quote it accurately and I think it preserved your original intended format, but need to confirm that. Please review what is displayed now.\nA space in that location will cause problems with any tool expecting fasta format. Please clarify, then we can get that removed with a different tool, and proceed with the Filter Fasta tool.\nFAQ for fasta formatting. Don\u2019t worry about the description line content, you want that preserved for your current use case, so it can be filtered on. But the reads do need to have a correct identifier. Each part of the \u201c>\u201d fasta title line is explained in the FAQ if this seems confusing. https://galaxyproject.org/learn/datatypes/#fasta\nThanks!"}, {"role": "user", "text": "Thank you for clarifying. There is not a space. Below is an example from one line of the FASTA file. Other than that your corrected format matches my file.\n>contig00004 len=193 cov=13.3 corr=0 origname=NODE_863_length_193_cov_13.322222 sw=shovill-spades/1.0.4 date=20190915\nGAAGGAAAGACCGCGGGGGGAGGGAAGAGATC\n\nnote: admin reformatted"}, {"role": "system", "text": "@Gabriella_Quinn\nOk, good!\nTry this expression to find reads with coverage at or above 500.0\n^.+cov=([5-9][0-9][0-9])|([1-9]\\d{4}\\d*)\\..+$"}], [{"role": "user", "text": "Hi, I am trying to align my sequences with the silva 138.1 version but the downloaded file is empty (same for version 132),\ncan anyone advise please?\nI am looking for bacterial and archaeal community in my samples amplified with 341F and 806 R for 16S V3V4 region.\nThank you"}, {"role": "system", "text": "Hi @Funnyme186\nTry this data provider as a source: https://www.arb-silva.de/\nYou\u2019ll want the fasta version of the data. Example link: Archive\nTutorials: Galaxy Training!"}, {"role": "user", "text": "hi @jennaj Thank you for your reply!\nI did download the uncompressed version of 132 (9.9 GB), and used it to align my sequences because I didn\u2019t really know how to extract the fasta from arb.\nThe problem now is that when I get to cluster.split , it is taking too long and error at the end.\nDo you have an idea of how to resolve this problem?\nIf yes, could you please advise?\nRegards\nStephanie"}, {"role": "system", "text": "\n\n\n Funnyme186:\n\nThe problem now is that when I get to cluster.split , it is taking too long and error at the end.\n\n\nHum \u2013 is the datatype fasta or fasta.gz. Expand the dataset to check. If fasta.gz you might just need to uncompress it. Click on the pencil icon for the dataset to bring up the Edit Attributes forms. The tab for Convert has a drop-down menu with an \u201cuncompress\u201d function. Some tools do not interpret compressed fasta well. If that works, please let me know.\nOr, maybe the full fasta is too large for the server to process. There are several posts at Biostars discussing how others are sub-setting the fasta to just the regions of interest.\n\nExtract V3-V4 region for 16S\nExtract V3-V4 regions from 16s sequences\n\nEMBOSS Fuzznuc is a tool in Galaxy already (referenced in the second post above).\n\n\n\n Funnyme186:\n\nI did download the uncompressed version of 132 (9.9 GB), and used it to align my sequences because I didn\u2019t really know how to extract the fasta from arb.\n\n\nThe https://www.arb-silva.de/ site has a search/filter function. I haven\u2019t used it much but might be another way to subset and output fasta. The site has documentation and examples. Many functions involve using the ARB tool package for data manipulation. That said, most functions can probably be translated to alternative tools in Galaxy.\nIf that is not enough, where are you working now (server URL)?\n\nIf usegalaxy.org, would you please send in a bug report from the new error? Include a link to this topic in the comments so I can find it. Would like to review it.\nIf working at a different public Galaxy server, you can share your history with me in a direct message here. You won\u2019t be able to do that yet \u2013 so write back if needed and I\u2019ll start one up. That keeps your shared history link private. If you don\u2019t care about privacy, the history share link can just be posted back in this thread. How to: Galaxy Training!. Also post back the dataset number with the failure to make sure we are looking at the same thing.\n\nLet\u2019s start there. Can bring in some domain specialists if needed, but better to rule out technical issues first."}, {"role": "user", "text": "Hi @jennaj\nThank you again for your prompt reply.\nMy datatype is fastqsanger.\nYes I tried to create a custom reference file with my regions but didn\u2019t use it.\nYes the full silva 132 fasta is too large (9.9 GB) but the readme to compress it was too complicated, I will try to extract the fasta again.\nI am working on the usegalaxy.org\nI shared my history with your email address.\nI sent in a bug report today the new error of cluster.split.\nThank you a lot for your help regarding this issue, I really appreciate it.\nBest regards\nStephanie"}, {"role": "system", "text": "HI @Funnyme186\nWell, there were some issues at the server last week. Those may have impacted your work. It looks like you have had a successful rerun by now with the same inputs that originally timed out and failed.\nApologies for the delayed reply \u2013 I also needed to wait for things to get back to (mostly) normal before reviewing/replying. All should be Ok now. Everyone should expect slightly longer job queue timing until the banner on the server clears, but everything else is fine."}], [{"role": "user", "text": "My account were active last night and running data, however this morning I tried logging in to find a message saying the account had been marked as deleted. How do I recover the data that was in the account?"}, {"role": "system", "text": "This notice applies to everyone who has found their account deleted unexpectedly from usegalaxy.org.\nThere is an end-user limit of one account per user per public Galaxy server, including Galaxy Main https://usegalaxy.org.\nMany end-users were found to be associated with two or more accounts. Several were deleted to meet posted Terms and Conditions. Duplicated account reviews and deletions will continue. Any single user may have one account, and one account only, at Galaxy Main https://usegalaxy.org.\nAttempts to subvert usage terms through the creation of multiple registered accounts or using multiple unregistered/unconfirmed accounts will result in the deletion of all accounts associated with any single end-user.\nAccounts at Galaxy Main have a very generous per-account quota size of 250 GB. Should these resources not be sufficient, then exploring alternative ways to use Galaxy is the path forward.\n\nEcosystem https://galaxyproject.github.io/\n\nChoices overview https://galaxyproject.org/choices/\n\nWays to use Galaxy https://galaxyproject.org/use/\n\nTeaching with Galaxy https://galaxyproject.org/teach/\n\n\nEach end-user can update account details at any time (email, password). Excess account data can be downloaded and then removed from the server to recover quota space. Registering two or more accounts is never needed.\nFAQs: https://galaxyproject.org/support/\n\nGetting an account at Galaxy Main (http://usegalaxy.org)\nThe account usage quota seems incorrect\nChecking for active vs deleted vs permanently deleted (purged) datasets and histories\nReset password or Change email, username, password\nDownloading Data\n\nAccount suspension is not intended to be punitive in the vast majority of cases. Some end users have older legacy accounts left over from training, prior employment/education, and other reasons. Others did not adequately review and understand the account creation terms during registration. A few accounts picked up and deleted may not be actually duplicated, and the primary account will be restored once confirmed. Unfortunately, a few end users also actively and knowingly violate the usage terms, consuming the public resources unfairly at Galaxy Main.\nThis account was found to be associated with one or more duplicated accounts. To have a single account restored, please send an email, from the registered account email address to be retained, to galaxy-bugs@lists.galaxyproject.org. Note: If your registered account email address is from a public email service, include in the cc of the email your work/academic email address and be very clear about which account will be used as your single account going forward.\nOnce and if restored, please do not create additional accounts at Galaxy Main. Do not use other\u2019s accounts, even if working in the same group focused on a joint project. Use sharing functions or consider setting up a dedicated Galaxy server for the project.\nEnd users are permitted to create a single account at each other public Galaxy server \u2014 most if not all have a \u201cone account per end-user\u201d requirement.\nIf an end-user recreates or uses more than one account going forward, knowingly, and especially after being directly re-informed about the usage terms, all associated accounts will be permanently removed from the Galaxy Main server, including data, without prior notice.\nFor those that may have multiple accounts at Galaxy Main, who have not had their duplicated accounts deleted yet, please send an email to the same mailing list above. Proactively deleting any non-primary accounts will prevent account deletion and potential data loss.\nThis notice applies to everyone who has found their account deleted unexpectedly from Galaxy Main.\nWe are enforcing account terms to help keep Galaxy Main a fair public resource for everyone!\nThank you! Jen, on behalf of the Galaxy Team and Community"}, {"role": "system", "text": ""}, {"role": "system", "text": "Update Nov 1, 2019\nDuplicated accounts are detected as an ongoing process. If you found your account recently deleted, or have not but do have duplicates and want to avoid problems, write to us at galaxy-bugs@lists.galaxyproject.org and list the following:\n\nThe registered email for the account you wish to keep\nIf that email is from a public domain service, include your non-public primary email address. See \u201cReminders\u201d below if you do not understand the difference.\nList out your duplicated account email addresses. All, not just those you happen to be using right now.\nConfirm that you now understand the terms of usage, and will follow them going forward.\n\nReminders:\n\n\nOnly one account will ever be restored. Unfair usage means data loss in other accounts.\nTerms of usage can be found here: https://usegalaxy.org/static/terms.html\n\nThere is never a valid reason for multiple accounts\nIf we discover duplicated accounts before they are proactively declared, your data is at risk of being lost\nA public domain email address is from an anonymous service like Gmail, Hotmail, Nate, QQ, Yahoo or similar.\nA non-public primary email address is from your university or place of work.\nAct quickly and be honest.\nAccount terms are only re-clarified once per user, as a courtesy.\n\nREAD THIS FAQ: Getting an account at Galaxy Main (http://usegalaxy.org) Creating a \u201cnew\u201d account will not solve duplicated account problems and instead cause more. You need to write in to get your issues resolved, correctly.\n\nThe simplified quota rule is one account per user per public Galaxy server.\nEnd users are permitted to create a single account at each other public Galaxy server \u2014 most if not all have a \u201cone account per end-user\u201d requirement.\nWhen presented with the activation link in the emails anyone received when creating an account, the account terms were very clearly explained, and you agreed to these terms.\nQuote:\n\nBy clicking on the above link and opening a Galaxy account, you are also confirming that you have read and agreed to Galaxy\u2019s Terms and Conditions for the use of this service (https://usegalaxy.org/static/terms.html). Terms include a quota limit of one account per user. Attempts to subvert this limit by creating multiple accounts or through any other method may result in the termination of all associated accounts and data.\n\nThanks for understanding and for helping us to keep Galaxy a free and fair public resource, for everyone, worldwide.\nJen & the Galaxy team"}], [{"role": "user", "text": "This had been working fine 6 months ago. I am now analyzing a new data set and it looks like it is running fine. The plots come out and look just fine as well as normalized count files but the list for the results is empty. I have rerun it over the course of the last few days and each time the list is still empty."}, {"role": "system", "text": "Hi @acrocker\nPlease double-check that you are using the most current version of the tool:\n\nWhat will display at the top of the tool form: DESeq2  Determines differentially expressed features from count tables (Galaxy Version 2.11.40.6)\nTool repository: https://toolshed.g2.bx.psu.edu/view/iuc/deseq2/0696db066a5b\n\n\nIf this is the version you are using, please confirm that you are working at the public Galaxy Main server at https://usegalaxy.org.\nThanks!"}, {"role": "user", "text": "Yes. I am using 2.11.40.6 and  believe I am working at the public instance of galaxy at usegalaxy.org.\nAlso I tried on the european site usegalaxy.eu and worked fine with the same current version of the tool.\nThank you"}, {"role": "system", "text": "Thanks for the extra info @acrocker\nI wasn\u2019t able to reproduce this with a test yesterday at usegalaxy.org.\nPerhaps there is something special about your run configuration that is hitting a novel corner-case problem. If you would like a review, sharing the history that demonstrates the problem is the quickest way to communicate.\nI am an administrator at both usegalaxy.org and here at Galaxy Help, so there are a few ways you could do this:\n\n\nIf your registered account at both places uses the same email address, post back publically and state so, and note the name of the history and the datasets (by number) involved.\n\n\nIf your registered accounts use different email addresses, send me a private direct message here with the email address you use at usegalaxy.org. Note the name of the history and the datasets (by number) involved.\n\n\nIf your registered accounts use different email addresses, send me a private direct message here with a share link to the history. Note the datasets (by number) involved. When generating the share link, be sure to also check the box to \u201calso share objects\u201d in the history to make this go quicker. If you are not sure if the objects are shared, toggle the history share link off/on to check or make changes.\n\n\nYou do not need to post email addresses publically and you never need to share your password with anyone, no matter who they say that are \u2013 an administrator doesn\u2019t need it \u2013 admins just need to know what data to review.\nYou could decide to post your history share link publically. Many do and that is fine. It depends on how concerned you are about data privacy around your work.\nTo send a direct message here in Galaxy Help, click on the other person\u2019s \u201c@\u201d and you\u2019ll see an envelope icon to use. I\u2019m @jennaj. When you do this from within a topic, the message is branched off privately from the topic (adding helpful context).\nFor any choice: the history should contain all input and output datasets in an undeleted state, or help will be limited.\nI added a tag to your post for \u201cshare-or-publish\u201d \u2013 click on it and review prior Q&A if you are not sure how to generate a history share link with datasets also shared. Or, review these FAQs:\n\nSharing and Publishing your work\nData Privacy Features\n\nSorry you had problems but let\u2019s try a review and find out if this is some usage problem (and explain how to avoid it) or a novel tool issue (and report it/get it fixed)."}, {"role": "system", "text": "Thanks for the report, I can reproduce this and we will release a fix shortly."}, {"role": "system", "text": "Thanks again for the report, this should be fixed with DESeq2 version 2.11.40.6+galaxy1 (https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.6+galaxy1)"}], [{"role": "user", "text": "I\u2019m currently using the following line to get tags of datasets to the underlying tool:\n#set tags = ','.join(str(t.value) for t in $dataset.tags)\n\nThis however this does not return the original tags, they get truncated (losing  #  and the  name:), converted to lowercase and special characters are removed. Is there a better way to get access to tags in the tool xml?"}, {"role": "system", "text": "You need to get  t.name and t.value. .name is the first part before the :"}, {"role": "user", "text": "Thanks @mvdbeek, can the tool also get access to unsanitized or at least preserved casing of tag values? e.g. #hello:YoU vs #hello:you"}, {"role": "system", "text": "There\u2019s no sanitization anywhere, but tags are linked via lowercase, so maybe you\u2019d need to use\nuser_tname and  user_value"}], [{"role": "user", "text": "Hi everyone!\nI have used the Stacks series a few times before but now have an error that I can\u2019t seem to get around.\nI did the demultiplexing via Stacks V2.55, having three enzymes I activated the option \u201cDisable checking if the RAD site is intact\u201d and only entered 2 of them (one frequent and one rare cutter). For the rest, I used the same parameters as I used before. The demultiplexing went fine.\nNow, as for the assembly, the de novo wrapper always ends on \u201cUnable to finish job\u201d. Funny enough, all the files that are not lists (the summary statistics, the logs) are available and can even be downloaded. I can see the list of the loci and where there are consensus, for example. But as for the lists, they just do not exist, so I don\u2019t have the catalog of loci\u2019s actual sequences for example, which is quite a bother.\nIt took between one day and a week for the data to stop running. I have PE data, 2 species for which I have 14 and 10 individuals. I used only the default options.\nAnybody could have an idea of the problem?\nThank you in advance."}, {"role": "system", "text": "Hi @Joelle99\nPartial outputs could be for a few different reasons\u2026 it could be a technical problem or the tool ran out of resources during execution.\n\n\n\n Joelle99:\n\nI used only the default options.\n\n\nUsing default settings may be the problem since the original data wasn\u2019t quality filtered as stringently as your prior runs (if I am understanding correctly!). Meaning, you may need to be stricter at this step to get a successful result. Tutorial (older version of the tool suite, but still helpful): RAD-Seq de-novo data analysis\nSo, please try this:\n\n\nSend in a bug report from the current error and include a link to this help forum post in the comments. The EU team can review it closer with that context/information. Try not to delete any datasets yet.\n\n\nStartup another rerun exactly the same as originally ran in that same history. Might as well get that started since it took several days to process the first time. If there was some transient server issue, a rerun is usually the solution.\n\n\nNext, review the assembly options, and consider making the settings stricter. You might want to try a few different combinations. The tutorial above has some examples of that, plus links to the tool suite documentation and forum (also at the bottom of the tool forms).\n\n\nLet\u2019s start there  It is evening in the EU right now, but this covers the first steps in troubleshooting. You can always cancel jobs that aren\u2019t needed once you get more feedback. Perhaps the tool needs more resources the EU admins can allocate, or there was some cluster problem, or maybe the job as-is is actually one that will never run completely as it is currently set up and tuning assembly parameters are needed."}, {"role": "system", "text": "Related Q&A Stacks: 3 Rad-seq Demultiplexing"}, {"role": "user", "text": "Hi there, and thank you for your return! I have already completed #1 and #2, but I\u2019ll now try like you said to diminuish the tool\u2019s burden by being more strict in the options. Thank you!"}, {"role": "user", "text": "To follow up a little bit, I have yet to receive any news from the support. On the other hand, I tried multiple solutions with sadly no positive results. I have tried reducing the number of samples by using only forward reads in my paired-end experiment, as well as choosing options that would be stricter. @jennaj, should I filled another ticket for the support?\nThank you."}, {"role": "user", "text": "Hello,\nIf anyone shares this problem, I \u201csolved\u201d it by using not the V2 of Stacks but the version 1.46, which apparently works for this type of situation."}], [{"role": "user", "text": "I am trying to develop a tool based on R script. The tool should provided several methods for correlation calculation. Then there should be a parameter with options for different methods. I have set the parameter, but \u201cthere is no options available\u201d on my local galaxy.\nHere is my code:\n    <param name=\"method\" type=\"select\" label=\"methods for correlation calculation\"/>\n\n            <option value=\"pearson\">Pearson </option>\n            <option value=\"spearman\">Spearman  </option>\n            <option value=\"kendall\">kendall </option>\n    </param>\n\nIn the corresponding R script:\nspec_list$expr <- c(\u2018expr\u2019, \u2018f\u2019, \u20181\u2019, \u2018character\u2019)\nspec_list$method <- c(\u2018method\u2019, \u2018m\u2019, 1, \u2018character\u2019)\nspec_list$correlationMatrix <- c(\u2018correlationMatrix\u2019, \u2018c\u2019, 1, \u2018character\u2019)\nspec <- t(as.data.frame(spec_list))\nopt <- getopt(spec)\n\u2026\nif(!is.null(opt$method)){\ncorMatrix <- cor(expr, method=opt$method)\n}"}, {"role": "system", "text": "I am unclear here, does the select element render fine for you in the toolform? (it should)\nHow do you pass it to the R script? Can you show us the command from the tool?"}, {"role": "user", "text": "Hi,\nI have copied the R code, I think the problem is in the tool definition xml file."}, {"role": "user", "text": "Here are the R code:\nspec_list<-list()\nspec_list$help <- c(\u2018help\u2019, \u2018h\u2019, \u20180\u2019, \u2018logical\u2019)\nspec_list$expressionData <- c(\u2018expressionData\u2019, \u2018f\u2019, \u20181\u2019, \u2018character\u2019)\nspec_list$method <- c(\u2018method\u2019, \u2018m\u2019, 1, \u2018character\u2019)\nspec_list$correlationMatrix <- c(\u2018correlationMatrix\u2019, \u2018c\u2019, 1, \u2018character\u2019)\nspec <- t(as.data.frame(spec_list))\nopt <- getopt(spec)\nexpressionData <- read.table(opt$expressionData)\nif(!is.null(opt$method)){\ncorMatrix <- cor(expressionData, method=opt$method)\n}\nwrite.table(corMatrix)"}, {"role": "system", "text": "Could you please show us your <command> from the xml tool file?"}, {"role": "user", "text": "Here are my commands:\n<command>\n    <![CDATA[\n        Rscript '${__tool_directory__}/correlationMatrix.R'\n            -f $expressionData\n            -m $method\n            -c $correlationMatrix                  \n            \n            \n            \n    ]]>\n</command>"}, {"role": "system", "text": "<param name=\"method\" type=\"select\" label=\"methods for correlation calculation\"/>\n\nthere is a typo \"/ \"\nyou close the parameter block before the options"}], [{"role": "user", "text": "I was trying to run a workflow but ran into some problems, which I think are related to featurecounts. Basically, once instance of featurecounts in a workflow>it runs. Two instances in a workflow>it does not run. Remove either featurecount from the workflow>it runs again. These kinds of problems do not occur if I use htseq instead, but I would prefer to use featurecounts because it counts more reads that are of interest to me."}, {"role": "system", "text": "Hi @cjain\nI tested a workflow with two featureCounts, the same input files, but different settings (gene_id vs gene_name for count aggregation) and cannot reproduce the issue. What do you mean by \u2018workflow does not run\u2019? Some jobs fail or you don\u2019t see outputs? Make sure that you \u2018activated\u2019 output files in the workflow or check hidden files.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nThank you for looking into this issue. What I meant is that if I have two featurecounts each of which have the same GFF gene identifier but different GFF feature types (e.g., tRNA and rRNA), then when I press the \u201crun workflow\u201d button, the workflow does not get activated. However, if I delete either featurecount module and press the \u201crun workflow\u201d button, the workflow starts right away.\nIf you have been able to get a workflow with two featurecount modules to run, is it possible to send it to me so that I can compare your workflow with mine? Perhaps there is something about my choice of options that is preventing my workflow from working properly.\nThanks,\ncjain"}, {"role": "system", "text": "Hi @cjain\nthank you for the additional description. I never experiences the situation you\u2019ve described.\nMy workflow:\nhttps://usegalaxy.org.au/u/igor/w/featurecountstest\nThe outputs of featureCounts jobs were renamed based on alignment file plus label for 1st or 2nd job. For the best experience use a BAM file named something like sample_name.bam.\nin both featureCounts jobs the reads were counted for \u2018exon\u2019 and aggregated on gene_id and gene_name, respectively.\nKind regards,\nIgor"}, {"role": "system", "text": "Hi @cjain\nhave you checked the annotation file for presence of rRNA and rRNA features? Can you run featureCounts on both settings?\nI submitted my workflow with two featureCounts, 1st: exon and gene_id; 2nd: gene and gene_name, and it was completed.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nI tried your workflow and it runs fine. Your workflow has inputs that are configured somewhat differently from mine, which might be the reason why yours worked and mine did not. Thank you for your time!\nRegards,\nChaitanya"}, {"role": "system", "text": "Hi Chaitanya,\nI\u2019ve built the workflow \u2018from scratch\u2019 in Workflow editor. Just add two input boxes, for alignment and annotation, add meaningful description, add tools, and you\u2019ll be fine.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi everyone,\nI am currently analyzing RNASeq data obtained from dog cell culture. I used HISAT2 for the alignment to the canis familiaris reference genome in Galaxy. Then, to count the reads, I tried both htseq-count (Galaxy Version 0.9.1) and featureCounts (Galaxy Version 1.6.4+galaxy1). As gene annotation file, I took this file from iGenomes: https://usegalaxy.eu/library/list#folders/Fcf0661b18aa5ad51\nIt contains 32461 lines, among them 15178 exons (feature type). As I run htseq-count/featureCounts with \u2018exon\u2019 as feature type, I expected 15178 lines in the outout of these tools. In contrast, I get both in htseq-count and featureCounts only 2023 lines, even though exons with 0 counts are included in the output.\nDo you have any idea what could have happened to the remaining exons?\nThank you.\nBest\nHannah"}, {"role": "system", "text": "Hi @HanMue\nThe counts are summarized by the meta-feature \u201cgene\u201d at default settings. This is specified by the Advanced option \u201cGFF gene identifier\u201d set to \u201cgene_id\u201d.\nThere can be one or more exon per transcript, then one or more transcript per gene.\nAs a test to see how this is arranged in gtf annotation, try using the tool Select with a simple query that just contains the name of one of your genes against the gtf dataset. Can check a few if you want. For each, you\u2019ll see how the formatting is organized for that particular gene and can compare it to the gtf datatype specification in the FAQ below.\nTo instead count up by feature (exons), toggle the Advanced option \u201cOn feature level\u201d to be \u201cYes\u201d. (No is the default). There are more options that can be adjusted \u2013 could try a few different ways and compare to better understand how each impact your particular data.\nThe above options are for FeatureCounts. Htseq-count is similar but with fewer options.\nFAQs: https://galaxyproject.org/support/\n\nCommon datatypes explained\nExtended Help for Differential Expression Analysis Tools\n\nThanks!"}, {"role": "user", "text": "Hi @jennaj\nthank you for your kind reply.\nI checked the gtf annotation file. As a standard gtf-file, it has 9 fields containing: \nTo check the formatting on the gene level as you recommended, I randomly checked the gene \u2018BCL2\u2019 and this is what I got:\n\ngrafik1258\u00d7294 23.6 KB\n\nI also have checked a few other genes and they have something between 1 and aprox. 15 lines each.\nThat is why I think counting by the meta-feature \u201cgene\u201d, the 15178 exon lines in the gtf annotation shrink to 2023 genes.\n\n\n\n jennaj:\n\nTo instead count up by feature (exons), toggle the Advanced option \u201cOn feature level\u201d to be \u201cYes\u201d. (No is the default)\n\n\nDoing this, I get 15179 lines as I expected in the beginning, but know the counts look like this:\n\nThe counts of the different exons of one gene are know split into different lines (before, when counting by the meta-feature \u201cgene\u201d it was only one line per gene). But will this have any adavantage for DeSeq2 because the information is missing to which exon the counts belong? Then, what was confusing, I added the counts of e.g. all BCL2 lines in the picture above and compared it to the count number of BCL2 in the first featureCounts run counting by the meta-feature \u201cgene\u201d and it does not match.\nMy main problem is, that 2023 genes are a very small number that makes downstream analysis like differential gene expression with DeSeq2or a PCA difficult/sketchy\u2026\nDo you think I should try another annotation, e.g. the ensembl-version?\nI have nerver worked with the organism canis familiaris (dog) before, so I do not know how well it is annotated, perhaps the small number of genes is due to the quality of annotation?\nMany new questions in this post, but your answer solved my original question! Thank you!\nHannah"}, {"role": "system", "text": "Hi @HanMue\nThe iGenomes version of the annotation is based on RefSeq from several years ago.\nAnnotation from Ensembl will be more current but will require some manipulations to have the chromosome identifiers match up with the built-in canFam3 indexes that are sourced from UCSC.\nOr, you can also use the matching Ensembl fasta version of the genome and use that as a Custom Reference Genome/Build with tools. Be aware that the dog genome is very large and may exceed server processing resources when used this way at a public Galaxy server. But you can try.\nDo not assign the \u201cdatabase\u201d as canFam3 if you decide to use all Ensembl data. You\u2019ll also need to remap. The Ensembl version of the data does not have the same chromosome identifiers as the UCSC version. Every step in an analysis needs to use data that is all based on the same exact genome/build.\nFAQs: https://galaxyproject.org/support/\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\n"}], [{"role": "user", "text": "Dear Sir/Madam,\nThanks in advance for your help. I would like to share you my problem regarding collecting the unmapped contigs using Bowtie2 tool. I have tried several times and I am still facing the same problem (please see attached images of error report\n\nBowtie2-problem344\u00d7549 15.1 KB\n\n\n)"}, {"role": "system", "text": "Hi @Adnan,\nwould you mind sharing your history with me? I\u2019ll have a look at it. This video explains how to share the history in case you need it: Share history - YouTube\nRegards"}, {"role": "user", "text": "Hi\nThanks for your reply and please find attached the link of my history page:\nhttps://usegalaxy.org/u/lahuf79/h/tomato-1\nPlease know that I am facing a problematic issue with application of SPAde assembly tool.\nAgain thanks so much for you help\nAdnan"}, {"role": "system", "text": "Hi @Adnan,\nI\u2019m trying to reproduce the Bowtie2 error; could you provide me some additional information about the problem that you are facing with Spades?\nRegards"}, {"role": "user", "text": "Dear Gallardoalba,\nThanks for your attention. Regarding SPAde tool issue please find below the history share link:\nhttps://usegalaxy.org/u/lahuf79/h/tomato-1\nRegards\nAdnan"}, {"role": "system", "text": "SPAdes has run out of memory, this is probably due to a recent backend change - we are now running a few tools, including SPAdes, on a different cluster from what it was previously, and this cluster has slightly less memory. I\u2019ll work on this and let you know when it should be working."}, {"role": "system", "text": "Hi, sorry for the wait. There was a bug in the way that SPAdes was running that prevented it from properly using the amount of memory allocated for the job. This has been corrected and you should be able to successfully run it now. Please give it a try and let us know if it\u2019s still not working for you."}], [{"role": "user", "text": "Hi, I\u2019m Yoon.\nEvery works on Galaxy eu are impossible.\nI tried \u2018Bowtie2 (with ATAC-seq data)\u2019, but it wasn\u2019t work.\nSo, i tried \u2018Faster Download and Extract Reads in FASTQ format from NCBI SRA\u2019 on Get Data, but it also wasn\u2019t work.\nOn https://usegalaxy.org/, i can do that work, but i need \u2018Genrich\u2019 program which is exist on only Galaxy eu.\nIs this a serve problem or it dosen\u2019t work because this is holliday?\nPlease let me know what happened, because i need to submit some data to professor.\nAlways thank you for your work!"}, {"role": "system", "text": "Hi @Yu_eun_Min,\ncan you try now again, please?\nCiao,\nBjoern"}, {"role": "user", "text": "Oh, it work now.\nThank you!"}, {"role": "user", "text": "Hi, as you said, i tried it.\nIt is loading, but not finished.\nI think it takes so long time compared to usual.\nCan you check it and reply to me?"}, {"role": "system", "text": "Hi,\nI don\u2019t know what I should look for. But if its yellow and spinning than it is running \nCiao,\nBjoern"}, {"role": "user", "text": "Thank you.\nI think server is fixed.\nBut my new work is not working. And i think it takes so long time. Can you check it again?\n\u201cThis is a new dataset and not all of its data are available yet\u201d sentences are shown, and it is not working.\nThank you for your work!"}, {"role": "user", "text": "Oh\u2026 it work now. thank you. Happy new year!"}], [{"role": "user", "text": "im doing an rna seq analysis and after aligning with Star and filtering after Deseq2 a question remains. and that is:how to replace these ID with official gene names? i found a tool names annotatedmyIDs but it isnt working"}, {"role": "system", "text": "The tool annotateMyIDs does have a small problem right now (all versions). The tool is not ignoring header lines even when that is set on the tool form.\n\n\n\nAnnotateMyIDs Fatal error\n\n\nTo use the tool now , first run the tool Remove beginning of a file to remove the first line (header), then run the tool annotateMyIds .\n\n\n\n\n\nAnnotateMyIDs Fatal error\n\n\nticket for the correction here if interested: https://github.com/galaxyproject/tools-iuc/issues/2319\n\n\nCross-reference post: AnnotateMyIDs Fatal error\nIf that does not solve the problem, check that the Organism and ID Type are a match for the transcripts or genes in your data. The tool UniProt ID mapping and retrieval is an alternative for many use cases.\nWhich tool to use depends on where the existing annotation was sourced (meaning: what ID content they currently represent). The format of the transcript/geneIDs can also cause problems. I\u2019ve seen problems come up when a version is added to the end of the value \u2013 that should be removed before using either tool if present. Example: NM_130786.2 should be modified to be NM_130786.\nIf this does not address your issue, please share a few of the IDs and the source/genome build of the reference GTF used with whatever counting tool you used as input to DESeq2. We can probably help with determining the right starting \u201cID\u201d type."}, {"role": "user", "text": "how i should remove the last part of my data ?\nim just want to try this and if it didnt work i will share my data ID and other information  and my gtf.maybe im doing somethings wrong"}, {"role": "user", "text": "how remove the last number ?\nNM_130786.2  to NM_130786.???"}, {"role": "system", "text": "Hi -\nSince the identifier is in the first column, Text transformation with sed will work without a complex expression. Try this:\ns/\\.[0-9]+//\n\nOther options include using regular expressions with tools like:\n\nText reformatting with awk\nReplace parts of text\nReplace Text in entire line\nReplace Text in a specific column\n\nOr, if you don\u2019t want to use a regular expression, the column can be isolated, the \u201cdot\u201d replaced with a \u201ctab\u201d, then all the data rearranged back again into one file. This could be put into a workflow if you plan to run it again. Example tool order: Cut > Convert delimiters to TAB > Paste > Cut.\nAlso, your IDs are Emsembl transcripts, so choose that as the input type with annotateMyIDs.\nHope that helps!"}], [{"role": "user", "text": "Hello\nIm running Trimmomatic Pair-end(separate data set) with multiple data sets. Every data was fine till two of them showed error.\nThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource.\nTrimmomaticPE: Started with arguments: -threads 6 -phred33 fastq_r1.fastqsanger.gz fastq_r2.fastqsanger.gz fastq_out_r1_paired.fastqsang\nwhat kind of error is this? and how can i fix it?\nThanks\nSihae"}, {"role": "system", "text": "Dear Sihae,\nI am not sure based on the message. Check first your input files again. Are they in the right format?\nYou can either make an error report or share the history with me. Send me a message here in this forum and I give you my email address so you can share the history with me.\nCheers,\nFlorian"}, {"role": "system", "text": "\n\n\n Sihae:\n\n-phred33\n\n\nThis option designates that the reads are based on an earlier Illumina protocol version 1.3-1.7 (Phred+33 quality score scaling). The datatype for reads in that format is fastqillumina or  fastqillumina.gz in Galaxy.\nYour inputs are labeled as fastqsanger.gz. Both fastqsanger and the compressed version fastqsanger.gz have quality scores from Sanger/Illumina protocols 1.8 or higher (Sanger Phred+64 quality score scaling).\nTry running FastQC on your reads. The report will note which quality score scaling was detected. The assigned datatype should match the content, or all sorts of odd issues/failures can result.\nFrom a quick look, maybe the option \u201cPerform initial ILLUMINACLIP step?\u201d was set as \u201cYes\u201d and the adaptor chosen is not an actual match for the protocol that created the reads, or the assigned datatype attribute metadata?\nThe first few FAQs here describe these different fastq formats in more detail. Most data is in fastqsanger/fastqsanger.gz now, and it is the fastq read datatype nearly all tools downstream from QA expect as an input, but you can confirm that with your own data, and make processing adjustments as needed. Galaxy Support\nPlus I added a few tags to your topic that link to prior Q&A that explains these concepts with more even detail, tutorial links, etc that may be able to help.\n@Flow will also be able to help with this and other potential issues by reviewing your history, but I wanted to give some help for what popped out for me as a potential data attribute vs content vs parameter problem that could lead to an error.\nUpdate: There is another problem that you should address with priority first. There is a quota of one account per person at each distinct public Galaxy server. One account at each or all is fine, but not more than one at any. You currently have three older accounts and one new account at UseGalaxy.org. You must consolidate your data into one account, and download anything that doesn\u2019t fit into one account that is still important to you. Which account you choose to use ongoing is your choice. Delete the others under User > Preferences immediately or all will be suspended by administrative processes. This can result in data loss, so please fix the problem yourself before that happens. How-to is explained here: Registering Accounts and here Account quotas. Write in to the mailing list in the FAQ should you need help with this."}, {"role": "user", "text": "Dear Florian\nThanks for the reply.\nThe data im trying to work with is already published. I think that might be easier for me to explain my problems.\nI used PRJNA591665 study seq data.\nAmong them, SRR10538402 and  SRR10538403 had problems with Trimmomatic. \u201cperform initial ILLUMINACLIP step?\u201d was set as \u201cNo\u201d\nThis might be a very beginner\u2019s question, How can I know the datatype?\nI downloaded from ENA  fastq.gz files and I didn\u2019t know that it was fastqsanger.gz\nIn addition, like @jennaj  recommended, i did run FastQC.\nOne of each data failed FastQC. Is this related to the datatype?\nThanks again for your attention.\nSihae"}, {"role": "system", "text": "Dear Sihae,\nSorry for the late reply but I applied Trimmomatic and FastQC to the SRR10538402 data, which took quite some time to finish, and it worked for me. Can you please check that you have selected the newest version of the tools?\nYou actually know the sequence device and version from FastQC (first table, Sanger / Illumina 1.9 encoding) and it is also written that they have used the HiSeq X Ten device. So should be a newer Illumina version.\nCheers,\nFlorian"}], [{"role": "user", "text": "I have a local Galaxy installation that so far works fine.\nI now work with data collections.\nI have Illumina data (128 files) in one collection (each file around 20-30 Mb, so not very big).\nI now try to run jobs on that collection. For example FastQC.\nNow Galaxy behaves strange: When I select the collection as FastQC input and press \u201cSubmit\u201d, Galaxy starts to add the jobs into the History while the \u201cSending\u2026\u201d button is shown at the tool page so, I do not see the green \u201csubmitted\u201d page yet.\nHowever, after approx. 60 sec. Galaxy starts to submit the job again (while the \u201cSending\u2026\u201d button is still showing). The \u201cold\u201d job keeps being processed - it just adds a new one (with of course again 128 entries. After some time (~60sec) it adds it again and so on. This can easily add >10 jobs with over 1000 files from my inital list - that are of course all processed.\nThis strange behavior only stops at some point when Galaxy displays an error message saying the \u201cthe job submission failed\u201d (red box). From then on no \u201cnew\u201d jobs are added.\nThe thing is that even the first job submission was OK - it just needed some time to be processed due to the large number of files.\nTo me this looks like a timeout error that is triggered because the tool page is at the \u201csending\u2026\u201d level for to long.\nIs there any way how I can increase such a timeout to prevent this \u201cexplosion\u201d of jobs?\nMany thanks in advance! for any help!"}, {"role": "system", "text": "Hi @schmitts\nWould you be able to share some more details about your local Galaxy configuration?\n\nWhat version of Galaxy are you running?\nWhen was that last updated/retrieved from Github?\nOr is it a Docker Galaxy? From what version, source, and when was it retrieved?\nAre you the only user of the server? Or have you set up the server to allow for multiple users?\nIs this the first time you have used dataset collections on your server?\nHow much memory is available to Galaxy? 16 GB is the minimum default, but often more is needed.\nAny other configuration settings that may seem relevant. Examples: Are you using a cluster? Have you upgraded the database to use Postgres?\n\nIf you want to send that info in a direct message to me, instead of posting publically, that would be fine.\nThanks!"}, {"role": "user", "text": "Hi @jennaj\nMany thanks for your reply. Sure:\n\n\nWhat version of Galaxy are you running?\nIt is version 20.01\n\n\nWhen was that last updated/retrieved from Github?\nI cloned it Galaxy from Github at 10.4.2020, no docker (except for GIEs which run in containers)\n\n\nAre you the only user of the server? Or have you set up the server to allow for multiple users?\nIts setup to allow for multiple users (use_remote_user: true), however - I am current the only user that has access to it.\n\n\nIs this the first time you have used dataset collections on your server?\nI use Galaxy since half a year. I ran many jobs already on single files, this includes FastQC on files >4Gb. If I used dataset collections, they were only very small (3-4 entries). This worked fine. This is the first larger collection (but with small files).\n\n\nHow much memory is available to Galaxy? 16 GB is the minimum default, but often more is needed.\nI have made 32GB available for Galaxy. Furthermore, I monitored memory usage and CPU load before and during submission. The server consumes only a small amount during both processes (4 GB), CPU load increases during submission. I made 8 virtual cores available for Galaxy Server. See picture below. I have the following settings in the uwsgi-config: buffer-size: 16384, processes: 1, threads: 4, offload-threads: 2 (tried 1-4)\n\n\nAny other configuration settings that may seem relevant. Examples: Are you using a cluster? Have you upgraded the database to use Postgres?\nThe server runs on Centos 8.1, I use Galaxy in front of a Apache reverse proxy on the same server that uses ldap authentication. I run a separate server with Postgres for the Galaxy database.\n\n\nI assembles a small figure indicating the problem and also summarizing the load on the server. The time between the job resubmissions is almost exactly 60 sec. Intressting: For some tests with the same dataset the \u201csubmission error\u201d message already showed up after 60 sec (thus the job was submitted only once and ran fine) for other tests with the same dataset it took u to 6 min until the error message appeared (thus the job was submitted 6 times until it stopped submission). Its important to highlight that the submission did not really fail. Even after the 6 jobs were submitted they all got processed nicely.\nSee figure:\nError_Galaxy1176\u00d71133 349 KB\nMany thanks in advance for your help!\nUPDATE: I now also tested this without a dataset collection (just added all 128 files to the history and selected all the files in the tool \u201cmultiple datasets\u201d). Result is similar: I get the \u201csubmission failed\u201d message after some seconds - but the jobs are not resubmitted in the meantime. Seems to be really a problem with a timeout during job submission\u2026"}, {"role": "system", "text": "TImeouts during job submission do not stop Galaxy from processing your jobs, you just won\u2019t see a summary of what has been submitted. That\u2019s why you see your data appearing multiple times."}, {"role": "user", "text": "Hi @mvdbeek\nThanks - this makes total sense.\nAny chance that I can increase the timeout somewhere in the config or code?\nI had a look at galaxy.yml - but did not find a flag that would fit. Maybe default_job_resubmission_condition but I am not sure how to use this correctly\u2026\nThanks again!"}, {"role": "system", "text": "If your using nginx as your proxy and you connect via the uwsgi protocol you can increase the timeout by setting uwsgi_read_timeout 300; which would set the timeout to 5 minutes. If you\u2019re using uwsgi without a proxy you should be able to set http-timeout: 300 in the uwsgi: section of galaxy.yml (you can find all valid options for uwsgi in https://uwsgi-docs.readthedocs.io/en/latest/Options.html#http-timeout), but I\u2019m less certain about that being the right option."}], [{"role": "user", "text": "Hi,\nI am trying to install galaxy release_22.01 to our local server.  The server runs with centos 7, python 3.8, and postgresql 14.  The galaxy service can start with run.sh when the default configuration was used.  However, I couldn\u2019t get it switch to postgresql database.   When the configuration was changed to (database_connection: postgresql://galaxy:password@127.0.0.1:5432/), galaxy service (run.sh) didn\u2019t switch to postgresql database but still kept using default sqlite database.  In the command line, use \u201cpsql -U galaxy -d galaxy -h 127.0.0.1\u201d, the database can be accessed successfully with password.  Can anyone help with it?\nThank you!\nWei"}, {"role": "system", "text": "Hi @wyan999, that is a bug that we have to fix. For now you can fix this by activating galaxy\u2019s virtualenv (source .venv/bin/activate), and calling galaxyctl update --force. Your instance is still trying to start from galaxy.yml.sample. After that, restart your Galaxy instance and the changes should apply."}], [{"role": "user", "text": "Hi all\nI am trying to analyse my mRNAseq data using the galaxy reference based workflow (Reference-based RNA-Seq data analysis). I am very new to data analysis and also to galaxy. I tried two different approaches to align my data:\n1)RNA-STAR followed by feature counts: In this case the feature counts showed 0 counts.\n2) Next, I tried HISAT2 alignment\u2026and that showed just 177 reads\u2026\nI don\u2019t understand why the read number is so low\u2026do you have any suggestions?"}, {"role": "system", "text": "Hi @poojamuk19,\nwhich reference genome and annotation file are you using?\nRegards"}, {"role": "system", "text": "Hi @gallardoalba\nI have same problem. just few reads align to ref genome. my organism`s ref genome  is not supported by galaxy. So, I uploaded fasta file to the history then did alignment using hisat2 and star too.  i download fasta and gtf file from here. Index of /genomes/refseq/bacteria/Streptomyces_coelicolor/latest_assembly_versions\nAfter quality control (trimming) the reads are 32 bp and it is ribo seq data.\nMany thanks"}, {"role": "system", "text": "Hi @pshtiwan_babane\nCheck to make sure that your inputs are correctly formatted, are a match with each other, and tool parameters that make use of attributes in the GTF are actually present in your GTF dataset. You may also need to do some more QA on your reads.\nFAQs: Galaxy Support\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\n\nExtended Help for Differential Expression Analysis Tools \u2013 useful tips, even if performing other types of analysis\n\nGTN Tutorials: https://training.galaxyproject.org\n\nMany tutorials cover read QA/QC steps specific to different analysis goals/tools\nThis one is a good place to start for basics: NGS data logistics\n\n\nI also added a few tags to your topic that link to prior Q&A that cover the above items in the context of other people\u2019s analysis.\nThanks!"}, {"role": "system", "text": "hi @gallardoalba\nI am sorry I can`t do it.  must I imported fasta file (ref genome) through ftp or is it ok i uploaded it. then I used NormalizeFasta still doesn\u2019t work!\nkindly, can you help me more?\nMany thanks"}, {"role": "system", "text": "Hi @pshtiwan_babane,\nI\u2019ll reproduce the analysis by using that datasets. I\u2019ll inform you as soon as I get some results.\nRegards"}], [{"role": "user", "text": "Hi everyone,\nI was trying to map my sequences to the latest mouse genome assembly (GRCm39/mm39) but I couldn\u2019t find it on galaxy. How do I make a request for the mouse genome to be updated on galaxy?\nIn the meantime, I successfully got the fasta file of the GRCm39/mm39 from the NCBI FTP site and tried to map my sequences to it using Bowtie2, but it gave an error with no information about the error.\nPlease help, I\u2019m new to galaxy \nThank you."}, {"role": "system", "text": "Hi @Sedem_Dankwa,\nwe are working in order to include this version in Galaxy. Regarding the error, could you send an error report (bug icon)?\nRegards"}, {"role": "system", "text": "Hi @Sedem_Dankwa\nI checked your bug report sent in from UseGalaxy.org.\nA very large custom genome will likely exceed resources at any public Galaxy server. That said, UseGalaxy.eu is sometimes able to scale even large jobs \u2013 so maybe try there.\nYou\u2019ll need to correct a few items when you run a test there:\n\nThe full genome fasta should not be extracted from the UCSC Table Browser. The result will be truncated. Instead, capture the URL for the genome\u2019s fasta from their Downloads area and paste that into the Upload tool in Galaxy to get it into your history as a dataset. Allow Galaxy to choose the \u201cdatatype\u201d and do not assign a \u201cdatabase\u201d.\n\n\nUCSC Genome Browser Downloads >> Index of /goldenPath/mm39/bigZips >> you\u2019ll probably want to choose this version: mm39.fa.gz\nIf the fasta loads compressed (fastq.gz), uncompress it before using it as a custom genome (pencil icon > Edit attributes > Convert > uncompress). That will create an uncompressed version as a new dataset. Then you can purge the dataset representing the compressed version to recover the quota space.\n\n\nThe associated reference annotation should also not be sourced from the UCSC Table Browser. The gene_id and transcript_id attributes in the GTF will be the same value, which can lead to scientific content problems later on. The data might also output as truncated due to the size.\n\nUCSC creates correctly annotated GTF\u2019s for priority genomes and places them in their Downloads area.\nFind the version based on RefSeq genes/transcripts here: Index of /goldenPath/mm39/bigZips/genes >> refGene.gtf.gz\nAgain, load the data by URL into Galaxy using default settings then uncompress as needed.\n\n\nIt looks as if the forward and reverse reads were entered on the Bowtie2 tool form in the opposite order. Instead, enter the reads as: R1 = forward and R2 = reverse.\n\nHope that helps!"}, {"role": "user", "text": "Thank you so much for taking the time to provide this detailed guide! I really appreciate it.@jennaj"}, {"role": "user", "text": "\n\n\n jennaj:\n\nIndex of /goldenPath/mm39/bigZips\n\n\nThanks! @gallardoalba"}, {"role": "system", "text": "usegalaxy.eu has mm39 now included. I hope that helps!"}], [{"role": "user", "text": "Hello!\nI was advised to use $GALAXY_SLOTS variable in order to obtain number of processor cores. Workflow automation? - #2 by mvdbeek But when i print this variable in the tools, i get exactly one despite i have 16 - 48 logical processors.\nWhy it is not set automatically? E. g why not add\n elif [ `nproc` > 0 ]; then \n     GALAXY_SLOTS=`nproc`\n\ninto /data/galaxy/lib/galaxy/jobs/runners/util/job_script/CLUSTER_SLOTS_STATEMENT.sh ?"}, {"role": "system", "text": "We assume and highly recommend that jobs submitted by Galaxy are run on a HPC system with a job scheduler (like SLURM, PBS, Torque, LSF) or kubernetes. You then configure the destinations in the job_conf.xml file to request the required resources, and you assign tools to the destination. If you don\u2019t have this available you can use local_slots as you found out, but you still need to map tools to destinations.\nNote that this is not recommended, as there is no way for Galaxy to know what resources are available, so you will start consuming more resources than are available if you queue up many jobs.\nConnecting Galaxy to a compute cluster walks through installing slurm using ansible on the head node \u2026 while that\u2019s still not ideal it is better than using the local job runner."}], [{"role": "user", "text": "Hi,\nI tried running DESeq2 using output data from upstream Salmon analysis. The samples I\u2019m investigating are RNA-seq, paired-end. The run failed and the following error message came up when investigating the bug reports:\n\nimage720\u00d7290 11.2 KB\n\nI saw that other people encountered similar issues before, but with different errors (error message from DESeq2/EdgeR - #4 by jennaj). Some recommendations for this other type of error were to try run datasets with option \u201cFiles have a header\u201d off.\nI tried that (just in case), but still had the same error message (only the line number changed). So maybe I did something wrong upstream.\n\nimage731\u00d7291 11.2 KB\n\nAlso, not sure if this is of relevance here, but the upstream Salmon report mentioned that there are newer versions of Salmon, with bug fixes, etc. Could this be some sort of bug?\nIf somebody had any suggestions on how to sort this, that would much appreciated!"}, {"role": "system", "text": "Hi @Egle\nDouble check that these settings are correct:\n\nChoice of Input data \u2192 TPM values (e.g. from kallisto, sailfish or salmon\nProgram used to generate TPMs \u2192 Salmon\nGene mapping format \u2192 input the same annotation (GTF or tabular) as input to Salmon.\n\nIf you don\u2019t have item 3, you\u2019ll need to back up and rerun Salmon with that annotation to produce the proper inputs for DESeq2.\nThe option on the Salmon form is: File containing a mapping of transcripts to genes\n\nIf this file is provided Salmon will output both quant.sf and quant.genes.sf files, where the latter contains aggregated gene-level abundance estimates. The transcript to gene mapping should be provided as either a GTF file, or a in a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab.\n\nquant.genes.sf files is the Salmon output that is input to DESeq2, along with the annotation providing transcript-to-gene information.\n\n\nPlease be aware that DESeq2 expects all sample counts to be in individual dataset inputs. This error might come up if a matrix of counts was input. The \u201c8 elements\u201d is not expected \u2013 that number of columns does not match the column count of any of the expected inputs:\n\n\nquant.genes.sf files has 5 columns and \u201ca single header or not\u201d is specified on the tool form\n\ntranscript to gene has 2 columns and should not contain any header lines (# or others)\n\nGTF has 9 columns and should not contain any header lines (# lines).\n\nTry this:\n\nCheck that all of the inputs match the help above\nRerun as needed, and be sure to use the most current Galaxy tool-wrapper version of all tools. \u201cVersion\u201d can be navigated at the upper right corner of any tool form, but the messages about tool versions in stderr/stdout logs are from the original tool and usually do not mean anything relevant when running a tool wrapper in Galaxy.\nIf that still fails, please send in a bug report from the error(s) to the server admins\nAt the same time, you can follow up here by posting back a #sharing-your-history link. That link can be posted back publicly or ask for a moderator to start up a private chat. We\u2019ll need all inputs and outputs to be undeleted and for you to note which dataset numbers are involved.\n\nLet\u2019s start there "}, {"role": "user", "text": "Dear @jennaj,\nThank you very much for your advice. I have now tried running both Salmon and DeSeq2 a few more times, but the issue remains.\n\nDouble check that these settings are correct:\n\nChoice of Input data \u2192 TPM values (e.g. from kallisto, sailfish or salmon\nProgram used to generate TPMs \u2192 Salmon\nGene mapping format \u2192 input the same annotation (GTF or tabular) as input to Salmon.\n\nIf you don\u2019t have item 3, you\u2019ll need to back up and rerun Salmon with that annotation to produce the proper inputs for DESeq2.\n\nYes, the first run was wrong. I now corrected 1 and 2. Unfortunately, for 3. I only have tabular data from BioMart. I reran everything several times, but issue remained. I removed the title row from the transcript/gene ID table, the error message changed to the following:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nreading in files with read.delim (install \u2018readr\u2019 package for speed up)\n1 2 3 4\nError in $<-.data.frame(*tmp*, \u201cTXNAME\u201d, value = character(0)) :\nreplacement has 0 rows, data has 274080\nCalls: get_deseq_dataset \u2192 $<- \u2192 $<-.data.frame\n\nPlease be aware that DESeq2 expects all sample counts to be in individual dataset inputs. This error might come up if a matrix of counts was input. The \u201c8 elements\u201d is not expected \u2013 that number of columns does not match the column count of any of the expected inputs:\n\n\nquant.genes.sf files has 5 columns and \u201ca single header or not\u201d is specified on the tool form\n\ntranscript to gene has 2 columns and should not contain any header lines (# or others)\n\nGTF has 9 columns and should not contain any header lines (# lines).\n\n\nI think my input was individual datasets (nothing was merged, concatenated, etc.). Also, I don\u2019t have any GTF data, but the formats of other datasets seem to be correct and the number of columns seems to match.\nAs advised, I sent the bug report.\nThese are the datasets I used for the most recent round of runs (the ones that resulted in altered error message):\nSalmon:\nTranscripts fasta file: 9 (Fasta normalised reference transcriptome).\nInput datasets: paired data, 1-8 (trimmed reads; 1,3,5,7 at Mate pair 1; 2,4,6,8 at Mate pair 2).\nFile containing a mapping of transcripts to genes: 10 (transcript ID-gene ID table from BioMart. I think I changed it to tab-delimited. I also took off title row).\nOutputs were 11-18.\nDeSeq2:\nComparison group 1: datasets 12 and 16.\nComparison group 2: datasets 14 and 18.\nGene mapping format - tabular.\nTabular file with Transcript-ID to Gene-ID mapping: dataset 10 (the BioMart table, with transcript and gene ID versions).\nIf full history is required, I would prefer to share it privately."}, {"role": "user", "text": "An update. I spent some time playing around with \u201ctranscript to gene\u201d table formats, swapping \u201ctranscript/gene stable ID versions\u201d with \u201ctranscript/gene IDs\u201d in various combinations, different ways of importing the table (TSV instead of CSV), converting the table to tab-delimited format, adjusting the titles, removing the titles etc.\nUnfortunately, none worked for DESeq2, despite upstream Salmon quantification working well. I suspect that perhaps there was some mismatch in ensembl transcriptome data in fasta format (maybe I messed up something when normalising it or used wrong reference dataset) and the BioMart table (I tried various versions).\nWhat worked was downloading ensembl reference transcriptome in gtf format. I used it unmodified in Salmon and downstream DESeq2 instead of the Transcript-to-Gene table. The result was this:\n\nimage1081\u00d7298 28.6 KB\n\nI guess my next question to @jennaj and other experts is the following: can I somehow validate if DESeq2 worked correctly? Of course, if something did not work, that will come out during downstream analyses. But maybe there is a tool or type of analysis that allows flagging up issues early?\nThank you!"}], [{"role": "user", "text": "Hi,\nI have some files obtained by small non-coding RNA sequencing in humans. My aim is to identify the miRNA and do differential expression.\nI am following this protocol: Hands-on: miRNA differential expression analysis - YouTube\nIt says that I need some reference files, but I\u2019m new in the field and i don\u2019t understand what\u2019s the difference between them or where I can download them. These files are:\n\nannotation.gtf\ntranscriptome.fasta\nstar_miRNA_seq.fasta\nmature_miRNA.fasta\nmiRNA_stem-loop_seq.fasta\n\nI searched in miRBase and I found some reference files, but I am not sure which are the correct ones or where are the ones missing. I attach a picture.\n\nCaptujra921\u00d7607 48.7 KB\n\nCan anyone tell me where I can download them and what\u2019s the difference among them (especially between annotation and transcriptome file, not only the format; also between star miRNA and the other two). Thank you so much! "}, {"role": "system", "text": "Hi @LaiaGutierrez,\nsome background on the files:\n\n\nAnnotation.gtf\nThe Gene Transfer Format is a file format used for describing genes and other features of DNA, RNA, and protein sequences. You can download it from ENCODE; this file corresponds to the Comprehensive gene annotation dataset.\n\n\nTranscriptome.fasta\nThis file includes the sequence of all the transcripts in the human genome. You can download it from ENCODE; this file corresponds to the Transcript sequences dataset.\n\n\nmature_miRNA.fasta\nThis file includes the functional miRNA sequences. You can download it from miRBase; this file corresponds to the mature miRNA sequences dataset.\n\n\nmiRNA_stem_loop_seq.fasta\nThis file contains the hairpin structure of the precursor miRNA. You can download it from miRBase; this file corresponds to the hairpin miRNA sequences dataset.\n\n\nstar_miRNA_seq.fasta\nThis file includes the sequences generated after processing the pre-miRNAs, which are assumed to be non-active. However, currently, this notation has been modified, since experimental results indicate that those sequences are usually functional. For this reason, star sequences are not currently available in miRBase (because have been integrated with the mature sequences). This file is optional.\n\n\nI suggest you to have a look at the training, since it can be useful for understanding the pipeline: miRNA data analysis.\nRegards"}], [{"role": "user", "text": "Can anyone clarify what is expected in the following block of code in cli.py?\n\nif state is None:\nif ajs.job_wrapper.get_state() == model.Job.states.DELETED:\ncontinue\n        external_metadata = not asbool(ajs.job_wrapper.job_destination.params.get(\"embed_metadata_in_job\", DEFAULT_EMBED_METADATA_IN_JOB))\n        if external_metadata:\n            self._handle_metadata_externally(ajs.job_wrapper, resolve_requirements=True)\n\n        log.debug(\"(%s/%s) job not found in batch state check\" % (id_tag, external_job_id))\n        shell_params, job_params = self.parse_destination_params(ajs.job_destination.params)\n        shell, job_interface = self.get_cli_plugins(shell_params, job_params)\n        cmd_out = shell.execute(job_interface.get_single_status(external_job_id))\n        state = job_interface.parse_single_status(cmd_out.stdout, external_job_id)\n        if not state == model.Job.states.OK:\n            log.warning('(%s/%s) job not found in batch state check, but found in individual state check' % (id_tag, external_job_id))\n    if state != old_state:\n        log.debug(\"(%s/%s) state change: from %s to %s\" % (id_tag, external_job_id, old_state, state))\n        if not state == model.Job.states.OK:\n\n\n            # No need to change_state when the state is OK, this will be handled by `self.finish_job`\n            ajs.job_wrapper.change_state(state)\n    if state == model.Job.states.RUNNING and not ajs.running:\n        ajs.running = True\n    ajs.old_state = state\n    if state == model.Job.states.OK:\n        log.debug('(%s/%s) job execution finished, running job wrapper finish method' % (id_tag, external_job_id))\n        self.work_queue.put((self.finish_job, ajs))\n\nIf the galaxy job gets queued by the cluster Galaxy marks it as OKay and deletes the job directories\u2026  Log output:\n\ngalaxy.jobs.mapper DEBUG 2019-04-22 15:07:56,035 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Mapped job to destination id: LocalShell\ngalaxy.jobs.handler DEBUG 2019-04-22 15:07:56,063 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Dispatching to cli runner\ngalaxy.jobs DEBUG 2019-04-22 15:07:56,086 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Persisting job destination (destination id: LocalShell)\ngalaxy.jobs DEBUG 2019-04-22 15:07:56,087 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Working directory for job is: /Dedicated/clingalproddata/database/jobs_directory/000/594\ngalaxy.jobs.runners DEBUG 2019-04-22 15:07:56,103 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] Job [594] queued (40.204 ms)\ngalaxy.jobs.handler INFO 2019-04-22 15:07:56,123 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Job dispatched\ngalaxy.jobs.command_factory INFO 2019-04-22 15:07:56,246 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] Built script [/Dedicated/clingalproddata/database/jobs_directory/000/594/tool_script.sh] for tool command [echo \"Running with '{GALAXY_SLOTS:-1}' threads\" > \"/Dedicated/clingalproddata/database/files/000/dataset_671.dat\"]\ngalaxy.jobs.runners DEBUG 2019-04-22 15:07:56,320 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) command is: rm -rf working; mkdir -p working; cd working; /Dedicated/clingalproddata/database/jobs_directory/000/594/tool_script.sh; return_code=?; cd \u2018/Dedicated/clingalproddata/database/jobs_directory/000/594\u2019;\n[ \u201cGALAXY_VIRTUAL_ENV\" = \"None\" ] && GALAXY_VIRTUAL_ENV=\"_GALAXY_VIRTUAL_ENV\u201d; _galaxy_setup_environment True\npython \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/set_metadata_KNv21u.py\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/registry.xml\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/working/galaxy.json\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_in_HistoryDatasetAssociation_806_uXnJC8,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_kwds_HistoryDatasetAssociation_806_hQcOzn,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_out_HistoryDatasetAssociation_806_gc_BfH,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_results_HistoryDatasetAssociation_806_adwo6t,/Dedicated/clingalproddata/database/files/000/dataset_671.dat,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_override_HistoryDatasetAssociation_806_Hi6XQ8\u201d 5242880; sh -c \u201cexit $return_code\u201d\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:07:56,341 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) submitting file: /Dedicated/clingalproddata/database/jobs_directory/000/594/galaxy_594.sh\ngalaxy.jobs.runners.cli INFO 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) queued with identifier: submitted\ngalaxy.jobs DEBUG 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) Persisting job destination (destination id: LocalShell)\n128.255.162.194 - - [22/Apr/2019:15:07:59 -0500] \u201cGET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A55.000Z&qv=False&qv=False HTTP/1.1\u201d 200 - \u201chttps://clinical-galaxy.iihg.uiowa.edu/?job_id=64c391652c2ec90d&__identifer=m1o72f2talq\u201d \u201cMozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:60.0) Gecko/20100101 Firefox/60.0\u201d\n[pid: 38337|app: 0|req: 8/8] 128.255.162.194 () {46 vars in 2172 bytes} [Mon Apr 22 15:07:59 2019] GET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A55.000Z&qv=False&qv=False => generated 1304 bytes in 36 msecs (HTTP/1.1 200) 3 headers in 124 bytes (1 switches on core 6)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:03,060 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job not found in batch state check\n128.255.162.194 - - [22/Apr/2019:15:08:03 -0500] \u201cGET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A59.000Z&qv=False&qv=False HTTP/1.1\u201d 200 - \u201chttps://clinical-galaxy.iihg.uiowa.edu/?job_id=64c391652c2ec90d&__identifer=m1o72f2talq\u201d \u201cMozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:60.0) Gecko/20100101 Firefox/60.0\u201d\n[pid: 38337|app: 0|req: 9/9] 128.255.162.194 () {46 vars in 2172 bytes} [Mon Apr 22 15:08:03 2019] GET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A59.000Z&qv=False&qv=False => generated 2 bytes in 28 msecs (HTTP/1.1 200) 3 headers in 124 bytes (1 switches on core 0)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,281 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) state change: from new to ok\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,281 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job execution finished, running job wrapper finish method\ngalaxy.jobs.runners ERROR 2019-04-22 15:08:06,286 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594/submitted) Job output not returned from cluster: [Errno 2] No such file or directory: \u2018/Dedicated/clingalproddata/database/jobs_directory/000/594/galaxy_594.o\u2019\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,406 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594/submitted) User killed running job, but error encountered during termination:\n"}, {"role": "system", "text": "Reference: https://docs.galaxyproject.org/en/release_19.01/admin/cluster.html#cli"}, {"role": "user", "text": "I appreciate the link, however I wouldn\u2019t have been able to write the Shell runner without reading the  documentation for the cli\u2026  I also have read https://docs.galaxyproject.org/en/release_18.09/dev/build_a_job_runner.html.\nMy code:\nfrom logging import getLogger\nimport re\nfrom galaxy.model import Job\njob_states = Job.states\n\nfrom ..job import BaseJobExec\n\nlog = getLogger(__name__)\n\nERROR_MESSAGE_UNRECOGNIZED_ARG = 'Unrecognized long argument passed to sge CLI plugin: %s'\n\nargmap = {'destination': '-q',\n          'Execution_Time': '-a',\n          'Account_Name': '-A',\n          'Checkpoint': '-c',\n          'Error_Path': '-e',\n          'Group_List': '-g',\n          'Hold_Types': '-h',\n          'Join_Paths': '-j',\n          'Keep_Files': '-k',\n          'Resource_List': '-l',\n          'Mail_Points': '-m',\n          'Mail_Users': '-M',\n          'Job_Name': '-N',\n          'Output_Path': '-o',\n          'Priority': '-p',\n          'Rerunable': '-r',\n          'Shell_Path_List': '-S',\n          'job_array_request': '-t',\n          'User_List': '-u',\n          'Variable_List': '-v'}\n\n\nclass sge(BaseJobExec):\n\n    def __init__(self, **params):\n        self.params = {}\n        for k, v in params.items():\n            self.params[k] = v\n\n    def job_script_kwargs(self, ofile, efile, job_name):\n        pbsargs = {'-o': ofile,\n                   '-e': efile,\n                   '-N': job_name}\n        for k, v in self.params.items():\n            if k == 'plugin':\n                continue\n            try:\n                if not k.startswith('-'):\n                    k = argmap[k]\n                pbsargs[k] = v\n            except KeyError:\n                log.warning(ERROR_MESSAGE_UNRECOGNIZED_ARG % k)\n        template_pbsargs = ''\n        for k, v in pbsargs.items():\n            template_pbsargs += '#$ %s %s\\n' % (k, v)\n        return dict(headers=template_pbsargs)\n\n    def submit(self, script_file):\n        return 'qsub %s' % script_file\n\n    def delete(self, job_id):\n        return 'qdel %s' % job_id\n\n    def get_status(self, job_ids=None):\n        return 'qstat'\n\n    def get_single_status(self, job_id):\n        return \"qstat -u svc-clingalprod | grep %s | awk '{print $1,$5}'\" % job_id\n\n    def parse_status(self, status, job_ids):\n        rval = {}\n        for line in status.strip().splitlines():\n            if line.startswith(\"job-ID\"):\n                continue\n\t    if line.startswith(\"-\"):\n                continue\n            line_parts = re.compile(\"\\s+\").split(line)\n            #if len(line_parts) < 9:\n                #continue\n            id = line_parts[0]\n            state = line_parts[4]\n            if id in job_ids:\n                # map SGE job states to Galaxy job states.\n                rval[id] = self._get_job_state(state)\n        return rval\n    def parse_single_status(self, status, job_id):\n        for line in status.splitlines():\n            line = line.split()\n            if line[0].strip() != None:\n                return self._get_job_state(line[1].strip())\n        # no state found, job has exited\n        return job_states.OK\n    def _get_job_state(self, state):\n        try:\n            return {\n                'r': job_states.RUNNING,\n                'Rr': job_states.RUNNING,\n                'qw': job_states.QUEUED,\n                'hqw': job_states.QUEUED,\n            }.get(state)\n        except KeyError:\n            raise KeyError(\"Failed to map sge status code [%s] to job state.\" % state)\n\n__all__ = ('sge',)\n\nUnder parse_single_status why is it working intermittently?  Is it a timing isssue with the cli.py or have I done something ncorrectly in my shell job class?"}, {"role": "user", "text": "Example output from get_single_status:\n\n[svc-clingalprod@clinical-galaxy job]$ qstat -u svc-clingalprod | grep 8791197 | awk \u2018{print $1,$5}\u2019\n8791197 Eqw\n"}, {"role": "system", "text": "It looks like your submit() method is outputting the full qsub output. The cli runner splits stdout on whitespace and uses the last element as the job id, which in your case appears to be the word submitted:\ngalaxy.jobs.runners.cli INFO 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) queued with identifier: submitted\ngalaxy.jobs DEBUG 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) Persisting job destination (destination id: LocalShell)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:03,060 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job not found in batch state check\n\nIf you update your submit() method to output just the SGE job ID that should get you past the issue you\u2019re seeing here."}], [{"role": "user", "text": "https://huttenhower.sph.harvard.edu/galaxy/\nHi Sir/Madam,\nWhen I try Lefse analysis, I often get this kind of error \u201cJob ran longer than the maximum allowed execution time (runtime: 0:15:15, limit: 00:15:00), please try different inputs or parameters\u201d.\nCan you advise how to fix it?\n"}, {"role": "system", "text": "User opinion:\nIs your input that big?\nMy lefse runs on hutlab are always done in ~5min. Using ~200 features.\nLook at this: Job and Tool Error Help.\nAlso, remember that hutlab is a 3rd party server."}], [{"role": "user", "text": "hello, I have loaded my sequences in galaxy because I want to do 16S Microbial Analysis with Mothur, and in the step of make.contings, the files are not created, a message that says \u201cThis job is currently running\u201d appears, many hours have passed but Many, I do not know if I\u2019m doing something wrong or it\u2019s normal, could you help me please?"}, {"role": "system", "text": "Which Galaxy server are you using? In general bioinformatic tools can run a long long time, even days. This is dependent on the tool in question, the size of the input data size and how busy the server is. So I wouldn\u2019t worry and wait a little bit longer.\nYou also might want to have a look at our training material about 16S analysis with Mothur."}], [{"role": "user", "text": "Hello, I\u2019m trying to do GO analysis using goseq but I get this error:\nScreenshot 2023-07-22 123611349\u00d7630 71.6 KB\n<<Warning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: paral>>\nas input I had this:\nScreenshot 2023-07-22 124256384\u00d7580 72 KB\nScreenshot 2023-07-22 124140327\u00d7529 33.7 KB\nTo add more details :\nDetails\nExecution resulted in the following messages:\nFatal error: An undefined error occured, please check your input carefully and contact your administrator.\nTool generated the following standard error:\nWarning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: parallel\nAttaching package: \u2018BiocGenerics\u2019\nThe following objects are masked from \u2018package:parallel\u2019:\nclusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\nclusterExport, clusterMap, parApply, parCapply, parLapply,\nparLapplyLB, parRapply, parSapply, parSapplyLB\n\nThe following objects are masked from \u2018package:dplyr\u2019:\ncombine, intersect, setdiff, union\n\nThe following objects are masked from \u2018package:stats\u2019:\nIQR, mad, sd, var, xtabs\n\nThe following objects are masked from \u2018package:base\u2019:\nanyDuplicated, append, as.data.frame, basename, cbind, colMeans,\ncolnames, colSums, dirname, do.call, duplicated, eval, evalq,\nFilter, Find, get, grep, grepl, intersect, is.unsorted, lapply,\nlengths, Map, mapply, match, mget, order, paste, pmax, pmax.int,\npmin, pmin.int, Position, rank, rbind, Reduce, rowMeans, rownames,\nrowSums, sapply, setdiff, sort, table, tapply, union, unique,\nunsplit, which, which.max, which.min\n\nLoading required package: Biobase\nWelcome to Bioconductor\nVignettes contain introductory material; view with\n'browseVignettes()'. To cite Bioconductor, see\n'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\nLoading required package: IRanges\nLoading required package: S4Vectors\nAttaching package: \u2018S4Vectors\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\nfirst, rename\n\nThe following object is masked from \u2018package:base\u2019:\nexpand.grid\n\nAttaching package: \u2018IRanges\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\ncollapse, desc, slice\n\nAttaching package: \u2018AnnotationDbi\u2019\nThe following object is masked from \u2018package:dplyr\u2019:\nselect\n\nUsing manually entered categories.\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: goseq \u2192 reversemapping \u2192 [ \u2192 [.table \u2192 NextMethod\nAny help would be much appreciated.\nedit: from what I noticed, the order of the genes in the two inputs(Gene IDs and differential expression, and Gene IDs and length) is different , could this be the problem? and if yes, how can I edit them so both inputs have the same order of genes? edit2: I saw a video on YouTube where the genes on the two inputs had different order and it still worked"}, {"role": "system", "text": "Hi @dimitris\nThe error message posted implies some technical problems. But it could just be the data, so troubleshoot that first. If none of the help below works out, you could try contacting the admins of that server. It wasn\u2019t clear which you are working at.\n\n\n\n dimitris:\n\nedit: from what I noticed, the order of the genes in the two inputs(Gene IDs and differential expression, and Gene IDs and length) is different , could this be the problem? and if yes, how can I edit them so both inputs have the same order of genes? edit2: I saw a video on YouTube where the genes on the two inputs had different order and it still worked\n\n\nYes, sort the inputs in the same order or that can cause problems. How to is in this tutorial Data Manipulation Olympics\nTechnically\u2026 all genes in the first file (true/false) need to be in the second file (lengths), and genes in both need to be in the same sort order. That second file can contain extra lines (will be ignored).\nI\u2019m not sure how others did it differently, and I\u2019m wondering if they got proper results or are missing some of the output! But it doesn\u2019t matter, try this way. Sorting files is quick and simple.\nPrior Q&A about goseq: Search results for 'goseq' - Galaxy Community Help\nTutorials that incorporate goseq:  Search Tutorials\nHope that helps!"}, {"role": "user", "text": "Thank you for your response.\nI did it again and this time everything was in the same sort order , however I still got the same error.\nI have no idea what causes the error cause I followed every step from this tutorial : Reference-based RNA-Seq data analysis\nIs it possible because my names of genes are in the form of ENSMUSG00000001.1 and not ENSMUSG00000001, to cause the error? I can not think of something else. Pls let me know if you can think of another reason that can cause this error.\nThanks!"}, {"role": "system", "text": "\n\n\n dimitris:\n\nIs it possible because my names of genes are in the form of ENSMUSG00000001.1 and not ENSMUSG00000001, to cause the error? I can not think of something else. Pls let me know if you can think of another reason that can cause this error.\n\n\nHi @dimitris\nYou could also remove the headers from all files \u2013 maybe try that first? The length file had a header in your original screenshot. You\u2019d need to remove that before sorting so maybe it is gone already?\nThen you could try stripping out the versions to see what happens but I don\u2019t think that is needed either.  But, I could be wrong! Be good to get that clarified, so let us know what happens. The tool form and the tutorial both have the headers removed. And, this is an R tool, so \u201csimple\u201d values tend to be better interpreted. If confirmed, we can add some more help comments in both places."}, {"role": "user", "text": "Hey again and thank you for the reply ,\nI removed the header, sorted in an order as you can see below:\n\n\nbut I still got the same error.\nwhile in this YouTube video ( at 2:04:00) GTN Training - Transcriptomics - Reference-based RNA-seq - YouTube , you can see that goseq analysis works, even if she used the header but also didnt do the sorting step.\nI am using galaxy.embl.de and is not listed under \u201cAvailable at these Galaxies\u201d that the tutorial has, so maybe this explains the error"}, {"role": "system", "text": "\n\n\n dimitris:\n\nI am using galaxy.embl.de and is not listed under \u201cAvailable at these Galaxies\u201d that the tutorial has, so maybe this explains the error\n\n\nYes, maybe, if the installed tool has some configuration problem or is running an older version of the tool wrapper. You could also try running this same data through at a supported server to compare.\nBut first \u2013 I would try removing the .N version from the two files next. Maybe the dot is causing a mismatch. After removing the version, confirm that all lines have a unique gene.\nYou could also do a check to make sure all the genes in the first file are actually in the second file."}], [{"role": "user", "text": "Hi,\nI have tried to use the Download Client to download an authorized dataset from EGA. I set up my EGA login and password in the Galaxy Preferences menu. I tried to use DESeq2 tool but in the \u201ccount file\u201d section I read \"(unavailable) EGA Download Client: authorized datasets. Why?\nThanks\nBarbara"}, {"role": "system", "text": "Hi @Barbara_Mandriani,\nhave you uploaded the count table into Galaxy? If yes, click at the name of this dataset in Galaxy history and check the datatype (format: text_string_to_check). Does it say tabular? If the datatyep (format) is anything but tabular, e.g., \u2018data\u2019 or \u2018txt\u2019, change it to tabular by clicking at Edit Attributes (pencil icon) > Datatypes tab in the middle window > select \u2018tubular\u2019 in the pull-down menu.\nJust in case: you can import data from a remote site into Galaxy directly using URL. Copy the URL on the remote site, usually with right mouse click > copy YRL or link, and paste it into Galaxy Upload menu, Paste/Fetch data section.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear Galaxy community,\nI am trying to build database for Mycobacterium tuberculosis H37Rv using GFF annotation. I tried to use .fna and .gff files from ncbi but am getting following error:Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/539/24539611/_job_tmp -Xmx7g  -Xms256m\nException in thread \u201cmain\u201d java.lang.ArrayIndexOutOfBoundsException: 1\nat org.snpeff.SnpEff.parseArgs(SnpEff.java:947)\nat org.snpeff.SnpEff.cm\nIs this something related to my computer or am I doing something wrong?\nI also tried .fasta and .gff3 files but came back with the same result.\nAlternatively, I tried to snpEff download and received following error message: Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/539/24539793/_job_tmp -Xmx7g  -Xms256m\n00:00:00\tSnpEff version SnpEff 4.3t (build 2017-11-24 10:18), by Pablo Cingolani\n00:00:00\tCommand: \u2018download\u2019\n00:00:00\tReading configuration fil\nI wonder if anyone can help me get through this.\nThank you"}, {"role": "system", "text": "Hi Sanjay\nNice to meet you. It sounds like there might be a problem with the data that you are trying to analyse.\nFirstly when it comes to M. tuberculosis, the snpEff database to use is Mycobacterium_tuberculosis_h37rv. This corresponds to the NCBI NC000962.3 genome (i.e. H37Rv). You can either download this (using the snpEff download tool) and use it from your history or just use the text string in your snpEff.\nI don\u2019t know what you are using for variant calling, but if you use snippy with Genbank reference input (i.e. the one from here https://www.ncbi.nlm.nih.gov/nuccore/NC_000962.3) it will run snpEff on your VCF for you automatically. This works for me with the 4.3.6+galaxy2 snippy tool."}], [{"role": "user", "text": "hello, I performed the differential analysis using the following pipeline, trimmomatic \u2192 kallisto quant \u2192 deseq . I get the volcano plot on the screen below using the volcano plot tool with Significance threshold = 0.05 .\n\nthere is a lot of genes at the top,  what\u2019s wrong with it ?  if it\u2019s normal what\u2019s the interpretation of it ?"}, {"role": "system", "text": "Hi @azzahrae,\ncould you try to sort the DESEQ result file according to the p-values and share a screenshot of it? I think that the plot is  fine, just those genes at the top are those whose p-value is extremely low.\nRegards"}, {"role": "user", "text": "hi @gallardoalba, the attached field is the result of deseq without sort, there is some problem in the galaxy server I can\u2019t do the sorting task.\ndeseq2 result"}, {"role": "system", "text": "Hi @azzahrae,\nit seems that some genes show such a low p-value that is considered as 0 (because the smallest floating-point value in R is 2.225074e-308, so anything smaller is rounded to 0), and the  -log of a number tending to 0 is infinite.\nRegards"}, {"role": "user", "text": "thank you, what we can say about these genes as an interpretation?"}, {"role": "system", "text": "According to the DESeq2 results, you could hypothesize that those genes are the most relevant from a biological point of view (the ones whose differential expression is more significant from a statistical point of view). But anyway, I suggest you consider in the downstream steps all those genes whose Adjusted p-value is lower than 0.05.\nRegards"}], [{"role": "user", "text": "Hi,\nI was trying to run count.seqs on my data but got this error:\n\nUntitled748\u00d7354 11.7 KB\n\nWhat does this error mean? How do I fix this?\nThank you."}, {"role": "system", "text": "Hi @cp1,\nthis information is not enough for inferring the problem; could you share your history with me? My email is \nRegards"}, {"role": "user", "text": "Hi Gallardo,\nYes, here is the history:\nhttps://usegalaxy.org/u/cp1/h/v6-amplicons\nThank you for looking into it!"}, {"role": "system", "text": "Hi @cp1,\nsince you are using make.groups for generating the groups file, it is necessary to collapse the FASTA collection into a single file. Here you can see what I mean: test history.\nRegards"}], [{"role": "user", "text": "Hello guys, please excuse my english by advance but its not my main language.\nSo Im trying to \u201cinvoke\u201d or \u201crun\u201d a tool of my galaxy instance using the bioblend package. I\u2019ve already tried this with a very simple tool without any problem but now im trying with a more complex tool : NCBI-blastp\nHere is the error I got:\n\nfrom the python code:\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.12.192236\u00d71020 233 KB\n\nHere im trying to get a query fasta file and a big database fasta from differents history. (everything has been uploaded successfully before), it seems to work with the query but not the database\u2026 im using those weird method \u201cset param\u201d, but maybe should I try it from another way?\nHere is the config XML file of my tool:\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.10.451922\u00d71428 428 KB\n\nedit : here is a detailed error report\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.46.431952\u00d71454 325 KB\n\nIts very strange, Subject database/sequences is locked on \u201cdb\u201d, but I\u2019ve set the param on \u201cfile\u201d\u2026\nI\u2019ve try everything :\u2019( do someone already did this with success ?"}, {"role": "system", "text": "Welcome!\nCan you check myinputs.set_param(name=\"database\", value=' ') ? It\u2019s the only one without a value;\nIn the detailed error report, CommandLine has empty value for parameter -db and Tool Standard Error is telling you need a name for you BLAST database.\n\n\nRead me\nPS: ton anglais est bon\n"}, {"role": "user", "text": "Hi, thx. That \u201cset_param(name=\u201cdatabase\u201d, value=\u2019 ')\u201d was added by me but it doesn\u2019t do anything. When I delete this line the same error appear.\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 20.22.271252\u00d7854 148 KB\n\nThe final tool input looks like this :\n{\u2018query\u2019: {\u2018src\u2019: \u2018hda\u2019, \u2018id\u2019: \u2018a7db2fac67043c7e\u2019}, \u2018db_opts_selector\u2019: \u2018file\u2019, \u2018subject\u2019: {\u2018src\u2019: \u2018hda\u2019, \u2018id\u2019: \u2018b472e2eb553fa0d1\u2019}, \u2018blast_type\u2019: \u2018blastp\u2019}\nthe database come from another history than the query_file, can this cause a problem ?"}, {"role": "system", "text": "\n\n\n Hudson_5:\n\nthe database come from another history than the query_file, can this cause a problem\n\n\nI guess it makes sense.\nDid you test with a db in the same history?"}, {"role": "user", "text": "Hello, I just test with the same history but still nothing.\nI think I\u2019ve found the origin of the problem, when there is a \u201cselected\u201d tag on \u201cTrue\u201d in an \u201coption\u201d tag, the param is stuck on that option.  The same problem appear here with the \u201cout_format\u201d param which is stuck on \u201cext\u201d option.  The question is, can we change that with the api (without modifying the xml file) ?\n\n\n"}, {"role": "user", "text": "Hello, I finally did it by invoking a workflow with the tool inside instead of a tool alone, hope it will help someone in the future \n\n\nCapture d\u2019\u00e9cran 2021-05-31 \u00e0 14.55.381048\u00d7298 31 KB\n\n\nCapture d\u2019\u00e9cran 2021-05-31 \u00e0 14.50.04279\u00d7531 24.8 KB\n"}], [{"role": "user", "text": "Hello all, I am new to ChIPseq and I\u2019m having problems with Diffbind analysis. My control (WT) and treated (3xTG) samples was not assigned properly for differential analysis. Under the replicate column, Diffbind calls WT and 3xTG as replicate 1,2,3,4 instead of replicate 1,2 for both conditions (as shown in the attached picture 1). In addition, under the caller column, it shows as raw instead of bed. I have included narrowpeak bed file called from MACs as well as the BAM files (shown in picture 2).\n\nPicture 1742\u00d7645 12.3 KB\n\n\nPicture 21154\u00d7686 34.2 KB\n\nI would really appreciate if anyone could show me how to arrange my samples so that the output would be WT (replicate 1 and 2) and 3xTG (replicate 1 and 2). Thank you!"}, {"role": "system", "text": "Welcome, @Zhen_Kai_Ngian\nThe column set for the score column is not appropriate for files from MACS2. Expand your dataset to find the correct column, probably c5, unless you manipulated the files?\nThe Galaxy version of the tool requires sample replicates.\nQuote from the tool form down in the help section, sometimes missed.\n\nNote this DiffBind tool requires a minimum of four samples (two groups with two replicates each).\n\nIf you want to do \u201cdata exploration\u201d with only two samples, that might be possible with the command-line version of this tool. But check before you invest time in that \u2013 some of the other tools by the same authors had changes that require the use of replicates now, too, at the source (so impact command-line versions). All Bioconductor tools wrapped for Galaxy have always required replicates. Three replicates per condition is the minimum ideal for scientific reasons but two is enough to satisfy the technical requirements. Bioconductor Forum\n\n\n\n Zhen_Kai_Ngian:\n\nMy control (WT) and treated (3xTG) samples was not assigned properly for differential analysis. Under the replicate column, Diffbind calls WT and 3xTG as replicate 1,2,3,4 instead of replicate 1,2 for both conditions (as shown in the attached picture 1). In addition, under the caller column, it shows as raw instead of bed. I have included narrowpeak bed file called from MACs as well as the BAM files (shown in picture 2).\n\n\nAll of this is due to how the underlying tool is creating data structures in R. Your screenshot of the input design from the Dataset Details view explains how the data is logically input to this tool, and is correct. The issue is with missing replicates \u2013 in this view that would show up as two datasets in each section, and is the same as how you would input the data on the tool form.\nThe order per group matters for the peak-bam pairings but don\u2019t worry about that yet. If you get a job failure once inputting replicates, you can read about this in the help section and reorder the datasets in the history (copy dataset > new history, done in the order that you want the tool to use them). Or, bundle and process the samples within collections, and this \u201corder\u201d will work out automatically and not be as tedious. The good news: this is only tool with that \u201cdataset order in the history\u201d wrinkle and it is because of the pairing of the inputs. There wasn\u2019t another good way to model the data back when the wrapper was written. You job didn\u2019t have replicates yet so the pairings were fine.\nResources\n\nhttps://training.galaxyproject.org/training-material/search?query=collections\n\nhttps://training.galaxyproject.org/training-material/search?query=ChIP-seq \u2013 note this tool is not specifically covered but other related tools are and the usage is similar\nSearch results for 'diffbind order:likes' - Galaxy Community Help\n"}], [{"role": "user", "text": "Hello everyone,\nI\u2019m using galaxy to trim and align data from targeted  DNA-seq. Using BWA-MEM I\u2019ve seen that between built-in genomes there is not the hg38 canonical you can find in bowtie2.\nHow is it possible to use that genome version to assembly my data?\nThank you in advantage.,\nG."}, {"role": "system", "text": "usegalaxy.eu does have hg38 as a built-in rerefernce genome for bwa-mem. Just start typing hg in the select box and you should find it."}, {"role": "user", "text": "Yes there is a hg38 but it is not the same as the \u201chg38 Canonical\u201d I\u2019ve already use with bowtie2.\nOr I\u2019m wrong and effectively is that the same as the one I can find in BWA-MEM?\nThanks!"}, {"role": "system", "text": "Ah, yes, you\u2019re right (sorry for not reading the question carefully the first time).\nhg38 should be identical to hg38 Full, but it\u2019s different from hg38 Canonical.\nSo, no, you cannot use hg38 Canonical as the reference in a bwa-mem alignment.\nThat said, you should also not do that anyway. Aligning against just canonical chromosomes can cause misalignments of reads that originate from non-canonical sequences, simply because there is no better match for them than a stretch of canonical sequence.\nThe better approach is to align against the full genome, then eliminate non-canonical mappings by filtering or by using the canonical genome during variant calling. Agreed, this can be tricky depending on downstream tools. Some variant callers, for example, may refuse to work with sequences mentioned in the input BAM header that are not found in the reference genome, in which case you would have to reheader your BAM dataset first.\nAll in all, my recommendation would be to rerun your bowtie2 jobs using the full hg38 version, not to look for solutions for making bwa-mem work with the canonical version.\nAt the same time, it\u2019s probably true that we should offer hg38 Canonical for bwa-mem if we do so for bowtie2 so thanks for bringing this up."}], [{"role": "user", "text": "Hi there,\nI am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is.\nHere the error:\n\nFatal error: Exit code 1 ()\nTool generated the following standard error:\ngetting samples and corresponding read numbers\nConverting input files\nbuilding bowtie index\nmapping mature sequences against index\nmapping read sequences against index\nMapping statistics\n#desc\ttotal\tmapped\tunmapped\t%mapped\t%unmapped\ntotal: 12382657\t343630\t12039027\t0.028\t0.972\nseq: 12382657\t343630\t12039027\t0.028\t0.972\nmapping star sequences against index\nanalyzing data\nExpressed miRNAs are written to expression_analyses/expression_analyses_galaxy/miRNA_expressed.csv\nnot expressed miRNAs are written to expression_analyses/expression_analyses_galaxy/miRNA_not_expressed.csv\nCreating miRBase.mrd file\nmake_html2.pl -q expression_analyses/expression_analyses_galaxy/miRBase.mrd -k dataset_21574970.dat -y galaxy  -o -i expression_analyses/expression_analyses_galaxy/dataset_21574970.dat_mapped.arf -j expression_analyses/expression_analyses_galaxy/dataset_21574968.dat_mapped.arf -l  -M miRNAs_expressed_all_samples_galaxy.csv\nmiRNAs_expressed_all_samples_galaxy.csv file with miRNA expression values\nparsing miRBase.mrd file finished\ncreating PDF files\n\nAny clues on what\u2019s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\nThanks\nLuca"}, {"role": "user", "text": "Found out, there were mismatches between fasta identifiers in the provided miRNAs (precursors, star and mature)"}], [{"role": "user", "text": "I am looking to find differentially expressed genes in the disease severity sample groupings within the cell type clusters (scRNAseq with scanpy) rather than across all clusters. Using two separate Strings in the groupby section yields an error, so how can this specific subgrouping be specified through a file?"}, {"role": "system", "text": "Hi @Kashish_Kumar\nDid you try use the file option for groupby instead yet? I think that that field directly on the form requires a single term. Or, it may require a dash between the values. Not sure but the tutorials here will probably help.\nhttps://training.galaxyproject.org/training-material/search2?query=scanpy"}, {"role": "user", "text": "Thank you for your response! Yes, I am unable to find the information on the format/parameters I should pass through the file groupby."}, {"role": "system", "text": "Hum, the file is probably one key/value per line.\nThe direct input seems to be comma separated.\nMany ways to slice up and label data, then to reference the same for calculations/plots, are covered in this specific tutorial: Clustering 3K PBMCs with Scanpy"}, {"role": "user", "text": "I recieved Keyerrors for both separating by a comma and the line-separated file. The tutorial does not specify a method for what I am aiming for. I will try to separate it based on groups, but I don\u2019t think this will provide cell-specific marker DE genes."}, {"role": "system", "text": "\n\n\n Kashish_Kumar:\n\ncell-specific marker DE genes\n\n\nhi @Kashish_Kumar\nThe tutorials are just examples of converting over methods from the tool developer \u2013 or some publication \u2013 into the Galaxy version of the tools. You can do this directly, too.\nMeaning, the underlying tools are the same, Galaxy just puts a GUI on top of it. Most functions are usually available, and if not for some reason, the help section will usually state why.\nThis sounds similar to what you are trying to do: Visualizing marker genes \u2014 Scanpy documentation"}, {"role": "user", "text": "Thank you, I have changed some of the \u201cAdvanced Settings\u201d inputs so the command running is the following. This might result in what I require. I need DE genes expression tables rather than plots.\nscanpy-find-markers --save diffexp.tsv --n-genes \u2018500\u2019 --groupby \u2018CoVID-19 severity\u2019 --method \u2018t-test_overestim_var\u2019 --use-raw  --groups \u2018celltype\u2019 --reference \u2018rest\u2019 --filter-params \u2018min_in_group_fraction:0.25,max_out_group_fraction:0.5,min_fold_change:2.0\u2019   --input-format \u2018anndata\u2019 input.h5   --show-obj stdout --output-format anndata output.h5"}], [{"role": "user", "text": "Hi, I\u2019m trying to run annotatemyid on my featurecounts table. I put as input ID type Entrez, but a error occurs .\nFatal error: Exit code 1 ()\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :\nline 2 did not have 8 elements\nCalls: read.table \u2192 scan\nCan anyone help me?\nThanks"}, {"role": "system", "text": "Hi - On the annotateMyIds tool form, try setting the option \u201cFile has header?\u201d to \u201cYes\u201d."}, {"role": "user", "text": "Tried but nothing, same error.This is the first part of my table. !\nfeaturecount.png421\u00d7513 7.7 KB\n"}, {"role": "system", "text": "Ok, thanks for trying that. I ran a few tests, at different servers and using different versions of the tool \u2013 all fail in a similar way. I\u2019m not sure why so will reply again once known.\nTo use the tool now, first run the tool Remove beginning of a file to remove the first line (header), then run the tool annotateMyIds. That worked in my tests, across servers and tool versions.\nAppreciate the follow up \u2013 this was unexpected!"}, {"role": "system", "text": "Update 11/16/2022\nThe indexes are restored at UseGalaxy.org. Prior failed runs should now complete successfully.\nThanks to all who reported the issue for their patience!\n\n\nUpdate: This was a bug. I opened a ticket for the correction here if interested: annotateMyIDs (several versions) is not really ignoring header line when option set \u00b7 Issue #2319 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\nUntil that is completed, continue to remove header lines from any inputs to this tool, no matter which server or tool version you may be using."}], [{"role": "user", "text": "Hello,\nI wonder if it is possible to use a .txt file as input parameter for instrument which accepts text field parameter. I want to use this option in a workflow: 1) get user\u2019s simple text input, 2) process this text with some instruments, 3) use the processed text as input parameter for the text field of other instrument. Can I do that?"}, {"role": "system", "text": "Hi @Alex_Shap\n\n\n\n Alex_Shap:\n\nI wonder if it is possible to use a .txt file as input parameter for instrument which accepts text field parameter.\n\n\nYes. Please see the functionality described in the tutorial below for the options.\nUsing Galaxy and Managing your Data \u2192 Using Workflow Parameters"}], [{"role": "user", "text": "Hi,\nI\u2019m on Galaxy Main trying to run a simple workflow that takes paired-end RNAseq reads and turns them into a count table via a process of cutadapt \u2192 mapping using RNA STAR \u2192 featureCounts, along with some quality control steps on the way. The workflow is designed to take two paired-end RNAseq reads and a gene annotation file and can be found here: https://usegalaxy.org/u/kevin-ma/w/paired-end-rnaseq-reads-to-counts-table\nWhenever I press \u201cRun Workflow\u201d like in the following screenshot, the button briefly changes to \u201cSending\u2026\u201d for a fraction of a second, and then absolutely nothing happens and the button returns to \u201cRun Workflow\u201d. Nothing appears in my history or my Workflow Invocations tab. Please let me know if more info would be helpful, and any help is appreciated, thank you!\n\nScreen Shot 2021-03-18 at 11.55.34 AM1792\u00d71350 293 KB\n"}, {"role": "system", "text": "Hi @Kevin_Ma,\nthis bug will be fixed in few hours. You will need to remove one of the tools and to add it again in order to run your workflow. Thanks for your report.\nRegards"}, {"role": "user", "text": "Thank you so much!"}, {"role": "user", "text": "I tried running it again and it seems that it\u2019s still not working, do you have any suggestions? I tried removing a tool and adding it back but it didn\u2019t seem to work."}, {"role": "system", "text": "Hi @Kevin_Ma,\ncould you try it again?"}], [{"role": "user", "text": "I have 24 untrimmed fastq files for RNA-seq. They are paired files. I have them in a data collection and run fastQC on them with no problem, but when I try to aggregate fastQC results with multiQC it only reports two files; forward and reverse, instead of each of the 24 individual files. What I\u2019m I doing wrong? I\u2019ve followed the tutorials and have set all parameters per those tutorials but keep getting the same result."}, {"role": "system", "text": "@stealsh\nI\u2019ve seen this before, too, for about the last 5-4 months when collections changed a bit in the 21.09 release. These two tools don\u2019t work in a series the way they used to.\nDetails: The problem comes from the way the data is organized and where the sample names are derived from when in a nested collection. They are named the same in the top level of the nested structure \u2013 \u2013 one \u201cforward\u201d and one \u201creverse\u201d. The actual sample names are one level deeper.\nI couldn\u2019t figure out how to solve it before, gave up, and no one else reported the issue. Will create the test again and ask others to review it. There is probably a solution, and it would probably involve organizing the collection differently. \u201cFlatten collection\u201d was one option I reviewed but that didn\u2019t produce the MultiQC output properly either (forward and reverse from the same sample had the same \u201cidentifier\u201d that MultiQC was interpreting instead, so again there was data loss from common naming). \u201cRename collections\u201d was problematic, too, but I forget why.\nIf there isn\u2019t a good workaround, will open up a ticket. For either case, expect another reply tomorrow with an update. The FastQC tool itself might need a change \u2013 or maybe MultiQC (although that tool is tricker to change).\nMeanwhile, one of these might work, and probably only the latter:\n\nExpand the collection and drag and drop the datasets from inside to the MultiQC tool input. This involves a LOT of clicking.\nOr \u2013 unhide the datasets in your history, then multi-select those for the input. I think this worked only when all forward were combined, then all reverse, but not together. Warning that this will make a lot of clutter in the history. Maybe copy just the FastQC output into a new different history and try it there, so any tests are easier to get rid of.\n\nThanks for reporting this! And @gbbio if you can think of a way to do this, feel free to add more to our replies. It is easily replicated: put any two pairs in a collection then run FastQC > MultiQC. MultiQC is only able to report back one pair, not both, no matter how the collection is arranged. I guess one option is to create some new collections just for input to MultiQC but that doesn\u2019t combine by sample ID. Maybe I missed something obvious that fresh eyes will find "}, {"role": "user", "text": "Hi,\nThanks for the detailed reply. Yes, I see now that all the files when paired were named either forward or reverse. I will try the work arounds later today, when I have some free time."}, {"role": "user", "text": "Update, I attempted both of the suggested work arounds but got the same result, multiQC only outputting forward and reverse."}, {"role": "system", "text": "Hi @stealsh\nOk, it was worth trying. These two specific tools won\u2019t work together for now when inputting the collection as a whole.\nThis will need a ticket, I\u2019ll get to it this week, and post that link back here for reference/tracking. I can\u2019t estimate how long it will take for the review and actual change to make it back to the server, so don\u2019t wait for that. The individual FastQC reports can be reviewed as an alternative for now.\nThanks for reporting the problem and so sorry there isn\u2019t some easier or immediate solution.\n\nUpdate\nLooks like this is a known issue still pending a correction: MultiQC - Use \"element_identifier\" as \"sample name\" for all tools \u00b7 Issue #1595 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub"}, {"role": "system", "text": "Hi @stealsh\ntry \u201cflatten collection\u201d from Collection Operation section on collection of paired reads before the FastQC step. Datasets in flatten collection have unique names.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all, I am working on galaxy and have a genome file in genbank format. To use featurecounts for my RNAseq, I need to convert the genbank format to a GTF format because that\u2019s the format the featurecounts tool in galaxy expects. Now, I tried to convert thse genbank to GFF3 using GenbaktoGFF3 tool in galaxy and managed to do so. Also I tried to use the gffread tool to convert the GFF3 file to GTF but this resulted in a GTF with no written lines and I can\u2019t understand why.\nIs there a way to directly convert genbank to GTF in galaxy (or even outside galaxy, but preferably in galaxy). Also, any idea why gffread is not working properly in my case? I am using default parameters (leaving everything as is) and only giving GTF as feature file output and track name as \u201cword\u201d.\nThanks,"}, {"role": "system", "text": "Hi @anubhavbasu,\ncould you share your history with me? I\u2019ll have a look at it."}, {"role": "user", "text": "Thanks, I have shared the history with you. The GFF3 file should be #481 BT_Degnan.GFF3. It was made from the genbank file #480 NC_000001.gb"}, {"role": "system", "text": "Hi @anubhavbasu,\nit seems that this problem has been fixed recently, but the new tool version isn\u2019t available in usegalaxy.org yet; I have requested to update it; meanwhile, I suggest yo  use usegalaxy.eu, since gffread works fine in that instance.\nRegards"}], [{"role": "user", "text": "Dear All,\nApologies to bother you again. Another Salmon quant question. While checking the output files, I noticed that the number of lines in the output files (\u201cGene quantification\u201d and \u201cQuantification\u201d) was inconsistent. For instance:\n\n\nI understand that \u201cGene quantification\u201d vs \u201cQuantification\u201d output files show different things: one the counts for genes while the other for transcripts. While that could possibly explain different numbers of rows, I am somewhat puzzled by higher numbers of lines in the \u201cGene quantification\u201d file - I would expect more transcripts (and their versions) than genes.\nAlso, some files have the same number of lines: ~190,000.\nSecond thing, when I was trying to set-up the pipeline for my analysis and ran DESeq2, the PCA spaced the datasets a bit weirdly: I expected them to group by test condition but they didn\u2019t. Of course, sample size was too small for PCA (only 4 datasets in total) but having noticed this difference in line numbers, I now began to worry that maybe I ended up incorrectly or partially mapping/aligning/referencing the datasets and/or maybe they are partially counted by Salmon and/or partially compared when running DESeq2.\nThird concerning thing was that my transcript-to-gene table I tried when testing the pipeline earlier had ~270,000 lines (probably because of transcript versions). I dropped that when DESeq2 did not work and switched to ensembl cDNA reference in .gtf format (~3,200,000 lines).\nProblem is, I did not notice this issue with differences in numbers of lines when running the pilots (they were all ~190,000 lines, irrespective of which dataset or output file it was). And I am sure I used the same parameters for all the datasets - pilot and the remaining ones.\nWould appreciate your opinion on whether this is a problem and if so, how you would recommend to solve it.\nThanks!"}, {"role": "user", "text": "Did a little bit of follow-up reading and found some threads that may be relevant:\nhttps://github.com/COMBINE-lab/salmon/issues/800\nhttps://github.com/COMBINE-lab/salmon/issues/214\nBoth suggest that Salmon discards duplicate transcripts. I checked the reports of the output files and there were some issues that seem to be similar:\n\nimage857\u00d7199 4.96 KB\n\n\nimage839\u00d7204 6.52 KB\n\nThe interesting thing is that the second thread also mentions some mismatches between the fasta and .gtf files. Is this something you have ever run into when doing your analyses?\nI am thinking of perhaps trying different mapping/alignment and count tools "}, {"role": "system", "text": "\n\n\n Egle:\n\nThe interesting thing is that the second thread also mentions some mismatches between the fasta and .gtf files. Is this something you have ever run into when doing your analyses?\n\n\nThis is likely your problem, too. And many posts at this forum are about format/content issues with either fasta, GTF, or both when used together. Tools are literal \u2013 any identifier that is compared between files needs to exactly match between all inputs.\nShort list of absolute requirements for Salmon \u2192 DESeq2\n\nTranscript fasta\n\n\ndid the file completely Upload to Galaxy? Truncated fasta is very hard to detect. So, just try to reproduce it by loading the file again and comparing\nhas correct formatting \u2013 try running it through the tool NormalizeFasta using the options to wrap at 80 bases and checking the box to remove \u201ceverything after the first whitespace\u201d\ndoes not contain any duplicated transcripts_id values \u2013 these are on the > title lines\nif you get the fasta from a data provider, all in one file, it might have problems the first and second will address but you won\u2019t need to worry about third\n\n\n\nGTF\n\nshould be from the same data provider where you sourced the fasta\nmake sure it is also completed Uploaded, and use \u201cauto-detect\u201d for the datatype. If Galaxy doesn\u2019t guess gtf there is a problem to fix\nthe most common problem is header lines # at the top. Remove the headers, then \u201credetect\u201d the datatype until Galaxy guesses gtf. If it won\u2019t be produced, then you don\u2019t have a GTF (maybe is GFF3 which will not work well?)\ninspect the identifiers in that file \u2013 does the transcript_id attribute exactly match those in the fasta (minus the >)?\ndoes the file contain gene_id attributes? are those different from the transcript_id attributes? if they are the same value, you need to find a GTF that has actual gene_id values.\n\n\n\nExecute all Salmon runs with the same fasta, along with the same GTF that you plan to use with DESeq2.\n\n\nThese FAQs cover the \u201chow-to\u201d. The UCSC hosted Ensembl genes track is probably what you want to use. Get the fasta from the Table Browser but get the GTF from their downloads area (only \u2013 not the TB).\n\n#working-with-very-large-fasta-datasets\n#working-with-gff-gft-gtf2-gff3-reference-annotation\n#detecting-the-datatype-file-format\n\nI also added some tags to your post that will find related prior topics here."}], [{"role": "user", "text": "Is it possible to use have workflow \u2018inputs\u2019 that allow multiple sample selection?\ne.g. First step of \u2018Workflow A\u2019 uses starSOLO, which allows selection of multiple lanes. Both R1 files (and both R2) are handled by a single run of the tool (rather than a batch job run twice).\n\nIf I set it up with \u2018input datasets\u2019 (below), upon running the pipeline I can select either a single input from a dropdown, or, hit \u2018multiple datasets\u2019 which creates a batch job. (\" This is a batch mode input field. Separate jobs will be triggered for each dataset selection.\"). Neither of which is what I want.\n\nIf, instead, I leave the inputs out as so:\n\nI get the \u2018ugly\u2019 full version of the inputs rather than the nice parameterised summary (its petty, but this matters to me, I\u2019m trying to make it nice for useability), but I can select the multiple files to run correctly.\n\nimage1110\u00d7786 128 KB\n\nHowever, that has knockon effects. My \u201cWorkflow A\u201d is essentially a subworkflow to be called by \u201cWorkflow B\u201d. In workflow B, I cannot connect any input to the R1/R2 inputs of workflow A, so when I launch it there is no box for this input. If I hit \u2018expland to full workflow form\u2019, right now* I can\u2019t see any further options to expland inputs for this subworkflow.\n\nimage1253\u00d775 11.9 KB\n\n\nI thought i had previously seen this to give the \u2018single dropdown\u2019 or batch selection options(so unable to run correctly), but that might have been when I had the subworkflow A set to take an \u2018input\u2019.\n\nI\u2019m a fairly new galaxy user, so understand I might not be coming at this from the right angle.\nWhat is the ~galaxy-way~ tm of structuring this kind of input?"}, {"role": "system", "text": "\n\n\n swbioinf:\n\nI\u2019m a fairly new galaxy user, so understand I might not be coming at this from the right angle.\nWhat is the ~galaxy-way~ tm of structuring this kind of input?\n\n\nUsing Dataset collections and possibly group tags is what you are looking for. Using Galaxy and Managing your Data.\nTry using a list collection with barcodes (R1), and another list collection with reads (R2) to solve the multiple input selection in the first vs second screenshot.\nAnd, I cross-posted this over to the IWC chat. They may reply here or there, and feel free to join the chat. You're invited to talk on Matrix"}, {"role": "user", "text": "Thank you, that may well be it!  Missed that option in starSOLO. Going to give it a try."}, {"role": "user", "text": "So progress. A collection input is definitely the way. I\u2019m now struggling with allowing a paired input collection as a workflow input.\nI have two pairs (lanes, one sample) of R1/R2 fastq files that I\u2019d like to input to one run of the StarSOLO tool. They are setup as a paired collection, and if I run that directly it works. Fantastic!\nSo I changed the input type in my workflow too. And then created a \u2018collection input\u2019 (from the inputs menu). By default, it wouldn\u2019t let me connect via noodle (fair, it needs a collection of pairs). So I changed the type in the collection to \u2018list:paired\u2019, and then I could noodle that into the starsolo input.\nBut if I configure this in my workflow as below, I get two independant starSOLO runs, one per lane.\nNot sure how I can specify that the whole input collection is to be sent to one run (as per manual running) rather than split to one-pair-per-run? Any thoughts?"}, {"role": "system", "text": "\n\n\n swbioinf:\n\nSo I changed the input type in my workflow too. And then created a \u2018collection input\u2019 (from the inputs menu). By default, it wouldn\u2019t let me connect via noodle (fair, it needs a collection of pairs). So I changed the type in the collection to \u2018list:paired\u2019, and then I could noodle that into the starsolo input.\n\n\nFor this part, when changing the input type, disconnecting all the downstream noodles, then reconnecting in the order of processing will \u201creset\u201d the workflow metadata. I\u2019m not sure if that was what you did. If not yet, try that first. It solves many workflow-runtime issues.\n\n\n\n swbioinf:\n\nBut if I configure this in my workflow as below, I get two independant starSOLO runs, one per lane.\n\n\nIf still a problem after reconnecting the noodles and trying a rerun, would you please post back a screenshot of the setting you want to use on the tool form? The one you want translated into the workflow? And maybe the section of that same tool form within your workflow (the side panel)? Sort of what you posted originally. These visually provide a lot of useful information \nI don\u2019t think this tool \u201cpools\u201d samples but let\u2019s make sure."}], [{"role": "user", "text": "On my workstation I use winzip to unpack these.  I hoped to unpack these with the online tools, but none of the ziptools recognized the fq.gz files.  How can I unpack these here on galaxy?  Or is it possible to analyze and assemble gz.files unpacked."}, {"role": "system", "text": "Yes, you can just work with fastq.gz datasets. If you really want to uncompress the data, you can click on the dataset\u2019s pencil icon, switch to the Convert tab and select the uncompressed format as Target datatype."}], [{"role": "user", "text": "I am trying to run Extract Genomic DNA on a bed file in which I have an ID, the start coordinate and the end coordinate. The sequences I am targeting represent products for potential primers (~180 of them total).\nThe Coordinate file set up:\nChrom            Start          End\nchr1000         184143     184871\nThe file I am matching it to is a set of ORFs that I extracted from a Blast search in the following format:\n\nchr1\tGTGCTCACCGCCGCCGCGNNNNNNN\u2026\nchr10     CTGGCTGCGGCGCACCTACGGCCTCACCGTCGC\u2026\nchr100   ATGGTGGCCACCGCTCCCCCGGCGAACCGGGCCAGGATGGCACAG\netc\u2026\n\nI have both files assigned to my custom build for the organism. When I try to run this, however, I get the error:\n181 warnings, 1st is: Unable to fetch the sequence from \u2018184143\u2019 to \u2018171\u2019 for chrom 'chr1000 '.\nSkipped 181 invalid lines, 1st is #1, \u201cchr1000 184143 184314\u201d\nI think I have made the formats of the chromosome names as identical as they get, but why will this still not function? I was wondering if it came to a matter of tabs vs spaces?\nAny ideas are helpful.Thanks!\nEDIT: I tried converting my bed file to tab delimited (just in case even though I saved it as tab-delimited) and tried again, and it still is saying all my lines are invalid."}, {"role": "system", "text": "I\u2019m sorry you\u2019re having this issue. Before going any further I\u2019d suggest seeing if the tool \u201cbedtools GetFastaBed\u201d would work instead. In most cases it can perform the same job as \u201cExtract Genomic DNA\u201d."}, {"role": "user", "text": "Thank you for the suggestion - I tried this and got the following (it showed up green at the end but still failed to extract any of my sequences):\nWARNING. chromosome (chr1000) was not found in the FASTA file. Skipping.\nI double checked the fasta file and it definitely is still there:\n\u201c>chr1000 ATAGCTCGCCCCGCCGGGCGGCCCGGGCGAGGGCTCCGGGGCCTCGTCGGTGAACGGCGTCGCCCCACCGGCCATCCGCTCCAGGGCCGGCCC\u2026\u201d\nDoes this program required different formatting?\nThanks"}, {"role": "system", "text": "Hi @aroebuck\nFew questions:\n\n\nDid you run the tool NormalizeFasta on your custom genome before using it, and before promoting it to a custom build?  If not try that first as a solution. You\u2019ll need to name the custom build with a different \u201cdatabase\u201d name (dbkey) \u2013 or delete the old one first.\n\n\nHow many sequences (\u201cchromosomes\u201d) are in your custom genome? If the fasta has 1000s of chromosomes, that might be problematic.\n\n\nCheck to make sure the bed file is formatted correctly.\n\n\nPlease confirm that you are working at usegalaxy.org. If somewhere else, please describe.\n\n\n\nFAQs: Galaxy Support\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\nFormat help for Tabular/BED/Interval Datasets\nCommon datatypes explained\n\nI also added some tags to your post that link to prior Q&A for custom genome/build troubleshooting.\nLet\u2019s start troubleshooting more from there."}, {"role": "user", "text": "Hi @jennaj, I did try normalizing, but I will try again. I am actually using an instance hosted on my own computer.  I am trying to extract sequences for primer design. I started with a single WGS, then obtained a series of unique ORFs from which I am trying to extract the sequences that will be putative targets for strain-specific primers.\nOriginally I tried using the initial genome (one long sequence) as my \u201ccustom build\u201d but that didn\u2019t work because (I am guessing) it isn\u2019t broken into chromosomes. I have since been trying to use the fasta file containing the unique ORFs as my \u201ccustom build\u201d since it has multiple sequences that can be treated as chromosomes. However, there are over 47 000 sequences in that file so could that be the issue?\nCould it also be an issue that the file I used to make the custom build is the same file as the reference genome? I\u2019m afraid I don\u2019t understand why there needs to be a build to begin with since I thought it would just match the headers in the reference genome and the coordinate file. I have checked the formatting of both files and I think they\u2019re both good (ran a replace spaces with tabs and condense multiple tabs to one).\nAny more advice you can give based on this would be amazing.\nThank you very much,\nAndrea"}, {"role": "system", "text": "Apologies for the delay @aroebuck Did you solve the problem?\nIf not, some more feedback:\n\n\n\n aroebuck:\n\nOriginally I tried using the initial genome (one long sequence) as my \u201ccustom build\u201d but that didn\u2019t work because (I am guessing) it isn\u2019t broken into chromosomes.\n\n\nThis would be a fasta containing one sequence. As long as the format is in fasta, that is fine (and actually quite common: bacteria, etc). My guess is that the bed content was not based on that sequence, which resulted in an error.\n\n\n\n aroebuck:\n\nHowever, there are over 47 000 sequences in that file so could that be the issue?\n\n\nIf the \u201creference genome\u201d fasta has that many \u201cchromosomes/sequences\u201d, you may run into problems (memory) with some tools, but I wouldn\u2019t expect this tool to have a problem.\nWhen using a custom genome with this tool, the expected inputs are:\n\n\nbed input with at least 3 columns. If you include more columns, they must follow the bed format.\n\nfasta input that has been run through NormalizeFasta.\nthe \u201cchromosome\u201d identifiers in the first column of the bed file are an exact match for the identifiers in the fasta file (minus the \u201c>\u201d at the start and no description content on the title line \u2013 just the identifier).\nBoth are selected on the tool form from the working history\u2019s set of datasets.\nSee the Datatype\u2019s FAQ in the original reply if you are not sure how to format the bed input.\n\nThe alternative tool is bedtools GetBedFasta, but if your bed and fasta are not a match, or the bed input has format problems, this tool will fail as well.\nMaybe try at a public Galaxy server then compare to your own server to eliminate technical issues on your local? That way you could also share the history (privately if you want) for more feedback. Or, capture some screenshots of the bed and fasta \u2013 some example data that you think should be matching up \u2013 and post those back.\nThanks!"}, {"role": "user", "text": "Thank you @jennaj for the suggestions. I was able to resolve the issue by the following:\n\n\nSplit my large file containing the ORFs of interest (~47 000 sequences) into separate files each containing 500 sequences each\n\n\nUsed one of the files produced from the above step to find putative primers and generate a bed file with the coordinates (Used Primer3).\n\n\nCreated a custom build on galaxy using the file (same one as selected for Step 2) that I used to find the primers. This same file was also used as the reference genome for the Extract Genomic DNA search.\n\n\nAssigned the bed file and the reference genome to the custom build.\n\n\nSuccess running Extract Genomic DNA.\n\n\nI actually didn\u2019t need to normalize the data following procedure, but that may have been happy coincidence.\nThanks again for the suggestions!\nAndrea"}], [{"role": "user", "text": "Hello,\nThe most strange thing happened. I aligned a sample of raw RNA-seq data with HISAT2 (default settings). Then I used the function [Filter SAM or BAM, output SAM or BAM files on FLAG MAPQ RG LN or by region] to cut a 20 kb region that I am interested in. I don\u2019t have a lot of storage space. So far so good. But when I opened it in IGV, there were more than 20 kb, way more. I could see the entire chromosome. I checked two more chromosomes and they were empty.\nThis happened with only one sample but I have 9 from the same batch that are processing right now.\nI have followed this protocol before and nothing like this happened. The only time when I could see more outside my region, it was solely because the ends of the exon were exceeding it. Which made sense.\nDo you have any opinions about it?\nBTW I checked and the region that I selected was the right one."}, {"role": "system", "text": "Isn\u2019t this tool producing a second output with your query turned into json format? This looks like a great opportunity to use this dataset and share its contents here."}, {"role": "user", "text": "No, it only produces a smaller BAM, unless I am missing something"}, {"role": "system", "text": "Please try modifying your regions term.\nCurrently is similar to:\n\nchrX: 1-1,000\n\nWhen should be written as a contiguous term, with no space after the :\n\nchrX:1-1,000\n\nor this (the inclusion of commas , is optional):\n\nchrX:1-1000\n\nWith the extra space included, the filter is malformed, and effectively ending up with an applied filter based on only the chromosome name (the first \u201cword\u201d in the term).\nTool form option and help:\n\nfilter-bam-select-regions.png1276\u00d7194 17.7 KB\n"}], [{"role": "user", "text": "Hi everyone\nI am trying to install locally galaxy, I am using conda 4.14.0 with ubuntu (20.04.4 LTS). The installation seems correct after running ./run.sh it seems everything went on fine, and after a while the installation freeze at \u2018\u2018solving enviroment\u2019\u2019, i left it like that for hours and nothing happened.\nI saw in other post that this problem was fixed using virtualenv:\nI installed virtualenv, desactivate conda, activate virtualenv, and the run the ./run.sh and\nI get this error\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [5 lines of output]\n      running egg_info\n      writing mercurial.egg-info/PKG-INFO\n      writing dependency_links to mercurial.egg-info/dependency_links.txt\n      writing top-level names to mercurial.egg-info/top_level.txt\n      Python headers are required to build Mercurial but weren't found in /usr/include/python3.8/Python.h\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n\nother try show this\nERROR: Could not install packages due to an OSError: [Errno 13] Permiso denegado: '/home/julia/galaxy/.venv/lib/python3.8/site-packages/zipstream'\nCheck the permissions.\n\n\n"}, {"role": "system", "text": "I think it has to do with your python installation. You may need to install sudo apt-get install python3-dev\nhttps://stackoverflow.com/questions/12870186/python-headers-are-required-to-build-mercurial"}, {"role": "user", "text": "Hi, thanks for your quick response.\nI had my python already installed, even though i run it again to show you the message I get when doing it with conda\n==> /home/julia/galaxy/database/gravity/log/celery-beat.log <==\n[2022-09-16 09:49:02,380: DEBUG/MainProcess] beat: Synchronizing schedule\u2026\n[2022-09-16 09:49:02,387: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n[2022-09-16 09:54:02,484: DEBUG/MainProcess] beat: Synchronizing schedule\u2026\n[2022-09-16 09:54:02,489: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n[2022-09-16 09:59:02,588: DEBUG/MainProcess] beat: Synchronizing schedule\u2026\n[2022-09-16 09:59:02,594: DEBUG/MainProcess] beat: Waking up in 5.00 minutes."}, {"role": "system", "text": "You may have python installed but next to that you also need python-dev. This contains some packages that mercurial needs, for example Python.h (https://www.cyberciti.biz/faq/debian-ubuntu-linux-python-h-file-not-found-error-solution/)\nWhat you are showing here does not look like errors. If something went wrong during installation with conda or venv it sometimes helps to remove the conda enironment or .venv folder. Maybe try to make a venv without using a conda environment first. Or just with conda alone, you only need to activate the base environment and let galaxy do the rest."}, {"role": "user", "text": "I tried to update conda, installed a few libraries and runned again the installation, but it has been loanding since sunday and after 48h it still running.\n\n==> /home/julia/galaxy22/galaxy/database/gravity/log/celery-beat.log <==\n[2022-09-27 11:37:47,332: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 11:37:47,334: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 11:38:03,147 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 11:38:04,149 INFO spawned: 'gunicorn' with pid 48973\n2022-09-27 11:38:19,180 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 11:42:47,433: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 11:42:47,433: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 11:43:09,687 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 11:43:10,689 INFO spawned: 'gunicorn' with pid 49577\n2022-09-27 11:43:25,854 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 11:47:47,532: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 11:47:47,533: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 11:48:17,367 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 11:48:18,369 INFO spawned: 'gunicorn' with pid 49924\n2022-09-27 11:48:33,477 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 11:52:47,632: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 11:52:47,634: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 11:53:24,093 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 11:53:25,095 INFO spawned: 'gunicorn' with pid 50411\n2022-09-27 11:53:40,820 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 11:57:48,496: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 11:57:48,499: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 11:58:31,454 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 11:58:32,459 INFO spawned: 'gunicorn' with pid 51090\n2022-09-27 11:58:48,337 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 12:02:48,599: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 12:02:48,600: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 12:03:38,865 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 12:03:39,867 INFO spawned: 'gunicorn' with pid 51311\n2022-09-27 12:03:55,623 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 12:07:48,699: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 12:07:48,700: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 12:08:45,134 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 12:08:46,136 INFO spawned: 'gunicorn' with pid 51811\n2022-09-27 12:09:02,007 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n[2022-09-27 12:12:48,799: DEBUG/MainProcess] beat: Synchronizing schedule...\n[2022-09-27 12:12:48,800: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n2022-09-27 12:13:51,510 INFO exited: gunicorn (exit status 1; not expected)\n2022-09-27 12:13:52,512 INFO spawned: 'gunicorn' with pid 52226\n2022-09-27 12:14:08,220 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 15 seconds (startsecs)\n\n\n\n"}, {"role": "system", "text": "First of all to install with conda you only have to activate your base environment and run galaxy.\nconda activate base\nsh run.sh\n\nIf something has gone wrong earlier it sometimes helps to starts over, and remove the automaticly created conda environment.\nconda remove --name _galaxy_ --all\n\nAnd/or remove the .venv folder\nrm -rf .venv\n\nAs far as I know you don\u2019t need additional packages if you do it like this. My earlier answer was specifically about a python/venv environment.\nThe log itself does not look necessarily wrong from on top of my head. How did you test if galaxy works or not?\nOn the following locations you could find logs that could help.\ngalaxy/galaxy.log\ngalaxy/database/gravity/supervisor/supervisord.log\ngalaxy/database/gravity/log\n\nIf you execute sh run.sh you will always see info lines on your terminal. If you want to start galaxy and run it in the background you need to do sh run.sh --daemon with the command sh run.sh --stop-daemon you can stop it again."}], [{"role": "user", "text": "Hi, I tried installing SLURM on my Ubuntu 22.04 server by following the tutorial from the Galaxy Training platform (Connecting Galaxy to a compute cluster) and ran into the following error during the installation:\n\nfatal: [192.168.123.108]: FAILED! => changed=false\nmsg: \u2018Failed to update apt cache: E:The repository \u2018\u2018Index of /natefoo/slurm-drmaa/ubuntu jammy Release\u2019\u2019 does not have a Release file., W:Updating from such a repository can\u2019\u2018t be done securely, and is therefore disabled by default., W:See apt-secure(8) manpage for repository creation and user configuration details., W:https://repos.influxdata.com/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details., W:https://packages.grafana.com/oss/deb/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u2019\n\nI was wondering if SLURM will be getting a release file for jammy and if so, if this is already under development. If not, are there any alternatives supported by Galaxy which could be used in a production environment?\nThanks in advance!"}, {"role": "user", "text": "Hi @hxr, sorry for bothering you, but would you by any chance happen to know any alternatives, since I doubt any new releases for compatibility with Ubuntu 22.04 are going to be released soon. Otherwise, I will just keep running the jobs one by one without connecting to a cluster and use local runners for now. Thanks in advance!"}, {"role": "system", "text": "Hi! Yes, apologies. I\u2019m filing the issue here: slurm-drmaa for Ubuntu Jammy \u00b7 Issue #73 \u00b7 natefoo/slurm-drmaa \u00b7 GitHub\nAlternatives:\n\nCompile slurm-drmaa yourself (quite some work)\nUse a different DRM (maybe less work but, completely different things to learn and there\u2019s no nice tutorial for it.)\n\nThe code for slurm-drmaa is all here, you might just be able to ./configure && make (assuming you have the dependencies) but it\u2019s probably non-trivial.\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - natefoo/slurm-drmaa: DRMAA for Slurm: Implementation of the DRMAA C...\n\n  DRMAA for Slurm: Implementation of the DRMAA C bindings for Slurm - GitHub - natefoo/slurm-drmaa: DRMAA for Slurm: Implementation of the DRMAA C bindings for Slurm\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "Sorry for the wait! A 22.04/jammy deb is now available in the PPA. Please give it a try and let me know if there are issues."}, {"role": "user", "text": "Sorry for the late reply and thank you for the update! I tried installing it and got the following error:\n\nfatal: [xxx.xxx.xxx.xxx]: FAILED! => changed=false\nmsg: \u2018Failed to update apt cache: W:Nate Coraor in Launchpad                                                                                         aa/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/e                                                                                         tc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details., W:h                                                                                         ttps://repos.influxdata.com/ubuntu/dists/jammy/InRelease: Key is stored in legac                                                                                         y trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt                                                                                         -key(8) for details., W:https://packages.grafana.com/oss/deb/dists/stable/InRele                                                                                         ase: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the                                                                                          DEPRECATION section in apt-key(8) for details., E:Repository \u2018\u2018https://packages                                                                                         .grafana.com/oss/deb stable InRelease\u2019\u2019 changed its \u2018\u2018Origin\u2019\u2019 value from \u2018\u2018graf                                                                                         ana stable\u2019\u2019 to \u2018\u2019. stable\u2019\u2018, E:Repository \u2018\u2018https://packages.grafana.com/oss/de                                                                                         b stable InRelease\u2019\u2019 changed its \u2018\u2018Label\u2019\u2019 value from \u2018\u2018grafana stable\u2019\u2019 to \u2018\u2019.                                                                                          stable\u2019\u2018\u2019\n\nThe error is caused by the deprecation of the APT key management utility, which I think is part of the Ubuntu 22.04 update. Would it be possible to add/change the variable in order to get rid of the error, or is it up to the user to move the apt-key?\nThanks in advance!\nMore about the APT-key deprecation:\n\nAPT-KEY(8)\nNAME\napt-key - Deprecated APT key management utility\n\u2026\nDEPRECATION\nExcept for using apt-key del in maintainer scripts, the use of apt-key is deprecated. This section shows how to replace existing use of apt-key.\n   If your existing use of apt-key add looks like this:\n\n   wget -qO- https://myrepo.example/myrepo.asc | sudo apt-key add -\n\n   Then you can directly replace this with (though note the recommendation below):\n\n   wget -qO- https://myrepo.example/myrepo.asc | sudo tee /etc/apt/trusted.gpg.d/myrepo.asc\n\n   Make sure to use the \"asc\" extension for ASCII armored keys and the \"gpg\" extension for the binary OpenPGP format (also known as \"GPG key public ring\"). The\n   binary OpenPGP format works for all apt versions, while the ASCII armored format works for apt version >= 1.4.\n\n   Recommended: Instead of placing keys into the /etc/apt/trusted.gpg.d directory, you can place them anywhere on your filesystem by using the Signed-By option in\n   your sources.list and pointing to the filename of the key. See sources.list(5) for details. Since APT 2.4, /etc/apt/keyrings is provided as the recommended\n   location for keys not managed by packages. When using a deb822-style sources.list, and with apt version >= 2.4, the Signed-By option can also be used to include\n   the full ASCII armored keyring directly in the sources.list without an additional file.\n\n"}, {"role": "user", "text": "Edit: I got rid of the error using the following command:\nsudo apt-key export 12788738 | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/slurm.gpg\n\nBut immediatly ran into a new error:\nfatal: [192.168.123.108]: FAILED! => changed=false\n  checksum: 7d99e1e5d268baa1c56acfee8616bdf4d4525ab4\n  msg: Destination directory /etc/slurm-llnl does not exist\n\nAny ideas on how to fix this error?"}, {"role": "system", "text": "This should be fixed as of version 1.0.2 of the galaxyproject.slurm role."}], [{"role": "user", "text": "Hi I have a similar problem. When I am adding a control Bam file to my sequence I am getting 0 narrow peaks and 0 bed. When I do not have a control I am getting the results. Could you please help me?"}, {"role": "system", "text": "Hi, @Manolis1\nDid you solve your problem? We have the problem too in our CUT&RUN analysis. We have 4 histone modification antibodies, and one IgG negative control in CUT&RUN. 3 histone data showed very good results. But this is no peaks after MACS2 in one histone data, even though we used all the same parameter.\nIf you solve your problem, could you please kindly let us know?\nThank you very much for your help!\nBest wishes!"}, {"role": "system", "text": "Hi @gallardoalba\nDid you solve this problem from @Manolis1 ?\nWe have the same problem now. Could you please show us how to solve it?\nThank you very much for your help!\nBest wishes!"}, {"role": "system", "text": "Hi @nourmahfel and @Heystone\nI reviewed a shared history from @nourmahfel yesterday.\nThe immediate problem seemed to be that the mapped read results in the upstream BAM datasets were filtered with a very low mapq threshold (10). That can result in many multi-mapped reads remaining which can lead to no/low resulting significant peaks found in treatments when compared against the background. I suggested refiltering with a mapq of 20, then 30, and comparing the all results after to see what happens.\nThe GTN tutorials cover BAM filtering by mapQ values, see: Search Tutorials. Start with the first tutorial for a technical overview of where this value is located in BAM/SAM datasets. The others will cover practical items with context. This including other steps you might want to incorporate to clean up ChiP-seq data (eg \u201cMark duplicates\u201d). That site can be searched with keywords or navigated by analysis domain.\nThis FAQ covers the most common reasons for technical issues in Galaxy (and bioinformatics analysis generally): Understanding input error messages (or odd putatively successful \u201cgreen\u201d results as not all tools fail due to technical problems).\nThis wasn\u2019t the problem @nourmahfel was having (technically, the inputs were fine), but you should review it quickly @Heystone to eliminate anything simple to fix. Examples: what does FastQC report about the read quality and does that data need more QA or was the applied QA too aggressive? Is all the data mapped against the same exact reference genome (mismatches happen!)? All samples were mapped using the same tool? Is the alignment rate different between the samples, and if so, was that expected or explained by read quality differences?\nIf your data looks fine technically, then you are having a scientific content problem that might be remedied with changes to your methods. Or the data content itself might be the problem (no actual significant difference in the treatment versus the background). Consider examining regions in the BAM files in a genome browser (IGV, UCSC) where you expected peaks to be called.\nYour own search here (directly, or use the tags I added) will find more advice at this forum. In short: eliminate technical issues first, then explore your data content and tune analysis methods if needed.\n\n\nMore ways to get help with this type of protocol\nGTN Tutorials (query = \u201cChIP-seq\u201d): Search Tutorials\nMACS2 has a google forum \u2013 a search should find it. Other bioinformatics help sites will have more advice. MACS2 has all been around for some time.\nAnd this is a good blog post that covers more about what mapQ values represent, plus a bit about how that varies across common alignment tools.\n\nIf you are still stuck after doing those checks, share back a link to your history and we can review.\n\nTroubleshooting errors\nWhat information should I include when reporting a problem?\n\nThanks!"}, {"role": "system", "text": "Hi @Heystone\nUnfortunately, my problem remains unsolved. I haven`t found yet a way to solve it.\nIn case I will find something, I will post it. Would you please do the same if you find a solution as well?\nGreetings and wishes for a great day."}, {"role": "user", "text": "Hi Jennifer,\nI have had Mapq of 30 before but changed it according to the bioinformatician\u2019s recommendation at uni. I didn\u2019t get any results with Mapq 30 either. I tried mapping it using BWA and bowtie and it did not work either. I have tried different parameters as well and different q-value thresholds. I have obtained the data from SRA and I am not sure what the problem could be with the data. Could you please help to identify the underlying issue with the data?\nThank you,\nKind Regards,\nNour"}, {"role": "system", "text": "This comment makes me think that the person who reviewed the data thought it was too sparse to call peaks, so they suggested less aggressive filtering. That is certainly something to try, but all of this is a balance. If you tried something like filtering only the control at 30 (strict), and treatments at 10 (permissive), that may even produce results, but the scientific quality of the result would be questionable, and in a different way than not including a control at all (which already did produce questionable results for one of you).\n\n\n\n nourmahfel:\n\nI have had Mapq of 30 before but changed it according to the bioinformatician\u2019s recommendation at uni. I didn\u2019t get any results with Mapq 30 either.\n\n\nIf there is a suspected data content problem, these are the items to examine:\n\nRun FastQC on the reads before and after other QA is applied (trimming, etc) and review << if the data doesn\u2019t appear to have sufficient quality it may not be usable\nReview the BAM mapping rates << this is likely where the problem first starts to impact the analysis, I\u2019m guessing low coverage\nReview the BAM alignments in a genome browser (details in original post + consider adding in an annotation GTF \u2013 UCSC has several for each mouse build in Galaxy)\nTry different filtering methods: mapQ is not the only item, getting rid of optical duplicates is also important (also in original post, plus see protocol advice at other forums \u2013 one example Did you remove ChIP-seq duplicates)\nUse the most current version of the genome build for the species (mm10, not mm9)\nAnd, sometimes using a mapper like BLASTN+, with a subset of reads as a query against a target database that contains reads from many species (WGS) can give some information: are the reads really from the species you think they were originally from? Is there contamination of some type?\n\nOther items like checking that the control and treatment samples are not mixed up is part of this.\nGet the processed reads from archives like SRA, and not the submitted reads, unless you understand how to manipulate the submitted reads into the current Illumina format that most tools know how to interpret. The processed fastq reads from SRA are already in the right format (fastqsanger). You can test this by allowing Galaxy to guess the format and also by running FastQC after the data is loaded (the format is reported).\nNot all public datasets are high quality, and that can impact the ability to use reads for analysis. Reads that you created yourself or obtained from a collaborator could also have quality/usability problems. This is a judgement call on your part, and the help above and in the original post are the ways to explore data for quality/performance with tools leading up to the problems with MACS2.\nSmall note: The usegalaxy.org server is undergoing maintenance. I wouldn\u2019t expect the problem in the history I reviewed to be impacted \u2013 but this can double checked by running the same analysis at a different usegalaxy.* server: usegalaxy.eu or usegalaxy.org.au."}], [{"role": "user", "text": "I wat to view my dataset on UCSC browser. I know I am supposed to expand the dataset to see the \u201cDisplay at UCSC browser main\u201d option. But this option isn\u2019t showing up at all. Please what might be the reason for this?\nI have attached an image of the expanded dataset.\nThanks\n"}, {"role": "system", "text": "Hi @lordtsage\nclick at Visualize icon (bar chart) to see links to UCSC browser and IGV.\nKind regards,\nIgor"}, {"role": "user", "text": "Thanks Igor,\nI did this and found that the option to view on UCSC browser is available only for datasets uploaded from the browser. The other datasets I operated on do not throw up this option. Any idea what might be causing this?"}, {"role": "system", "text": "Hi @lordtsage\nDo you have dbkey (database value) assigned to datasets without link to UCSC browser? On the screenshot in your original post it is hg19. Galaxy uses dbkey value for communication with remote servers such as UCSC Genome Browser. A bit of background: when reads are mapped/aligned to a built-in genome, Galaxy assigns a proper dbkey to output files like BAM alignments, and the value propagate through downstream analysis. When a user uploads data, for example, intervals in BED format, the database (build) has to be specified manually either during upload or later via Edit Attributes (pencil icon) > select a proper value from Database/Build pull-down menu.\nHope this helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you so much Igor. You have been most helpful. I still have a little challenge, if you don\u2019t mind. I have attached an expanded picture of the exon dataset which I got from UCSC database (BED format), and the operated dataset (tabular format). You can see that they both have the same database(hg18), which is as it should be. Do you think this problem is related to the difference in format? I can\u2019t really know why this is acting up.\n\nimage1040\u00d7642 65.8 KB\n\n\nimage1052\u00d7529 54 KB\n"}, {"role": "system", "text": "Hi @lordtsage\nhere the issue is in format: tabular does not work with the browser. It is too generic. Galaxy handles datasets based on labels (datatypes) assigned to files. For example, a BED file with txt, tabular and bed datatypes is treated differently: there is no columns in txt file, tabular is not compatible with the browser, hence no UCSC GB and IGV links.\nBased on content on the first four columns, dataset #7 contains information about intervals. If it is BED, change the format (datatype) from tabular to bed: Edit attributes (pencil icon) > Switch to Datatype tab in the middle window and select an appropriate datatype in the pull-down menu.\nJust in case: start positions in BED are zero based. Make sure you compensate for this in generic data manipulation.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI have an existing dataset from nanopore run in fast5 format. I\u2019m using GALAXY to convert it into fastq format. The tools which I\u2019ve trialled from the tools list are : 1) extract fastq in tabular format from a set of fast5 files and 2) extract read in fasta or fastq format from nanopore files.\nI got either 1 line or 0 bytes from these runs,no output.\nDoes anyone has any idea why? or which step did i miss or input the wrong thing?\nThank you."}, {"role": "system", "text": "Hi @jas\nThis error indicates that a tar archive was loaded and used for the input. For many cases when a tar archive is uploaded, only the first file in the archive is loaded. That first file can be an index, which won\u2019t contain any reads.\nThese tools are working at UseGalaxy.org using the tool\u2019s test data (raw data URL link). Review to see if/how your input dataset differs and make adjustments as needed. The input was directly loaded by URL with the datatype \u201cautodetected\u201d. Galaxy | Accessible History | nanopore tar upload test\nIf you get odd results again after adjusting the input (as needed), or if you cannot figure out what adjustments are even needed, please write back and specify:\n\nWhich public Galaxy server you are working at (server URL)? The topic was posted under the UseGalaxy.org category, but sometimes that is misassigned.\n\nOr, if using a different type of Galaxy deployment, please describe. Galaxy version, how/where sourced and installed, and the last date it was updated.\nUpdate your own Galaxy if needed. There are point-updates between full releases \u2013 and those are usually important to capture (tend to be bug fixes important enough to include before the next full release) Releases \u2014 Galaxy Project 20.09 documentation. Please be aware that the next full release is expected to be published in the next week or so (if all goes as planned!). The pre-release 21.01 is already installed at UseGalaxy.org for integration testing \u2013 but don\u2019t install that version yet on your own server.\nHow and from where tools were installed also matters. The most current version should be sourced from the Galaxy Main ToolShed https://toolshed.g2.bx.psu.edu/. Install directly from the Admin area of the Galaxy application and use the built-in dependency resolution function for the simplest way to make sure the install is complete.\n\n\nWhat is the \u201cdatatype\u201d currently assigned to the dataset when the tools failed to produce output?\nWas that \u201cdatatype\u201d autodetected during Upload? Or did you assign it directly? What datatype is assigned if you try to \u201cdetect datatype\u201d under the function: pencil icon > Edit attributes > Datatypes? Does a rerun work with that datatype assigned (if it differs)?\n\nLet\u2019s start troubleshooting more from there . We may ask for you to generate a share link to the history containing the work so we can review it in more detail and provide the best feedback. That can be posted into a branched direct message (we\u2019ll start one up \u2013 as you won\u2019t be able to yet since are new to this forum) \u2013 unless you are Ok with posting the share link publically in a reply to this topic.\nThanks!"}], [{"role": "user", "text": "Hello!\nI have local Galaxy installed. Also I used Data Manager for creating indexes.\n\nGalaxy - 19.05\nHISAT2 - 2.1.0+galaxy5\n\nI have just tried to run Hisat2 on usegalaxy.org with one of my files and it run successfully.\nI uploaded this file to galaxy:\n\nhttp://212.150.245.226/~mali/Galaxy/RNA_seq/a1.fastq\n\nThen  I run HISAT2 using this file as input (single end) fastq and hg38 as index , that\u2019s all, very simple\nIn usegalaxy.org it works, in local galaxy it runs forever\u2026\nI don\u2019t have any error, just eternal job. This only one thing that I can found this message:\n\nCould not display BAM file, error was:\nfile has no sequences defined (mode=\u2018rb\u2019) - is it SAM/BAM format? Consider opening with check_sq=False\n\nAlso I try the same run in console and notice the same behavior."}, {"role": "system", "text": "Welcome @artem\nIt seems that your computer is running out of resources when running the mapping job locally.\nTools use about the same resources whether run line-command or in Galaxy. The hg38 database is very large, and your input fastq data may be large, too.\nMake sure that you set up your local properly to work with large data. Instructions/tutorials for configuration can be found at:\n\n\nhttps://getgalaxy.org \u2013 then follow the links to the basics https://galaxyproject.org/admin/get-galaxy/#configure-for-production\n\n\nhttps://github.com/galaxyproject/dagobah-training \u2013 largest/current overview tutorials, but there are other current branches (gcc-2019) https://github.com/galaxyproject/dagobah-training/tree/2019-pennstate\n\n\nI\u2019ll also ping the developers at Gitter to see if they can tell what else might be going wrong. Feel free to join in the conversation. They may reply here or there: https://gitter.im/galaxyproject/Lobby?at=5daa1be5e3646f24c74d65ea\nThanks!"}, {"role": "user", "text": "@jennaj\nThanks for your answer!\nRight now I use AWS instance t3.2xlarge (8 CPU and 32Gb RAM), HISAT2 job has been started more then 4 hours ago, but any result. Also don\u2019t see any errors or etc.\nMaybe I missed something, I\u2019m a technical specialist, and unfortunately I do not understand the intricacies of Galaxy.\nThanks!"}, {"role": "user", "text": "I have modified file \u201cjob_conf.xml\u201d, add \u201clocal_slots\u201d option. And noticing, that hisat2 running with \u201c-p 8\u201d option, but only one processor is utilizing and not use RAM.\nI have waiting 24h, but HISAT2 job not finished."}, {"role": "system", "text": "Thanks for the clarification about your Galaxy, @artem\nSo you have tested in both a GVL cloud Galaxy instance and a 19.05 local version of Galaxy, same problem (job errors)? But the same job works at the public Galaxy server https://usegalaxy.org?\nOne of these could be going wrong:\n\n\nThe genome was not fully indexed. Any genome should have Data Managers run in this order: Indexing reference genomes with Data Managers: Resources, tutorials, troubleshooting. The cloud version of Galaxy would have this genome already indexed. In a local Galaxy, you\u2019ll need to do the indexing.\n\n\nYour data is in fastqsanger format and the first read has an exact human genome match (hg19, hg38), so the problem doesn\u2019t seem to be with either of those \u2013 unless you manually assigned the datatype to be fastqsanger.gz (compressed fastq). When the assigned datatype is a mismatch for the actual data content, errors are produced.\n\nTry running FastQC to see if it was fully uploaded. If that tool fails, the dataset was likely truncated upon Upload. Loading again is usually how to fix that type of problem. It doesn\u2019t seem like that original dataset is truncated, if it worked at usegalaxy.org. Be sure to use \u201cautodetect\u201d for the datatype assignment.\n\n\n\nFAQs: https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nCommon datatypes explained\n\nThanks!"}], [{"role": "user", "text": "\nimage348\u00d7740 27.9 KB\n\nThe uploader on the website is not taking in any files. Can anyone point me to the issue or help me why it is not working.\nThanks!"}, {"role": "system", "text": "It works fine in my testing. Please describe your issue in more detail."}, {"role": "system", "text": "Hi @bdhakal\nI can now reproduce the Upload problem/delay with local file browsing at UseGalaxy.org. And, someone else reported it here: usegalaxy.org is super slow: 11/8/2022 Confirmed job delays, please leave queued jobs queued - #5 by jennaj\nMore feedback soon, we are looking into what might be going on.\nThanks for reporting the issue!"}, {"role": "user", "text": "Hi @jennaj @marten\nI should have explained the issue better. But looks like @jennaj found the issue I was having. I was unable to upload any file using local file browsing option. I have seen the issue since the last few days.\nThe uploader does not even start uploading the data. it just get stalled.\nThanks!"}, {"role": "system", "text": "When I upload the file I also have this same issue, however after a while I get this error:\n\"An error occurred with this dataset:\nformat data database ?\nUnable to run this job due to a cluster error, please retry it later\"\nI am unsure if it is similar to your issue, but I thought it was worth clarifying my above comment.\nThank you!"}, {"role": "system", "text": "Update 11/09/2022\nThe Upload issues are now resolved. Anyone that had an issue similar to the other descriptions, please try running your jobs again.\nThanks to all for the feedback!\n\n\nHi @cmpetit and @bdhakal\nThanks for the extra information! This is exactly what I found, too. We will be looking into this in a few hours US time tomorrow. Updates will post back to this thread until resolved.\n\nUpload (local file browsing)\nIt looks like this at first using the Upload tool. Stalls for hours\u2026 I haven\u2019t had an error result yet, but it will probably end up that way the same as yours did.\n\nupload-tool-stalls2016\u00d71276 230 KB\n\nRemote file loading (SRA example)\nThis did already fail with a server-side error.\n\nremote-file-errors574\u00d71208 63.3 KB\n"}], [{"role": "user", "text": "Hello!\nI have to fill parameters manually into my workflow which are mostly file names and sample names. I have to select from about 7 files about 20 times and enter similar but slightly different sample names (e. g. wn0552.A7CE75785, wn0552.A7CE75785_tumor, wn0552.A7CE75785_normal) for another 20 times. As you may guess, it is time consuming, error prone and in general not exactly the kind of work i dreamed of.\nBut i suspect i can automate this process. If it were linux command line, all i need were echo, cat, ln etc., and some rarely used ascii symbols. However in galaxy i found only cat.\nSo essentially i want:\n\n\nTool that receives one file and sends it to the output intact so i could connect its output to all steps using some specific file and set its name only once. In linux it may be cat or cp, but i would prefer ln or even variable definition to save disk space and time.\n\n\nTool that receives some string and output it intact like echo.\n\n\nTool that receives two string and concatenates them like echo $a$b . So i could set my sample name to e. g. wn0552.A7CE75785, derive other important strings from it and use these strings in all my steps using \u201cadd connection to module\u201d.\n\n\nAnd also tool that gives the number of processors on current machine to set the number of threads in some tools.\n\n\nIs this possible in galaxy? Or maybe there is an easier way to automate workflows? Or i have to write wrappers for all these tools myself?\nThanks in advance."}, {"role": "system", "text": "Please check out Using Workflow Parameters, I think it covers all of this, minus the last part. If tools can use multiple threads or cores they can consume $GALAXY_SLOTS, which is set as appropriate based on the resources the admin has configured."}], [{"role": "user", "text": "Hello!\nIs it possible to tell galaxy not to delete temporary tool working directory?\nThanks in advance."}, {"role": "system", "text": "Have a look at this\nhttps://docs.galaxyproject.org/en/master/admin/options.html?highlight=amqp_internal_connection#cleanup-job"}], [{"role": "user", "text": "Hello guys,\nI am running EGSEA, and it was working fine until I started to run into errors.\nI have the file in tabular format.\n\n\n\n\nGeneid\nFFSSCLCR3\nFFSSCLCR2\nFFSSCLCR1\niPSC1\niPSC2\niPSC3\n\n\n\n\n2018\n2867.300115\n4384.063805\n3697.321171\n0\n0\n0\n\n\n196047\n2139.832269\n3254.124667\n2938.558938\n0\n0\n3.544768899\n\n\n27063\n6174.085968\n2953.349501\n6630.032231\n0\n7.378716416\n5.317153348\n\n\n\nAnd have a factor as tabular format\n\n\n\n\nSample\nCell Type\n\n\n\n\nFFSSCLCR3\nFFSSCLC\n\n\nFFSSCLCR2\nFFSSCLC\n\n\nFFSSCLCR1\nFFSSCLC\n\n\niPSC1\niPSC\n\n\niPSC2\niPSC\n\n\niPSC3\niPSC\n\n\n\nand my annotation as tabular as well\n\n\n\n\nENTREZID\nSYMBOL\n\n\n\n\n100287102\nDDX11L1\n\n\n653635\nWASH7P\n\n\n102466751\nMIR6859-1\n\n\n100302278\nMIR1302-2\n\n\n645520\nFAM138A\n\n\n79501\nOR4F5\n\n\n729737\nLOC729737\n\n\n102725121\nDDX11L17\n\n\n\nBut\u2026 when I tried to run EGSEA, it says\ngroupGOTerms: \tGOBPTerm, GOMFTerm, GOCCTerm environments built.\nEGSEA analysis has started\nLog fold changes are estimated using limma package \u2026\nlimma DE analysis is carried out \u2026\nEGSEA is running on the provided data and h collection\n\u2026globaltestcamera.ora*\nEGSEA is running on the provided data and c5 collection\n\u2026cameraglobaltestora*\nEGSEA is running on the provided data and gsdbgo collection\n\u2026globaltestcamera.ora*\nEGSEA is running on the provided data and kegg collection\n\u2026globaltestcamera.ora*\nEGSEA analysis took 12.34 seconds.\nEGSEA analysis has completed\nEGSEA HTML report is being generated \u2026\nReport pages and figures are being generated for the h collection \u2026\nHeat maps are being generated for top-ranked gene sets\nbased on logFC \u2026\nError in plotHeatMapsLogFC(gene.sets = gsets.top, fc = logFC, limma.tops = limma.tops,  :\nAll featureIDs in the gs.annot list should map to\na valid gene symbol\nCalls: egsea.cnt \u2192 egsea\nand tried to troubleshoot, but I gave up after three days of trying\u2026\ncan anyone help me to figure out what I am doing wrong here?\nThank you in advance"}, {"role": "user", "text": "Hello guys,\nCould it be because I am using hg38?\nShould I try with hg19 and run EGSEA again?\nI tried to remove all \u201cNA,\u201d \u201cLOC###,\u201d and \u2018XXX-AS\u2019 genes from my list. It still doesn\u2019t run, and I am running out of idea how to make it run.\nThank you,"}, {"role": "system", "text": "I am experiencing same problem. Did you manage to sort it?"}, {"role": "system", "text": "@Filip_Filipsky @incho\nThe problem is reported in the error message here:\n\n\n\n incho:\n\nError in plotHeatMapsLogFC(gene.sets = gsets.top, fc = logFC, limma.tops = limma.tops, :\nAll featureIDs in the gs.annot list should map to\na valid gene symbol\n\n\nTool form instructions:\n\nSymbols Mapping file\nA file containing the Gene Symbol for each Entrez Gene ID. The first column must be the Entrez Gene IDs and the second column must be the Gene Symbols. It is used for the heatmap visualization. The number of rows should match that of the Counts Matrix.\n\nChecking that both datasets contain the same number of rows is a basic check, but you could also compare the IDs and make sure they are a match. You might also need to adjust the header lines in the files \u2013 these deviate from the example data.\nSome tools have built-in expectations for how the data are labeled in headers that the Galaxy wrapper around the tool cannot auto-adjust. This can lead to spurious error messages. So, I\u2019d recommend following the same header labeling as in the examples just to eliminate that from being a problem, too. The sample names can differ of course, but columns of data representing gene/symbol information could have standardized labels in the first row.\nUsing the most current version of any tool is also usually important, to capture bug fixes and wrapper enhancements, and to make further troubleshooting easier. If you are not working at a usegalaxy.* server, you might want to try the tool at one of those as a comparison. UseGalaxy.org and UseGalaxy.eu are the best choices \u2013 UseGalaxy.org.au usually follows the same server (but not cluster) configuration as UseGalaxy.eu so testing/comparing between those is not normally needed.\nIf the tool still fails after those adjustments are made/confirmed, please confirm:\n\nThe URL of the usegalaxy.* server where you tested.\nThe complete tool name and version \u2013 find this info at the top of the tool form.\n\nLet\u2019s start there. We may ask for a history share link to help more, and that can be posted back if you are not concerned about keeping the data private, or let us know if you do want to keep it private and a moderator will start a private message thread for sharing. If there is some bug with the tool, we can help to sort that out versus a usage issue."}], [{"role": "user", "text": "Hi, I launched a few assembly analyses with Unicycler on Monday and they are still queued (in grey).\nThe ones I did last week took no more than 2 hours, so I\u2019m starting to wonder if there is an issue here or a maintenance that prevents the analyses to keep going?\nOf note, I have been running other analyses (with QUAST) and they completed in minutes.\nThanks\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nTo confirm, you are working at Galaxy Main https://usegalaxy.org?\nWe have had recent reports of delays for both Trinity and Unicycler now at this server. More feedback about that soon.\nMeanwhile, please leave queued jobs queued. If you delete and rerun, the job will move back to the end of the queue, extending wait time.\nThanks for reporting the problem."}, {"role": "user", "text": "\n\n\n jennaj:\n\nTo confirm, you are working at Galaxy Main https://usegalaxy.org?\n\n\nYes I am. Everything else but Unicycler seems to work, it\u2019s been 3 days now.\nI won\u2019t delete anything, Thanks"}, {"role": "system", "text": "Update:\nThanks for the clarification. The jobs were delayed by a server-side issue that was just now corrected.\nPlease do allow queued jobs to remain queued and they will eventually process."}, {"role": "user", "text": "It seems that all the jobs with Unicycler are now finished, except most of them failed (in red).\nDo you think it is a real failure, or I should launch again the ones who failed, in case this is related to the delay in being processed?\nThanks"}, {"role": "system", "text": "@vanina.guernier Please try a rerun. For any job failure, at least one rerun is recommended to eliminate cluster issues.\nIf those fail, then check the error messages and the inputs. There might be a problem that could be fixed, or the data could be too large to execute on the public server.\nThis FAQ helps with identifying and addressing the most common data input problems that lead to tool errors:\n\nMy job ended with an error. What can I do?\n\nMore comprehensive help, including links to GTN tutorials, is here. Unicycler is covered in a GTN tutorial under the topic \u201cAssembly\u201d.\n  \n    \n    \n    Troubleshooting resources for errors or unexpected results usegalaxy.org support\n  \n  \n    Check to see if there is a known usegalaxy.* server issue. \n\nServer Status: https://status.galaxyproject.org/\n\n\nIf the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\n  \n\n"}], [{"role": "user", "text": "Greetings!\nI want to study ribosomal RNA expression and potential processing sites in bacteria. To such effect, I want to perform an RNA-Seq experiment in order to analyze potential rRNA transcripts isoforms and processing sites.\nConventional RNA-Seq experiments are not designed for the analysis of rRNA expression and generally include a RNA depletion step, which reduces rRNA signal. Notice rRNA in bacteria accounts for ~90% of total RNA.\nIn the context of studying rRNA processing, would the RNA-Seq approach be appropriate? If so, what rRNA treatment would you recommend to study rRNA expression levels and processing?\"\nThank you in advance."}, {"role": "system", "text": "Hi @Fernando,\nthe mir Vana  miRNA Isolation Kit has demonstrated been useful for isolating rRNA. In order to study expression levels and processing probably Nanopore sequencing is the best alternative, since it allows accurate isoform quantification and eliminates PCR bias by using direct RNA sequencing."}, {"role": "user", "text": "Hi there! I was thinking of skipping the rRNA depletion step, so nearly 90% of my RNA will be rRNA. Maybe the mir Vana miRNA Isolation Kit is more useful when the ribosomal portion of total RNA is smaller (e.g. humans)?\nThank you about the Nanopore suggestion, will definitely have a look at it"}, {"role": "system", "text": "Hi @Fernando,\naccording to different publications , such as RNA Preservation Agents and Nucleic Acid Extraction Method Bias Perceived Bacterial Community Composition, this protocol should work fine with bacterial samples.\nRegards"}, {"role": "user", "text": "Sorry @gallardoalba, I see no why should this method work for the question I posed. Why not using samples without any rRNA isolation? I mean, if rRNA accounts for ~90% of total RNA then rRNA purification would  not make such a big difference"}, {"role": "system", "text": "Sorry @Fernando, I didn\u2019t express it properly; the cited protocol can be used for extracting total RNA, not just rRNA. Any protocol for RNA extraction will be fine."}], [{"role": "user", "text": "Jennaj,\nI am running into problems with the Unique.seq tool in Galaxy.  I use Galaxy online to teach a course and we use the 16S Metagenomics (Mouse) dataset as the training module. I have been using this approach for a couple of years now, but we have run into a Fatal Error when running the first step of the Unique.seq tool.  We have tried to run Unique.seqs from multiple accounts (students and mine), but always using Galaxy online. I have tried it from our computers at work (faster internet) and from home. Again, over 400 students in the class have done this over the years and we have not encountered the problem until now. Any suggestions? Belwo is the error messages we get from Unique.seq.  There is an error for each of the three output files from Unique.seqs.\nAn error occurred with this dataset:\nformat mothur.names database ?\ne[He[J"}, {"role": "system", "text": "Hi @Ouverney\nPlease send in a bug report from one of the errors, leaving all inputs and outputs undeleted and we can try to help. Please also share the workflow you are using, and shared history link where the same was done successfully before if at all possible \u2013 both links can go into the comments of the bug report. If you didn\u2019t generate the log for the failed run, please run one more time after checking that box and leave it also undeleted in that same history.\nAnother person reported a similar problem, and this was the solution. Maybe check to see if that helps first?\n\nThe tool is expecting another input, the second on the form, the label is: \u201cname file or count table\u201d.\nThe output from Screen.seqs can be used.\n\nThe final option is to try using a workflow from the tutorials. You can adjust parameters to match your data and check for differences between your workflow and tutorial\u2019s most current workflow.\n\ntraining-material/search?query=mothur\n"}, {"role": "user", "text": "Jennaj,\nHere is the history in the USA Galaxy where the problem with Unique.seq happened:\nurl: https://usegalaxy.org/u/ouverney/h/tutorial091222\nJust today I ran the same set of data (the Galaxy mouse tutorial for 16S metagenomics) in the Australian Galaxy and Unique.seqs worked.\nAnd here is the History for that test in the Australian Galaxy:\nurl: https://usegalaxy.org.au/u/cleber/h/tetsting-uniqueseq-aus\nI will check the suggestions you listed in your email once I get a break. Thanks for any help you can provide.\nI appreciate it."}, {"role": "system", "text": "Great, thanks for sharing the links.\n\nThe version of the tool used at the AU server is: 1.39.5.0\n\nThe version of the tool used at the ORG server is: 1.36.1.0, but the updated version 1.39.5.0 is also available. Would you please retry using it?\nIf you are incorporating a workflow, maybe that needs to be updated, too? The older version of any of these Mothur tools might have technical problems or otherwise not work well together with mixed versions (some had known bugs, others were replaced with updates re dependencies). The most current of each is the best choice.\n"}, {"role": "user", "text": "Thanks. I will try the ORG server once I get a chance.\nYes, I am teaching a class that was created during the pandemic (summer 2020).\nI noticed there is an \u201cextended\u201d version of that module.\nIf I understand you correctly, I should update my course to include the updates from this new extended version?"}, {"role": "system", "text": "Updates are routinely applied at the training site. See the \u201clast updated\u201d information at the top of every tutorial. For these two, only the extended version has had changes applied since 2020."}, {"role": "system", "text": "Update April, 2023\nThe tool has been adjusted. Please rerun prior technical failures.\nThanks to everyone who reported the issue, and to all that helped fix it \n\n\nUpdate:\nThe Unique.seqs tool has a bug at UseGalaxy.org (only).\nDetails are here Mothur Unique.seqs -- fails when an optional input is not provided at ORG \u00b7 Issue #5163 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\nThe choices are to use UseGalaxy.eu instead, or to supply the optional input until the fix is live on the other server.\nThanks for reporting the problem!"}], [{"role": "user", "text": "Hi,\nI have started managing my own Galaxy instance using a virtual machine (virtualbox, not docker). I would like to add an administrator and a user, but I cannot use any SMTP server. How can I register those two users?\nBest."}, {"role": "system", "text": "Hi @SamGG,\nto add an administrator you need to add user\u2019s email to Galaxy\u2019s configuration file (usually config/galaxy.yaml) and restart Galaxy - see details at https://galaxyproject.org/admin/#setup-admin-user\nIdeally this user already exists in Galaxy, if it does not you can either create it through the UI or if you want to use the commandline as you mentioned you can make an API call (see details here). This is easier when you use some API library like Galaxy bioblend."}, {"role": "user", "text": "Hi @marten,\nI read the links you gave and I think they definitively answer the question. You answer leads me to think that what I need is very dummy. I installed Galaxy by cloning the Github repo. I run it without any problem. But I can\u2019t remember if I had to setup an admin account at some step of the installation. Could you point me to the documentation step I forgot? Alternatively, you can join me offline.\nThanks."}, {"role": "system", "text": "Galaxy does not require to have an admin user, so during the first startup it just initialized empty Galaxy without any users. All users are subsequently registered (usually using the interface) and any number of them can be turned into admins by adding their associated email to the configuration file.\nDoes that help?"}, {"role": "user", "text": "Thanks. My memory is not that bad. In the current case you clearly summarized, how could I add my first user without SMTP server?"}, {"role": "system", "text": "SMTP server has nothing to do with user registration, you do not need it. You can go to the URL where your Galaxy is running and click on \u2018Register\u2019 in the top menu."}], [{"role": "user", "text": "Hi,\nThe FATSQC job has been waiting to run for a while and I am not sure why. I have added the link to my history below.\nurl: https://usegalaxy.org/u/nourmahfel/h/chip-seqfinal"}, {"role": "system", "text": "Same thing is happening for me, maybe it is something on their end."}, {"role": "system", "text": "Same for me, I have 4 workflows in queue that seem to be stalled.  Started them yesterday.  Is there any way of knowing if galaxy offline?  (Sorry, haven\u2019t had this issue before.)"}, {"role": "system", "text": "Same for me, both FastQC and bowtie2. Nothing has run for hours"}, {"role": "system", "text": "I am experiencing the same problem. Jobs are queued up for almost 24 hours waiting to be submitted."}, {"role": "system", "text": "\nMy jobs aren\u2019t Running!\nUnderstanding job states\nWill my jobs keep running?\n\n\nThe most effective strategy is to queue up analysis jobs, then to let them complete.\n\nPublic servers are shared resources. Jobs will run when resources become available.\nQueue and execution time depends on the tool and how busy the server is.\nPlease avoid deleting and rerunning jobs in hopes that they will run quicker. It has the opposite result: the new job is added back at the end of the queue extending wait time.\nA wait of a few hours to a few days can be normal for computationally expensive tools.\n\n Tool forms and workflows include options to send yourself notifications by email.\n\n\nDataset Collections and Workflows are worth learning about.\n\n\nMost Galaxy Training Network (GTN) tutorials include at least one associated Workflow to get your started.\n\n\nNot sure how to use, create, import, or customize Collections and Workflows?\n\nUsing Galaxy and Managing your Data\n\nA collection of microtutorials explaining various features of the Galaxy user interface and manipulating data within Galaxy.\n\n"}], [{"role": "user", "text": "I have a problem while performing a Deseq2 in galaxy web.\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in read.table(file = file, header = header, sep = sep, quote = quote,  :\nno lines available in input\nCalls: get_deseq_dataset \u2192 lapply \u2192 lapply \u2192 FUN \u2192 read.table\n\nScreenshot_2022-10-31-16-04-00-06720\u00d71105 121 KB\n\n\nScreenshot_2022-10-31-16-05-18-67720\u00d7902 125 KB\n"}, {"role": "system", "text": "Hi @Sohailul_Hannan\nThis error\n\n\n\n Sohailul_Hannan:\n\nError in read.table(file = file, header = header, sep = sep, quote = quote, :\nno lines available in input\n\n\nusually means that one or more of the count input datasets has no content (is empty). Please check for that situation first. If none are empty, then try at least one rerun to eliminate a transient cluster issue from being a factor.\nIf a rerun fails and the inputs look Ok, you can generate a share link to your history and post that back publicly for review. Or, you can ask for a moderator to start up a private message chat to share it privately instead. How to generate the link: sharing-your-history"}, {"role": "user", "text": "Thank you for your assist.\nAfter checking my count input datasets i found that my control 2 and control 4 has no line (empty).\nWhy they were empty i don\u2019t understand.\nMay I proceed to next Deseq2 step without selecting Control 2&4  as input?\n\n"}, {"role": "system", "text": "\n\n\n Sohailul_Hannan:\n\nWhy they were empty i don\u2019t understand.\nMay I proceed to next Deseq2 step without selecting Control 2&4 as input?\n\n\nYou should find out why there are no results from the upstream tool. Maybe it can be fixed.\nWhether to drop datasets from the analysis is a scientific decision. Just keep in mind that this tool technically requires at least two factor levels and at least two count files per factor level. With a minimum of three per factor level considered best to generate meaningful results."}], [{"role": "user", "text": "Hello,\nWe are trying to analyze our files following this RNA Seq tutorial: https://galaxyproject.org/tutorials/nt_rnaseq/\nOur DESeq2 outputs, however, are giving us MSTRG gene IDs, which are not very useful as the labels are only relevant internally. In researching how to convert MSTRG to a gene name, we found that a better \u201creference file to guide assembly\u201d should be used to run StringTie so a gene name is outputted. However, the galaxy StringTie job fails when we use the Homo_sapiens_NCBI_GrCh38.tar.gz reference file. We have tried unzipping it to .tar and also using a DEXSeq annotation as the reference file, then ran StringTie, but this also failed. Also, the option to use a built in reference file is disabled. Currently, the only method that successfully outputs a DESeq2 file is when no reference file is used, however we are unsure how to convert the MSTRG gene ID it outputs to a gene name/prevent MSTRG outputs.\nAny help is greatly appreciated; thanks in advance!\n-Preformatted textAnanya"}, {"role": "system", "text": "Hi Ananya_P,\nWere you able to resolve this issue? What worked for you? We are having the same issue currently.\nBest,\nJohnathon"}, {"role": "user", "text": "Hi Johnathon.\nMy apologies for the late reply. Unfortunately, we have not been able to resolve this issue. Please let me know if your team has found any helpful solutions.\nThanks,\nAnanya"}, {"role": "system", "text": "Hello @Ananya_P @Johnathon_Anderson\nSorry the original question got missed.\nThis FAQ explains how/where to get a human reference annotation dataset that will work with these tools: https://galaxyproject.org/support/diff-expression/\nI also added some tags to your post that includes other Q&A about this. In short, you need a GTF dataset that matches the UCSC version of the genome/build (if you are mapping against the built-in genome indexes). Correct format matters or tools will fail. The FAQ and linked FAQs explain with full details and common sources.\nGRCh38 is the same genome as UCSC\u2019s hg38.\nGRCh37 is the same genome as UCSC\u2019s hg19.\nBut the \u201cbuild\u201d may differ between sources. Check your chromosome identifiers and make sure they are a match.\nThanks!"}], [{"role": "user", "text": "gives an error message: Uncaught exception in exposed API method"}, {"role": "system", "text": "Hi @stanislove\nWhat is the server URL?"}, {"role": "user", "text": "https://usegalaxy.org/login"}, {"role": "system", "text": "Update\nThe account registration in function has been restored at UseGalaxy.org.\nFor everyone reading, please see the banner on the server and let us know about any other odd behavior.\n\nGalaxy has recently been updated to a pre-release of the next Galaxy stable release (22.05), and you might encounter problems as we iron out the wrinkles. Please report any issues you encounter using the bug icon on error (red) datasets, or email galaxy-bugs.\n\n\nThanks.\nHave you tried again yet? If not, please do.\nThe server was undergoing maintenance and was just updated again later today to the 22.05 pre-release. This function is working from tests.\nIf you have more problems, please explain a bit more about your situation.\nCorrection \u2013 registration is failing with this error at UseGalaxy.org. We are looking into the issue. Updates will post back here.\nFor now, you can work at UseGalaxy.eu and/or UseGalaxy.org.au. Having one account at each public server is common and grants access to distinct resources.\nThanks!"}], [{"role": "user", "text": "Hi,\nAfter updating my (Ubuntu 22.04) server, I suddenly get the following error when running the ansible-playbook galaxy.yml command:\n\nTASK [galaxyproject.nginx : Set additional config options] ***********************************************************************************************************\nfatal: [192.168.123.108]: FAILED! => changed=false\n  checksum: 3cb6cfac4f721ecabef8f03be6774942e82eeb0f\n  msg: Destination directory /etc/nginx/conf.d does not exist\n\n\nI never got this error before the update. After checking the /etc/nginx directory, I found out that the directory was completely gone. I ran the ansible-galaxy install requirements.yml command again just in case, but this ofcourse didn\u2019t change anything. I am stuck not knowing where to go from here. Any help is appreciated!"}, {"role": "user", "text": "The error was ultimately fixed by running the following commands as root (sudo su):\napt-get purge nginx nginx-common nginx-full\n\nFollowed by:\nansible-galaxy install requirements.yml\n\nAnd finally:\napt-get install nginx\n\nIt is still unknown what caused the error, but this is what fixed it."}], [{"role": "user", "text": "Hi all,\nI would like to create an authentication system with username and password (without storing the email of the user, for security reason). So I just send by mail the username and password to the user that contact me to create the account (but i don\u2019t store the email itself). Is there a way?"}, {"role": "system", "text": "The email field of a user record is required and must be unique, but is only required to be valid for the following scenarios:\n\nYou have enabled new account activation by email in the Galaxy config.\nResetting forgotten passwords.\nBug reporting: the user email is used as the return address (but users can specify a valid email with the bug report if they wish.\nFeatures such as emailing on workflow step completion (there\u2019s no workaround for this).\n\nThere may be cases I\u2019m forgetting, but most of Galaxy\u2019s functionality should work fine without a valid email address."}], [{"role": "user", "text": "Hi,\nI\u2019m trying to  input a file from kallisto and I keep getting a key input error on the annotate my IDs function. My headers are called target_id_eightfiveoneseven and est_countseightfiveoneseven. The file has two rows; the first column  is the transcript ID from ensembl and the second is the est count column. The transcript names look odd for example one looks like ENST00000631435.1. Even though I select the ensembl transcript ID it still doesn\u2019t seem to recognize the input.since I get a key error.\nError in .testForValidKeys(x, keys, keytype, fks) :\nNone of the keys entered are valid keys for \u2018ENSEMBLTRANS\u2019. Please use the keys method to see a listing of valid arguments.\nCalls: select \u2026 select -> .select -> testSelectArgs -> .testForValidKeys\nMy table headers are unique and are called target_id_eightfiveoneseven and est_counts_eightfiveoneseven.\nDoes anyone spot errors in the names of the input files?\nThanks!"}, {"role": "user", "text": "*Update- I\u2019ve Switched the name of the identifier in column one to ENSEMBLTRANS but I still get the same key error message. I left my est_count title the same.\nK"}, {"role": "system", "text": "Hello @Kiera_Drew45\nTry removing the .N (where N is some number) from the identifiers.\nTool choices include: Replace parts of text\ndata-manipulation-olympics/tutorial.html#cheatsheet\nHope that helps!"}], [{"role": "user", "text": "I would like to ask help for the following problem with a workflow: I need to connect two tools. The first it outputs a list of files and I would like to connect it to a second tool that needs in input only a specific file from the list generated from the previous tool. Is there a way to do it, or a link to a comprehensive documentation on the workflows?\nHave a nice day, and many thanks."}, {"role": "system", "text": "There probably is a way to do it, how do you decide which dataset to use from the list ?\nDoes it have a specific name or position in the list ?"}, {"role": "user", "text": "Yes it has a specific name! (i.e File_1)"}, {"role": "system", "text": "Then you can use the Extract dataset from a list tool. This tool doesn\u2019t set the datatype in advance, so if you\u2019re connecting this to a downstream step you also need to change the datatype to a compatible datatype.\n\nScreen Shot 2019-07-17 at 14.51.47.png951\u00d7810 69.7 KB\n"}], [{"role": "user", "text": "I want my local Galaxy instance allow all users(anonymous and registered) free to browse around and see what\u2019s available without registering, but only the registers can upload data or run jobs.\nAccording to bgruening  suggestion, I could set limits in my job_conf.xml file and configure resources for users that are not registered. Below is the configuration of my job_conf.xml file, but it doesn\u2019t work. Could anyone help me see what went wrong?Thank you.\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n    </plugins>\n    <destinations>\n        <destination id=\"local\" runner=\"local\"/>\n    </destinations>\n    <limits>\n        <limit type=\"registered_user_concurrent_jobs\">2</limit>\n        <limit type=\"anonymous_user_concurrent_jobs\" id=\"local\">0</limit>\n    </limits>\n</job_conf>\n"}, {"role": "user", "text": "I tried it with 1, it workd fine on my local Galaxy instance. I think it might be a bug with 0 on anonymous_user_concurrent_jobs , I hope someone can help fix it."}, {"role": "system", "text": "\n\n\n istevenshen:\n\nI want my local Galaxy instance allow all users(anonymous and registered) free to browse around and see what\u2019s available without registering, but only the registers can upload data or run jobs.\n\n\nHave you considered enabling account activation? That way only registered and activated users can run jobs."}, {"role": "user", "text": "Yes, I tried.\nIf I set user_activation_on: true and require_login: false, the both anonymous and registered free to browse around and run jobs.\nIf I set user_activation_on: true and require_login: true, only registered and activated users can run jobs, but the anonymous can\u2019t browse around and see what\u2019s available. This seems to be a better method, but it is not what I really want.\nWhat I really want is that the anonymous and registered users free to browse around and see what\u2019s available without registering, but only the registers can upload data or run jobs."}, {"role": "system", "text": "It looks like there may be a bug with setting the limit to 0. In lieu of this, you can also create a dynamic job rule that will cause any jobs submitted by anonymous users to fail.\nThe job configuration would look like:\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n    </plugins>\n    <destinations default=\"require_login\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"require_login\" runner=\"dynamic\">\n            <param id=\"type\">python</param>\n            <param id=\"function\">require_login</param>\n        </destination>\n    </destinations>\n    <limits>\n        <limit type=\"registered_user_concurrent_jobs\">2</limit>\n    </limits>\n</job_conf>\n\nAnd then create a file <galaxy_root>/lib/galaxy/jobs/rules/require_login.py with contents:\nfrom galaxy.jobs.mapper import JobMappingException\n\n\ndef require_login(user_email):\n    if user_email is None:\n        raise JobMappingException('Please register an account to use this tool')\n    return 'local'\n"}], [{"role": "user", "text": "Hello,\nI have recently installed a local version of Galaxy and installed FreeBayes 1.3.6 from the tool shed. After indexing the genomes hg19 and hg38 as explained in many posts, I ran HiSat2 to generate the BAM files for input in to FreeBayes (version 1.3.6). This version of FreeBayes fails to recognize the BAM files generated by HiSat2 and gives the following error: \u201cSequences are not currently available for the specified build\u201d. The Galaxy.eu server version of FreeBayes is able to identify and process the BAM files generated by HiSat2 without any issues.\nWhen I downgrade the FreeBayes version to 1.3.1 on the local instance of Galaxy, FreeBayes is able to recognize the BAM files and proceeds to process them. I am wondering if this anomalous behaviour is related to the latest version and how can I fix it (other than downgrading the FreeBayes version)?\nThe FreeBayes version that I am using on the Galaxy.eu server is 1.1.0.46\nThank you"}, {"role": "system", "text": "Update\nThis was bothering me so decided to dig a bit more \nThe root problem is one of these:\n\nThe input BAM did not have a database assigned\nThe input BAM has a different or mismatched database assigned\nThe genome fasta was not originally indexed by using this Data Manager data_manager_fetch_genome_dbkeys_all_fasta, or manually indexed to do the same (involves more than just the fasta.fai index itself). All done without any conflicts like a duplicated dbkey or missing indexes (including indexes that point to other indexes )\n\nDocs Galaxy Community Hub - Galaxy Community Hub\nThe \u201cfirst step\u201d for all newly indexed genomes is very important.\n\n\nGalaxy has a built-in list of dbkeys for known/common genomes to enable consistent labeling across all (most?) Galaxy installs. Those keys are indexed themselves for UI functions (Upload tool, assigning/changing database metadata).\n\nIf you create a new dbkey it is either added to that master index of all dbkeys available to all users (when using the \u201cfetch\u201d DM) or to just that specific user\u2019s custom version of that master dbkeys index (when creating a custom genome build).\nFor both, the fasta index is also created since the actual sequences are now available.\n\n\n\nDuplicated dbkeys lead to all sorts of issues across tools and functions and are tedious to fix. If this seems to be the problem and the Galaxy install is new, is usually better to just start over with a fresh Galaxy base install. Then decide to use CVMFS, or your own local indexes, or both.\n\nIf a dbkey  already exists, use that when fetching a new genome, or expect problems.\nIf a dbkey does not exist yet, you can create a new one for you local indexes, but it must be different from any that already exist. For brand new dbkeys, DM will run a few more steps, including updating the master dbkeys index.\n\n\n\ndbkey is the technical label in files/tables for what is displayed as the database metadata in the end-user portion of the UI.\n\n\nA known dbkey is the same reserved \u201ckey:value\u201d pairing that data providers like UCSC also use to label specific exact assemblies. (all of it \u2013 the dbkey, the fasta title lines, AND the sequences).\n\n\nA dbkey is what enables direct connections to/from external sites or applications. Some of those external resources also support the creation/use of custom dbkeys. So, if a key is an exact \u201cmatch\u201d between the applications is used, useful functions like dataset displays are possible \u2013 directly.\n\n\nSo, the specific error is produced by Galaxy and is related to data indexes. Why the two wrappers perform differently, I don\u2019t know. But these are the relevant technical items beyond the indexes.\n\nDependency Resolvers in Galaxy \u2014 Galaxy Project 22.05.1 documentation\n\nGalaxy wrapper change log History for tools/freebayes - galaxyproject/tools-iuc \u00b7 GitHub, and this is the diff for the first change away from version 1.3.1 Updating tools/freebayes from version 1.3.1 to 1.3.5 (#4259) \u00b7 galaxyproject/tools-iuc@cce7fcd \u00b7 GitHub\n\n\nFreeBayes Issues \u00b7 freebayes/freebayes \u00b7 GitHub\n\n\nSAMtools Issues \u00b7 samtools/samtools \u00b7 GitHub.\n\n\n\nHi @prao123\nMaybe upgrade Freebayes to the latest version instead of some prior version?\nVersion 1.3.6 isn\u2019t even hosted at some of the public servers. The most current (and recommended) version is 1.3.6+galaxy0. This is expected to be paired with the most current release of Galaxy 22.05 Releases \u2014 Galaxy Project 22.05.1 documentation.\nAll versions are stored in the ToolShed Galaxy | Tool Shed with links out to the development Github repository that tracks all the changes if you want to investigate more about exactly why 1.3.6 isn\u2019t working. I don\u2019t remember why and it may not matter if you can get the more current version installed, and it works.\nIndexes can be tricky to produce directly. At a minimum, all distinct assemblies should have at least four core Data Managers applied, in order, then layer in tool-specific indexes as needed. I guessing that one of these steps was missed, or has a typo, or maybe an extra space, although the latter is much more common when NOT using Data Managers.\n\n\n\n prao123:\n\n\u201cSequences are not currently available for the specified build\u201d\n\n\nDid you know that indexes are available for a local Galaxy servers through CVMFS? training-material/search?query=cvmfs\nThe error you had can come up when the database metadata assigned to the BAM inputs (by an upstream mapping tool) is not an exact match for the build key (also called the dbkey) used to create/label all of the related built-in indexes. You can either adjust your current indexes to be consistently named, fix typos, etc \u2013 or easier \u2013 link in the CVMFS indexes. These include the full suite of hg19 and hg38 indexes (across tools) plus everything else you see indexed at usegalaxy.* sites. All is hosted from a remote file system, and just slices of data are used when called by a tool.\nHope that helps!"}, {"role": "user", "text": "Hello @jennaj,\nThank you for looking in to the issue more deeply. My post should have been a bit more detailed. My FreeBayes version that was giving me issues is 1.3.6+galaxy0. I have since then downgraded to version 1.3.1. My Galaxy build is 22.05.\nAdditionally, I created the index files replicating the steps outlined in this post by Jennifer Hillman Jackson:\nhttps://biostar.usegalaxy.org/p/19371/\nBriefly, I ran the\n\n\nCreate DBKey and Reference Genome fetching tool\nSAM fasta index builder\npicard index builder\n2bit index builder\nand then finally HiSAT2 index builder\n\nmaking sure that the next index builder is applied only after the previous one has completed. The indexes were built on the Ensembl human genome fasta file (hg38) downloaded separately.\n\n\n\n jennaj:\n\nSo, the specific error is produced by Galaxy and is related to data indexes. Why the two wrappers perform differently,\n\n\nI believe this to be the root cause of the issue because even with the previous human genome build (hg19), the issue recurs.\nI think the CVMFS may be the best route to go with.\nThank you for linking the tutorial."}, {"role": "system", "text": "\n\n\n prao123:\n\nindexes were built on the Ensembl human genome fasta file (hg38) downloaded separately\n\n\nThe problem is probably with the format of that fasta. Specifically, the > lines don\u2019t match what UCSC used, so sequences lengths cannot be found. That is already part of the \u201ccomes with Galaxy\u201d master list of dbkeys (includes length-per-identifier).\nEither get that fasta\n\nfrom UCSC with the DM tool (the original source of hg38) by starting over for that specific dbkey\nor use CVMFS\nor give the Ensembl version of the assembly a distinct dkbey\n\nFor any, you\u2019ll need to strip out your existing indexes (most of that is trial and error) or start over with a fresh Galaxy install first.\n\n\n\n prao123:\n\nJennifer Hillman Jackson\n\n\nThis is me "}], [{"role": "user", "text": "Hello,\nPlease I want to call the peaks from bigwig file, so I converted it to bedgraph formate, and run MACS, it worked really fine and I got the output, however suddenly it turned red and gave me this message:\nThe job creating this dataset has been resubmitted\n NFO  @ Sun, 02 Jun 2019 12:45:55: \n# Command line: callpeak -t /data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat --name bigWigToBedGraph_on_data_13 --format BAM --gsize 2700000000 --keep-dup 1 --qvalue 0.05 --mfold 5 50 --bw 300\n# ARGUMENTS LIST:\n\nI have no idea what happened! and how to fix it!\nCould you please help me!\nThank you in advance!\nSham"}, {"role": "system", "text": "Hi @shamjdeed\nWhat public server are you working at?\nAppears to be Galaxy EU https://usegalaxy.eu. But please confirm.\nIf working somewhere else, please note the URL (if public) or describe more about your local/cloud Galaxy (what Galaxy version, where/how you sourced and installed it, last time it was updated).\nThere are some known issues with metadata assignments with a fix in progress but not finalized yet/deployed to all public servers yet.\nMeanwhile, allow the job to rerun. If it fails, try running it again. As far as I know, only some clusters are presenting with this problem as a server-side issue (or at least at Galaxy Main https://usegalaxy.org \u2013 which doesn\u2019t automatically rerun failed jobs, so you are not working there).\nThe problem could also be due to inputs, parameters, or other issues. Once we know where you are working, we can follow-up based on that information.\nThanks!\ncc @bjoern.gruening @hxr @nate"}, {"role": "user", "text": "Hello @ jennaj,\nThank you for your reply\nYes I am using Galaxy EU, and I am in Hungary! (Galaxy Version 2.1.1.20160309.6), I did not install it I use it on line after signing in in my account!\nI run it again and it did not work!\nThank you!"}, {"role": "user", "text": "Hello @ jennaj,\nThank you for your reply\nYes I am using Galaxy EU, and I am in Hungary! (Galaxy Version 2.1.1.20160309.6), I did not install it I use it on line after signing in in my account!\nI run it again and it did not work!\nThank you"}, {"role": "system", "text": "Hi @shamjdeed\nWhenever you have errors like these, we recommend submitting a bug report. That\u2019s often a fast way to get help.\nLooking at the (i) information icon, and checking the stderr and stdout, I see the following.\nINFO  @ Sun, 02 Jun 2019 12:45:55: \n# Command line: callpeak -t /data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat --name bigWigToBedGraph_on_data_13 --format BAM --gsize 2700000000 --keep-dup 1 --qvalue 0.05 --mfold 5 50 --bw 300\n# ARGUMENTS LIST:\n# name = bigWigToBedGraph_on_data_13\n# format = BAM\n# ChIP-seq file = ['/data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat']\n# control file = None\n# effective genome size = 2.70e+09\n# band width = 300\n# model fold = [5, 50]\n# qvalue cutoff = 5.00e-02\n# Larger dataset will be scaled towards smaller dataset.\n# Range for calculating regional lambda is: 10000 bps\n# Broad region calling is off\n# Paired-End mode is off\n \nINFO  @ Sun, 02 Jun 2019 12:45:55: #1 read tag files... \nINFO  @ Sun, 02 Jun 2019 12:45:55: #1 read treatment tags... \nException struct.error: 'unpack requires a string argument of length 4' in 'MACS2.IO.Parser.BAMParser.tsize' ignored\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/bin/macs2\", line 4, in <module>\n    __import__('pkg_resources').run_script('MACS2==2.1.1.20160309', 'macs2')\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 750, in run_script\n    self.require(requires)[0].run_script(script_name, ns)\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1534, in run_script\n    exec(script_code, namespace, namespace)\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 617, in <module>\n    \n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 57, in main\n    \n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/MACS2/callpeak_cmd.py\", line 73, in run\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/MACS2/callpeak_cmd.py\", line 399, in load_tag_files_options\n  File \"MACS2/IO/Parser.pyx\", line 880, in MACS2.IO.Parser.BAMParser.build_fwtrack (MACS2/IO/Parser.c:13703)\n  File \"MACS2/IO/Parser.pyx\", line 893, in MACS2.IO.Parser.BAMParser.build_fwtrack (MACS2/IO/Parser.c:13111)\n  File \"MACS2/IO/Parser.pyx\", line 877, in MACS2.IO.Parser.BAMParser.get_references (MACS2/IO/Parser.c:12886)\nstruct.error: unpack requires a string argument of length 4\n\nmaybe this is helpful to you?"}], [{"role": "user", "text": "Hi,\nI am new at using Galaxy and trying to follow the tutorials of Immune Repertoire pipeline and SHM & CSR pipeline using tutorial test data sets. But the analyses ended up with some errors as below. How can I fix these??\nThanks,\nERROR from Immune Repertoire pipeline:\ncat: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/samples.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 55: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/productive_counting.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 149: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/AAMedianBySample.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 344: 7za: command not found\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 350: 7za: command not found\n\nERROR from SHM & CSR pipeline:\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/shm_csr/f387cc1580c6/shm_csr/baseline/script_imgt.py:20: DeprecationWarning: 'U' mode is deprecated\n  with open(args.ref, 'rU') as ref:\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/shm_csr/f387cc1580c6/shm_csr/baseline/script_imgt.py:20: DeprecationWarning: 'U' mode is deprecated\n  with open(args.ref, 'rU') as ref:\n"}, {"role": "system", "text": "On which galaxy server are you working? Is it perhaps this one? https://argalaxy.researchlumc.nl/"}, {"role": "user", "text": "Yes I\u2019m using that server"}, {"role": "system", "text": "Then you may get a better and faster response if you contact that maintainer. The error looks like some technical issues but I can never be sure."}], [{"role": "user", "text": "Hello,\nFollowing the tutorial \u201cFrom peaks to Genes\u201d I have an error uploading data with UCSC Main. The error is: The uploaded file contains invalid HTML content.\nThe settings used are those:\nimage1280\u00d7747 43.7 KB\nimage1280\u00d7638 31.5 KB\nThanks for any help!"}, {"role": "system", "text": "Hi @apventeo\nThanks for posting publicly about the error. The behavior can be reproduced, and is unexpected. Your settings are correct.\nI am looking into why and how to solve this, and will post updates back here once more is known.\nThanks!"}, {"role": "system", "text": "Hello again @apventeo\nThe problem seems to be with how the data is moving into Galaxy. We\u2019ll need to figure out why.\nThe file content itself is fine. I was able to download the file directly (instead of checking the \u201csend to Galaxy\u201d box), then upload the file to Galaxy. You can get a copy from my history.\nHow to:\n\nClick on this shared history link to open it in a browser window https://usegalaxy.eu/u/jenj/h/test-galaxy-intro-peaks2genes\nUse the \u201cImport\u201d button found in the top left corner\nGo into the User \u2192 Histories menu in the top masthead, and navigate to the imported history\nCopy the dataset into your history. Copy a dataset between histories\n\nOr, you can just work in the imported history. It is yours to work in, copy data out of, run more jobs in, then later even delete as you want :). Copies of histories are fully independent, so if anyone else runs into this problem before we can fix it, they can do the same when working with this tutorial, or can download/upload the particular file they need.\nUpdate2: The issue has been reported to the UCSC team at their mailing list and is pending a correction. https://groups.google.com/a/soe.ucsc.edu/g/genome/c/-OLkG9MnwMk\nThanks again for reporting the problem!"}, {"role": "system", "text": "The ucsc genome browser is redirecting to https now for better security.\nYour client can either be made to support the redirect, or much better is to update galaxy to use https:// urls for UCSC Genome Browser now. This include the callback to UCSC go grab the contents after a user does hgTables sendToGalaxy checkbox and button. Galaxy is seeing a web page that is also a redirect when it fetches a http:// page.\ncommandlinePrompt> curl -v Table Browser\n > GET /cgi-bin/hgTables HTTP/1.1\n > User-Agent: curl/7.29.0\n > Host: genome.ucsc.edu\n > Accept: */*\n >\n < HTTP/1.1 302 Found\n < Date: Fri, 15 Sep 2023 05:39:08 GMT\n < Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_fcgid/2.3.9\n < Location: https://genome.ucsc.edu/cgi-bin/hgTables\n < Content-Length: 224\n < Content-Type: text/html; charset=iso-8859-1\n <\n <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n <html><head>\n <title>302 Found</title>\n </head><body>\n <h1>Found</h1>\n <p>The document has moved <a href=\"https://genome.ucsc.edu/cgi-in/hgTables\">here</a>.</p>\n </body></html>\n"}, {"role": "system", "text": "Oh thank you @Galt_Barber for helping to clarify what is going on!\nFor Galaxy users\nWe are currently addressing the change in the Galaxy code base. Details: UCSC Genome Browser redirects http to https. Use https URLs instead. Change was made for improved user security. \u00b7 Issue #16700 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nWhen the final changes are determined and made, those will need to be implemented by the administrators of Galaxy servers. This is still in progress. For now, downloading and uploading data from the UCSC Table Brower to Galaxy is the workaround, and this applies to all Galaxy servers, public or private.\nGeneral updates will post back to this topic. The issue ticket linked above will have the technical details.\nThanks!"}, {"role": "system", "text": "Update:\nAs of today, UCSC has rolled back the change and all Table Browser queries should work as they do normally. Meaning, no need to download/upload as separate steps.\nThe change will now be implemented in about two weeks. It involves some server-level modifications to allow the direct data transfer. How end-users experience these functions will not change.\nThanks to everyone to reported the problem & helped to get it solved!"}], [{"role": "user", "text": "Hi, all there,\nI\u2019m trying to build a local galaxy that anybody can access with different devices on the same internet, so I set the uwsgi part of galaxy.yml as \u2018http: 127.0.0.1:8080\u2019, then it showed :\n#####################################\nGalaxy server instance \u2018main.web.1\u2019 is running\nStarting server in PID 9063.\nserving on http://127.0.0.1:8080\n#####################################\nBut with this setting, we could not access galaxy from the web browser on any other PC.\nCould you give me some advice on how to set of my galaxy or linux environment?"}, {"role": "system", "text": "What happens if you set it to: 0.0.0.0:8080? "}, {"role": "user", "text": "still can\u2019t connect, is there any protocol to follow up? ::"}, {"role": "system", "text": "I am no expert but in that case the first thing that comes to mind are firewalls etc. As a test you can try to ping the server/computer that is running galaxy from another computer in the same network.\nEDIT:\nA protocol to follow could be:\nhttps://docs.galaxyproject.org/en/latest/admin/production.html\nor\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Galaxy Installation with Ansible\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBut I think it is better you get this working first. (Do keep in mind that if you want to use galaxy with multiple users you need to check out that galaxy production page)\nEDIT2:\nAlso make sure you restart galaxy after the change"}, {"role": "system", "text": "HI\uff0c Have you success?"}, {"role": "system", "text": "Generally your computer has to be reachable over the network for the other devices to be able to see your services like Galaxy. That means it has to have an IP address that the others know and can send requests too. This is very much out of scope of what Galaxy community can help you with since we have no knowledge of your network\u2019s topology. I recommend you contact your network administrator and ask for help."}], [{"role": "user", "text": "I am attempting to follow the Galaxy tutorial for reference-based RNA-sequencing (https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#visualization-of-the-differentially-expressed-genes).\nUsing Galaxy, I have successfully (1) extracted the most DE genes from the normalized counts table and (2) extracted the normalized counts into a table. Currently, I am attempting to visualize my differential expressed genes. I have been unable to plot the heatmap with my normalized counts using the tool heatmap2. I could not find others in the community with a similar issue. Any help would be greatly appreciated!\nI am using the tool heatmap2 (toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_heatmap2/ggplot2_heatmap2/2.2.1+galaxy1).\nParameters used were:\nData transformation - Log2(value) transform my data\nEnable data clustering - True\nLabeling columns and rows - Label columns and not rows\nColoring groups - Blue to white to red\nData scaling - Do not scale my data\nError Details\nTool generated the following standard error:\nFatal error: Exit code 1 ()\nAttaching package: \u2018gplots\u2019\nThe following object is masked from \u2018package:stats\u2019:\nlowess\nError in hclust(x, method = \u201ccomplete\u201d) :\nNA/NaN/Inf in foreign function call (arg 11)\nCalls: heatmap.2 -> hclustfun -> hclust"}, {"role": "system", "text": "I met the same error and changed the parameter \u201cData transformation\u201d to - Log2(value+1) transform my data. Finally it plotted the heatmap successively. I think there should be \u201czero\u201d in your data too. So after \u201cLog2(value)\u201d it got the Inf value in the data.\n(Sorry for my poor English.)"}], [{"role": "user", "text": "If I download a file, fasta or zip of lets say 3GB the downloaded file is always max 1GB. Where do I need to look to solve this?\nI tried to add proxy_max_temp_file_size 0; to my nginx.conf but that is not working.\nIn my galaxy.log I get the following error:\nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n\nEDIT:\nI am still searching for a solution (and a cause) so here an update. My problem is not solved yet.\nAt the moment I have two galaxy servers running, both a different server. Using x-accel-redirect solves my problem only for one of the servers. The difference between them is that on the one were it does not work the database/files folder is located on a mounted volume. If I click the save icon to download some files I get a sort of page not found error, I will check the actual error in a moment because it is slightly different. I already found that it can have to do with reading rights but I gave the full rights to every one on the database/files folder.\nEDIT 2:\nI just mounted a volume on the galaxy where I did not had problems and everything still works. It is clearly not a galaxy problem. If I find the cause I will post it.\nEDIT 3: The solution and cause\nI still not fully understand it myself but it is solved and I know the cause. So in my case the path to the files folder looks like this: /media/galaxy/database/files.\nThe galaxy folder had the rights drwxrwxrwx (777), I just changed the rights to drwxr-xr-x (755) and now it works."}, {"role": "system", "text": "Yeah, my first guess here would be proxy_max_temp_file_size, too.  You restarted nginx after setting that value, right?\nOn second thought, it sounds like maybe you haven\u2019t set up nginx to serve the files directly?  You might want to try x-accel-redirect via the following instructions, if you haven\u2019t already.\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/nginx.html#sending-files-using-nginx"}, {"role": "user", "text": "First of all thanks for your reply, I will edit my original question with an update and maybe you or some one else can still point me in the right direction. I am working on the x-accel-redirect solution"}], [{"role": "user", "text": "I am trying to install the Frogs version https://toolshed.g2.bx.psu.edu/view/frogs/frogs_2_0_0/76c750c5f0d1\nUnfortunately, i am not able to resolve following dependencies\nFROGS Demultiplex reads frogs\nperl-io-zlib\nperl-io-gzip\nperl\nKindly help me to resolve the problem faced for my Galaxy installation."}, {"role": "system", "text": "Hello,\nFirst, make sure that your Galaxy install is up to date. The most current release is 19.01: https://docs.galaxyproject.org/en/master/releases/index.html\nFrom there you have choices:\n\nUpgrade to use the most current version of the tools: https://toolshed.g2.bx.psu.edu/view/frogs/frogs_3_1_0/59bc96331073\n\nTry to reinstall the older package from the ToolShed, instead of line command.\n\nIf it fails again, or you want to install line command, either contact the repository owners (http://frogs.toulouse.inra.fr/) or open a ticket at the development repository reporting the problem (https://github.com/geraldinepascal/FROGS-wrappers).\nFrom the error message you shared, I\u2019m not sure of the scope of problems that could be involved. The developers that created the tools will be able to offer more help to troubleshoot \u2013 and let you know about any known issues with the older tool version. It could be a problem as simple as an incompatible perl version or something else (disc space exceeded?)."}, {"role": "system", "text": "Hi,\nI encountered the same problem. Installing version 3.1.0 solved dependencies problem.\nCheers"}], [{"role": "user", "text": "Please note that I have already posted this message below since Oct. 20 at the\nusegalaxy.org support, but so far I got no response. thus, I am reposting it here in case someone could help. Many thanks in advance.\nInitial post in usegalaxy.org support:\nHello,\nI am using as input for the Volcano plot tool a file derived from DESeq2. The whole pipeline works great and I do get the corresponding Valcano graph but I have two little issues:\n\nFew genes are highly expressed and at the same time have p-value equal to 0; thus, these genes are depicted in the corresponding Volcano at the very top:\n\nimage678\u00d7682 74.6 KB\n\n\n\nMy question is: How should I treat those genes? I definitely want to have them due to are highly expressed, but at the same time, the plot doesn`t look proper in the way they are presented. I am not saying that is wrong, just saying that is odd. Should I exclude those genes? Should I do anything else with them?\n\nOn some occasions the DESeq2 file that I am using as an input does show for LogFC, p-value, and p-adjust of a few genes the entry NA, I guess standing for Not Applicable, instead of any real value. Should I exclude these particular genes from the input that I am going to use for the Volcano?\n\nI will highly appreciate any help,\nGreetings"}, {"role": "system", "text": "Hello @Manolis1\nA p-value of 0 should be filtered out for not being significant. Instead, it looks like a filter was not applied, or that tool/step had a problem interpreting the content of the values evaluated.\nThis tutorial has examples for creating meaningful plots: Visualization of RNA-Seq results with Volcano Plot"}, {"role": "user", "text": "Hi @jennaj,\nMany thx for your response; I appreciate it: )\nCould you please explain to me why p-value=0 should be excluded?\nAs I understand it the p-value for the corresponding genes (gathering to the top of the graph) was very very very small and due to that the program (Volcano Plot) could not show it properly, thus gave zero. Moreover, correct me if I am wrong but, the smaller the p-value the more highly significant is the difference among the diff. samples of the experiment, which in turn means small variation among the different replicates of each sample type; in that sense please have a look just below at the values from the rLog-Normalized counts file that I got from the DESeq2 as part of the same analysis, for ALL of the genes appearing to the very top of my Volcano:\n\nimage975\u00d7721 165 KB\n\nCould it be that due to the diff. replicates (for of the two sample typers Wt & Deletion) show extremely small variation among each other, and thus the p-value that the DESeq is calculating is zero? If that is the case then why should I exclude these particular genes, which is highly reproducible and homogeneous regarding their normalized counts?\nSorry for insisting, I just want to understand the issue that I have, I hope that you do understand\n: )\n@ This tutorial has examples for creating meaningful plots: [Visualization of RNA-Seq results with Volcano Plot.\n\nI did use this great tutorial to produce my Volcano indeed.\n\n@Instead, it looks like a filter was not applied.\nFilter was actually applied:.\n\nGreetings.\nP.S.: For your convenience and just in case I do attach the same Volcano with the gene of the top annotated:\n\nimage975\u00d7982 158 KB\n"}, {"role": "system", "text": "Update\nI can reproduce your graph clumping up near the top by faking extremely small p-values (e100, e200, e300, e400 \u2026 e900).\nIt seems this is a known limitation of R data representation - R largest/smallest representable numbers - Stack Overflow\nAnd the Bioconductor tool authors are aware, example topic (warning, older posts!) P Value of 0? but you can also search there for how others have scientifically interpreted these tiny values and any current additional advice.\nGalaxy history that uses the inputs and workflow from the tutorial, plus an adjusted input with \u201cextreme-pvalues\u201d for the test. Everything is tagged and I\u2019ll leave it shared as a reference. Galaxy | Accessible History | test rna-seq-viz-with-volcanoplot\nHope that helps!\n\n\n\n\n\n jennaj:\n\nthat tool/step had a problem interpreting the content of the values evaluated.\n\n\nIt looks like you are losing significant digits in the table (Excel?).\n\n\n\n jennaj:\n\nA p-value of 0 should be filtered out for not being significant.\n\n\nTools can report all kinds of odd values when there is a problem. A pvalue that is actually just \u201c0\u201d means that something went wrong. In your case, this seems to be related to losing significant digits in intermediate applications and that should be addressed.\nTry running these tools in Galaxy if you are not already, and see what results. You won\u2019t lose significant digits."}], [{"role": "user", "text": "Does a job using CVMFS take a long time? I implemented CVMFS configuration on my local galaxy with reference to https://galaxyproject.org/admin/reference-data-repo/  Then I executed bwa job for test using  hg38, but it is still running after about 6 hours. Smaller one such as dm3 finished but it took about 50 minutes. It seems to be slow for the size of the data.  In both cases, input data is the same, paired-end fasq filest about 400MB each and the second run with dm3 finished  less than a minute.\nI think it\u2019s not a problem of our network because downloading data from other sites, such as UCSC chromosome data, comes out to 2-3MB/s. I would like to know if the galaxy with CVMFS performance is normal, and if there is a setting that would improve performance.\nThank you,\nYukie"}, {"role": "system", "text": "What have you set as your cache size for cvmfs? This can affect production loads."}, {"role": "system", "text": "Which Stratum 1 was selected and the connection speed to that stratum 1 would also make a major difference. If you\u2019re going to make heavy use of the CVMFS repo it\u2019s recommended to at least run a local squid cache if not a full stratum 1 (which can be private to you)."}, {"role": "user", "text": "Hi @hxr ,\nI set CVMFS_QUOTA_LIMIT=\u201c100000\u201d. I would like to avoid re-downloading once downloaded as much as possible. Is it too large?\nThanks,\nYukie"}, {"role": "user", "text": "Hi @nate ,\nThank you for telling me about the squid. I don\u2019t use it that much now, but I\u2019ll try when I need it.\nThanks,\nYukie"}, {"role": "user", "text": "Follow-up comment. I tried to change Stratum1 server and it led to improved performance.  Bwa job with same input files and dm3 finished less than 10 minutes. Thanks for the advice, both of you.\nBest,\nYukie"}], [{"role": "user", "text": "Hello everyone! Which appliance on Galaxy would you recomend for Eukaryotic genomic annotation? It would be even more helpful if there was one capable of intersecting or cross-linking information from two or more Exomes. The Galaxy tutorials used GEMINI and Funannotate but those appear to no longer be availiable.\nThank you in advance, Anna"}, {"role": "system", "text": "hi @Anna\nthe GTN tutorials has information about compatibility(?0 with popular Galaxy servers. For example, both Gemini and Funannotate are available at usegalaxy.eu. It is OK to have account on different Galaxy servers. Data can be moved directly between Galaxy servers: click at Copy Link (chain) icon at the dataset to be copied, and paste the link into Upload menu on the target Galaxy server, into Paste/Fetch Data tab.\nWhat do you mean by cross-linking information from two or more exomes? Do you mean something like multi-sample VCF file?\nKind regards,\nIgor"}, {"role": "user", "text": "Hey @igor! Thank you again for all the help!\nI have two VCFs, one from a normal blood sample and one from diseased tissue from the same patient (not tumor, so standard cancer gene tools may not be helpfull). I\u00b4d like to compare them for somatic mutations only on the diseased tissue and possible loss of heterozigosity. I was able to generate a VCF of the differences between the two with VCF-VCF intersect as you suggested and I have annotated both of the VCFs with VEP but can seem to annotate the \u201cintersected\u201d VCF, the error message states \u201cthe index file is older than the data file\u201d. I was planning to use GEMINI to read them."}, {"role": "system", "text": "Hi @Anna\nI believe if you intersect annotated VCF files, the output file will have the annotations present in the input file(s).\nGalaxy servers usually have several intersection tools. Maybe try different tools.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi, I\u2019m Yoon.\nI\u2019ve tried to do DEseq process and featureCount process, but it still not downloaded.\nI tried it since 2 days ago, and i tried it repeatedly.\nAlso, i checked Internet state and storage.\nAs i write, i checked some possible causes.\nIt still not work(it presents gray color), and it is not move on to the next level(yellow or green) .\nPlease let me know what should i do and what is the problems.\nThank you for your work."}, {"role": "system", "text": "I\u2019ve got the same issue too when running DEseq today, I\u2019ve sent in the job 3 hours ago and it still hasn\u2019t started running. Is there an issue with tool or is the server just slow? Thank you!"}, {"role": "system", "text": "Hi @Yu_eun_Min and @Lim_Sing_Wei,\nlonger queued stages (gray datasets) are usually caused by a heavy load on servers. Which Galaxy instances are you using?"}, {"role": "system", "text": "Hi all \u2013\nIf you are working at UseGalaxy.org, there is one very busy cluster. A new banner on the server explains:\n\nTools impacted include by the busiest cluster: Unicycler, SPAdes, and Trinity\n\nOther clusters that run different tools are also very busy. Leave jobs queued for the quickest processing and to not lose your place in the queue. Three hours is not very long to wait \u2013 give it a few days \u2013 most jobs will process sooner than that, but the queue time is dependent on the server load as @gallardoalba stated.\nThe above applies to jobs that have valid (successful) inputs. It may be worth checking those upstream inputs for problems. If any are in an error or paused state, that could lead to downstream jobs pausing until the input is corrected \u2013 if you need to make a change to an input, that would be the exception for waiting. You should correct the inputs, then rerun.\nFAQs: Galaxy Support\n\nDatasets and how jobs execute\nExtended Help for Differential Expression Analysis Tools\n\nNote: When one public Galaxy server is very busy, you can try working at another. UseGalaxy.eu is one alternative. It is common to have one account at each distinct public Galaxy server. Each utilizes different processing resources.\nThanks!"}], [{"role": "user", "text": "Hello!\nI have local installed Galaxy 19.05 on t3.2xlarge instance (8 CPU and 32Gb RAM) and sometimes when I try update page or follow to another job I get next error:\n\nAn error occurred while updating with the server. Please contact a Galaxy administrator if the problem persist.\n\nFor me only helps close and open new tab on browser, but this very uncomfortable.\nDetails of error:\n\n{\n\u201cuserAgent\u201d: \u201cMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\u201d,\n\u201conLine\u201d: true,\n\u201cversion\u201d: \u201c19.05\u201d,\n\u201cxhr\u201d: {\n\u201creadyState\u201d: 4,\n\u201cresponseText\u201d: \u201c{\u201cerr_msg\u201d: \u201cUncaught exception in exposed API method:\u201d, \u201cerr_code\u201d: 0}\u201d,\n\u201cresponseJSON\u201d: {\n\u201cerr_msg\u201d: \u201cUncaught exception in exposed API method:\u201d,\n\u201cerr_code\u201d: 0\n},\n\u201cstatus\u201d: 500,\n\u201cstatusText\u201d: \u201cInternal Server Error\u201d\n},\n\u201coptions\u201d: {\n\u201cparse\u201d: true,\n\u201csilent\u201d: true,\n\u201climit\u201d: 500,\n\u201coffset\u201d: 0,\n\u201cdetails\u201d: \u201c92b83968e0b52980\u201d,\n\u201ctraditional\u201d: true,\n\u201cdata\u201d: {\n\u201climit\u201d: 500,\n\u201coffset\u201d: 0,\n\u201cdetails\u201d: \u201c92b83968e0b52980\u201d,\n\u201corder\u201d: \u201chid\u201d,\n\u201cv\u201d: \u201cdev\u201d,\n\u201cq\u201d: [\n\u201cdeleted\u201d,\n\u201cpurged\u201d,\n\u201cvisible\u201d\n],\n\u201cqv\u201d: [\n\u201cFalse\u201d,\n\u201cFalse\u201d,\n\u201cTrue\u201d\n]\n},\n\u201cemulateHTTP\u201d: false,\n\u201cemulateJSON\u201d: false,\n\u201ctextStatus\u201d: \u201cerror\u201d,\n\u201cerrorThrown\u201d: \u201cInternal Server Error\u201d\n},\n\u201curl\u201d: \u201chttp://18.130.115.156/api/histories/f597429621d6eb2b/contents?limit=500&offset=0&details=92b83968e0b52980&order=hid&v=dev&q=deleted&q=purged&q=visible&qv=False&qv=False&qv=True\u201d,\n\u201cmodel\u201d: [],\n\u201cuser\u201d: {\n\u201cid\u201d: \u201cf2db41e1fa331b3e\u201d,\n\u201cusername\u201d: \u201cadministrator\u201d,\n\u201ctotal_disk_usage\u201d: 881918595,\n\u201cnice_total_disk_usage\u201d: \u201c841.1 MB\u201d,\n\u201cquota_percent\u201d: null,\n\u201cis_admin\u201d: true,\n\u201cpreferences\u201d: {},\n\u201ctags_used\u201d: [\n\u201cname:tissueA\u201d,\n\u201cname:tissueB\u201d\n],\n\u201cdeleted\u201d: false,\n\u201cpurged\u201d: false,\n\u201cquota\u201d: null\n}\n}\n\nI use Nginx as frontend HTTP server and use SQL lite by default.\nThanks!"}, {"role": "user", "text": "Log from journalctl:\n\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]: galaxy.web.framework.decorators ERROR 2019-10-22 10:16:26,345 [p:14873,w:1,m:0] [uWSGIWorker1Core3] Uncaught exception in exposed API method:\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]: Traceback (most recent call last):\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/web/framework/decorators.py\u201d, line 282, in decorator\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     rval = func(self, trans, *args, **kwargs)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/webapps/galaxy/api/history_contents.py\u201d, line 82, in index\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     return self.__index_v2(trans, history_id, **kwd)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/webapps/galaxy/api/history_contents.py\u201d, line 891, in __index_v2\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     user=trans.user, trans=trans, view=\u2018detailed\u2019, **serialization_params))\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/managers/base.py\u201d, line 692, in serialize_to_view\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     return self.serialize(item, all_keys, **context)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/managers/hdas.py\u201d, line 367, in serialize\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     return super(HDASerializer, self).serialize(hda, keys, user=user, **context)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/managers/datasets.py\u201d, line 604, in serialize\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     serialized = super(DatasetAssociationSerializer, self).serialize(dataset_assoc, keys, **context)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/managers/base.py\u201d, line 605, in serialize\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     returned[key] = self.serializers[key](item, key, **context)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/managers/hdas.py\u201d, line 374, in serialize_display_apps\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     for display_app in hda.get_display_applications(trans).values():\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/model/init.py\u201d, line 2757, in get_display_applications\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     return self.datatype.get_display_applications_by_dataset(self, trans)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/datatypes/data.py\u201d, line 551, in get_display_applications_by_dataset\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     value = value.filter_by_dataset(dataset, trans)\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/datatypes/display_applications/application.py\u201d, line 309, in filter_by_dataset\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     if link_value.filter_by_dataset(data, trans):\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/datatypes/display_applications/application.py\u201d, line 95, in filter_by_dataset\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     if fill_template(filter_elem.text, context=context) != filter_elem.get(\u2018value\u2019, \u2018True\u2019):\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:   File \u201clib/galaxy/util/template.py\u201d, line 91, in fill_template\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]:     raise first_exception or e\nOct 22 10:16:26 ip-172-31-13-19 run.sh[14713]: NotFound: cannot find \u2018site_id\u2019\n"}, {"role": "user", "text": "I found this commit:\nhttps://github.com/galaxyproject/galaxy/pull/8104\nBut this was merged to 19.09, I tried use it for my 19.05, but have the same error."}, {"role": "system", "text": "Maybe it helps to check the galaxy.log file, it can have more informative errors. It will be created if you start galaxy as a daemon (sh run.sh --daemon)."}, {"role": "user", "text": "Thanks you for answer!\nI noticed in log next line:\n\n/srv/galaxy/.venv/local/lib/python2.7/site-packages/Cheetah/Compiler.py:1630: UserWarning:\nYou don\u2019t have the C version of NameMapper installed! I\u2019m disabling Cheetah\u2019s useStackFrames option as it is painfully slow with the Python version of NameMapper. You should get a copy of Cheetah with compiled C version of NameMapper.\n\"\\nYou don\u2019t have the C version of NameMapper installed! \"\n\nSo I installed cheetah3:\n\npip install cheetah3\n\nAnd now it works well!\nThanks!"}], [{"role": "user", "text": "I ran a test file (xxx.txt) containing fasta formatted sequences that I uploaded from my desktop. There are only <10 sequences in the file. I wanted to test this because I was having issues with running megaBLAST on a larger dataset collection. At any rate, I think there is something happening with this python script that is causing megaBLAST to fail with large or small files.\nThis was the error message:\nFile \u201c/project/galaxy/galaxy-py3/galaxy-base/database/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/megablast_wrapper/fb2e0e1dac89/megablast_wrapper/megablast_wrapper.py\u201d, line 68\nprint megablast_command\n^\nSyntaxError: Missing parentheses in call to \u2018print\u2019. Did you mean print(megablast_command)?\nI also tried to upload a pic of the error screen, not sure if it made it\u2026\n\nerror screen1377\u00d7841 140 KB\n\nThis was the input file (for some reason it\u2019s not seeing the >'s at the start of each sequence title, but they are on there):\n\nA00129:980:H5TVGDSX2:4:1107:23547:158121:N:0:TGTCGCAT+AAGCTGAC\nTAGTAGATTTAAAACGAAGGTCTTGGGGGGTGGTACTTCCCTCAATTCCAATAACCTAAAATAAAATTAAAATTAATAAGTATTTAAAATTTAATATTACGCGCTAGCGTAACATCTGACATCCATTTCTTCCCATGTGATAAGGT\nA00129:980:H5TVGDSX2:4:1114:6777:151701:N:0:TGTCGCAT+AAGCTGAC\nGTCGGAATGGAATCGTAAATTGGACGGGACTTATACTAAATATTCTAGTCTTACCAGGTATTCTGCATTTTTCCACAGGCACACATGTATCTTTCAAACAATCCGTGAATATAGTGTGAGGCTTTATTCCCTTCTTCCTCATTAACTGAG\nA00129:980:H5TVGDSX2:4:1124:1524:98311:N:0:TGTCGCAT+AAGCTGAC\nCACCTAACCTAGAGTACCACTTTAATCGTAACTAAAACTACTCTACTACGGAGCTGAGTCTTTAGAGGCAACTCAACCTTCCTCACATATAAGCTAAAAGTGGTGGTCTAATCCACTCTGCTTTTAGACTCTTATATGCAAGTTCACTCC\nA00129:980:H5TVGDSX2:4:1126:11279:367461:N:0:TGTCGCAT+AAGCTGAC\nACCCCATCCGAATGCCAACTCTAGCGCTTGTTTAGCATTCTCAATGGTTGCTACTCGACGACCCAATCCTCGAGCATGTGTCCAATTGGTTGTTCCTTCTATAGAAACATTATCCAGATTGGCTAGAAACACGGGTCTTGTTGGATGTTT\nA00129:980:H5TVGDSX2:4:1127:7699:341781:N:0:TGTCGCAT+AAGCTGAC\nCGGATAAAGGCAAATCAGTAATACCTAACCAAGCTAACCTAATTAACAAACAATTTGAAATTGTATTCAATATGTCCGTTATTGGAGAACCTGATGGAATTCCGCAAGGTACTCGGTACACCAAATCGCGACATAGATGACTAGGCG\nA00129:980:H5TVGDSX2:4:1140:5041:243771:N:0:TGTCGCAT+AAGCTGAC\nCCACCGTAACAAACTAGCACGACATGTCTAGAAAATTCGGATAAAGGCAAATCAGTAATACCTTGCCAAGCCAATCGAATTAACAAACAATTCGAAATAGTATTCAAAATGTCCGTAATTGGTGATCCAGAAGGAATACCGCATAGTACG\nA00129:980:H5TVGDSX2:4:1169:13883:227481:N:0:TGTCGCAT+AAGCTGAC\nTATATTTTTCTTTCTTACTCCAATTACTTTCTTCTACGGCAAATACAAGTTATTTCATTTCTTTGCGCTTTATAAGTAACCGTTGTAACTTAGTAACTCTTAGTTCGAGGAATTATTTAGAAAGCTTTTTAAGTAACAATATATTT\n"}, {"role": "system", "text": "Hi @Vanessa_Corby-Harris\nI cannot reproduce megablast error on usegalaxy.org. I tested several small query datasets.\n\u2018>\u2019 character should be visible on data preview. If it is absent, there is no proper name line.\nCan you run any other tool on the input/query file, for example, convert FASTA to tabular? If yes, recreate the FASTA file using tabular-to-FASTA. Do you see \u2018>\u2019?\nKind regards,\nIgor"}, {"role": "user", "text": "Hi @igor,\nThank you for answering! I can convert my FASTA to tabular. If I take a file from my collection, the forward reads, I input something that looks like this:\n\nfasta snip1111\u00d7201 85.8 KB\n\nand I get something like this:\n\ntable snip1228\u00d7153 66.3 KB\n\nI don\u2019t know of a tabular-to-FASTA tool. Is it buried within something else? When I search for that, nothing comes up."}, {"role": "user", "text": "@igor\nHang on a minute\u2026 I might know what is going on."}, {"role": "user", "text": "@igor\nI was using an old version of megaBLAST (circled in red). I am using the USDA SciNet Galaxy, and this is an option still.\n\nCapture megaBLAST1679\u00d7520 102 KB\n\nI don\u2019t see it on the usegalaxy.org (non-USDA SciNet) page. I guess they need to deprecate/retire/take down that option. But I still don\u2019t know why it wouldn\u2019t work correctly. Maybe something associated with that tool needs updating or whatever.\nAt any rate, thank you for taking time out for this.\nV"}], [{"role": "user", "text": "Hi everyone, when trying to import a dataset (large .csv file) from my active galaxy history to Rstudio with gx_get(19056) I get the following error:\n\nDEBUG:root:Host IP determined to be b\u2019172.17.0.1\\n\u2019\nDEBUG:root:TestURL url=https://usegalaxy.eu obj=False\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): usegalaxy.eu:443\nDEBUG:urllib3.connectionpool:https://usegalaxy.eu:443 \u201cGET /api/histories?key=###HTTP/1.1\u201d 200 None\nDEBUG:root:TestURL url=https://usegalaxy.eu state=success\nDEBUG:root:Downloading gx=GalaxyInstance object for Galaxy at https://usegalaxy.eu history=0cd1246fe45a8830 dataset=19056\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): usegalaxy.eu:443\nDEBUG:urllib3.connectionpool:https://usegalaxy.eu:443 \u201cGET /api/histories/0cd1246fe45a8830/contents?key=###HTTP/1.1\u201d 200 None\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/get\u201d, line 19, in \nresults = get( args.id, args.identifier_type, args.history_id )\nFile \u201c/usr/local/lib/python3.8/dist-packages/galaxy_ie_helpers/init.py\u201d, line 199, in get\ndatatypes_all.append({ds[identifier_type]: ds[\u2018extension\u2019] for ds in history})\nFile \u201c/usr/local/lib/python3.8/dist-packages/galaxy_ie_helpers/init.py\u201d, line 199, in \ndatatypes_all.append({ds[identifier_type]: ds[\u2018extension\u2019] for ds in history})\nKeyError: \u2018extension\u2019\n\nThe dataset loads normally in my locally installed Rstudio.\nAny suggestions?"}, {"role": "system", "text": "Hi @feanor1914\nI also noticed a problem with all three of the gx_ commands at both usegalalxy.eu and usegalaxy.org late yesterday, but haven\u2019t had a chance to follow up yet.\nLet\u2019s ask the developers if they know what is going on at Gitter. They may post back here or at Gitter, and feel free to join the chat. galaxyproject/dev - Gitter"}, {"role": "user", "text": "Hi @jennaj\nThanks for responding, I forgot to mention that uploading the zipped dataset with the \u201cupload\u201d button in the Rstudio server failed as well returning a \u201cunexpected response from server\u201d message."}, {"role": "user", "text": "@jennaj after seeing the people at Gitter saying gx_* worked for them, I tried again (just now) and managed to fetch my data. Not sure if it\u2019s related but before trying again I created a new history with my dataset and a new Rstudio environment. Hope this helps!"}, {"role": "system", "text": "Glad worked out! No idea what was wrong originally but seems working on .org and .eu now. Maybe it was a data compression issue for you \u2013 or maybe the IE environment got corrupted. But if working now considering it all good.\nI was doing something else (and will be doing more today). That involves these same commands. I\u2019ll also use a fresh session.\nThanks for all the followup! "}], [{"role": "user", "text": "Hello there,\nI am an amateur user of Galaxy and have been trying to convert a bedgraph file generated from the Methyldackel tool into a bigwig format for visualisation using the Wig/Bedgraph - to - Bigwig converter tool. I am receiving this error message -\n\u201cUnrecognized line 1 of stdin:\nchr1 3000826 3000828 80 4 1\u201d\nCan you please tell me what is wrong and how I can solve it?\nThanks!!\nAbishek"}, {"role": "system", "text": "Hi @srinivasa_abishek,\ncould you share your Galaxy history with me? I\u2019ll have a look at it. My email is\n\nRegards"}, {"role": "user", "text": "Hello Gallardo,\nThanks for replying.\nI have shared the history with you. Kindly check if you are able to access it.\nWith regards,\nAbishek"}, {"role": "user", "text": "Hello @gallardoalba ,\nDid you have a chance to look into this problem?\nAbishek"}, {"role": "system", "text": "HI @srinivasa_abishek,\nsorry, I wasn\u2019t able to fix it yet. I\u2019ll try to work on it throughout the week.\nRegards"}, {"role": "system", "text": "Hi @srinivasa_abishek,\nthe problem was that the converter requires a BedGraph with only four columns; so you in your case you need to generate three different bedgraph files, one for each track.\nTest history: Galaxy | Accessible History | imported: MIWI2 WGBS_abi\nSorry for the late solution, regards!"}], [{"role": "user", "text": "Hello,\nI am trying to build a local Galaxy server (basically followed the training materials up to step 7, version 22.01).\nMy problem is that when I run BWA/BWA-MEM I receive the following error:\n\nimage1305\u00d7658 38.3 KB\n\nAs far as I can tell the data tables and CVMFS seem to work properly.\n\nimage1390\u00d7288 21.6 KB\n\nubuntu@galaxy-server:~$ ls /cvmfs/data.galaxyproject.org/byhand/hg38/hg38full/bwa_index_v0.7.10-r789/\nhg38full.fa  hg38full.fa.amb  hg38full.fa.ann  hg38full.fa.bwt  hg38full.fa.pac  hg38full.fa.sa\n\nAny help is appreciated!"}, {"role": "system", "text": "Hi @OtimusOne\nWould you please try again now? It looks like the CVMFS server had a little trouble around the time you posted. https://status.galaxyproject.org/\nOther than that, things to check are:\n\nUsing the most current version of a tool:  true\n\nUsing the most current version of Galaxy: ?? Should be January 2022 Galaxy Release (v 22.01) \u2014 Galaxy Project 22.01.2.dev0 documentation\n\nThe indexes are connected. One of these:\n\nReference Data with CVMFS\nReference Data with CVMFS without Ansible\n\n\n\nIf that is not enough help, please post back the relevant configurations lines and we can bring in some expert help for more review.\nIt also might be helpful to test this on another index or two, just to see what happens. Maybe a different genome\u2019s BWA index and a tool that uses a different index from this same genome.\nLet\u2019s start there "}, {"role": "user", "text": "Hello,\nI\u2019ve managed to track down the issue to Singularity. I\u2019ve rolled back my configuration to the CVMFS tutorial and BWA works properly. It\u2019s after I apply the changes from the Singularity tutorial that it fails to locate the index files. I assume that the /cvmfs directory isn\u2019t accessible to the containers.\nSeems like adding < env id=\u201cSINGULARITY_BIND\u201d>/cvmfs< /env> did the trick."}], [{"role": "user", "text": "Hi,\nI am the admin of local galaxy installation. Files more than 2GB fail to upload and run.\nHow to upload data from a local disk directly so that I can continue with my work."}, {"role": "system", "text": "I believe you can\u2019t (but some one need to confirm), and I am assuming that you are not using a recent galaxy version. I think out of my head the 2GB limit was gone after version 18.05.\nMy quick fix at the time was to enable FTP ftp_upload_dir: '' in this FTP folder you can create a folder with your email as the name for that folder. And then upload it manually."}, {"role": "user", "text": "I am using the recent version galaxy 19.05."}, {"role": "system", "text": "Maybe settings in nginx or apache if you are using that. Do you get an error?"}, {"role": "system", "text": "The 2GB browser upload soft limit is long gone, it was more of a browser\u2019s issue than Galaxy\u2019s. If you already have data local to Galaxy instance the best way is to not upload them, just to import them with Data Libraries using links instead of copying. This is how: https://galaxyproject.org/data-libraries/#user-folder"}], [{"role": "user", "text": "I am using Galaxy 20.09 (last pull January) and found a strange error when trying to open some shared workflows (trace below). The workflows themselves work fine, both execution and loading the editor from the \u201cworkflow\u201d menu. But from the \u201cshared workflow\u201d menu you get 500 if you click on it. I cleared the template cache to no avail. This effect only some workflows, other shared workflows on the same instance can be opened. New basic workflow can be opened as well. If I download the problematic workflow and import it the error will occur again. If I import it to another instance of Galaxy, it will also cause server 500 error. One of the failing workflow can be seen exported here: 500.ga \u00b7 GitHub\ntrace:\n86.49.250.134 - - [10/Mar/2021:15:14:35 +0000] \"GET /u/marten/w/mzmlconvert HTTP/1.1\" 500 - \"https://umsa.cerit-sc.cz/workflows/list_published\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\"\nTraceback (most recent call last):\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/error.py\", line 153, in __call__\n    app_iter = self.application(environ, sr_checker)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/paste/recursive.py\", line 85, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/statsd.py\", line 33, in __call__\n    req = self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/paste/httpexceptions.py\", line 640, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/base.py\", line 141, in __call__\n    return self.handle_request(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/base.py\", line 220, in handle_request\n    body = method(trans, **kwargs)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/galaxy/controllers/workflow.py\", line 230, in display_by_username_and_slug\n    return self._display(trans, stored_workflow)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/galaxy/controllers/workflow.py\", line 272, in _display\n    user_is_owner=user_is_owner)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/base/webapp.py\", line 972, in fill_template_mako\n    return template.render(**data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/template.py\", line 476, in render\n    return runtime._render(self, self.callable_, args, data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 883, in _render\n    **_kwargs_for_callable(callable_, data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 920, in _render_context\n    _exec_template(inherit, lclcontext, args=args, kwargs=kwargs)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 947, in _exec_template\n    callable_(context, *args, **kwargs)\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/base/base_panels.mako.py\", line 124, in render_body\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/display_base.mako.py\", line 234, in render_center_panel\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/display_base.mako.py\", line 269, in render_render_content\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 256, in render_render_item\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 227, in do_inputs\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 134, in render_do_inputs\nKeyError: 'input_files'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/batch.py\", line 80, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/request_id.py\", line 15, in __call__\n    return self.app(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/xforwardedhost.py\", line 23, in __call__\n    return self.app(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/translogger.py\", line 70, in __call__\n    return self.application(environ, replacement_start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/error.py\", line 163, in __call__\n    exc_info)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/translogger.py\", line 69, in replacement_start_response\n    return start_response(status, headers, exc_info)\nSystemError: <built-in function uwsgi_spit> returned a result with an error set\n\nanother workflow has the same trace except the middle part is:\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 134, in render_do_inputs\nKeyError: 'input_smiles'\n\nbut there is no input_smiles value when looking at the .ga file"}, {"role": "user", "text": "I found a suspicious thing - the workflow runs the tool galaxyp/thermo_raw_file_converter/thermo_raw_file_converter/1.3.2+galaxy0 however the downloaded file has version 1.2.3+galaxy0 instead. Moreover I am unable to change the version of this tool, no matter what I select in the workflow editor it will always download as 1.2.3.\nThis would explain the input_files mismatch, since that was input name of the previous tool version."}, {"role": "user", "text": "So if I manually change the version of the tool in the .ga file to 1.3.2 and then import - everything works well. Moreover the original (broken) .ga file contains the following, which includes the newest changeset corresponding for the 1.3.2 version\u2026\n            \"tool_shed_repository\": {\n                \"changeset_revision\": \"92ac8e086317\",\n                \"name\": \"thermo_raw_file_converter\",\n                \"owner\": \"galaxyp\",\n                \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n            },\n"}, {"role": "system", "text": "Sounds like a variation of [20.09] Incorrect tool version saved in workflow \u00b7 Issue #10914 \u00b7 galaxyproject/galaxy \u00b7 GitHub?"}, {"role": "user", "text": "Looks like it, I will try tomorrow, thank you!"}, {"role": "user", "text": "\n\n\n wm75:\n\nSounds like a variation of [20.09] Incorrect tool version saved in workflow \u00b7 Issue #10914 \u00b7 galaxyproject/galaxy \u00b7 GitHub?\n\n\nThis was the right direction. To add to it you also need to delete all problematic nodes from the workflow and re-add them. "}], [{"role": "user", "text": "Hi,\nI have been attempting to use the Mothur pipeline to conduct my 16S community analysis. I was onto the beta-diversity step, using the Dist.shared tool. However, when running this tool I received the following error message, \u201cFatal error: Exit code 139 (), The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\u201d\nI was unable to locate the empty dataset in any of my input datasets or any of the datasets used in the proceeding tools that may have caused the issue. After attempting to re-run the tool a number of times to no avail, I attempted to conduct my entire analysis again.\nHowever this time, when running summary.seqs tool after the sequence alignment step was completed using the align.seqs tool, this gave the same error. I assumed something went wrong in the align.seqs tool so I reattempted to run this again with the same data set but then this gave me the same error message again even thought I successfully ran the align.seqs tool with the same dataset that worked a few hours earlier.\nUnsure why this is happening as I have successfully used Mothur in the past with no issues, any help would be fantastic, Ryan"}, {"role": "system", "text": "Hi @RyanNThompson\nThis message usually means that one of the inputs is empty (possibly has only headers and no data lines) or sometimes that the one or more input datasets was entered more than once on the tool form.\n\n\n\n RyanNThompson:\n\n\u201cFatal error: Exit code 139 (), The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\u201d\n\n\nIf you still need help, you can post back a share link to your history publicly as a reply, and we can try to help. If you\u2019d rather share the link privately instead, please confirm the URL of the public server where you are working (actually UseGalaxy.eu?) and state that you\u2019d like a moderator to start up a direct message to post the history share link in.\nFAQs\n\n\n#troubleshooting \u2013 the \u201cDataset Information\u201d view (  icon within any dataset) is a good way to review the inputs and parameters for a job in a summary. This is the same place where the job stderr/stdout logs are located for any dataset, errored or successful.\n#understanding-input-error-messages\n#sharing-your-history\n\nTutorial reference training-material/search?query=mothur\nLet\u2019s start there! Thanks"}], [{"role": "user", "text": "Hi,\nI have Chipseq data and 2 replicates for each condition. I did MACS2 peak calling and tried to perform DiffBind. But I always get the same error. [1] \u201cen_US.UTF-8\u201d\nCan you please help me with this.\nThanks"}, {"role": "system", "text": "Hi @dush,\ncould you provide additional information about the error report? You can find it in the bug icon:\n\n  \n    \n  \n\n\nRegards"}, {"role": "user", "text": "\nDetailed Job InformationJob environment and execution information is available at the job info page.\n\n\n\n\n\nJob ID\n36606423 (bbd44e69cb8906b530f67995209c8a3a)\n\n\n\n\nTool ID\ntoolshed.g2.bx.psu.edu/repos/bgruening/diffbind/diffbind/2.10.0\n\n\nTool Version\n2.10.0\n\n\nJob PID or DRM id\n30689932\n\n\nJob Tool Version\nR version 3.5.1 (2018-07-02) \u2013 \u201cFeather Spray\u201d, DiffBind version 2.10.0, rjson version 0.2.20\n\n\n\n\n\n\nJob Execution and Failure Information#### Command Line\nln -s '/galaxy-repl/main/files/057/429/dataset_57429239.dat' 'Treated-MACS2_callpeak_on_T-peaks.bed' &&   ln -s '/galaxy-repl/main/files/057/429/dataset_57429110.dat' 'Filter_SAM_or_BAM__T-bamreads.bam' && ln -s '/galaxy-repl/main/files/_metadata_files/004/628/metadata_4628948.dat' 'Filter_SAM_or_BAM__T-bamreads.bai' &&      ln -s '/galaxy-repl/main/files/057/429/dataset_57429139.dat' 'Filter_SAM_or_BAM__N-bamcontrol.bam' && ln -s '/galaxy-repl/main/files/_metadata_files/004/628/metadata_4628955.dat' 'Filter_SAM_or_BAM__N-bamcontrol.bai' &&      ln -s '/galaxy-repl/main/files/057/429/dataset_57429192.dat' 'Control-MACS2_callpeak_control62121_narrow_Peaks-peaks.bed' &&   ln -s '/galaxy-repl/main/files/057/429/dataset_57429080.dat' 'Filter_SAM_or_BAM__C-bamreads.bam' && ln -s '/galaxy-repl/main/files/_metadata_files/004/628/metadata_4628952.dat' 'Filter_SAM_or_BAM__C-bamreads.bai' &&              Rscript '/cvmfs/[main.galaxyproject.org/shed_tools/toolshed.g2.bx.psu.edu/repos/bgruening/diffbind/194e3f2c1d86/diffbind/diffbind.R](http://main.galaxyproject.org/shed_tools/toolshed.g2.bx.psu.edu/repos/bgruening/diffbind/194e3f2c1d86/diffbind/diffbind.R)'  -i '[[\"Condition\", [{\"Control\": [\"Filter_SAM_or_BAM__N-bamcontrol.bam\"]}, {\"Control\": [\"Filter_SAM_or_BAM__C-bamreads.bam\"]}, {\"Control\": [\"Control-MACS2_callpeak_control62121_narrow_Peaks-peaks.bed\"]}, {\"Treated\": [\"Filter_SAM_or_BAM__N-bamcontrol.bam\"]}, {\"Treated\": [\"Filter_SAM_or_BAM__T-bamreads.bam\"]}, {\"Treated\": [\"Treated-MACS2_callpeak_on_T-peaks.bed\"]}]]]' -o '/galaxy-repl/main/files/058/461/dataset_58461363.dat' -t 0.5 -f interval -p 'XXXX'  -n \"8\"\n\n\n\nstderr\nMACS2_callpeak_control62121_narrow_Peaks MACS2_callpeak_control62121_narrow_Peaks  Control  NA raw\nMACS2_callpeak_on_T MACS2_callpeak_on_T  Treated  NA raw\nWarning message:\nNo contrasts added. Perhaps try more categories, or lower value for minMembers. \nError in pv.DBA(DBA, method, bSubControl, bFullLibrarySize, bTagwise = bTagwise,  : \n  Unable to perform analysis: no contrasts specified.\nCalls: dba.analyze -> pv.DBA\nWarning message:\nNo contrasts added. Perhaps try more categories, or lower value for minMembers. \n\n\n\nstdout\n[1] \"en_US.UTF-8\"\n\n\n\nJob Information\nNone\n\n\n\nJob Traceback\nNone\n\n\nThis is an automated message. Do not reply to this address."}, {"role": "system", "text": "Hi @dush,\nplease have a look at this entry; it covers your case and provide some additional info about how to use Diffbind.\n\n  \n    \n    \n    DiffBind_ChIP-Seq_Error_message usegalaxy.eu support\n  \n  \n    Hi @shamjdeed \nAt least two replicates per condition are required. And for Diffbind, the first condition must be specifically named \u201cCondition\u201d (quirk in how the underlying wrapped tool works). \nFAQ: https://galaxyproject.org/support/ \n\nExtended Help for Differential Expression Analysis Tools\n  \n\n\nRegards"}], [{"role": "user", "text": "\nCapture 21620\u00d7877 60.5 KB\n\n\nFastQC job created yesterday and only half processed. The rest of the jobs are in the waiting or unprocessed mode. First time encountering this as similar jobs were processed within 4 hours earlier this week."}, {"role": "system", "text": "Hi @ktian811,\ncould you share your history with me? Regards"}, {"role": "user", "text": "Thanks so much for the reply. May I have your email address for sharing history please?"}, {"role": "user", "text": "I found it and shared my history with you!"}, {"role": "user", "text": "Sorry to bother you on a weekend. Are there any update on the server as the status of the job is still the same as 2 days ago. Many thanks!"}, {"role": "system", "text": "Hi @ktian811,\nsorry for the late response; I just saw that the jobs finished succefully.\nRegards"}], [{"role": "user", "text": "I am new to Galaxy and I am trying to learn how to use the Galaxy API with the BioBlend package.\nWhen describing the Galaxy API, Galaxy mentions that it is useful to use the API and tools such a BioBlend in the following cases:\n\n\nRunning a workflow against multiple datasets (which can be done with the web interface, but is tedious)\n\n\nWhen the analysis involves complex control, such as looping and branching\n\n\nI was therefore hoping that I would be able to programmatically create workflows with BioBlend, not only invoke them. however, the documentation for BioBlend doesn\u2019t seem to indicate that I can make workflows. Am I correct in thinking that I cannot create workflows? Is this something that Parsec might be able to do?\nAn example of I was hoping to do. I want to automate RNAseq differential expression starting from a single matrix produced with featureCounts. I have 3 groups I need to compare (so 3 inputs for edgeR). However, in the Galaxy GUI, I have been unable to seperate my matrix count into 3 noodles going into edgeR. In other words, if I want edgeR to have 3 input groups, I need to create 3 separate matrix counts. See image below. I was hoping to be able to programmatically create a workflow with BioBlend that would help me around this problem.\n\nScreenshot from 2018-12-17 11-47-41.png1064\u00d7925 106 KB\n\nAny help would be deeply appreciated."}, {"role": "system", "text": "Yes, you can create workflows via the API, but this is probably not going to help you with this problem.\nTo create a workflow you would need to create json file that specifies the workflow steps and input connections that follows the Galaxy workflow format, but that isn\u2019t fully described / specified at this point. This is essentially what the workflow editor does for you, or any other API client (bioblend, parsec (which uses bioblend), blend4j \u2026).\nThere\u2019s ongoing work to simplify this using format 2 workflows (https://github.com/jmchilton/gxformat2), but again I don\u2019t think this has fully stabilized yet.\nSo the attached screenshot looks fine, I assume this works for you but you would like to generalize to an arbitrary number of comparisons that is driven y the input data ?\nWe\u2019ve been working on enabling this using group tags, we\u2019re not entirely there yet, but a big step towards this is using group tags and then inferring comparisons using the group tags. That last part I was planning to address once https://github.com/galaxyproject/tools-iuc/pull/2167 is merged. Doing the same for edgeR would be trivial."}, {"role": "user", "text": "Thanks for your answer.\nWould it be possible for you to point me in the direction of some examples of bioblend usages? For now I have only used the following links but I was wondering if I can find real life examples (of people also doing RNAseq for example). Also, I was reading about parsec being a wrapper of bioblend but does it actually offer more functionality than bioblend?\nhttps://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#objects-api\nhttps://bioblend.readthedocs.io/en/latest/api_docs/galaxy/docs.html#export-or-import-a-workflow\nRegarding my attached screenshot, yes that is indeed what I would like to do. For now, I am forced to use separate matrices for each egdeR group but it sounds as if your group tag features may be the solution. But this is not yet implemented in Galaxy, is that correct?"}, {"role": "system", "text": "bioblend consumes the Galaxy API, so it\u2019ll only ever be able to do what the API offers. At this moment bioblend offers a significant subset of API calls, but not all of them. Parsec in turn uses bioblend to provide functionality. Think of it as reusable recipes making use of bioblend.\nUsage documentation \u2014 BioBlend 0.16.0 documentation is the right link for importing Workflows into galaxy. You could in principle script your analysis to create a workflows (by programmatically writing JSON workflows) for whatever scenario you want to enable, but that\u2019ll be a lot of work and won\u2019t be easy to use for users.\nGalaxy can already do everything that is necessary for group tags, the tool just needs to be merged and deployed.\n\n\n\n m93:\n\nFor now, I am forced to use separate matrices for each egdeR group\n\n\nRight, that is the way to go IMO. This nicely generalizes to any datatype (multisample tables only work for tabular data). I\u2019m sorry that we haven\u2019t made more progress there, but I\u2019ll try to write this up in more details soon."}, {"role": "user", "text": "It seems it now is possible to use a single matrix as input - so need more need to have one matrix per sample.\n\nScreenshot from 2019-02-01 14-05-34.png905\u00d7258 17.7 KB\n\nHowever, I am having trouble using a single factor file for a multifactorial analysis (but I have posted a separate question about this)."}], [{"role": "user", "text": "I am trying to upload files via FileZilla onto my Human Cell Atlas instance, which I have done before without any issue as recently as a few days ago. However, now when I log on and click \u2018Upload Data\u2019 I only have an option for \u2018Choose Remote Files\u2019, which does eventually give me access to my FileZilla folders, but would involve manually selecting each file individually from subfolders. What I used to be able to do, and I can see this is still available on usegalaxy.org, is click on an option called \u2018Use FTP files\u2019, which lets me upload the entire FileZilla folder at once. The \u2018Choose Remote Files\u2019 button is now showing instead of this.\nCould you please advise how to get this \u2018Use FTP files\u2019 option back within the Human Cell Atlas instance?"}, {"role": "system", "text": "Hi @carolyn_nielsen\nThe https://humancellatlas.usegalaxy.eu/ Galaxy server is a domain-specific server hosted by the UseGalaxy.eu team. They do track this forum, plus have a Gitter channel.\nServer Directory Page: Human Cell Atlas\nI pinged them at their Gitter chat. They may reply here or at Gitter, and feel free to join the chat. usegalaxy-eu/Lobby - Gitter\nThanks!"}, {"role": "system", "text": "Hi @carolyn_nielsen,\nthe interface has changed a little bit as we now offer much more options to get your data.\n\ngrafik949\u00d7639 78.4 KB\n\nPlease see this dialog and choose the top option \u201cFTP Directory\u201d.\nHope that helps,\nBjoern"}], [{"role": "user", "text": "Hello,\nI am just getting started with developing Galaxy.\nI cloned the repository onto my local computer - MacOS running High Sierra 10.13.6.\nNext, I created a virtual environment, activated it, and then tried the run.sh script.\nA lot of things were downloaded and installed, but then the script died with this error:\nRequirement already satisfied: setuptools in ./.venv/lib/python2.7/site-packages (from sphinx==1.7.2->-r ./lib/galaxy/dependencies/dev-requirements.txt (line 32)) (41.0.1)\nUnsetting $PYTHONPATH\nActivating virtualenv at .venv\ngalaxy.tools.deps DEBUG 2019-05-30 17:32:26,193 Unable to find config file './dependency_resolvers_conf.xml'\nexecuting: .venv/bin/uwsgi --module 'galaxy.webapps.galaxy.buildapp:uwsgi_app()' --virtualenv /Users/aloraine/galaxy/.venv --pythonpath lib --threads 4 --buffer-size 16384 --http localhost:8080 --static-map /static/style=/Users/aloraine/galaxy/static/style/blue --static-map /static=/Users/aloraine/galaxy/static --die-on-term --hook-master-start 'unix_signal:2 gracefully_kill_them_all' --hook-master-start 'unix_signal:15 gracefully_kill_them_all' --enable-threads --py-call-osafterfork \nTraceback (most recent call last):\n  File \".venv/bin/uwsgi\", line 6, in <module>\n    from uwsgi import run\nImportError: dlopen(/Users/aloraine/galaxy/.venv/lib/python2.7/site-packages/uwsgi.so, 2): Library not loaded: /usr/local/opt/pcre/lib/libpcre.1.dylib\n  Referenced from: /Users/aloraine/galaxy/.venv/lib/python2.7/site-packages/uwsgi.so\n  Reason: image not found\n\nThe python I am running in my virtual environment (called \u201cgalaxy\u201d):\n(galaxy) $ python\nPython 2.7.10 (default, Oct 6 2017, 22:29:07)\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)] on darwin\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.\nI used pip to install wsgi in my environment, and it seems to have worked:\n(galaxy) $ pip install uwsgi\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won\u2019t be maintained after that date. A future version of pip will drop support for Python 2.7.\nRequirement already satisfied: uwsgi in /Users/aloraine/.virtualenvs/galaxy/lib/python2.7/site-packages (2.0.18)\nTips on what to do next would be great!\nMany thanks,\nAnn Loraine"}, {"role": "system", "text": "The most current Galaxy release can always be found here:\nhttps://docs.galaxyproject.org/en/master/releases/index.html\n\nExample step-by-step for a basic local Galaxy install with an admin account created\n\nGo to home directory using Terminal on MAC OSX\n\n\ncd\n\n\nSet up a place to keep the local galaxy. I often need to do testing with fresh checkouts, so tend to use the version as the dir name. Wherever you put/name this, avoid nesting the location too much, or the path will get long and possibly present with Conda errors later on (it doesn\u2019t play well with longer paths).\n\n\n$ mkdir 1901\n\n\n$ cd 1901\n\n\nClone Galaxy\n\n\n$ git clone -b release_19.01 https://github.com/galaxyproject/galaxy.git\n\n\nLet that run to completion ^^. Then go into the galaxy dir.\n\n\n$ cd galaxy\n\n\nSet up the virtualenv\n\n\n$ virtualenv -p /usr/bin/python .venv\n\n\nInitial startup of Galaxy\n\n\n$ sh run.sh\n\n\n\nThen that runs using all default settings, ending with the server hosted at the local URL. Will then create my \u201cadmin\u201d account in that web browser (but it won\u2019t be admin yet!).\n\n\nAfter that will shut it down, change the galaxy.yml to become admin, start it up again, and log into my account in the web browser to make sure that I set up the admin config correctly.\n\n\nMore can be done after that to customize your instance.\n\n\nI hope this works for you, too \n\n\n\nYou probably know where these resources are, but for others reading:\n\nhttps://galaxyproject.org/admin/get-galaxy/\nhttps://docs.galaxyproject.org/en/master/admin/index.html\nhttps://github.com/galaxyproject/dagobah-training\n\nIn particular, have a look at:\n\nhttps://docs.galaxyproject.org/en/master/admin/framework_dependencies.html\n"}], [{"role": "user", "text": "My workflow does not execute any step after being successfully invoked. Under \u201cStep scheduling\u201d there is written: Loading step state summary and under \u201cJob execution\u201d there is written: Loading job summary. Nothing has changed for over 40 hours and the single steps that are included in workflow do work properly on my input dataset, so there is no mistake in the workflow nor in the input data. It just does not \u201ckick off\u201d.\nPlease help me,\nMonika"}, {"role": "system", "text": "Welcome, @Monika\nIs this still a problem? If so, please review the help in the prior posts linked below.\nThe most important parts are:\n\nMake sure the workflow includes \u201cinputs\u201d\nDisconnect/reconnect the \u201cnoodles\u201d between tools when an existing \u201cinput\u201d type or a tool parameter is modified, or if \u201cinput(s)\u201d are added in.\n\nConnect the noodles starting from the inputs through downstream tools (left side of the workflow canvas to the right side). This will reset metadata/logic internal to the workflow.\n\n\n\n\nSave a copy of your original workflow back before making more edits, in case you need to start over again for some reason.\n\nNote: Not all \u201cinput\u201d changes/additions or tool parameters changes can trigger workflow execution problems. The steps above are where to start and resolve workflow execution problems in most cases.\nPlease review the prior help for more details, add inputs/reconnect noodles first, test it, and we can troubleshoot from there if still a problem. How to stage and share the history + workflow involved is included in the second post. Share links can be sent into the mailing list or in a direct private message here, or you can post it publically as a reply (your choice). I\u2019m an admin at both. Never share your account password \u2013 administrators will never ask for or need it.\n\n  \n    \n    \n    Workflow is not operating on a collection usegalaxy.org support\n  \n  \n    Hi, \nI\u2019m trying to use an RNAseq analysis workflow on a collection constructed by NCBI fastq dump of paired-end reads. \nMy workflow involves Trim Galore!, FastQC, HISAT2 and featurecounts; all I believe set to the needed settings to deal with paired-end collections. However despite the workflow claiming to be invoked nothing happens. Similar workflows just for paired-end files will work however the workflow is not operating for collections. I can use the programs individually on the collection a\u2026\n  \n\n\n\n  \n    \n    \n    Workflow doesn't execute all steps after successfully being invoked. \n  \n  \n    Hi, \nI\u2019ve recently created a workflow for my ChIP-Seq analysis. I used the workflow to analyse new experiments and while there are 5 steps, only 3 steps are executed. I dont even see the other steps at all in the history. \nAny suggestions on how to resolve this would be greatly appreciated. \nThanks, \nKris\n  \n\n\nThanks!"}, {"role": "system", "text": "Good day, I am having the same problem. I run a workflow on a collection and the new history is empty. I already add Input at the beginning of the workflow, I chose type list:paired, I reconnected the noodles, I try sending more than once the workflow, but still the history is empty. How can I know that the workflow is running, I waited but it is the same. After running the workflow it stucks here.\nWorkflow Invocation State  (invoked at 2020-06-21T17:49:17.581194)\nStep Scheduling\nLoading step state summary\u2026\nJob Execution\nLoading job summary\u2026\nWhat can I do?\nThis is the link to the workflow\nhttps://usegalaxy.org/u/sguauque/w/metagenomics-healthy-workflow\nThank you"}, {"role": "system", "text": "Update: Hi, I fixed the data type in fastqtosam and now the workflow is working but just with a list of paired dataset collections contaning just one sample, It is not working for a list of dataset collection with several samples. Also, I have to send the job more than oncec until finally it could populate the history of results."}, {"role": "system", "text": "I had the same issue. I came to this thread after google search. I had to restart the galaxy server and all the workflow steps were loaded and ran fine."}], [{"role": "user", "text": "Hello everyone!\nI tried running a basic workflow of Quality Control and Mapping for RNAseq, with Cutadapt, RNA star and HTseq, with FASTQC/MULTIQC and qualimap. But everytime I try to run it, it gives me this error and Cutadapt does not run\nimage1305\u00d7685 28.5 KB"}, {"role": "system", "text": "Hi @GACG1\nPlease try a rerun \u2013 this looks like a server-side problem. Both usegalaxy.org and usegalaxy.eu had transient issues yesterday.\nhttps://galaxyproject.statuspage.io/\nThis feature is somewhat new, a new release of Galaxy itself is pending, and this may be related to prior known but soon-to-be-fixed issues when Galaxy upgrades. I would also suggest reviewing the work that completed already \u2013 were the inputs intact (contain successful, non-empty, content)?\nIf that fails again (is reproducible, with upstream data confirmed), we can followup more from there. My guess would be that there is a problem within the workflow itself \u2013 are you really inputting two different collection types into the same tool at the same step? That probably won\u2019t work for a few reasons. But we can review the details after the rerun/data check.\nThanks!"}, {"role": "user", "text": "After rechecking, it seems like cutadapt is somehow the problem. I can run it, with the same configurations, without using workflow. But when I use the workflow, it always fails.\nOne important point is that every time I try do edit my workflow, the connection between the paired list and cutadapt disapears.\nIn this image, both are cutadapt with exactly the same configs, but the one on the left I just added to the workflow1920\u00d7937 149 KB\nAnother example, I just tried to run this and it couldn\u2019t feed the paired list to cutadapt. When I open it to edit, cutadapt always disconnect from the list:paired input, even though I can run it without the workflow (with same configs)\nimage873\u00d7790 119 KB"}, {"role": "system", "text": "hi @GACG1\nThanks for the screenshots. I had forgotten that there are some known issues with Cutadapt and paired-end collections when used in workflows that have not been resolved yet (to my knowledge, did start a test just now to check, still scheduling/running).\nTrimmomatic or fastp are commonly used alternatives that both function with this type of input within a workflow. You may need to use one of those instead.\nThis is the part of your screenshot that explains what problem came up before that prior issue would manifest. And knowing how to address it will apply to many other use cases.\n\nSummary: Workflows contain metadata about the relationships between inputs, tools, and tool output (which are often inputs to downstream tools). When the workflow\u2019s internal metadata doesn\u2019t match up, the solution is to disconnect all of the workflow\u2019s \u201cnoodles\u201d then reconnect them.\nStart with the initial \u201cinputs\u201d and connect to downstream tools in the order of execution (\u201cleft to right\u201d within the editor).\nIn our test, connecting a paired-end collection list wasn\u2019t a problem, so you should be able to get that far following the advice above. Then try to run the workflow. If Cutadapt fails or stalls again, then you\u2019ll need to pick a different trimming tool.\nIt is known that the \u201cdisconnect/reconnect\u201d usage is not ideal \u2026 but also that it is required for now. This tends to show up when new inputs or adjusted input types are added to existing workflows.\nYour problem is a bit more complicated since it involves adjusting the workflow AND it still might fail given the prior known issues with Cutadapt. But you can certainly try the workflow metadata reset, rerun, and see what happens before deciding to incorporate a different read trimmer tool.\nHope that helps with some options to get this going. Even if Cutadapt needs more changes, and that is prioritized, the updated tool wouldn\u2019t be immediately available. This has been an open issue for over a year, isn\u2019t a simple fix, and there are functional alternative tools available.\nBest"}, {"role": "system", "text": "Update:\nTurns out that Cutadapt is working fine in a workflow now. Based on that result, reconnecting the workflow \u201cnoodles\u201d will likely solve your entire issue as well. Make sure you are incorporating the most current tool version (all steps/tools) to avoid older-but-fixed problems.\nTest history and workflow if you want to compare your work to our test. I\u2019ll leave this shared for a while, may help others, too.\n\nGalaxy | Accessible History | test cutadapt listpaired\nGalaxy | Accessible Workflow | Workflow constructed from history 'test cutadapt listpaired'\n"}], [{"role": "user", "text": "Has anybody had issues running goseq on galaxy? the first time I ran goseq it failed after 3 days (error message said it exceeded max. run time) so I fixed an issue with my dataset and hit execute again. It is still running and has been for 20 hours? Is there an issue I am missing or something I can do to speed it up? Thanks!"}, {"role": "user", "text": "using public galaxy main"}, {"role": "system", "text": "Hi @Niamh\nThanks for letting us know which server you are working at: Galaxy Main https://usegalaxy.org.\nThere were some busier server days over the last few weeks but it doesn\u2019t seem like that is your issue. Instead, the problem is most likely with the inputs (format or content).\n\nhttps://usegalaxy.org/ -- Server issues Feb 25, 2020 Resolved\n\nMy job ended with an error. What can I do? >> https://galaxyproject.org/support/tool-error/#type-execution-exceeds-maximum-allowed-job-run-time-walltime\n\n\nResolving input issues\nThere are three core inputs to this tool. Each is described in the tool form. It is very important that the format of each matches the examples. In particular, the first column (gene identifier) must be in the same format for all three of the inputs.\nMeaning, if any contain extra content, such as a trailing .N after the gene name (where N is a number, specifically the version), the \u201cdot\u201d and \u201cversion-number\u201d should be either present in all inputs, or better in most cases, removed from all inputs.\nPlease give this a review in your particular data. If all seems correct, then allow the job to fully process. If any are a mismatch, make corrections and rerun. If the job still ends with another error once the inputs seem OK, three options:\n\nPost back a few lines (2-3) from each of the three inputs. Please try to preserve formatting by quoting the text.\nGenerate a share link to the history and post that back. Make sure that all inputs and outputs are in an undeleted state, and that \u201cobjects\u201d in the history are shared.\nSend in a bug report from one of the failed datasets (\u201cbug icon\u201d in the red error dataset), then post back that you sent one in so we know when to look for it. Be sure to include a link to this Galaxy Help post in the comments so we can find it quickly and associate it with your question here.. If you already sent one in, go ahead and send it in again with the link, then let us know.\n\nFor any choice, you can post the information publically in this thread or post back to our direct message thread. Either is fine, it just depends on your comfort level with making a portion or all of your data public (for expanded community help with troubleshooting).\nIf you choose to keep this private, we won\u2019t post anything back public that is private, just summarize the usage issue and how to address it, in general terms, IF it will help others. This tool and the tutorial that includes it do not have any current known issues - but if one is uncovered, we can ticket/address that with anonymous data as well.\nLet\u2019s start there. Thanks!"}, {"role": "user", "text": "Gene differential expression file:\n\n\n\n\n1\n2\n\n\n\n\nENSSSCG00000024911\nTrue\n\n\nENSSSCG00000023305\nTrue\n\n\nENSSSCG00000023684\nTrue\n\n\n\ngene length file\n\n\n\n\n1\n2\n\n\n\n\nGENEID\nLength\n\n\nENSSSCG00000048769\n3130\n\n\nENSSSCG00000037372\n1717\n\n\nENSSSCG00000027257\n899\n\n\n\nand I currently have no gene category file as any annotations I have downloaded have given me errors. If there is anyone who knows which go annotation file might work properly that might fix the issue.\nhere is a link to my data, I had previous steps in a different history including mapping and alignment steps and QC but this is the history for DeSeq2 and GoSeq\nhttps://usegalaxy.org/u/niamh98/h/deseq2-research-project\nthanks "}, {"role": "system", "text": "Hi @Niamh\nTry Biomart. The query will look something like the screenshot below. Export/download in TSV format (tabular) from this site then update the file to Galaxy.\n\nOnly these two columns (Gene and GO term) should be input to the Goseq tool for the \u201cCategories\u201d reference file.\nDo not input any lines (genes) that do not have a GO term. Remove those first. There are a few ways to filter out genes that do not have a GO assignment, this is one:\n\nTool Select lines that match an expression\n\nOption: Matching\n\nRegular expression: ^E.+\\sG.+$\n\n\n\nMore related data can be exported if you want, for later use.\n\nTools like Cut and Join and others in the Data Manipulation tool group can be used.\n\n\n\n\n\nensembl-to-go-terms-biomart2172\u00d71108 316 KB\n\nThanks!"}], [{"role": "user", "text": "Hi all,\nWhen using the write unaligned reads (in fastq format) to separate file(s) option in the Bowtie2 tool (triggering the --un-conc parameter) on a paired-end library, I get two unaligned read output files: unaligned reads L and unaligned reads R. I assume that these separate files are generated to distinguish which file contains the unaligned reads from the R1 and R2 input files but can anyone clarify which file (L or R) refers to which input file (R1 and R2)?\nThanks!\nminor admin reformat for clarity"}, {"role": "system", "text": "Hi @Jroels\nUnmapped L reads are from the R1 input (first input on the tool form)\nUnmapped R reads are from the R2 input (second input on the tool form)\nThe read names can confirm this.\nThanks!"}], [{"role": "user", "text": "Hello,\nI submitted a job to unicycler over 24 hours ago to the main USA galaxy server and it is still grayed out waiting to run. How long does this typically take?\nI am prepping for a course I am teaching and I was going to have my students do the \u201cUnicycler assembly of SARS-CoV-2 genome with preprocessing to remove human genome reads\u201d tutorial and I want to ensure they have enough time to perform it."}, {"role": "user", "text": "After 4 days and no responses I purged my history and am attempting again. I also downloaded the fastq results up to that point to run locally in ubuntu. I had already attempted this before but got a spades error.\nIf anyone out there can help me I would greatly appreciate it. I would love my students to perform a de novo assembly."}, {"role": "user", "text": "Here is an link to my history.\nhttps://usegalaxy.org/u/mgh/h/unicycler-assembly-stand-alone"}, {"role": "system", "text": "@mharpole have seen your post here only now. Read your corresponding message in the Galaxy Training Network chat, but your issue wasn\u2019t so clearly described there.\nI cannot comment on Unicycler jobs on usegalaxy.org. There may indeed be a long queue you are in there, though 4 days is really long.\nWhat I do know is that people have followed that very same tutorial successfully on usegalaxy.eu last week so maybe you have more luck there.\nIn general, however, it is a good idea when planning to do a tutorial like this with a challenging tool like Unicycler (I think the training material mentions this bottleneck explicitly) to create an example history before the actual course.\nDuring the course let participants perform all steps up to that challenging one, but then explain them what computational complexity is hiding behind that next step and that this cannot be scaled reliably to a large number of participants. At this point, you can share your prepared history with them, they can copy the dataset they need from yours into their history, and continue.\nPlease also note that usegalaxy.eu offers TIaaS, which makes trainings a lot more reliable though in the end it\u2019s always good to have a strategy for when things don\u2019t work as they should."}], [{"role": "user", "text": "Hi everyone, I\u2019m trying to run LoFreq to call variants on sorted BAM, with default settings.\nI\u2019ve used it for long time and it\u2019s always worked fine\nSince a couple days it keeps crashing with this error message:\n\u201ccat: can\u2019t open \u2018/jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallel*/*.log\u2019: No such file or directory\u201d\nany clue? what\u2019s wrong?\nthx\nLuca"}, {"role": "system", "text": "Hi @LUCA_MOLOGNI\nmaybe submit error report: click at any output of failed job, click at error icon, the one looking like ladybird bug, in the middle window provide any info relevant to the failed job (optional but helpful to support people) and hit Report.\nKind regards,\nIgor"}, {"role": "user", "text": "Dear Igor,\nthank you for your reply.\nHere is the error report from Galaxy\nGalaxy Tool Error Reportfrom https://usegalaxy.org/### Error Localization\n\nDataset | 94921136 (bbd44e69cb8906b55884c1d3dfb7be88) |\n\n| - |\nHistory | 3916736 (932e860747bb9352) |\nFailed Job | 973: Call variants on data 947 (f9cad7b01a47213560aa91eeda719002) |\n\n\n\n\nDetailed Job InformationJob environment and execution information is available at the job info page.\n\nJob ID | 51231981 (bbd44e69cb8906b573341c47583a8294) |\n\n| - |\nTool ID | toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy2 |\nTool Version | 2.1.5+galaxy2 |\nJob PID or DRM id | 51231981 |\nJob Tool Version | 2.1.5 |\n\n\nJob Execution and Failure Information#### Command Line\nln -s '/jetstream2/scratch/main/jobs/51231981/inputs/dataset_1fd63bdb-91c2-4d7c-b0b2-9bf1c5e03474.dat' reads.bam && ln -s -f '/jetstream2/scratch/main/jobs/51231981/inputs/metadata_metadata_254480b5-65fa-4c3f-8c89-652a6bea7a13.dat' reads.bam.bai &&   lofreq call-parallel --pp-threads ${GALAXY_SLOTS:-1} --verbose  --ref '/cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa)' --out variants.vcf    --sig 0.01 --bonf dynamic   reads.bam 2>&1  || (tool_exit_code=$? && cat \"$TMPDIR/lofreq2_call_parallel*/*.log\" 1>&2 && exit $tool_exit_code)  && echo set_lofreq_standard\n\n\nstderr\ncat: can't open '/jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallel*/*.log': No such file or directory\n\n\nstdout\nINFO [2023-06-27 17:09:41,198]: Using 8 threads with following basic args: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam\n\nINFO [2023-06-27 17:09:41,222]: Adding 27 commands to mp-pool\nCRITICAL [2023-06-27 17:09:41,287]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr8:1-145138636\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/10.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/10.log 2>&1\nCRITICAL [2023-06-27 17:09:41,288]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr6:1-170805979\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/8.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/8.log 2>&1\nCRITICAL [2023-06-27 17:09:41,289]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chrX:1-156040895\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/25.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/25.log 2>&1\nCRITICAL [2023-06-27 17:09:41,289]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr4:1-190214555\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/6.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/6.log 2>&1\nCRITICAL [2023-06-27 17:09:41,289]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr7:1-159345973\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/9.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/9.log 2>&1\nCRITICAL [2023-06-27 17:09:41,289]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr11:1-135086622\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/13.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/13.log 2>&1\nCRITICAL [2023-06-27 17:09:41,290]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr5:1-181538259\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/7.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/7.log 2>&1\nCRITICAL [2023-06-27 17:09:41,292]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr9:1-138394717\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/11.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/11.log 2>&1\nCRITICAL [2023-06-27 17:09:41,302]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr10:1-133797422\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/12.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/12.log 2>&1\nCRITICAL [2023-06-27 17:09:41,303]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr2:1-121096764\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/2.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/2.log 2>&1\nCRITICAL [2023-06-27 17:09:41,303]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr12:1-133275309\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/14.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/14.log 2>&1\nCRITICAL [2023-06-27 17:09:41,304]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr2:121096765-242193529\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/3.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/3.log 2>&1\nCRITICAL [2023-06-27 17:09:41,304]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr1:124478212-248956422\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/1.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/1.log 2>&1\nCRITICAL [2023-06-27 17:09:41,306]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr13:1-114364328\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/15.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/15.log 2>&1\nCRITICAL [2023-06-27 17:09:41,307]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr14:1-107043718\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/16.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/16.log 2>&1\nCRITICAL [2023-06-27 17:09:41,314]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr15:1-101991189\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/17.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/17.log 2>&1\nCRITICAL [2023-06-27 17:09:41,314]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr1:1-124478211\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/0.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/0.log 2>&1\nCRITICAL [2023-06-27 17:09:41,315]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr3:99147780-198295559\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/5.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/5.log 2>&1\nCRITICAL [2023-06-27 17:09:41,318]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr3:1-99147779\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/4.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/4.log 2>&1\nCRITICAL [2023-06-27 17:09:41,319]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr16:1-90338345\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/18.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/18.log 2>&1\nCRITICAL [2023-06-27 17:09:41,322]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr20:1-64444167\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/22.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/22.log 2>&1\nCRITICAL [2023-06-27 17:09:41,322]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr18:1-80373285\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/20.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/20.log 2>&1\nCRITICAL [2023-06-27 17:09:41,324]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr17:1-83257441\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/19.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/19.log 2>&1\nCRITICAL [2023-06-27 17:09:41,328]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr19:1-58617616\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/21.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/21.log 2>&1\nCRITICAL [2023-06-27 17:09:41,329]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chrY:1-57227415\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/26.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/26.log 2>&1\nCRITICAL [2023-06-27 17:09:41,329]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr22:1-50818468\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/24.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/24.log 2>&1\nCRITICAL [2023-06-27 17:09:41,332]: Following command failed with status 1: lofreq call --verbose --ref /cvmfs/[data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa](http://data.galaxyproject.org/byhand/hg38/sam_index/hg38.fa) --sig 0.01 --bonf dynamic reads.bam --no-default-filter -r \"chr21:1-46709983\" -o /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/23.vcf.gz > /jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallelgo23nzea/23.log 2>&1\nCRITICAL [2023-06-27 17:09:41,336]: Some commands in pool failed. Can't continue\n\n\nJob Information\nNone\n\n\nJob Traceback\nNone\n"}, {"role": "system", "text": "Hi @LUCA_MOLOGNI\nThere seems to be a server side problem at UseGalaxy.org.\nWe\u2019ll get back to you here once we know more, but maybe also try a rerun since we are still exploring the scope?"}, {"role": "system", "text": "We\u2019ve had a misconfiguration of the temporary directories for containerized jobs on usegalaxy.org. It should be resolved now, please try running again. Sorry for the inconvenience and thanks for using Galaxy."}], [{"role": "user", "text": "Hi everyone, I am trying to run MimodD extract variant sites tool with default settings in Galaxy.org and I got the below error for a week ago. I have used this tool many times and it has worked\u2026\nFatal error: Exit code 127 ()\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 10: mimodd: command not found\nI first aligned my fastq file using Map with BWA-MEM tool, then ran FilterBAM, MarkDuplicates, CleanSAM, Realign reads, MimodD Variant calling, and then tried to MimodD extract variant sites.\nI re-ran it several times, but got same error, tried to run this tool with the data that worked in the past, but got same error now.\nCould you help me?\nThank you!\nHere is the bug report\nDetailed Job Information\nJob environment and execution information is available at the job info page.\n\n\n\n\nJob ID\n51284803 (bbd44e69cb8906b5ca7ad95501fbcc85)\n\n\n\n\nTool ID\ntoolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varextract/0.1.9\n\n\nTool Version\n0.1.9\n\n\nJob PID or DRM id\n42890615\n\n\nJob Tool Version\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 9: mimodd: command not found\n\n\n\nJob Execution and Failure Information\nCommand Line\nmimodd varextract \u2018/corral4/main/objects/3/a/b/dataset_3ab6f1f4-e4f9-4f97-9ef1-f70de1dbe8c5.dat\u2019 --ofile \u2018/corral4/main/objects/6/0/6/dataset_6069d9c4-28b2-40b3-868f-cba502a72c30.dat\u2019 --verbose\nstderr\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 10: mimodd: command not found\nstdout\nJob Information\nNone\nJob Traceback\nNone"}, {"role": "system", "text": "@nate I\u2019ve filed a bug report about this same problem. Can you take a look if you have time?\n@sen_99 there is nothing you\u2019ve been doing wrong. It\u2019s a problem with the tool installation on the server.\nThanks for reporting!"}, {"role": "user", "text": "Thank you! I was relieved.\nAs he says, I will try at EU for now.\n  \n    \n    \n    MiModD Troubleshooting usegalaxy.org support\n  \n  \n    Hi @WangNIE \nWe have confirmed the problem at the US server. Please try at EU or AU for now. \nConsolidating this this post, where we will post updates: MimodD extract variant sites \nThanks for the followup!\n  \n\n"}, {"role": "system", "text": "This should now be working at usegalaxy.org again. Sorry for the trouble, and please let us know if you continue to encounter issues."}], [{"role": "user", "text": "We\u2019ve been experiencing persistent error pasted below with our lab\u2019s cloud-based installation of the Galaxy suite via the Genomics Virtual Laboratory 4.4.0.  Specifically, whenever we attempt a Trinity de novo assembly of a collection of Illumina TruSeq stranded mRNA paired end reads, we get the errors below.  We\u2019ve had no problems with these same data sets when using other tools like HiSat2 and StringTie.   We\u2019ve poked around in the \u201cmanage tools\u201d and \u201cmanage dependencies\u201d area of the Galaxy Admin interface and updated to the latest version of Trinity and checked dependencies, and all seems to be OK; however, we must confess to being rank amateurs.  Can anyone point us toward some other things we might try?\n\nFatal error: Exit code 2 ()\nPossible unintended interpolation of @2 in string at\n/mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 46.\n\n\nTool Errors\nInternal Tool Error log\n\n\n\n\nTime\nPhase\nFile\nError\n\n\n\n\n2019-02-14 03:03:05.105596\nTool Loading\n./tools/phenotype_association/sift.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/sift_db.loc\u2019\n\n\n2019-02-14 03:03:05.104204\nTool Loading\n./tools/evolution/add_scores.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/add_scores.loc\u2019\n\n\n2019-02-14 03:03:05.103428\nTool Loading\n./tools/evolution/codingSnps.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/codingSnps.loc\u2019\n\n\n2019-02-14 03:03:05.100147\nTool Loading\n./tools/extract/liftOver_wrapper.xml\n\u2018liftOver\u2019\n\n\n\n\n\nAlso found additional error information via the Galaxy | Reports interface:\n> Error, 1 threads errored out at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/opt/trinity-2.8.4/util/insilico_read_normalization.pl line 963.\n> Error, cmd: /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/opt/trinity-2.8.4/util/insilico_read_normalization.pl --seqType fq --JM 1G  --max_cov 200 --min_cov 1 --CPU 4 --output /mnt/galaxy/tmp/job_working_directory/000/104/working/trinity_out_dir/insilico_read_normalization   --max_pct_stdev 10000  --SS_lib_type FR  --left /mnt/galaxy/files/000/dataset_64.dat,/mnt/galaxy/files/000/dataset_67.dat,/mnt/galaxy/files/000/dataset_70.dat,/mnt/galaxy/files/000/dataset_73.dat --right /mnt/galaxy/files/000/dataset_65.dat,/mnt/galaxy/files/000/dataset_68.dat,/mnt/galaxy/files/000/dataset_71.dat,/mnt/galaxy/files/000/dataset_74.dat --pairs_together --PARALLEL_STATS   died with ret 6400 at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 2689.\n> \tmain::process_cmd(\"/mnt/galaxy/tool_dependencies/_conda/envs/__trinity\\@2.8.4/opt\"...) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 3235\n> \tmain::normalize(\"/mnt/galaxy/tmp/job_working_directory/000/104/working/trinity\"..., 200, ARRAY(0x55a0498484e8), ARRAY(0x55a0498484d0)) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 3182\n> \tmain::run_normalization(200, ARRAY(0x55a0498484e8), ARRAY(0x55a0498484d0)) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 1319\n"}, {"role": "system", "text": "This looks like a tool wrapper bug (conda path quoting).\nIt might present in some Trinity tool wrapper versions and not in others.\nGVL 4.4.0 was released in September 2018 and is based on the Galaxy release 18.05. The most current version of the Trinity wrapper from the IUC was released in October 2018. The most current Galaxy version is now 19.01.\nI suggest updating the Trinity wrapper to the most current IUC version and testing that out. If the latest version still presents with the problem, please write back and we can help to get a ticket opened to review the issue, learn the (actual, not guessed) root cause, and get it fixed."}], [{"role": "user", "text": "Hello,\nIve been trying to download a large collection ~32GB of alignments, and despite attempts, ive not been able to download more than a GB before it cuts out. this includes browser and wget options.\nAny advice?"}, {"role": "user", "text": "I should have mentioned, this is the public usegalaxy.org server, not a local one"}, {"role": "system", "text": "Hello @Michael_Ridley\nFor large datasets, dataset collections and/or slower connections, use either curl or wget to download data. The exact commands are in the FAQ below.\nIf using MAC OSX, use the Terminal utility to get command-line access. If using a different OS, you can google around for the options.\nFAQ: https://galaxyproject.org/support/\n\nDownloading Data\n\nHope that helps!"}, {"role": "user", "text": "Hello Jennaj,\nEven using wget/curl it cuts out after almost exactly 1Gb, is there another command to specify for it keep going, or is this likely to be a network issue?"}, {"role": "system", "text": "Yes, this could be a network issue. A slower connection can drop. Your internet provider could have download limits. Other connection issues are possible but those are two most common reasons we have had reported when this has come up before.\nTry adding this argument to your wget string: -c\nFor more explanations, Google \u201cresume curl\u201d or \u201cresume wget\u201d for much discussion. Plus, the man page for each tool will have official argument descriptions. Both can resume but in slightly different ways that can also differ by OS. You\u2019ll need to test what works for you. If none work, confirming what your IP provider data transfer/time limits are is the next step.\nExample, this Q&A gives a good line-liner to \"resume transfer for curl\"."}], [{"role": "user", "text": "I have worked through the Single-Batch Processing hand-on tutorial three times and consistently get different results than what the tutorial says. After the UMI-tool count (last step of the Single-Batch Processing), the tutorial states there should be 2,140 lines in the matrix and 180 columns, but I am consistently getting 1,972 lines and 50 columns.\nIs anyone else experiencing this?\nDoes anyone have any ideas why this is happening? I made sure to use the Tutorial Mode on galaxy, so when I would select the next step it took me to the appropriate tool and version."}, {"role": "system", "text": "Hi @swren,\nI contacted the people who developed the training; probably they will provide you an answer in a few days.\nBest regards!"}, {"role": "system", "text": "Hi @swren, thanks for trying out the training, could you share your history with me so that I might be able to debug it? My user-name is \u201cmehmet-tekman\u201d\nI will be out of office for a few days, but will happily look at your history when I am back."}, {"role": "user", "text": "Hi @mtekman, thank you for reaching out! Here is the link to my history: Galaxy | Europe | Accessible History | Preprocessing of scRNA Data\nI have two tries on this history, and the second try is the one that I used tutorial mode for the links to the tools.\nThank you!"}, {"role": "system", "text": "Dear @swren - I am back from holiday, thanks for sharing your history.\nCan you try the analysis again, this time using slightly different input FASTQ files:\nhttps://zenodo.org/record/3253142/files/SRR5683689_1.fastq\nhttps://zenodo.org/record/3253142/files/SRR5683689_2.fastq\n\n(note the lack of \u201c_subset\u201d prefix)"}], [{"role": "user", "text": "I am trying to connect to galaxy via filezilla but i am getting this error message:\n\n|Status:      |Resolving address of usegalaxy.org|\n\n|\u2014|---|\n|Status:      |Connecting to 129.114.60.60:21\u2026|\n|Status:      |Connection established, waiting for welcome message\u2026|\n|Status:      |Initializing TLS\u2026|\n|Status:      |Verifying certificate\u2026|\n|Status:      |TLS connection established.|\n|Status:      |Logged in|\n|Status:      |Retrieving directory listing\u2026|\n|Command:|PWD|\n|Response: |257 \u201c/\u201d is the current directory|\n|Command:|TYPE I|\n|Response: |200 Type set to I|\n|Command:|PASV|\n|Response: |227 Entering Passive Mode (129,114,60,60,118,163).|\n|Command:|MLSD|\n|Error:        |Connection timed out after 20 seconds of inactivity|\n|Error:        |Failed to retrieve directory listing|\n|Status:      |Disconnected from server|\n|Status:      |Resolving address of usegalaxy.org|\n|Status:      |Connecting to 129.114.60.56:21\u2026|\n|Status:      |Connection established, waiting for welcome message\u2026|\n|Status:      |Initializing TLS\u2026|\n|Status:      |Verifying certificate\u2026|\n|Status:      |TLS connection established.|\n|Status:      |Logged in|\n|Status:      |Retrieving directory listing\u2026|\n|Command:|PWD|\n|Response: |257 \u201c/\u201d is the current directory|\n|Command:|TYPE I|\n|Response: |200 Type set to I|\n|Command:|PASV|\n|Response: |227 Entering Passive Mode (129,114,60,56,120,136).|\n|Command:|MLSD|\n|Error:        |Connection timed out after 20 seconds of inactivity|\n|Error:        |Failed to retrieve directory listing|\n"}, {"role": "system", "text": "Hello @tamerbio\nPlease try again if you have not already. FTP at usegalaxy.org is working from remote locations now, and the server didn\u2019t have any known issues around this function when you wrote in. But, perhaps the connection was poor. Or, you did not click to accept the secure certificate pop-up before the connection timed out?\nFAQ: https://galaxyproject.org/ftp-upload/\nThanks!"}, {"role": "user", "text": "Hi Jennaj,\nThank you so much for your reply. I check FileZilla on Mac and Windows OS and still failed to log into FTP."}, {"role": "system", "text": "Hi @tamerbio\nThe service is definitely up.\n\n\n\n jennaj:\n\nFAQ: https://galaxyproject.org/ftp-upload/\n\n\nThe FAQ has examples, but this is some more troubleshooting help that usually solves problems:\n\n\nMake sure you are using the latest version of Filezilla. The tool updates quite frequently and will prompt you to update when the app is opened. Those updates install quickly. I tend to just replace my current version with the update \u2013 but I also don\u2019t set up any special configurations (on purpose) \u2013 so I can\u2019t tell you if those are preserved or not after an update with the latest version. You could test it out or review/ask for help from those app developers if this is a concern. There are many FTP clients, and it can be used command-line \u2013 the FAQ I sent has more help about that.\n\n\nTry using all default settings. The connection will still be secure when connecting to usegalaxy.org. You\u2019ll be prompted to accept the security certificate when the connection starts up.\n\n\nMake sure to connect with your Galaxy registered email address as your Filezilla \u201cusername\u201d. Your Galaxy \u201cpublic name\u201d won\u2019t connect. The \u201cpassword\u201d is the same as used when logging into that same exact Galaxy server for that same account.\n\n\nThe FTP server name aka \u201chost\u201d will vary by the Galaxy server \u2013 sometimes it is the server\u2019s base URL (and that is true for \u201cusegalaxy.org\u201d) \u2013 and sometimes it is different. What to use is likely posted within the Upload tool for the server in the button > pop-up for the FTP function (tool used move FTP\u2019d files into a history as datasets). Another place to check is the server\u2019s homepage. If you cannot find it \u2013 you can ask here (we might know based on the public Galaxy server\u2019s URL) or you can contact those server administrators directly \u2013 not all follow this forum. Contact information for public Galaxy servers is often on the server\u2019s home page and/or in their directory listing here: https://galaxyproject.org/use/\n\n\nDo not enter a \u201cport\u201d for most cases (including usegalaxy.org). Advanced configuration is different \u2013 and the FAQ linked before has details about that. But you don\u2019t need to do that \u2013 defaults work fine for most people.\n\n\nIf the data transfer stalls out while in progress, which files failed is noted near the bottom of the Filezilla app in the tabs. Timeouts can happen with slower connections or larger datasets and it is possible to resume the transfer where it left off within Filezilla. Note that this will usually prompt another security certificate pop-up. Click to accept each time.\n\n\nIf you are transferring many files and having problems, try transferring less files at a time. The transfer will only go as fast as your internet connection allows. University and business wifi connection speed is usually much faster than home wifi or cellular connection speeds. For personal internet/cell, most ISPs maximize \u201cdown\u201d transfer speeds \u2013 and \u201cup\u201d data transfers can be much slower \u2013 for the same service. There are many \u201cspeedtests\u201d available \u2013 an internet search will find them if you want to check yours.\n\n\nSome universities or businesses block sending data outside of the \u201cintranet\u201d entirely and others may only block some external sites for security purposes. If you think that usegalaxy.org is being blocked, contact your local administrators.\n\n\nSome home/cell ISPs may also block or throttle where and/or how much data can be transferred. Total or within a given time window. Contact your ISP for help if you think that is the problem.\n\n\nReview prior Q&A about FTP at this forum. Search with the keyword \u201cftp\u201d to find those topics. You can also review prior Q&A at our older forum. The second post in this search is an identical issue to what you have, and using \u201call defaults\u201d solved their problem: https://galaxyproject.org/support/ >>  Galaxy Biostars: retired Q&A archive >> add \u201cftp\u201d (or \u201cmsld\u201d, part of the error message) into the keyword search already including \u201cbiostar\u201d >> https://galaxyproject.org/search/?q=biostar+MLSD#gsc.tab=0&gsc.q=biostar%20MLSD&gsc.page=1 >> https://biostar.usegalaxy.org/p/24134.1.html\n\n\nHope that helps! I don\u2019t think everything was in one post before, so maybe will help others too."}], [{"role": "user", "text": "Hi all,\nI was trying to run a pipeline these days. Unfortunately, I don\u2019t know the strandedness of the sample (fastq files) I have. Is there any way to check this? I tried already to convert my fastqs to BAMs with bowtie and then use Infer Experiment to check for that. I was wondering if there is any other way.\nThank you,\nAthanasios"}, {"role": "system", "text": "Hi, I am new to this area too! I have been reading about the same thing. I did an RNA seq experiment using the illumina TruSeq stranded mRNA kit and as part of this protocol strand specificity is achieved by replacing dTTP with dUTP in the SMM (Second Strand Marking Mix), followed by second strand cDNA synthesis using DNA Polymerase I and RNase H. The incorporation of dUTP in second strand synthesis quenches the second strand during amplification. I think that means that only the forward strand is taken forward so the library and resulting data is forward stranded. Hope this helps or that I can be corrected!\nAlysha"}, {"role": "system", "text": "Hi @doctorasgr and @Alysha\nTo check actual strandedness, as a QA/QC step for data with unknown protocols, or your own data with a known (to confirm), try this:\n\n\nMap the reads to the reference genome as unstranded\n\nRun the tool Infer Experiment on the resulting bam\n\n\n\nNote this will require a bed dataset with 12 columns\n\n\nDepending on how your obtained/created the bed data, it may have the datatype bed or bed12 assigned. Either is Ok, but check to make sure the data really has 12 columns\n\n\nUCSC\u2019s Table Brower is one source for complete bed data (tool: Get Data > UCSC main).\n\n\nIf your genome is not supported by UCSC, or you want to base the gene model on some other data provider\u2019s genome annotation, you can convert any gtf dataset to a bed12 dataset (tool: Convert GTF to BED12)\n\nDo not obtain gtf data from the UCSC Table Browser for most purposes\nUCSC has a gtf appropriate for RNA-seq tools for selected genomes in their Downloads area\nOr, you can choose a gtf from another source\nFAQ: https://galaxyproject.org/support/diff-expression/ (help here applies to any reference annotation usage. Format, content, and \u201cmatched\u201d inputs are important \u2013 or expected problems.\n\n\n\n\n\nMap the reads again to your reference genome setting the strand correctly based on the results of Infer Experiment. Use this bam result for analysis.\n\nMore Help:\n\n\nUCSC Table Browser options to get a bed with 12 columns\n\nSet the \u201cclade + genome + assembly\u201d\nPick a track from the \u201cgroup\u201d Gene and Gene Predictions\n\nSet \u201cregion\u201d = genome\n\nSet \u201coutput format\u201d = bed and check the box to send the output to Galaxy\nSubmit the query by clicking on the button \u201cget output\u201d\nNot done yet! There will be another sub-form presented next to specify more details\u2026\nChoose \u201cCreate one BED record per\u201d > \u201cWhole Gene\u201d to output a bed with 12 columns\nFinish by clicking on the button for \u201cget bed\u201d\nIf you are logged into a public Galaxy server known by UCSC (usegalaxy.org, usegalaxy.eu, and usegalaxy.org.eu are \u201cknown\u201d), the output will be sent to your active History.\nIf working somewhere else, or you want a downloaded copy, you can use the Table Browser directly (https://genome.ucsc.edu/cgi-bin/hgTables) and not check the box to send the output to Galaxy. Once the file is downloaded, use the Upload tool to get it into Galaxy.\n\n\n\n\n Generating and Interpreting the results from `Infer Experiment`\nSee these \u201cGalaxy Training Network\u201d (GTN) tutorials:\nhttps://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html#qc-strandness\nhttps://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#mapping\n\nMore More Help:\n\n\n\nBWA-MEM built-in genome(s)\n\n\nThese FAQs cover datatypes, expected formats, and related help, including how to make sure all of your inputs are a \u201cmatch\u201d. Using the same exact genome build/version for all analysis steps is important, otherwise tools can error or produce unexpected/scientifically incorrect results.\n\nStart here: https://galaxyproject.org/support/ >> https://galaxyproject.org/support/#getting-inputs-right\n\n\nMore help is also at this forum. Searching with keywords like \u201creference\u201d or \u201cannotation\u201d or even \u201chg38\u201d will find prior Q&A similar to yours \n\n\nA search with the keyword \u201cgtf\u201d will find more topics, too.\n\n\n\n\n doctorasgr:\n\nI tried already to convert my fastqs to BAMs with bowtie and then use Infer Experiment to check for that. I was wondering if there is any other way.\n\n\nConverting Fastq > BAM produces a bam dataset without any mapping information. If you actually mapped already with Bowtie (this wasn\u2019t clear to me!), maybe there was some other problem. RNA-seq data might not map well enough with a DNA mapping tool \u2013 use an RNA mapping tool like HISAT2 instead to see if that helps.\nTry the method above to produce a bam with mapping results for QA/QC purposes, run Infer Experiment on that, interpret the results, then run the analysis mapping with strand settings that match your data.\n\nHope that helps!"}], [{"role": "user", "text": "I have 3 muscles I am trying to perform differential expression with.\nFor each muscle type, I have 3 replicates.\nHow can I get pairwise comparisons between muscle 1 vs muscle 2, muscle 1 vs muscle 3, muscle 2 vs muscle 3. Within the DESeq2 tool I have factor set to muscle, then level 1 as muscle 1, level 2 and muscle 2, etc. Then within each level/,muscle type, I select the three replicate data count files.\nI read that DESeq2 takes the first input as the reference (so that would be muscle 1) and then compares everything to that. But that\u2019s not necessarily what I want. I have been using Cuffdiff to perform analyses but wanted to use a more updated tool but I can\u2019t seem to set it up as it needs to be. Can anyone give me some guidance?"}, {"role": "system", "text": "Trying to work through similar problems\u2026\nI\u2019m aware of the math to an extent, but ultimately, when you\u2019re building a model of expression change in a gene and comparing it to a normal distribution, the model incorporates each sample\u2019s expression level, so idk how to get pairwise comparisons of LFCs/significance from a matrix with multiple factors/variables and the levels therein.\nI have a bunch of RNAseq data from mutant mice at different ages; and the data clusters better after including age as an additional factor (genotype is my primary factor), and the test results are different if I remember right, but ultimately, how do you do a pairwise test (contrast) or examine interactions on Galaxy? The DESeq2 paper is very intuitive until you dig deeper into the methods. The documentation in Bioconductor for the DESeq2 R package is good, but is there the appropriate functionality here on Galaxy for this?"}, {"role": "user", "text": "It almost seems that it\u2019s not available as part of the Galaxy wrapper. Ultimately, I ended up ditching DESeq2 and ended up using edgeR as I found that their set up made more sense to me. I also found it easier to manipulate."}], [{"role": "user", "text": "hi there I just installed a local galaxy server on my computer, and I  loaded several fastq files with no problems while I have issues uploading 2 fastaq. I used these fastq in other analysis and they work just fine. this is the error I get:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 326, in \nmain()\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 319, in main\nmetadata.append(add_file(dataset, registry, output_path))\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 128, in add_file\nconvert_spaces_to_tabs=dataset.space_to_tab,\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/datatypes/upload_util.py\u201d, line 54, in handle_upload\nconvert_spaces_to_tabs=convert_spaces_to_tabs,\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/datatypes/sniff.py\u201d, line 759, in handle_uploaded_dataset_file_internal\nif not is_binary and check_content and check_html(converted_path):\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/util/checkers.py\u201d, line 28, in check_html\ntemp = open(file_path, mode=\u2018rb\u2019)\nIOError: [Errno 2] No such file or directory: u\u2019/Users/azimmer/galaxy/database/tmp/f2db41e1fa331b3e-1570157510288-2428886529\u2019\nthanks for any help!"}, {"role": "system", "text": "Can you post the job_conf.xml file? what is the size of the files?  if your files are bigger than 2Gb try to use ftp"}, {"role": "system", "text": "@nibhelim How did you load the files into Galaxy? Does the file /Users/azimmer/galaxy/database/tmp/f2db41e1fa331b3e-1570157510288-2428886529 exist on the file system? Does it have the correct permissions?\n\n\n\n pietro:\n\nif your files are bigger than 2Gb try to use ftp\n\n\nThat limit does not apply with any up to date browser anymore."}, {"role": "system", "text": "Hi, I am having similar issues. When I upload local files, they appear in my history, but they are empty and I\u2019m getting \u201c/bin/sh: /Users/viboud: No such file or directory\u201d. The upload path is \t\u201c/Users/viboud 1/galaxy/database/files/000/dataset_30.dat\u201d, but that file doesn\u2019t contain anything. Any advice?\nThank you!\nVincent"}, {"role": "system", "text": "@viboud12  You have a space in your path\u2026 Change it to something like:\n/Users/viboud_1/"}], [{"role": "user", "text": "Is there an issue with the servers? It\u2019s taking me a whole day just to upload some files, another day to run HISAT2, and I\u2019ve been waiting since yesterday afternoon for my Stringtie data."}, {"role": "system", "text": "Hi @Jodi_D\n\n\n\n Jodi_D:\n\nIt\u2019s taking me a whole day just to upload some files\n\n\nUploading data will take longer than usual if you are now working from a home internet connection instead of an institution/commercial connection. Home internet services are almost always optimized for data download speeds, not upload speeds.\nTry running a \u201cspeedtest\u201d to find out what your connection speeds are. Contact your ISP provider to find out what options are available to increase \u201cup\u201d speeds. There are several \u201cspeedtest\u201d options \u2013 an internet search will find those appropriate for where you are located, or you can ask your ISP provider for guidance.\nThe internet, in general, is a bit slower for most right now, or at least not what it was before. More people working over home wifi plus phone connections, more people streaming entertainment during the day, etc.\n\n\n\n Jodi_D:\n\nanother day to run HISAT2, and I\u2019ve been waiting since yesterday afternoon for my Stringtie data.\n\n\nThe Galaxy Main https://usegalaxy.org server is under very heavy load. This means that jobs will stay in the queued stage (grey dataset) longer. The execution stage (yellow/peach dataset) is about the same as before for most tools. There are simply more people using the server, for a variety of reasons, including distance learning.\nThe best strategy is to queue up jobs, then allow them to complete. Avoid deleting and restarting jobs in hopes of job running faster. Restarting a job places it at the back of the queue again, extending wait time. Only delete and rerun jobs if you already know there is some problem with the inputs/settings originally used. If you haven\u2019t used Workflows before, this might be a good time to learn/start using them.\nFAQ for jobs/datasets: Datasets and how jobs execute\nMany GTN Tutorials include workflows in the tutorial itself and most have a workflow that runs through all steps (except for data loading) that can be used as examples/templates. Those under the topic \u201cIntroduction\u201d cover simple cases for the \u201chow-to\u201d. Other topics cover intermediate/advanced methods for customization.\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy  server choices. If a tutorial is supported by a pre-configured Galaxy Docker  training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nNote: There are a few tools running with reduced memory resources. These are listed in the top banner on the server. The banner will update or be removed as the situation changes. Reduced memory doesn\u2019t impact how long a job takes to queue or execute but it can result in larger jobs failing for resources (even if they ran Ok before).\nIf work is urgent or you just want jobs to execute run faster, setting up your own Galaxy server is an option. Public servers are not the only choice. Cloudman is popular for scientists as it requires very little administration, resources can be scaled up for your own needs, and all of that (except for the initial step) is GUI based with built-in help. Galaxy itself is always free but commercial storage and computational resources are not. AWS has always offered grants for learning/research work, and they have extended that (or did last time I checked). I added a few tags to your post about Cloudman \u2013 please review for more details if this interests you, or see the links below for all choices.\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n\nDelays are frustrating, but I hope this helps to explain the factors involved in the different kinds of delays most public Galaxy servers are experiencing right now (not just usegalaxy.org), plus what you may choose to do to mitigate delays.\nGood question! Hopefully, the Q&A helps others to understand the current circumstances.\nBest, Jen"}], [{"role": "user", "text": "I have multiple VCF files corresponding to 40 different patients. I want to run a batch annotation and GEMINI analysis on them. Can I merge/concatenate these files and run 1 single VCF file keeping patient ID information?"}, {"role": "system", "text": "While you may be able to use the tool VCFcombine for that, please note that this is normally not what you would do because:\n\n\nstarting with separate VCFs for each patient you typically have no information about any variant site\u2019s status in \u201cunaffected\u201d patients other than that the variant wasn\u2019t called, i.e., if a site was judged homozygous ref in a patient it is not in the output of that patient and, thus, there are no stats about it.\n\n\nbecause you\u2019re lacking information you, generally, cannot rely on the INFO column after combining the files. GEMINI however relies on INFO column fields for many of the queries you can perform with it.\nSo unless you know exactly what you\u2019re doing you may get very wrong answers from such queries.\n\n\n\nAssuming that you expect your 40 patients (or subsets of them) to have something in common, joint variant calling assessing the data of all samples simultaneously can increase sensitivity in particular at low coverage sites.\n\nFor all of these reasons, I would recommend calling variants for all samples (or, at least, the ones that logically should be grouped) together with a tool like freebayes. You can then directly use the resulting multi-sample VCF dataset with GEMINI.\nIf all this sounds confusing, you may want to have a look at this tutorial:\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Exome sequencing data analysis for diagnosing a genetic d...\n\nGenetic differences (variants) between healthy and diseas...\n\n\n  \n  \n    \n    \n  \n  \n\n\nwhich illustrates joint variant analysis for a family trio."}, {"role": "user", "text": "Thank you for your answer!\nI followed the tutorial you suggested and also the \u201cIdentification of somatic and germline variants from tumor and normal sample pairs\u201d tutorial (https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/somatic-variants/tutorial.html#variant-annotation-and-reporting), successfully.\n1)I am interested in learning more about the GEMINI annotate and query and Join files to customize my analysis. I am trying to download the gnomad database but I am failing in downloading the dataset. It cannot recognize the vcf.bgz.tbi extension, I think.\nWould you suggest how can I download this dataset?\n2)during the tutorials I realized that the some commands are on usegalaxy.org and others on galaxy Europe. Is there any way to share the history between the two websites?\n\nLastly for annotate query, learning SQL language is always needed? Or I can use other command like filter and sort? (to remove frequent variant for example)\n\nThanks in advance for your help."}, {"role": "system", "text": "\nYou don\u2019t want to upload any .tbi files. These are index files that the GEMINI annotate tool knows how to generate transparently and on-the-fly for you. What you need to upload is just the (compressed) vcf.\nNo, there is no simple sharing between the websites. For following these tutorials there shouldn\u2019t be any missing tools on EU though since that\u2019s were these particular tutorials have been developed and tested.\nAs long as you want to stick to GEMINI, you have to use SQL for phrasing more complex queries (after running GEMINI load your data is stored in an sqlite database and this is not a plain text file. Note, however, that many GEMINI tools exist only for the purpose of avoiding explicit SQL syntax for common usage scenarios - e.g., GEMINI inheritance, GEMINI query in basic mode, etc.).\nWhat you can always do is formulate a partial query (one that doesn\u2019t do all filtering, etc. you\u2019d want), then use regular text processing tools to work on the tabular output.\n"}, {"role": "system", "text": "@wm75 Please see the other post and correct as needed.\nIn summary, @mvdbeek is making some changes so that compressed vcf can be recognized and used by tools. The \u201cVCF\u201d gnomAD datasets are loading with a vcf_bgzip datatype and are not currently recognized by tools (including Gemini).\nThe complete gnomAD datasets are also very large (too large, will probably fail for resources\u2026 or is that not correct at usegalaxy.eu?). Per-chromosome data are smaller.\nBoth use custom versions of hg19/hg38, so assigning a database will be troublesome (yes?). If the actual human genome fasta used to call the variants is identified and loaded, it will be too large to use as a custom genome/build (or at least at usegalaxy.org).\n(For data that is vcf.gz (no index, from other sources), those uncompress during Upload to vcf and tools recognize the data as valid VCF input.)"}, {"role": "system", "text": "@wm75 has added vcf_bgzip to gemini, so that should work starting with version 0.20.1+galaxy2 which is already available on usegalaxy.eu"}], [{"role": "user", "text": "Hi,\nI was wondering could I get suggestions on how to align multiple protein sequences that are in an excel file to a reference protein fasta sequence. I want to get an output of which align and which do not align.\nThanks for any tool suggestions."}, {"role": "system", "text": "Hi @stauntok,\nexcel files are usually a bad supported format. I suggest to generate a FASTA file with your sequences and then use the Align sequences to a reference tool.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nThanks for your advice. I have generated a fasta file using Tabular-to-Fasta. I then tried to use the Align sequences to a reference tool. However, I got the error below, do you know how to fix it? I need an alignment file right?\nTraceback (most recent call last):\nFile \u201c/usr/local/tools/_conda/envs/__python-bioext@0.19.7/bin/bealign\u201d, line 196, in \nretcode = main(\nFile \u201c/usr/local/tools/_conda/envs/__python-bioext@0.19.7/bin/bealign\u201d, line 82, in main\n_align_par(\nFile \u201c/usr/local/tools/_conda/envs/__python-bioext@0.19.7/lib/python3.8/site-packages/BioExt/uds/init.py\u201d, line 80, in _align_par\naln = Aligner(\nFile \u201c/usr/local/tools/_conda/envs/__python-bioext@0.19.7/lib/python3.8/site-packages/BioExt/align/init.py\u201d, line 93, in init\nraise ValueError(\u2018codon alignment requires a protein score matrix\u2019)\nValueError: codon alignment requires a protein score matrix\nThanks,\nKara"}, {"role": "system", "text": "Hi @stauntok,\nI cannot reproduce that error, which Galaxy instance are you using? This tool requires the reference as a sequence of nucleotides.\nRegards"}], [{"role": "user", "text": "Hello everyone,\nCan you tell me how can I solve this problem? \ndeseq821\u00d7401 24.3 KB\n"}, {"role": "system", "text": "Hi @ive_s\nThis looks like an input problem.\nThings to check. The first two are most likely to solve this specific error.\n\nContent of input datasets. Examples are on the tool form. Be sure to input each dataset just once.\nIf count files have headers, be sure to check the box for \u201cinputs have headers\u201d.\nThis FAQ has additional help (but I don\u2019t think you are running into these problems): Help for Differential Expression Analysis\n\nI added a tag to your post about prior Q&A troubleshooting using this tool for additional context/help.\n"}, {"role": "user", "text": "Hi @jennaj ,\nI checked first two things from your list, but I still get the same error. My count files have headers, and box for that is checked by default. I went trough my samples, there are no duplicates. This is how my count files from FeatureCounts look like:\ndeseq412\u00d7591 8.29 KB\n\nIn box Choice of Input data I put option count.\nThank you for help!"}, {"role": "system", "text": "Hi @ive_s\nThanks for checking and posting back.\nThe problem is with the count values. Those should be whole numbers (integers), not decimals. Check your settings with Featurecounts \u2013 the default output are counts by \u201cgene_id\u201d and will produce whole numbers \u2013 and that is required by the Bioconductor tools.\nUpdate: Hum, I might be wrong about that. Looks like EdgeR accepts fractions but DESeq2 does not. Maybe use EdgeR instead? Or you can use text manipulation tools in Galaxy to round off the values.\nReferences:\nhttps://support.bioconductor.org/p/100516/\nhttps://support.bioconductor.org/p/p132450/\nTutorials: Galaxy Training!"}, {"role": "user", "text": "Hi,\nI solved this by rounding numbers in RStudio, and then uploading data back on server. It worked! "}], [{"role": "user", "text": "I am currently enrolled in the intro to galaxy class with coursera and registered for a galaxy account. I tried to get shared data from the tab on the top and there is a blank screen.\nthanks"}, {"role": "system", "text": "Hello @Akat\nWhat is the URL of the public Galaxy server you are working at?\nThanks!"}, {"role": "user", "text": "usegalaxy.org,  my public name is histotech\nthank you"}, {"role": "system", "text": "Hi @Akat\nThank you for reporting the problem! We can confirm that Shared Data  > Data Libraries is not loading correctly.\nWe are working to resolve the issue. More feedback soon!\nJen"}, {"role": "user", "text": "thank you so much"}, {"role": "user", "text": "up and running\nthanks again "}], [{"role": "user", "text": "Hello,\nI recently noticed that the je-demultiplex tool is no longer accessible. Is there any explanation for why it was taken down, and will it be put back up? I see a few other je suite tools, such as je-demultiplex-Illu and je-clips, but I\u2019ve become reliant on je-demultiplex and would like to use it again, if possible.\nThanks for any information\nJake"}, {"role": "system", "text": "Welcome @jturo\nWould you please confirm that you are working at Galaxy EU https://usegalaxy.eu?\nIf somewhere else, what is the URL?\nThanks!"}, {"role": "user", "text": "Hi jennaj,\nThanks for the response. Yes, I am using https://usegalaxy.eu.\nFunnily enough, I just re-checked for je-demultiplex and it is there again after being gone from Monday-Wednesday. Did you have any hand in putting it back? If so, thanks very much! I\u2019m now doubting myself that it was ever missing, but it wasn\u2019t appearing when I searched for it in the tools section, nor was there a \u2018run again\u2019 button available when looking through my previous runs of the tool in my histories. Has this sort of thing happened before?\nTake care\nJake"}, {"role": "system", "text": "Hi @jturo\nThanks for confirming the server. Glad you can find these now, whatever went on.\nNo, I didn\u2019t make any changes to that server. But the EU server\u2019s administrators/team may have.\nThe \u201crerun\u201d button not being present is a bit odd \u2013 but if the tools were removed, or possibly removed then added back again (the same or different versions), that could be a plausible explanation for what occurred.\nPing @bjoern.gruening (EU administrator) in case he has more feedback.\nBest "}, {"role": "system", "text": "@jturo sorry for the problem. There was a Galaxy bug in the new 19.09 that prevented to load this tool. This was fixed in the meantime and everything should work now.\nThanks for reporting.\nBjoern"}], [{"role": "user", "text": "Hi,\nI noticed that upon running FastQC on my list of pairs containing PE Illumina reads, all the generated reports have the filename of the input reads set to either \u2018forward\u2019 or \u2018reverse\u2019, depending which set of reads it corresponds to. This isn\u2019t too big of an issue since the output of FastQC is another list of pairs itself, so I can easily figure out which \u2018forward\u2019 and \u2018reverse\u2019 it is referring to, however when I take the FastQC rawdata outputs and use them for MultiQC, the tool cannot summarize all my data as the FastQC files all refer to the same \u2018filename\u2019 even though they\u2019re all from different SRAs.\nI used the fasterqdump tool from the sra-tools to import many SRAs and that\u2019s how the list of pairs of my reads was created.\nWould appreciate any advice on how I can work around this, ideally so that FastQC actually picks up the correct filename for each read!\nCheers."}, {"role": "system", "text": "Hi,\nCan you use variables to auto-rename the outputs (like this) ?"}, {"role": "user", "text": "Hi @David,\nThanks for your reply. I don\u2019t think this is necessarily what I\u2019m looking for per se, since within the FastQC file itself, it references its input file \u2018forward.gz\u2019 or \u2018reverse.gz\u2019. Rather, I would ideally like to have it refer to the proper filename \u2018SRR000000-forward.fastq.gz\u2019 for example. That is because the contents of the FastQC file itself is being parsed by downstream tools; not so much the name of the FastQC output files. I\u2019ve attached a screenshot of the FastQC output for reference.\nThe FastQC command which is run by Galaxy looks like this, notice how in the first symlink step it names the symlink reverse.gz:\nln -s '/REDACTED/galaxy-21.01/database/files/030/dataset_30560.dat' 'reverse.gz' &&\nmkdir -p '/REDACTED/galaxy-21.01/database/jobs_directory/007/7581/working/dataset_30618_files' &&\nfastqc --outdir '/REDACTED/galaxy-21.01/database/jobs_directory/007/7581/working/dataset_30618_files'  --adapters '/REDACTED/galaxy-21.01/database/files/030/dataset_30591.dat'   --quiet --extract  --kmers 7 -f 'fastq' 'reverse.gz'  &&\ncp '/REDACTED/galaxy-21.01/database/jobs_directory/007/7581/working/dataset_30618_files'/*/fastqc_data.txt output.txt &&\ncp '/REDACTED/galaxy-21.01/database/jobs_directory/007/7581/working/dataset_30618_files'/*\\.html output.html\n\nI looked into the FastQC toolshed XML file some more and this is how the code starts off:\nset input_name = re.sub('[^\\w\\-\\s]', '_', str($input_file.element_identifier))\nset input_file_sl = $input_name + '.gz'\nln -s '${input_file}' '${input_file_sl}' && \n.....\n\ninput_file is the corresponding read file I select from my history. So essentially it seems like for my original sequencing read files, the input_file.element_identifier is returning either \u2018forward\u2019 or \u2018reverse\u2019 rather than a more useful name.\n\nScreen Shot 2021-05-28 at 11.26.10 AM953\u00d7426 106 KB\n"}, {"role": "system", "text": "Thanks for the detailed reply.\nMy guess would play with the set input_name as you can see is operating changes on the file name string.\nOr even rename the inputs to something like \u201c\u2018forward-SRR000000.fastq.gz\u201d."}, {"role": "user", "text": "I did some more digging, and the issue only arises when using list of pairs PE reads. If I just select one individual read file, the value for $input_file.element_identifier is indeed its proper name SRR0000-forward.fastq.gz, while if I have a collection, the element_identifier for all reads is either just forward or reverse. This seems to be an issue (feature?) inherent to Galaxy itself.\nNot quite sure how to move on from here, other than maybe running FastQC individually on each reads file in a list, rather than on the list of pairs itself.\nEDIT: flattening the list seems to solve the issue! easy enough workaround!"}], [{"role": "user", "text": "I am trying to follow the Hsp90 tutorial from this paper: Intuitive, reproducible high-throughput molecular dynamics in Galaxy: a tutorial | Journal of Cheminformatics | Full Text. However, in step 5, I cannot find a tool called \u201cMerge GROMACS topologies\u201d. I have tried looking through all the tools that are available and cannot find the one used in the tutorial. I would greatly appreciate any help!"}, {"role": "system", "text": "Hi @christos-efthymiou\nDifferent public Galaxy servers host different tools. The publication notes where the original tutorial was executed here: Intuitive, reproducible high-throughput molecular dynamics in Galaxy: a tutorial | Journal of Cheminformatics | Full Text\nTry working at UseGalaxy.eu \u2013 the tool is available at that server: Galaxy | Europe\nGTN Tutorials: Search Tutorials"}, {"role": "user", "text": "Thank you so much for the help. I am unfortunately still a bit confused. The tutorial shows these instructions:\n\nimage1060\u00d7848 152 KB\n\nBut the tool that you sent the link to only has a place to select the two TOP/ITP files, not the GRO files:\n\nimage1005\u00d7763 50.6 KB\n\nWhere/how do you select  the GRO files? I am trying to use this for the first time so I apologize for my complete confusion!"}, {"role": "system", "text": "Hi @christos-efthymiou, can you switch the tool version to 3.4.3+galaxy0?\nBest wishes,\nSimon"}], [{"role": "user", "text": "Hi, when I used freebayes to call SNPs with input filter parameters, there is a error in my local galaxy. when I closed the input filter parameters everything is OK. I just want to know how can I solve the problem.\nthe following is the error details\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 243, in prepare_job\njob_wrapper.prepare()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1161, in prepare\nself.command_line, self.extra_filenames, self.environment_variables = tool_evaluator.build()\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 462, in build\nraise e\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 458, in build\nself.__build_command_line()\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 483, in __build_command_line\ncommand_line = fill_template(command, context=param_dict, python_template_version=self.tool.python_template_version)\nFile \u201clib/galaxy/util/template.py\u201d, line 107, in fill_template\nraise first_exception or e\nFile \u201clib/galaxy/util/template.py\u201d, line 80, in fill_template\nreturn unicodify(t, log_exception=False)\nFile \u201clib/galaxy/util/init.py\u201d, line 1060, in unicodify\nvalue = str(value)\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/Template.py\u201d, line 1053, in unicode\nreturn getattr(self, mainMethName)()\nFile \u201ccheetah_DynamicallyCompiledCheetahTemplate_1638720981_7731435_86611.py\u201d, line 438, in respond\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/NameMapper.py\u201d, line 293, in valueFromSearchList\n_raiseNotFoundException(key, searchList)\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/NameMapper.py\u201d, line 178, in _raiseNotFoundException\nraise NotFound(excString)\nCheetah.NameMapper.NotFound: cannot find \u2018standard_filters\u2019"}, {"role": "user", "text": "I will be very pleasure hearing from you,thx"}, {"role": "system", "text": "Hi @yangchen,\nI\u2019m trying to reproduce the error; which tool version are you using? It would be great if you could do the analysis in usegalaxy.eu; in that way would be easier for us to identify the problem.\nRegards"}, {"role": "user", "text": "Hi gallardoalba,\nThanks for your reply, I use freebayes (1.3.5+galaxy0 1.3.5). I have tried most of versions in galaxy but all of them produced this error. I can analysis in usegalaxy.eu with the same version of freebayes (1.3.5+galaxy0 1.3.5) but it only produce this error in local galaxy. I have installed freebayes in my server and run it by command line without this error.I think my server might be absent of dependence environment but I do not know what I need to install.\nMy operating system is Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-40-generic x86_64)\nRegards\nYangchen"}, {"role": "system", "text": "It could be that you need to re-install the freebayes tool, see:\n  \n\n      github.com/galaxyproject/tools-iuc\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      Error run-in g latest version of freebayes\n    \n\n    \n      \n        opened 12:52PM - 03 Dec 21 UTC\n      \n\n        \n          closed 10:02PM - 03 Dec 21 UTC\n        \n\n      \n        \n          \n          gregvonkuster\n        \n      \n    \n\n    \n    \n  \n\n\n  \n    I'm running Galaxy 21.09, and executing this version of freebayes: \n```\ntoolsh\u2026ed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.5+galaxy0\n```\nproduces this error:\n```\ntool error\nAn error occurred with this dataset:\ncannot find 'standard_filters'\n```\nThis may be related to this recent change - https://github.com/galaxyproject/tools-iuc/pull/4250\n\nIs this a bug, or have I missed something during the tool install?\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI see you posted your question 16 days ago, but maybe you already had it installed for a few days."}, {"role": "user", "text": "thanks for your help, I re-install freebayes and I can use the input-filter parameters successfully. It works well now!\nYangchen"}], [{"role": "user", "text": "how can i turn a single end data, into the paired end data and opposite( paired end into single end data)\n?"}, {"role": "system", "text": "Hi @amir\nData that is paired-end or single-end refers to the way the reads were originally sequenced. Any type of manipulation to attempt to create one from the other, either way, is unlikely to produce meaningful scientific results (eg: combining paired-end reads into some type of \u201cfake\u201d single end read dataset).  Or, result in data loss (eg: just using the F or R reads from a paired-end dataset as a single end dataset).\nWhat is your goal? Analysis, tools, etc? Or was this just a general question?"}, {"role": "user", "text": "i have this data :\nIt is 42bp paired-end sequencing; NextSeq was set for 42 bp for read 1, 6 bp for the multiplexing index, and 42 bp for read 2, so a total of 90 reads, 84 of which are supposed to be RNA.   So, they should be 2x42bp reads.\nnow i can just work with 42 bp reads. and if i cut only 20 bp of it, it became useless. i thought maybe or some how combine the paired to get more nucleotide to work with\n\nadmin edit for \u201cnucleotide\u201d to remove problematic google search link\n"}, {"role": "system", "text": "\n\n\n amir:\n\ncombine the paired to get more nucleotide to work with\n\n\nHi, I\u2019m not sure what you mean by this, the link appeared to be a google search?\nAnyway \u2013 use the data as paired-end inputs. Not only does this preserve the true content, but read pairs are an important scientific content advantage for many analysis protocols."}, {"role": "system", "text": "You can not go from single-end to paired-end. Maybe you can think of something to simulate it but this data is not useable for doing an actual analyses. To go from paired-end to single-end (wrong terminology see first reply of jennaj) you could use something like FLASH. If you need to do some mapping, most tools can handle paired-end data."}, {"role": "system", "text": "\n\n\n gbbio:\n\nFLASH\n\n\nThank you @gbbio for adding in this detail. Agree with everything you state.\nFor some cases where the reads actually overlap, this type of micro \u201cassembly\u201d to generate a consensus is appropriate. And yes, that would not technically result in \u201csingle-end\u201d data and shouldn\u2019t be used that way.\n@amir To learn more about FLASH, see the tool form. A short description is in the help section and link to the publication is at the bottom. "}], [{"role": "user", "text": "I noticed workflow versions (v1,v2\u2026) in workflowhub and myexperiments but it\u2019s not available to the galaxy shared workflow.\nIs it possible to get all versions of a particular workflow??"}, {"role": "system", "text": "Hi @khairul0026\nYes, Galaxy now keeps track of workflow versioning. The Galaxy workflow version is part of each workflow\u2019s metadata. To view the version in Galaxy, open the workflow in the editor, and the current version will be listed in the far right panel. Navigate the available versions using the pull-down menu.\nNote that a workflow publication site may have its own versioning system. You can keep track of both and anything else important to you within your copy of the workflow \u2013 details below.\n\n\n\n khairul0026:\n\nIs it possible to get all versions of a particular workflow??\n\n\nIt depends on what the original authors published and how the workflow hosting system versions published content uploaded to the site. Example: This workflow has two published versions that can be navigated/downloaded SARS-CoV-2 Illumina Amplicon pipeline - iVar based\nTechnically, if the workflow was published in the last year or so, it will include at least one Galaxy version available in the workflow\u2019s metadata. The author may also have annotated the workflow with other details. Examples: source repository, citation information, related publications, or public data sources.\nWorkflow versioning within the workflow itself may not be included in workflows created/published before the functionality was added to Galaxy. This means that when you import a workflow without built-in versioning, the default version will be set by Galaxy. Any changes you make from that point forward will be tracked and incorporated.\nYou can also add more annotation to your copy of the workflow in the description. This could capture information about the source (URL, 3rd party version, dates published/obtained, authors/citations) or anything else informative that you want to track.\nRecent Galaxy release highlights about workflows:\n\nSeptember 2020 Galaxy Release (v 20.09) \u2014 Galaxy Project 21.05.1.dev0 documentation\nJanuary 2021 Galaxy Release (v 21.01) \u2014 Galaxy Project 21.05.1.dev0 documentation\n\nWorkflow authors and developers:\n\nWorkflows published by the wider Galaxy community (some are members of the two groups below) can be found here The WorkflowHub and here myExperiment - Workflows. Both sites have instructions and several use-cases are covered in the tutorials below. Help is also included in Galaxy on the \u201cWorkflows > Import\u201d form.\nWorkflows published by the IWC (\u201cIntergalactic Workflow Commission\u201d) are authored by this group GitHub - galaxyproject/iwc: Intergalactic Workflow Commission and are also published here Dockstore\n\nWorkflows published by the GTN (\u201cGalaxy Training Network\u201d) are authored by this group GitHub - galaxyproject/training-material: A collection of Galaxy-related training material and are primarily published here https://training.galaxyproject.org with one here (so far) Dockstore\n\n\n\n\n\n khairul0026:\n\nI noticed workflow versions (v1,v2\u2026) in workflowhub and myexperiments but it\u2019s not available to the galaxy shared workflow.\n\n\nIf a workflow is published at a public site or included in a publication, import it to the Galaxy server where you are working to use it. You may need to modify a workflow \u2013 open it in the editor and review any messages about updated tool versions or similar. If you are working at your own Galaxy server, you may need to install or update tools and reference data. Some workflow authors may also publish a pre-configured Docker Galaxy. One example is the GTN \u2013 find any associated docker images on the topic category pages in the section \u201cGalaxy instances\u201d and/or included in the \u201cAvailable on these Galaxies\u201d pull-down menu included directly on tutorial pages.\nGTN Tutorials about workflows:\n\nSearch Tutorials\n\nHope that helps!"}, {"role": "system", "text": "@khairul0026 Galaxy\u2019s internal workflow versioning and the external one you\u2019re seeing in the workflowhub, for example, are not the same thing. To get different workflowhub versions (we like to call them releases in the Galaxy world to distinguish them from Galaxy\u2019s internal versions), you will need to import them as separate workflows into Galaxy."}, {"role": "user", "text": "While I published a workflow in galaxy repository, it\u2019s always becoming a new workflow instead of versioning, but in my workflow list it\u2019s like a different versions like v1, v2\u2026 I am hoping to get this type of versioning (v1,v2\u2026) if I share same workflow in the galaxy repository"}, {"role": "user", "text": "Is it possible to share workflow using different versions in galaxy repositories?  For me it\u2019s becoming new workflow instead of versioning"}, {"role": "system", "text": "@khairul0026 are you talking about publishing a workflow on a Galaxy instance so that others can find it under \u201cShared Data\u201d \u2192 \u201cWorkflows\u201d?\nIf so, then the answer is: when people are importing a Galaxy-published workflow into their own account they will import only the specific state the workflow is in at that moment. Importing is a copy operation so afterwards the other user owns the copy and can start modifying, which will create new versions of their workflow.\nIf you modify the original workflow later, this will be reflected directly in the published workflow in Shared Data, but will not affect the other user\u2019s copy. If they want to get your latest changes, they will need to re-import the published workflow, i.e. create another copy.\nThe best way to deal with public-facing workflow versions is to export your workflow file and deposit it in a public version-control system. Dockstore and workflowhub are repositories tailored specifically to scientific workflows, but you can use, for example, a github repo to host your workflow and people can use Galaxy\u2019s workflow import functionality to get the workflow directly from there. This way everyone\u2019s aware of your versions and can refer to them properly."}], [{"role": "user", "text": "Hello, Galaxy!  \nI am running a local instance of Galaxy 22.01 on my institution\u2019s slurm cluster via DRMAA. I am configuring it to send jobs to our partition, where each job is given one node and all the cpus on that node (in job_conf.xml: param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=32 --partition=vgl</param>). For more background on this cluster, when I am submitting jobs via sbatch and I need a tool from a conda environment, I need to either a) activate the conda env from the interactive head node, then submit the job, or b) have the job script explicitly init my conda, then source ~/.bashrc, then activate the required env, within the job script. Otherwise, the job can fail because it did not find the tool.\nOnto the local Galaxy install!\nSometimes when I run a workflow, a tool works once but not another time, like in this screenshot:\n\nimage1357\u00d7458 40.7 KB\n\nOther times, a tool will not work when I try to invoke a workflow using it. These two BWA MEM jobs failed for different reasons in the same workflow. But then I tried invoking the workflow again, and then they both ran fine.\n\nimage (1)1404\u00d71014 100 KB\n\nAnother instance of BWA MEM working once then not again within the same invokation:\n\nimage (2)1366\u00d7344 38 KB\n\nI have also been seeing this with BUSCO and QUAST. Here is an example of the workflow-invoked BUSCO failing, but then it ran fine on the same dataset when I clicked \u201crerun job\u201d:\n\nScreen Shot 2022-04-18 at 3.14.38 PM1866\u00d71312 280 KB\n\nSometimes for BWA MEM I get a database locked error (might be unrelated to the previous ones\u2026):\nTraceback (most recent call last):\n  File \"/lustre/fs5/vgl/scratch/labueg/galaxy_22.01/galaxy/.venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1803, in _execute_context\n    cursor, statement, parameters, context\n  File \"/lustre/fs5/vgl/scratch/labueg/galaxy_22.01/galaxy/.venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 732, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nFor the above database locked error, this was when I was running the bwa-mem workflow a couple times to try to debug it, so maybe it is coming from trying to run the workflow twice at once or something? I have only started seeing this error recently, like last night when trying to run it a couple times.\nFor tools that I have not run into this issue with: meryl, merqury, and hifiasm have all been running without a hitch, I have not seen any of the above errors with them.\nThe workflows I am using are found here: HiC workflow (with BWA MEM) and [iwc/Galaxy-Workflow-Long_read_assembly_with_Hifiasm_and_HiC_data.ga at VGP \u00b7 Delphine-L/iwc \u00b7 GitHub](https://Hifiasm-HiC workflow (with BUSCO/QUAST)). Here is the rest of my job_conf.xml in case it is helpful:\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n            <param id=\"drmaa_library_path\">/vggpfs/fs3/vgl/store/labueg/programs/slurm-drmaa/slurm-drmaa-1.1.3/lib/libdrmaa.so</param>\n        </plugin>\n    </plugins>\n    <destinations default=\"slurm-vgl\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"slurm-vgl\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=32 --partition=vgl</param>\n        </destination>\n        <destination id=\"slurm-bigmem\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=64 --partition=vgl_bigmem</param>\n        </destination>\n    </destinations>\n</job_conf>\n\n\nAny help is appreciated, and please let me know if any more details would help! Thank you for your time! "}, {"role": "system", "text": "Hi @abueg\nI\u2019ve cross-posted this to the admin chat. They may reply here or there, and feel free to join the chat.\nMatrix: You're invited to talk on Matrix\nGitter: galaxyproject/admins - Gitter\nOne extra bit of information you might add to help others offer advice\u2026 have you implemented any of these? Which and how? Production Environments \u2014 Galaxy Project 22.01.1.dev0 documentation"}, {"role": "system", "text": "@abueg if the file-not-found error only appears randomly, can it be that a few nodes in your cluster do not have access to your conda environment? Every cluster node that receives jobs need to have access to the conda environments. Alternatively, you can try to use Docker or Singularity containers.\nThe database is locked is due to the sqlite database that you are using. This is not meant for production or heavy load. Normally you will use a proper database like postgresql instead.\nHope that helps,\nBjoern"}], [{"role": "user", "text": "Greetings!  We would like to run blasts with fairly high stringency, e.g. e values <e-50\nWhen we enter values in the \u201cSet expectation value cutoff\u201d field, it seems we are restricted to using strings of zeros.  Is there a way we can enter cutoffs in scientific notation."}, {"role": "system", "text": "Hi @tom_abrams_lab\nThe BLAST+ tools available in Galaxy are wrappers around the source tools from NCBI.\nMeaning, the parameter inputs are determined by the underlying tool. Filters for evalue are submitted and interpreted in decimal format.\nReference: https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices_Options_for_the_commandline_a\nHope that helps!"}, {"role": "user", "text": "Thanks for the info!  Will the blast work properly entering a long string (e.g. 50) zeros for the e value cutoff?"}, {"role": "system", "text": "@tom_abrams_lab\nI honestly don\u2019t know whether BLAST+ itself will accept that value. You could check the NCBI documentation for the tool and their public forums. If it does, a query with that low of a sensitivity threshold might be unlikely to succeed when working at a public Galaxy server. The intermediate results could grow into a very large memory cache and exceed the capabilities of the clusters. Much would depend on the query, the target database, and other parameters of course. You can certainly try.\nIf you are working at your own Galaxy, and are able to allocate sufficient resources, then any limits the tool can natively compute/handle should be possible using the same tool as wrapped for Galaxy.\nSo, the best answer I have is to test it out. Maybe on a small test query first, then scale up for the full analysis."}, {"role": "system", "text": "Hum, looking at the wrapper code, it does have a test for an evalue threshold in scientific notation now \u2013 as low as 1e-41 for megablast. Low evalues are present in tests for Blastx, and they are in scientific notation. I hadn\u2019t noticed that before!\n\nSearch \u00b7 evalue \u00b7 GitHub\ngalaxy_blast/ncbi_blastx_wrapper.xml at 1e3b152f1400a5f8d5c7eec840fe7396baf5d0ce \u00b7 peterjc/galaxy_blast \u00b7 GitHub\n\nMaybe just test it out? This tool is maintained by a dedicated developer who is responsive about questions and requests if open issues (bugs, enhancement requests) are reported.\nSo give it a try with scientific notation and follow the format as the tests use, and perhaps open an issue against the wrapper repository if you run into technical problems. If you get an error that looks to be server-side, or even if you are not sure, feel free to post back here. Include the public server URL where you are working for context. Or post back the stderr and stdout if working at your own server and we can try to help triage what is going wrong.\n(I don\u2019t personally use BLAST+ often anymore \u2026 but did about ~20 years ago (line-command)! Apologies for not being current on the exact status of the tool/wrapper  I\u2019m just looking up the same resources online as you probably are to answer for this)"}, {"role": "user", "text": "Thanks much Jenna.   This scientific notation for the cutoff e-value worked perfectly.  I am not sure whether there is a limit, but I tried both e-41 and e-60.  What was critical is that I needed to include the1 (as in 1e-50)."}], [{"role": "user", "text": "I have put the download papAnu.fa to history as my reference genome, but why it does not work when I do hisat2 for alignment with other fastq files"}, {"role": "system", "text": "Welcome @zhwdong9\nAre you sourcing the papAnu2 or papAnu4 genome from UCSC?\nhttp://hgdownload.soe.ucsc.edu/downloads.html#baboon\nMake certain that you get the fasta version of the genome. For papAnu4, this would be: http://hgdownload.soe.ucsc.edu/goldenPath/papAnu4/bigZips/ >> papAnu4.fa.gz (soft-masked version, but the hard-masked is also available).\nThis particular data does not need to be reformatted through NormalizeFasta as in the first FAQ listed below states (will already be formatted correctly from this source). But you should load it in the Upload tool without setting a \u201cdatatype\u201d. Allow Galaxy to uncompress the data and assign the final datatype (will be \u201cfasta\u201d).\nFAQs: https://galaxyproject.org/support\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\nExtended Help for Differential Expression Analysis Tools\n\nAlso, click on the tags I added for more help/prior Q&A for various related issues end users have had when first learning how to work with Custom Genomes/Builds. Some of these may inform you about how to resolve your problem.\nHowever, if none of that helps, please share more information:\n\nTry at least one rerun to eliminate transient server/cluster issues. Let us know if the job fails again (and how/why) \u2013 and if you confirmed that the genome fasta is intact (fully loaded) and has the correct datatype assigned.\nWhere are you working? Public Galaxy (URL)? Your own Galaxy?\nHow does the job fail?\n\nReview the contents of the \u201cbug\u201d icon report (it does not need to be submitted, but if you do, include a link to this post for context).\nClick into the \u201ci\u201d icon for job details and review the stderr and stdout reports. These often explain exactly what the problem is.\n\n\n\nFor reference, many Galaxy Tutorials from the GTN include the tool HISAT2. Start with those in the group \u201cTranscriptomics\u201d:\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials: Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy server choices. If a tutorial is supported by a pre-configured Galaxy Docker  training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nThanks!"}, {"role": "user", "text": "Thank you, I will try it again"}, {"role": "user", "text": "I did it like jennaj ever mentioned, but why the result did not show how many reads and the alignment rate. If I used the build-in genome (papAnu2) then ran hisat 2, I can get all the information."}, {"role": "system", "text": "Hi @zhwdong9\nThe statistics are there, for every HISAT2 run by default. But if for some reason there were many \u201cwarnings\u201d or processing details, the statistic report will be after those lines, and you won\u2019t see it in the expanded dataset view. Only the first few lines of stderr are captured here, as a \u201cpeek\u201d view.\nTechnically, the underlying HISAT2 tool reports the alignment statistics in a part of the job output named stderr. That\u2019s just how this particular tool works, and it doesn\u2019t necessarily indicate that there was an actual error/problem even though the name of that output includes \u201cerr\u201d (abbreviation for \u201cerror\u201d) in it. All tools have a stdout and stderr output by default, although not all tool authors report meaningful data in those places. The Galaxy wrapper around a tool can capture and (sometimes) reformat or add more information to the default stdout and/or stderr to provide more details. And some tool wrappers, like this one, have an option to report the interesting parts of those metadata as a distinct dataset in your history (just the statistics).\nHow to review the full stderr message (any tool).\nFor HISAT2, this is where the alignment statistics are captured (at the end, any content above it is worth reviewing but doesn\u2019t necessarily mean that anything went wrong. In my example, the warnings are simply informing that there were some very short reads in my fastqsanger inputs. Too short for the tool to map. A few of these kinds of warnings not unusual \u2013 but if my reads didn\u2019t map well overall, could explain one reason why (perhaps QA was too aggressive or the data was not of high quality originally).\nA job\u2019s stdout can also be reviewed this way. Plus, this is a method to review the tool parameters originally set/submitted in a simplified format (scroll further into the report than what is shown in the graphics below to find this information). This is a very useful report to learn how to review, and not only when something goes wrong (error or unexpected results).\n\nExpand the result bam dataset and click on the \u201cView details\u201d icon. This example happens to have too many warnings to show the statistics, and is probably similar to one of your jobs.\n\n\nhisat2-review-alignments-in-stderr-1.png632\u00d71026 119 KB\n\n\nThe \u201cJob details\u201d report will display in the center pane. Locate and click on the link to the stderr output.\n\n\nhisat2-review-alignments-in-stderr-2.png1218\u00d71252 125 KB\n\n\nOn the stdout report, scroll down past the warnings to find the statistics at the end. In this example, all of my warnings except for the last one before the actual statistics were not captured in the screenshot, on purpose, to keep the graphic smaller. The last line is from the wrapper and is just a processing detail that can be ignored.\n\n\nhisat2-review-alignments-in-stderr-3.png1224\u00d7678 67.4 KB\n\nHow to capture only the HISAT2 alignment statistics from the full stderr message into a distinct dataset report into the history.\nOn the HISAT2 tool form, expand the section \u201cSummary Options\u201d to access the parameter to \u201cPrint alignment summary to a file.\u201d It happens to be set to \u201cNo\u201d by default. In the graphic, I changed the option to be \u201cYes\u201d, and the extra output dataset was created during job execution. It contains just the alignment statistics \u2013 in a cleaned-up format (no \u201cwarnings\u201d or technical run details). The statistics content is exactly the same as those found at the end of the stderr output above.\n\nhisat2-print-alignments-to-a-dataset.png1136\u00d7540 40.2 KB\n\nHope that helps!"}], [{"role": "user", "text": "Hi,\nWhen using \u201cVCF to pg_snp\u201d (with idea to reformat later to gd_snp), Galaxy fails. The tool does not accept that some alleles have alternatives:\nAn error occurred with this dataset:\nformat interval database ?\nbad variant nt AAGA,AATG for nt 2 at /cvmfs/main.galaxyproject.org/migrated_tools/toolshed.g2.bx.psu.edu/repos/devteam/vcf2\nPlease advise on how to proceed, or on alternative tools. Thank you!"}, {"role": "system", "text": "I am currently having the same problem. Did you find any solution for this problem?"}, {"role": "user", "text": "I resorted to NOT using Galaxy for this.\nInstead, I manipulated my vcf in vcfR and poppr, both implemented in R."}, {"role": "system", "text": "Oh! Bummer! Thank you anyways!"}, {"role": "system", "text": "All of the Genome Diversity tools have been deprecated, and are no longer supported."}], [{"role": "user", "text": "Hello,\nTrying to use DESEQ2 for a couple of days now, I have the output of FeatureCounts, I also used Limma Voom successfully. However I need the tables with the p-value and the fold difference in expression so I try to get them via DESEQ2. I tried to compare the Control with each mutant (4) but I think I got it wrong. I\u2019m not sure how to import the files correctly and I couldn\u2019t find any proper explanation of the factors. Please help or an idea where to start from? "}, {"role": "system", "text": "Dear Sammy,\nOk where to start. Well \u2026 first here are the DESeq papers: DESeq2 and DeSeq. Then I would recommend to look into one of the Galaxy training tutorial that uses DESeq: Reference-based RNA-Seq data analysis. There is also a good biconductor post and a thorough document from the created and author here. Ah and for a visual and audio-tutorial you can watch this DESeq2 series, which is quite intuitively explained.\nFrom your explanation I just guess that you do not received any p-vlaues with DESeq2? If that is the case than the reason for that is probably that you only have one replicate for either mutant or control, or both. DESeq2 needs at least two replicates two calculate a p-value. This has something to do with the variance estimation.\nI hope I could help you and have a nice day!\nFlorian\n."}], [{"role": "user", "text": "Hi,\nI have the following workflow: https://galaxy.embl.de/u/descoste/w/tesyatacseqmm10\nIt processes paired-end atac-seq data using mm10.\nThis is a copy of an existing functional workflow to which I just modified trimmomatic to trim galore. I imagine that my problem is coming from this tool.\nIndeed, the workflow blocks after bowtie2 and does not load all steps in the history.\nMy question is, is there any log file where I can see where is the problem coming from?\nIs the only way to solve the problem to rebuild the entire thing step by step?\nThx,\nNicolas"}, {"role": "system", "text": "Welcome, @descostes!\nThe server/shared link is not publically accessible, so we can\u2019t help to review the problem in detail.\nI can let you know that certain earlier versions of Trimmomatic didn\u2019t work well in workflows for some use cases. Changes included in recent Galaxy releases (most current version is 19.01) and to the Trimmomatic tool itself (most current version 0.36.6) resolved those issues.\nI would suggest contacting the administrators of that server to make sure Galaxy/Trimmomatic are at the most current versions, then modify the workflow to include that updated version of Trimmomatic. If that doesn\u2019t resolve the problem, then also ask them to update the other tools included your workflow (including Bowtie2), if not at the most current version available in the ToolShed, to eliminate other potential technical issues as being a factor.\nOther items to note:\n\n\nWorkflows populate output datasets into the history in the order of execution. This effectively means that not all steps will appear at the very start, but will show up as the processing proceeds over time. Now, this is somewhat dependent on the server resources, the tools invoked, and whether or not dataset collections are included. So might want to check your history now and see if more content has populated since you first wrote in.\n\n\nYou could also ask the administrators if the server is busy, causing a delay at the mapping step with Bowtie2. This is a computationally intensive tool and usually takes longer to execute than Trimmomatic.\n\n\nThere is a known issue regarding \u201cinput\u201d changes and workflow execution. The issue ticket is here, and you could try the workaround steps if you think this might be a factor: https://github.com/galaxyproject/galaxy/issues/7431.\n\n\nWe are working to resolve the issue above and discussing ways to make workflow execution issues/conflicts clearer for ends users to aid with troubleshooting, directly in the interface, with priority.\n\n\nFinal option:\n\n\nTest at the public server Galaxy Main https://usegalaxy.org to compare results.\n\n\nDownload/import the workflow, update tool versions (as needed), create a history with sample input data, and test it out to see what results. You might need to edit the workflow to remove any tools not available at this server (likely to be downstream of Trimmomatic/Bowtie2).\n\n\nIf it works at Galaxy Main, that confirms the issue is at the other server.\n\n\nIf it fails at Galaxy Main in the same way, a share link to that version of the workflow could be posted back, along with the shared history link, and we can review/help more that way.\n\n\nHope that gives some actionable choices to get this resolved!"}], [{"role": "user", "text": "Hi,\nI have encountered a problem with Galaxy analyses of RNAseq data when I want to analyze the differential expression using DESeq2 or EdgeR. Below are the typical error messages. I have used HISAT2 for as the alignment program, Stringtie for assembly, and both htseq count and featurecounts to count the reads. I have tried to use the output from either htseq count, featurecounts and Stringtie into DESeq2 or EdgeR with error messages popping up after a few minutes. The original data are from Illumina next seq, paired end. I am working with Galaxy Main https://usegalaxy.org.\nBelow is from DeSeq2\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator. Error in data.frame(\u2026, check.names = FALSE) : arguments imply differing number of rows: 51\nBelow from EdgeR\nFatal error: Exit code 1 () Error in data.frame(\u2026, check.names = FALSE) : arguments imply differing number of rows: 51329, 58444, 53946 Calls: do.call -> cbind -> cbind -> data.frame"}, {"role": "system", "text": "Check the header , your input file must had it or you got to turning that off"}, {"role": "user", "text": "Hi,\nThank for the response. I have tried turning off header option as well but still get error messages. Please see info below.\nThe data are from Illumina NextSeq 40 bp paired, TruSeq stranded RNA prep kit used\nWith header option selected:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nError in data.frame(\u2026, check.names = FALSE) :\narguments imply differing number of rows: 4, 58443\nCalls: get_deseq_dataset \u2026 eval -> eval -> eval -> .Method -> cbind -> data.frame\nWith header option turned off:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nError in Ops.factor(a$V1, l[[1]]$V1) :\nlevel sets of factors are different\nCalls: get_deseq_dataset \u2026 DESeqDataSetFromHTSeqCount -> sapply -> sapply -> lapply -> FUN -> Ops.factor\nWarning message:\nIn is.na(e1) | is.na(e2) :\nlonger object length is not a multiple of shorter object length\nOutput HISAT2 sample"}, {"role": "system", "text": "\n\n\n markussen:\n\narguments imply differing number of rows: 4, 58443\n\n\nIt looks like your data does not have a header (where the first error comes from). This can be inspected to confirm (no need to guess) by clicking on the \u201ceye\u201d icon to view the first lines of one (or all) of the count datasets. Galaxy will create headers if none are included already in the inputs.\n\n\n\n markussen:\n\nlonger object length is not a multiple of shorter object length\n\n\nThis sometimes comes up (second error) when the wrong input is selected by mistake. Example: summary/report data was input instead of count data.\nIf you would like to compare to sample data/workflows, please see the RNA-seq tutorials. \u201cReference-based RNA-Seq data analysis\u201d includes DESeq2.\n\n  \n      \n\n      galaxyproject.github.io\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "if your header was correct\u2026 maybe you make a mistake in loading your data in Deseq2 or EdgeR. you cant use a data twice in one run of Deseq2 or EdgeR. i think you do something wrong in loading Steps.take a Sc of your analyse data"}], [{"role": "user", "text": "Hi,\nI\u2019m unable to upload simple small (400 bytes) test file (fasta).\nThe spinner keeps spinning and nothing gets uploaded.\nI\u2019m getting 504 gateway time-out error from \u201c/api/upload/resumable_upload/\u201d. The method works on \u201cusegalaxy.org\u201d. I\u2019ve tried different browsers and internet providers, I\u2019ve also tried to switch autodetect and fasta format, but with no success.\nIs there anything I can do on my end (pasting file content is no-go for me, as actual files are much larger).\nThank you"}, {"role": "system", "text": "I have encountered the same issue in the \u201cgalaxy.eu\u201d server.  Nothing uploads. Even small fasta files."}, {"role": "system", "text": "Hi @marek and @osman.merdan\nThe UseGalaxy.eu server had a maintenance event earlier but seems to be up now. Maybe try again?\nStatus: https://status.galaxyproject.org/\nChat: You're invited to talk on Matrix"}, {"role": "user", "text": "Hello,\nI don\u2019t use galaxy often and it seems it was bad luck.\nHowever, the upload still does not work for me. This time, it reports different error (502)\nXHRPOSThttps://usegalaxy.eu/api/upload/resumable_upload/\n[HTTP/1.1 502 Bad Gateway 13ms]\n\nFailed because: Error: tus: unexpected response while creating upload, originated from request (method: POST, url: /api/upload/resumable_upload/, response code: 502, response text: <!DOCTYPE html>\n<html>\n\n   <head>\n      <title>Galaxy Europe, error page</title>\n      <link rel=\"canonical\" href=\"https://usegalaxy-eu.github.io/error_pages/502.html\">\n   </head>\n   <body>\n     <iframe src=\"https://usegalaxy-eu.github.io/error_pages/502.html\" style=\"position:fixed; top:0; left:0; bottom:0; right:0; width:100%; height:100%; border:none; margin:0; padding:0; overflow:hidden; z-index:999999;\">\n        <a href=\"https://usegalaxy-eu.github.io/error_pages/502.html\">502</a>\n     </iframe>\n   </body>\n</html>\n, request id: n/a)\n, will retry in 10 seconds\n"}, {"role": "system", "text": "Hi, same problem here with some BED files. From yesterday, nothing can be uploaded in Galaxy."}, {"role": "user", "text": "As of now, the upload works.\nThanks"}], [{"role": "user", "text": "I have posted the original question here, if you\u2019d like to answer from there too:\nbiostars\nI have an (old) Galaxy installation at a cluster that makes use of default python 2.7. I can see that there are 2 symlinks called python2 and python2.7 that maps to a python binary inside /galaxy/.venv/bin and calling it with --version returns\n2.7.14\nI recently installed a tool using tool.xml that has the following directive:\n<configfiles>\n  <inputs name=\"inputJSON\" filename=\"myConfig.json\" data_style=\"paths\"></inputs>\n<configfiles>\n\nThe above directive tells galaxy to parse the entire UI from XML and create a JSON representation of the user choices, which is then passed to a perl script.\nWhen I ran the tool I got the following error:\nImportError: No module named xml.dom.minidom\nSince the tool and the wrappers are written in node and perl, I am guessing this has something to do with galaxy dependencies that is required to parse the xml directive above.\nI checked the default python from $PATH and it\u2019s version is 2.7.14 as well. When I go to /usr/lib64/python2.7 or /usr/lib64/python I can see that there is a folder structure xml -> dom -> minidom.py. So it means I already have this module installed.\nNext I went to galaxy/.venv and I see there is a folder called lib and another called lib64 which points to lib itself. Inside the lib there are many python scripts that are symlinks inside /usr/lib64/python2.7/. For example galaxy/.venv/lib/types.py maps to /usr/lib64/python2.7/types.py .\nAt this point I have some questions:\n\nHow can solve this dependency issue?\n\nShall I create a folder symlink called xml inside galaxy/.venv/lib that maps to /usr/lib64/python2.7/xml ? Would that be sufficient ? - Tried and this did not help\n\nShouldn\u2019t these dependency symlinks be refreshed/created when galaxy is restarted? Because when I restart galaxy, I still get the same error, meaning galaxy is still not aware that this module exists.\n\n\nAdditional information:\n\n\nlsb_release -a yields SUSE 15.1\n\nI also have a local clone of galaxy (on Ubuntu) from the repo that uses python 3.8 and CANNOT reproduce the error above\nIn both galaxy instances (the local one that uses 3.8 and one at SUSE that uses 2.7) there is a file at location galaxy/lib/galaxy/util/__init__.py and here I can see both galaxy instances are trying to import  xml.dom.minidom. However, in the older galaxy\u2019s __init__.py there is an additional line at the start of the script: from __future__ import absolute_import. Would this affect the search path or cause the problem in this question?\n"}, {"role": "system", "text": "Hi @ibowankenobi,\nusually, Galaxy instances are configured to run tools in a conda environment or container. Those tools are independent of the Galaxy environment. How old is your Galaxy and do you see in your tool a \u201c\u201d section?\nCiao,\nBjoern"}, {"role": "user", "text": "Hello Bjoern,\nThe galaxy instance I am testing the tool on is at least 5-6 years old, running on python 2.7. I am not sure if conda is used under the hood  but it looks like it is either using what is available in $PYTHONPATH or the virtual environment under galaxy/.venv/bin/python.\nThe thing I that confuses me is when I do: galaxy/.venv/bin/python -c \"import xml.dom.minidom; print(\\\"hello\\\")\"  from the terminal it works OK and exits with 0. So the python can import xml.dom.minidom but somehow when run from the galaxy UI, I get the below stack trace:\nCannot open the input file!\nTraceback (most recent call last):\n  File \"/some/path/to/database/job_working_directory/019/19094/set_metadata_cW4TpH.py\", line 1, in <module>\n    from galaxy_ext.metadata.set_metadata import set_metadata; set_metadata()\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy_ext/metadata/set_metadata.py\", line 24, in <module>\n    import galaxy.model.mapping  # need to load this before we unpickle, in order to setup properties assigned by the mappers\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/model/__init__.py\", line 44, in <module>\n    import galaxy.model.metadata\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/model/metadata.py\", line 21, in <module>\n    from galaxy.util import (in_directory, listify, string_as_bool,\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/util/__init__.py\", line 25, in <module>\n    import xml.dom.minidom\nImportError: No module named xml.dom.minidom\n\nThe Cannot open the input file! is from my script that is run, the rest if from galaxy\u2019s side"}, {"role": "user", "text": "Hi Bjoern,\nLooking at the python that is used by default, it is at least 5-6 years old. I am sure if it is using conda under the hood, but I can see that it is either using $PYTHONPATH, system path or what is available under /galaxy/.venv/bin/python.\nInterestingly, when I run: /some/path/to/galaxy/.venv/bin/python -c \"import xml.dom.minidom; print(\\\"hello\\\");\", it prints and exits with 0. But when the tool is run from the UI I get the below stack trace:\nCannot open the input file!\nTraceback (most recent call last):\n  File \"/some/path/to/data/derived/Galaxy/database/job_working_directory/019/19094/set_metadata_cW4TpH.py\", line 1, in <module>\n    from galaxy_ext.metadata.set_metadata import set_metadata; set_metadata()\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy_ext/metadata/set_metadata.py\", line 24, in <module>\n    import galaxy.model.mapping  # need to load this before we unpickle, in order to setup properties assigned by the mappers\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/model/__init__.py\", line 44, in <module>\n    import galaxy.model.metadata\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/model/metadata.py\", line 21, in <module>\n    from galaxy.util import (in_directory, listify, string_as_bool,\n  File \"/some/path/to/galaxy/galaxy/lib/galaxy/util/__init__.py\", line 25, in <module>\n    import xml.dom.minidom\n\nImportError: No module named xml.dom.minidom\n\nCannot open the input file! is from my script, the rest is from galaxy\u2019s."}, {"role": "system", "text": "I don\u2019t really know. xml.dom is a standardmodule, so should be available. Do you have a manipulated PYTHONPATH somewhere? For example an xml.py file somewhere which confuses Python?"}], [{"role": "user", "text": "Hi, I\u2019m trying to run AnnotateMyIDs but I get this error; please what does it mean? and what should I do, I am still a beginner.\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 9: R: not found\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 10: Rscript: not found"}, {"role": "system", "text": "Update 11/16/2022\nThe indexes are restored at UseGalaxy.org. Prior failed runs should now complete successfully.\nThanks to all who reported the issue for their patience!\n\n\nHi @Nahla_OUARD\n\n\n\n Nahla_OUARD:\n\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 9: R: not found\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 10: Rscript: not found\n\n\nTechnically, this means that the tool did not have any valid input to send to sub-functions the tool is executing. This can be due to input problems or a transient cluster issue.\nIf you are actually working at UseGalaxy.org (I see a bug report about this same issue) \u2013  the built-in annotation for this tool and the upstream Featurecounts tool is an open known issue we are still working on. The problem started about 3 weeks ago, and will likely not be fixed until later next week. I\u2019ll mark this topic for an update.\nYou can try at UseGalaxy.eu or UseGalaxy.org.au instead for now OR supply a reference annotation GTF from the history instead of using the built-in annotation.\nTutorial\ntraining-material/search?query=annotatemyids"}], [{"role": "user", "text": "Hi,\nI submitted a job to produce a Circos Plot over 72 hours ago and it is still grey and waiting to run. It\u2019s a fair amount of information, and I\u2019ve read the other topics on this and so understand the process but this seems even longer than most. Could you advise please?\nThanks!"}, {"role": "system", "text": "Hi @Miniimi54 , we currently have an issue with Circos, and we are working to fix it asap"}, {"role": "user", "text": "Thanks for the prompt response! No worries, hoping its back up and running soon "}, {"role": "system", "text": "It should work now."}], [{"role": "user", "text": "Dear community,\ni run meryl for k-mer coverage counting due to the following workflow: Galaxy | Europe | Accessible Workflow | Kcov .\nWhen doing so with a paired end Illumina dataset it first work well, i got results.\nWhen repating the same workflow with more data the job does not failed (no error message) but the resulting .meryldb file is not suitable for histogram creation within the next step, it resulted in an empty file.\nDoes anybody know what the problem is? Too much data? Too less quota?\n\nAll the Best\nThomas\nMeryl\nDataset Information\nNumber 22\nName Meryl on data 19: read-db.meryldb\nCreated Monday Jul 5th 4:38:13 2021 UTC\nFilesize 6.9 MB\nDbkey ?\nFormat meryldb\nFile contents contents\nHistory Content API ID\n11ac94870d0bb33a65184c01b231c955\nHistory API ID\ne0706b5a535dc122\nUUID a86e5aae-b668-409d-9f9e-1bdec2668a33\nFull Path /data/dnb03/galaxy_db/files/a/8/6/dataset_a86e5aae-b668-409d-9f9e-1bdec2668a33.dat\nTool Parameters\nInput Parameter Value\nOperation type selector count-kmers\nCount operations Count: count the occurrences of canonical k-mers\nInput sequences\n19: Collapse Collection on data 18, data 17, and others \n\nK-mer size selector provide\nK-mer size 21\nJob Information\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/meryl/meryl/1.3+galaxy3\nCommand Line\nexport GALAXY_MEMORY_GB=$((${GALAXY_MEMORY_MB:-8192}/1024)) && meryl count k=21 memory=$GALAXY_MEMORY_GB threads=${GALAXY_SLOTS:-1} /data/dnb03/galaxy_db/files/d/0/9/dataset_d09368a3-8058-42f6-bd3e-0ba9db38611a.dat output read-db.meryl && echo \u2018K-mer size: 21\u2019 && tar -zcf read-db.meryldb read-db.meryl\nTool Standard Output\nK-mer size: 21\nTool Standard Error\nFound 1 command tree.\nCounting 111 (estimated) billion canonical 21-mers from 1 input file:\nsequence-file: /data/dnb03/galaxy_db/files/d/0/9/dataset_d09368a3-8058-42f6-bd3e-0ba9db38611a.dat\n\nSIMPLE MODE\n21-mers\n\u2192 4398046511104 entries for counts up to 65535.\n\u2192 64 Tbits memory used\n119496818480 input bases\n\u2192 expected max count of 477987273, needing 14 extra bits.\n\u2192 56 Tbits memory used\n15 TB memory needed\n\nCOMPLEX MODE\nprefix # of struct kmers/ segs/ min data total\nbits prefix memory prefix prefix memory memory memory\n 1     2  P   285 MB    13 GM    17 MS  8192  B   142 GB   142 GB\n\n 2     4  P   278 MB  7122 MM  8905 kS    16 kB   139 GB   139 GB\n\n 3     8  P   271 MB  3561 MM  4341 kS    32 kB   135 GB   135 GB\n\n 4    16  P   264 MB  1780 MM  2115 kS    64 kB   132 GB   132 GB\n\n 5    32  P   257 MB   890 MM  1030 kS   128 kB   128 GB   129 GB\n\n 6    64  P   250 MB   445 MM   500 kS   256 kB   125 GB   125 GB\n\n 7   128  P   243 MB   222 MM   243 kS   512 kB   121 GB   121 GB\n\n 8   256  P   237 MB   111 MM   118 kS  1024 kB   118 GB   118 GB\n\n 9   512  P   231 MB    55 MM    57 kS  2048 kB   114 GB   115 GB\n\n10  1024  P   225 MB    27 MM    27 kS  4096 kB   111 GB   111 GB\n\n11  2048  P   221 MB    13 MM    13 kS  8192 kB   107 GB   108 GB\n\n12  4096  P   221 MB  7122 kM  6680  S    16 MB   104 GB   104 GB\n\n13  8192  P   227 MB  3561 kM  3231  S    32 MB   100 GB   101 GB\n\n14    16 kP   245 MB  1780 kM  1559  S    64 MB    97 GB    97 GB\n\n15    32 kP   289 MB   890 kM   752  S   128 MB    94 GB    94 GB\n\n16    64 kP   383 MB   445 kM   362  S   256 MB    90 GB    90 GB\n\n17   128 kP   578 MB   222 kM   174  S   512 MB    87 GB    87 GB\n\n18   256 kP   976 MB   111 kM    84  S  1024 MB    84 GB    84 GB\n\n19   512 kP  1780 MB    55 kM    41  S  2048 MB    82 GB    83 GB\n\n20  1024 kP  3392 MB    27 kM    20  S  4096 MB    80 GB    83 GB  Best Value!\n\n21  2048 kP  6624 MB    13 kM    10  S  8192 MB    80 GB    86 GB\n\n22  4096 kP    12 GB  7123  M     5  S    16 GB    80 GB    92 GB\n\n23  8192 kP    25 GB  3562  M     3  S    32 GB    96 GB   121 GB\n\n24    16 MP    50 GB  1781  M     1  S    64 GB    64 GB   114 GB\n\n25    32 MP   101 GB   891  M     1  S   128 GB   128 GB   229 GB\n\n26    64 MP   202 GB   446  M     1  S   256 GB   256 GB   458 GB\n\n27   128 MP   405 GB   223  M     1  S   512 GB   512 GB   917 GB\n\n28   256 MP   810 GB   112  M     1  S  1024 GB  1024 GB  1834 GB\n\n\nFINAL CONFIGURATION\nEstimated to require 107 GB memory out of 110 GB allowed.\nEstimated to require 4 batches.\nConfigured complex mode for 107.359 GB memory per batch, and up to 4 batches.\nStart counting with THREADED method.\nUsed 10.589 GB / 109.922 GB to store 0 kmers; need 0.000 GB to sort 0 kmers\nInput complete. Writing results to \u2018read-db.meryl\u2019, using 10 threads.\nfinishIteration()\u2013\nFinished counting.\nCleaning up.\nBye.\nTool Exit Code: 0\nJob API ID: 11ac94870d0bb33ad73bde806337fc6c\nDataset Storage\nThis dataset is stored in a Galaxy object store with id files10.\nInheritance Chain\nMeryl on data 19: read-db.meryldb\nJob Metrics\ncgroup\nMemory softlimit on cgroup 0 bytes\nWas OOM Killer active? No\nOOM Control enabled No\nMax memory usage (MEM+SWP) 106.9 GB\nMemory limit on cgroup (MEM+SWP) 8.0 EB\nMax memory usage (MEM) 106.9 GB\nMemory limit on cgroup (MEM) 110.0 GB\nFailed to allocate memory count 0\nCPU Time 21 minutes\ncore\nJob Runtime (Wall Clock) 26 minutes\nJob End Time 2021-07-05 19:04:55\nJob Start Time 2021-07-05 18:38:25\nMemory Allocated (MB) 112640\nCores Allocated 10\nhostname\nhostname vgcnbwc-worker-c125m425-7047.novalocal\nAWS estimate\n1.06 USD\nThis job requested 10 cores and 110 Gb. Given this, the smallest EC2 machine we could find is m4.10xlarge (160 GB / 40 vCPUs / Intel Xeon E5-2676 v3 (Haswell)). That instance is priced at 2.4 USD/hour.\nPlease note, that those numbers are only estimates, all jobs are always free of charge for all users.\nDataset peek\nCompressed binary file\ncancel"}, {"role": "system", "text": "Hi @fruitphytodoc,\nit seems a memory issue. Could you try to run meryl count on the collection instead of collapsing the collection in a single datafile? After that you can merge the collection of .meryldb files by using operation on kmers:union-sum.\nRegards"}, {"role": "user", "text": "Dear @gallardoalba,\nthanx i will try this! Give you a reply when I did it.\nAll the Best\nthomas"}, {"role": "user", "text": "Hi @hxr, @gaalardoalba,\nso due to the problem \u201cpossible error in meryl\u201d, it is still persistent also after using a smaller dataset, without and with collapsing th datasets.\nWhen using Illumina reads for k-mer counting the tool creates an output, which you can then use as input for histogram calculation. But it produces an empty file? Please how could we have a look on that issue?\nRegards"}, {"role": "system", "text": "Hi @fruitphytodoc,\nthe error seems to be related to the compressed files; I opened an issue about it Meryl: wrong outputs with compressed FASTQ files \u00b7 Issue #3801 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub. Could you try to decompress the fastq.gz files and relaunch the analysis? You can uncompress your files in that way:\n\n  \n    \n  \n\n\nRegards"}], [{"role": "user", "text": "Hello, I have a fastq file with this header.\n@1/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@2/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@3/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\nBut I need to replace the header from \u201c/1\u201d to \u201c/2\u201d\nso it looks like this.\n@1/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@2/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@3/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\nHow can I replace this to every line?\nIf the Galaxy tool is \u201cManipulate FASTQ\u201d, I wanna know the string syntax. Many thanks!\nMiae"}, {"role": "system", "text": "Hello @miaeyaho\nUpdate \u2013 Upon further review, have determined that Manipulate FASTQ is NOT a good tool choice for this particular task (for technical reasons around how the tool is designed). The tool does not interpret the entire string as a \u201cword\u201d. Instead, the function performs a direct translation, per character. This is useful in other situations, but not for this one.\nHow it works: the first character in the \u201cfrom\u201d string is translated to the first character in the \u201cto\u201d string; the second translated to the second; etc.\nHow this poorly impacts the result: All 1 characters will be translated to a 2 characters. Not just those following a /. The / is replaced by another /.\nApologies, I should have tested this out first. Try Sed as described in the other followup post. There is still likely a Python-version issue with the Manipulate FASTQ tool at usegalaxy.eu \u2026 and it will be addressed \u2026 but even once that is done, don\u2019t use this tool for /1 > /2 manipulation.\nThere are many ways/tools to \u201cfind/replace\u201d data content, across datatypes, and the Sed tool is one good choice. The \u201cfind\u201d portion of the substitution function can be customized to fit any text characters, and the \u201creplace\u201d can also be fully customized. Other \u201cfind/replace\u201d tools were created to make data substitution functions easier to perform \u2013 search the tool panel with the keyword \u201creplace\u201d to explore options if interested. Then test out your desired manipulation across the tools and see which produce the result you want \u2013 each act on data a little bit differently.\nThat said, sed is worth learning how to use for many, even if just the simpler functions (like substations, aka s/find/replace/. It is a common and stable command-line utility. The way it is used/interpreted in Galaxy is near exact to command-line usage. A web search will return many resources for sed usage, examples, tutorials, tips&tricks, Q&A. There are also many \u201cregular expression\u201d syntax resources (including web forms where expressions can be \u201ctested\u201d as interpreted across several common utilities/scripting languages). Tool forms generally note which \u201ctype\u201d of regular expression syntax is supported by the tool, along with basic syntax descriptions, examples, and link-outs to resources.\n\nYes, that would be a good tool choice.  Incorrect, apologies! Everything below is WRONG for the original usage, but I\u2019m leaving it posted since sometimes explaining what doesn\u2019t work helps everyone to learn, discover and understand what will work, and importantly, why.\nString translate the name/identifier. All reads in the file will be processed with the default \u201cMatch Reads\u201d settings.\n\nFrom: \\/1\n\nTo: \\/2\n\n\nScreenshot:\nmanipulate-fasta-seq-name-string-translate1300\u00d71036 63.2 KB\nThanks!"}, {"role": "user", "text": "Thank you Jennifer, but unfortunately things didn\u2019t go well.\nI\u2019ve got the error message of SyntaxError as below.\nimage662\u00d7925 23 KB\nWhat is the problem?\nThanks,\nMiae"}, {"role": "system", "text": "Hi @miaeyaho\nThe usegalaxy.eu server is pending some changes to adapt to Python 3. This error is related to that. Other tools may also fail for this same reason until addressed \u2013 possibly any written in Python. Ping @wm75 @bjoern.gruening @mvdbeek\nFor your purposes, try Sed instead. This is the \u201cprogram\u201d substitution expression to use. It is specific enough that quality scores will not be modified, just the format of your particular fastq sequence headers.\ns/(^@[0-9]+)\\/1$/\\1\\/2/ \n\nScreenshot:\nmanipulate-fasta-seq-name-sed1486\u00d7422 25.3 KB\nPlease give that a try. Thanks!"}], [{"role": "user", "text": "Now, I installed Galaxy Docker 20.05 and tried to join two datasets using \u2019 Join Two Datasets side by side 2.1.3\u2019. I can\u2019t get the tool to run it. Error like this : 'No module named \u2018galaxy\u2019. After searching previous list, I noticed that there were technical problem in this module and fixed it now. But, this fault is not fixed in Docker image 20.05. So, I tried to download other version of \u2018join two datasets side by side\u2019 on tool shed and didn\u2019t find anything. Are there any other option to solve this problem?"}, {"role": "system", "text": "Is this the only tool failing like this?"}, {"role": "system", "text": "Hi everyone! I had this same problem. But, I have a recent Galaxy installed in our local server, but not by the Docker.\nFor me, the error message is:\ngalaxy/tools/filters/join.py\", line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nAny tip to fix it?\nThanks"}, {"role": "system", "text": "Hi everyone,\nI am facing same problem. @lfdeoliveira did you solve this?\nI\u2019d highly appreciate if anyone can comment down the solution below."}, {"role": "system", "text": "Update:\nThe Galaxy Docker release has been updated since this question was originally asked, and dependencies for this tool would be managed correctly in that environment now using default settings.\nThe later question didn\u2019t involve a Docker image but a stand-alone Galaxy instance. The problem was due to a configuration issue that has been discussed in this Gitter chat. In short, a dependency was missing.\n\n@lfdeoliveira  First, make sure that you installed the most current Galaxy release, that you setup the installation directory correctly, and review that Gitter chat help. You are probably missing that same dependency.\nFor a very simple local Galaxy intended to be used by a single person, these instructions are still valid, and it it would be easiest to install tools from the ToolShed using managed dependencies, especially if you are new to server administration.\n\n  \n    \n    \n    Example step-by-step for a basic local Galaxy install with an admin account created \n  \n  \n    Hello, \nI am just getting started with developing Galaxy. \nI cloned the repository onto my local computer - MacOS running High Sierra 10.13.6. \nNext, I created a virtual environment, activated it, and then tried the run.sh script. \nA lot of things were downloaded and installed, but then the script died with this error: \nRequirement already satisfied: setuptools in ./.venv/lib/python2.7/site-packages (from sphinx==1.7.2->-r ./lib/galaxy/dependencies/dev-requirements.txt (line 32)) (41.0.1)\nUnsett\u2026\n  \n\n\nIf that does not resolve the problem, and you need more help, please ask your new question in a new topic and include enough details so we can help. This would include at a minimum: the version/source of Galaxy, details about the local environment (OS, python version), and any admin configuration changes you have done so far, including tool installs and how those were done. We might need to bring in expertise from the Galaxy Admin working group, but it is fine to get that started here."}], [{"role": "user", "text": "how can i detach my collection.  i turn my pair end data into collection because its easier for analyses. but now i what to detach them to use them separately in limma. is there tool or options to do that ?"}, {"role": "system", "text": "The original datasets that you created your collection with should be in your history, hidden. You can also use the Extract Dataset collection operation."}, {"role": "user", "text": "thanks for your reply. i dont need my original datasets. because i did some analysis to them. and know i just need these collections, detach. for example , i had 400 paired end samples. and know i have 2 collection which each have 200 samples in it. So i need those too detach so i can use them separately in next step. Extract dataset just give me one dataset to use. i need all of them"}, {"role": "system", "text": "\n\n\n marten:\n\nThe original datasets that you created your collection with\n\n\nSome extra clarification: \u201cOriginal\u201d means the data inside your target collection before extracting from it. Each collection includes hidden datasets."}, {"role": "user", "text": "i think i found them, but in large datasets like 500, its too hard too search them one by one in there, if there way a way to simply detach the collections, then analyses the large datasets would be a lot easier."}, {"role": "system", "text": "Have a look at this presentation from yesterday at GCC2019\nSlides: http://bit.ly/complextutorial\nIt covers many collection operations, tutorials that focus on collections, workflow features, and more. Most of this is fairly new and it links up to the newest GTN tutorials that cover the same concepts/functions.\nIn short, it is worth learning how to construct your collections with metadata that makes it easier to sort out the data downstream.\n\n\n\n amir:\n\nits too hard too search them one by one in there\n\n\nFor this part \u2013 maybe unhide the datasets and use the search at the top of the history panel? Searches will be easier if your collections have more metadata but perhaps will help for now? Or use the tools in the group \u201cCollection Operations\u201d. All collection datasets have element identifiers and there are tools to find out what that metadata currently is plus then manipulate it and/or extra datasets based on that info.\nThanks!"}], [{"role": "user", "text": "Hi,\nDoes anyone else have trouble running the PAP structural workflow?\nFor me it has been running for days now, and then it is now halted at #14: \u2018MGA to GFF3\u2019\nI have just uploaded my fasta file and run the workflow, with all default settings except for the \u2018Create or Update organism\u2019.\n/Lene"}, {"role": "system", "text": "Hi @lsmr\n\n\n\n lsmr:\n\nthen it is now halted\n\n\nI\u2019m not sure what this means. Is the job still running for a long time? Or, did it error? Or, did downstream tools end up in a paused state? Have you checked the last successful job\u2019s dataset to see if it contains data with no metadata warnings?\nA few more details would also help if that doesn\u2019t solve your problem:\n\nWhat server are you working at? URL?\nPublic Galaxy or private?\nShare link to the workflow. If associated with a tutorial, share a link to that as well will help with context.\n\nThanks!"}, {"role": "user", "text": "hi,\nit did error when trying to run \u2018MGA to GFF3\u2019 on Fasta file input without description (first step in the workflow) and on \u2018MetaGeneAnnotator\u2019.\nI tried to re-run it, and now it error at the \u2018MetaGeneAnnotator\u2019:\nFatal error: Exit code 127 ()\n/galaxy/database/jobs_directory/000/183/183563/tool_script.sh: line 9: mga_linux_x64: command not found\nIt is some of the very first steps in the workflow that errors, and all I do is to upload my fasta file for it to use."}, {"role": "system", "text": "It is still not clear where you are working, where the tool was sourced, and what workflow you are using.\nIt appears to be your own local Galaxy and I didn\u2019t find that tool in the ToolShed https://usegalaxy.org/toolshed.\nPlease clarify as requested above:\n\n\n\n jennaj:\n\nA few more details would also help if that doesn\u2019t solve your problem:\n\nWhat server are you working at? URL?\nPublic Galaxy or private?\nShare link to the workflow. If associated with a tutorial, share a link to that as well will help with context.\n\n\n"}, {"role": "system", "text": "This might be the workflow you are using: https://cpt.tamu.edu/galaxy-pub/u/cory-maughmer/w/pap-2019-structural-workflow-v810\nIf the workflow is erroring at different steps, the jobs may be exceeding the computational resources where you are working. Or something else is wrong. You\u2019ll need to contact their support \u2013 many of these tools are custom for this domain-specific server.\nEach public Galaxy is independently administered. If that server accepts bug reports, send one in. Or you can contact them directly. Some Galaxy servers post contact on the home page and/or here: https://galaxyproject.org/use/.\nContact for this server (and the workflow author) happen to be posted on the homepage of the server: https://cpt.tamu.edu/galaxy-pub plus also here https://galaxyproject.org/use/center-for-phage-technology-cpt/."}], [{"role": "user", "text": "Hi everyone,\nI would like to delete all lines from my vcf, where ALT=\".\" in order to only have the variants represented in my vcf. I thought I could youse the genotype filter in the tool vcffilter, but I actually want to keep those lines where an ALT allele is reported, even though GT=\"./.\".\nI would be glad for help.\nThanks a lot!\nRose"}, {"role": "system", "text": "Hi Rose,\nI cannot think of a dedicated tool for this job right now, but it\u2019s always good to remember that VCF is just a specialized (albeit rather complicated) form of tabular data. As such you can use on them many of the general Text Manipulation and Filter tools that most Galaxy instances offer.\nIn particular, you could use https://usegalaxy.org/root?tool_id=Filter1 as a simple means to achieve your goal though it will require you to specify the number of header rows in your VCF under Number of header lines to skip (which somewhat confusingly means \u201cnumber of header lines to except from filtering\u201d).\nIf you can\u2019t do that ( for example, because you need to perform this step as part of a workflow on input datasets with a variable number of header lines), you could use the more flexible, but also harder to configure https://usegalaxy.org/root?tool_id=Grep1 tool with a Matching regular expression pattern of ^((.+\\t){4}[^.]\\t)|#.+ (which should retain all lines where either the 5th tab-separated column is not a literal . or that start with a comment # symbol)."}, {"role": "user", "text": "Hi Wolfgang,\nthanks for the two options.\n\n\n\n wm75:\n\nhttps://usegalaxy.org/root?tool_id=Filter1\n\n\nThis works very well. I checked all vcfs (21) and they all have the same number of header lines. So I guess this too would even work when using it for a workflow.\n\n\n\n wm75:\n\nhttps://usegalaxy.org/root?tool_id=Grep1\n\n\nThis tool for some reason did not work for me with the expression ^((.+\\t){4}[^.]\\t)|#.+\nInstead of 4800 lines with variants (where ALT isn\u2019t \u201c.\u201d) and 60 header lines, I am left with only 4425 lines and 60 header lines.\nThanks for the fast help! I am glad one of the options worked on my data!\nAll the best Rose"}, {"role": "system", "text": "Glad to hear the first solution is working for you. Just to have this documented properly, here\u2019s the correct version of my second suggestion (the first version wasn\u2019t tested carefully, sorry for that).\nThis regex pattern should work: ^(([^\\t]+\\t){4}[^.])|#.+"}, {"role": "user", "text": "Hi @wm75 ,\nthanks for the correction. I tried the new pattern again and now it is working again. I thought the pattern through in order to understand it correctly and was wondering why I could not use the following: \t^((.+\\t.+\\t.+\\t.+\\t)([.]+\\t)(.*))|#.+\n\u201cMy\u201d pattern left me with 0 lines.\nMay I ask you for an explanation?\nThanks All the best Rose\nPS: I was using this pattern below in the Tool \u201cReplace text in entire line\u201d in order to be left only the first annotation in the ANN Info-Tag. This worked fine. With this knowledge in mind I came up with the pattern for the \u201cSelect lines which the expression\u201d tool.\nTool:https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/1.1.2 auf usegalaxy.eu\nFind pattern: (.+\\t.+\\t.+\\t.+\\t.+\\t.+\\t.+\\t)(.+;)(ANN=[^,\\t;]+)(,[^;\\t]+)(.*)\nReplace with: \\1\\2\\3\\5"}, {"role": "system", "text": "The main problem with your pattern is that .+\\t acts greedily, i.e. it tries to match as much of each line as possible while still enabling a match to the rest of the pattern. That is, the first .+\\t may actually already span several columns.\nIn your previous pattern with ANN=, you prevent this by anchoring the match at that string (provided that ANN= occurs only once per line in the INFO column). In your current pattern, however, the [.]+\\t is too ambiguous since you may have other columns after ALT ending in a . (my guess would be the FILTER column).\nThis is why my revised suggestion defines a column explicitly as at least one character, which is not a TAB and which is followed by a TAB. The complete pattern then requires four such columns followed by a character that is not a . (or, alternatively, a # as the first character on the line) followed by additional characters.\nIf you prefer, you could also use the following NOT Matching pattern, which is slightly shorter: ^([^\\t]+\\t){4}\\. and simply says: skip all lines that are starting with four tab-separated columns and a . at the start of the 5th.\nIn general, you can test your patterns locally in a Linux terminal with:\ngrep -P 'your_pattern_in_single_quotes' input_file   # for a Matching pattern\ngrep -vP 'your_pattern_in_single_quotes' input_file   # for a NOT Matching pattern\nSimilarly use grep -cP or grep -cvP if you just want to count the number of lines you\u2019d keep.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hi. I want to transfer a history that someone shared with me. I imported it into a Galaxy account I had on one server (temporary- as part of a class I did). Now I want to import it into my regular Galaxy account. I tried using the email option, but then it says I can\u2019t transfer to myself, so then I tried creating and copying the URL, but nothing appears in my Histories or Shared Histories. I used Import from File to do this - wrong way? Thanks."}, {"role": "system", "text": "If the server is still up and public, or you have downloaded the history archive, then moving the history to another server is possible.\nYou state that the accounts are on different Galaxy servers. This is why the share by email didn\u2019t work. To share with another user by email, both accounts must be on the same server. All accounts on the same Galaxy server will have distinct registered email addresses.\nGalaxy FAQs:\nOptions for Data Sharing\nLoading/Downloading Data\nTips:\n\nMake sure the history is set as \u201cShared\u201d at the original server. This is required.\nClick on the link in the archiving message (blue box) to get an updated status until noted as being completed and ready for import/download. The larger the history, the longer it will take the archive to build. Permanently deleting any data not needed will make the history smaller and the archive creation faster.\nOnce the archive is ready, clicking on the link a second time will launch a download dialogue (where to save it, etc). This can be dismissed if you are not going to download it. Copy the URL instead for a direct Galaxy-to-Galaxy transfer.\nYou can navigate away from the export page and do other thing while waiting (including logging out). But be sure to watch it, archived histories remain on servers for a set length of time (varies by the server, generally about a day or so).\nIf clicking on \u201cExport to File\u201d starts the archive creation again instead of taking you back to the original archive already in progress, the server might not support archive creation or you may have waited too long to check back.\n\nDownloading a history archive locally from one server, then uploading it to another, works for almost all cases regardless of firewalls and other server configuration. If transferring by URL fails, try that instead. Keeping a backup of your work is a good idea anyway, plus these archives can be uncompressed to use the included datasets for other purposes.\n"}], [{"role": "user", "text": "Hi, I need help. I have the genome of lens ervoides (legume) and need to find genes and symbiosis protein sequences associated with rhizobia using known protein sequences in other legumes. How do i start? How is the work flow?"}, {"role": "system", "text": "Hi @Dardo_Dallachiesa,\ndo you have a list of genes of interest?"}, {"role": "user", "text": "Yes @gallardoalba . NFR1 and NFR5 (Nod Factor receptor, kinase), NSP1 (nodulation signal pathway, transcrition factor) and CCamK (another kinase). Thanks"}, {"role": "system", "text": "Hi @Dardo_Dallachiesa,\nI think that the best approach would be to download the gene sequences of the most closely related species from the NCBI (in that case, by restrincting the search to the Fabaceae family), and then use the NCBI BLAST+ blastn tool in Galaxy to identify the target sequence in the reference genome. I performed a similar analysis which can be useful to guide you.\nRegards"}], [{"role": "user", "text": "Hi everybody. I\u2019m trying to upload my tool in the public tool shed following this guide (not using Planemo): How to publish a tool in the Tool Shed - Galaxy Community Hub\nIn my local instance of Galaxy, in the admin tab I can see the repository but it says that it is not possible to install it, which is confirmed also by the repository tab in the tool shed.\nI just loaded a tar file with the xml and the python code of the tool, am I missing something else?\nThank you for the help"}, {"role": "system", "text": "Hi @Andrea_Furlani,\ndoes it include the .shed.yml file? Could you share the repository link with me?\nRegards"}, {"role": "user", "text": "Hi, this is the repository link (just a test):\nhttps://toolshed.g2.bx.psu.edu/view/andrea.furlani/infr_test/bf610d149a69\nIt doesn\u2019t include a yml file, just the .py codes and .xml definitions"}, {"role": "system", "text": "Hi, I found an error while validating the sys_heatmap.xml file:\nApplying linter tool_xsd... FAIL\n.. ERROR: Invalid XML found in file: pheno_sys.xml. Errors [/tmp/tmpiy7_00_p:18:0:ERROR:SCHEMASV:SCHEMAV_CVC_COMPLEX_TYPE_3_2_1: Element 'param', attribute 'labels': The attribute 'labels' is not allowed.]\n\nI think that it is the problem; I recommend you to use planemo for developing the Galaxy wrapper.\nRegards"}, {"role": "user", "text": "Thank you for the suggestion. I\u2019m using planemo and I solved the error indicated in your reply, but still the lint fails retrieving this message:\nLinting repository /Users/andrea/Desktop/galaxy/tools/personal_tools/impc_tool\nApplying linter expansion... CHECK\n.. INFO: Included files all found.\nApplying linter tool_dependencies_xsd... CHECK\n.. INFO: No tool_dependencies.xml, skipping.\nApplying linter tool_dependencies_sha256sum... CHECK\n.. INFO: No tool_dependencies.xml, skipping.\nApplying linter tool_dependencies_actions... CHECK\n.. INFO: No tool_dependencies.xml, skipping.\nApplying linter repository_dependencies... CHECK\n.. INFO: No repository_dependencies.xml, skipping.\nApplying linter shed_yaml... CHECK\n.. INFO: .shed.yml found and appears to be valid YAML.\nApplying linter readme... CHECK\n.. INFO: No README found skipping.\n+Linting tool /Users/andrea/Desktop/galaxy/tools/personal_tools/impc_tool/impc_tool.xml\nApplying linter tests... WARNING\n.. WARNING: No tests found, most tools should define test cases.\n.. WARNING: No valid test(s) found.\nApplying linter output... CHECK\n.. INFO: 1 outputs found.\nApplying linter inputs... CHECK\n.. INFO: Found 43 input parameters.\nApplying linter help... CHECK\n.. CHECK: Tool contains help section.\n.. CHECK: Help contains valid reStructuredText.\nApplying linter general... WARNING\n.. WARNING: Requirement requests defines no version\n.. WARNING: Requirement pandas defines no version\n.. WARNING: Requirement lxml defines no version\n.. CHECK: Tool defines a version [0.1.0].\n.. CHECK: Tool defines a name [IMPC].\n.. CHECK: Tool defines an id [IMPC_query_tool].\n.. CHECK: Tool targets 16.01 Galaxy profile.\nApplying linter command... CHECK\n.. INFO: Tool contains a command.\nApplying linter citations... WARNING\n.. WARNING: No citations found, consider adding citations to your tool.\nApplying linter tool_xsd... CHECK\n.. INFO: File validates against XML schema.\nFailed linting\n\n\nRegarding the warnings: I saw that also in other tools already present in the toolshed the version of the requirements is not always indicated and the citation section often is not present or inserted in the help since they are not using bibtex or doi citations types.\nIn the folder there are: tool.py , tool.xml , .shed.yml\nHere\u2019s the temporary link to the repository I loaded manually."}, {"role": "user", "text": "I found the solution by using the following command for lint:\nplanemo shed_lint --tools --fail_level error\n\nIn this way it will not compile only in presence of errors."}], [{"role": "user", "text": "Hello when i use fastq spliter to separate the forward and reverse strand no error but the final result contain no sequence shows 0 byte result"}, {"role": "system", "text": "Hi - A bit more information will help to troubleshoot:\n1 - What version of the tool are you using? Click on the rerun button and copy the name/version of the tool back in the reply.\n2 - Where are you running the tool? If a public server, capture the base URL. If your own Galaxy, what version was installed?\n3 - What do the sequences look like? Please copy/paste back in a few complete fastq records (all four lines).\nThanks!"}, {"role": "system", "text": "Update: I was able to find your account at the Galaxy Main https://usegalaxy.org public server. No more information is needed.\nWhat to do:\nThe data is an interleaved/interlaced fastq dataset from NBCI. The tool you want to use instead is the Fastq De-interlacer \u2013 this will output two datasets, one representing the forward reads and one representing the reverse reads.\nFull details are in the Galaxy support FAQs: https://galaxyproject.org/support/#getting-inputs-right\n\nPlease see: Reformatting fastq data loaded with NCBI SRA. Specifically, here: https://galaxyproject.org/support/ncbi-sra-fastq/#interlaced-forward-and-reverse-reads\n\n"}], [{"role": "user", "text": "Hi,\nCould the admin for this repo (https://depot.galaxyproject.org/yum/package/slurm/18.08/7/x86_64/) help to recompile\n\nslurm-drmaa.x86_64 : DRMAA for Slurm\nslurm-drmaa-debuginfo.x86_64 : Debug information for package slurm-drmaa\n\nThe DRMAA rpm is breaking as it is not meant for the slurm version in this repo.\nThe slurm rpm in the repo provides .so.33 and drmaa in this repo requires .so.31\n\nError: Package: slurm-drmaa-1.1.0-1.el7.x86_64 (slurm-18.08)\nRequires: libslurm.so.31()(64bit)\nError: Package: slurm-drmaa-1.1.0-1.el7.x86_64 (slurm-18.08)\nRequires: libslurmdb.so.31()(64bit)\nYou could try using --skip-broken to work around the problem\n\nThank you."}, {"role": "user", "text": "Could anyone point me to a repo admin?"}, {"role": "system", "text": "\n\n\n lwj5:\n\nIndex of /yum/package/slurm/18.08/7/x86_64/\n\n\nHi @lwj5!!\nI\u2019ve asked the developers for some feedback/help at Gitter: galaxyproject/dev - Gitter\nThey may reply here or at the Gitter chat. Please feel free to jump in there for more interactive feedback. Just keep in mind that communications are asynchronous, as the developers are from all over the world in different time zones. You may hit a time when they are around, and might need to wait a day or so, depending on where you are \nSorry, this got missed originally but I\u2019ll keep watch over it until resolved one way or another.\nThanks!!"}, {"role": "user", "text": "Hi @jennaj\nNo worries, thanks for the reply and the post on gitter! I see natefoo, the owner of slurm-drmaa will be fixing the problem.\nThanks a lot for the help!"}, {"role": "system", "text": "This should be fixed now, sorry for the delay."}], [{"role": "user", "text": "I would like to perform an analysis of non-templated adenosine residues added to the end of RNAs in bacteria. In bacteria poly(A) sequences, if they are present, are generally quite short. What tools could I use to identify the location and length distribution of such non-templated residues using RNA-seq data?"}, {"role": "system", "text": "Hi @cjain\nI guess it depends on data and information you have. Maybe find a paper describing a similar analysis and follow the protocol.\nKind regards,\nIgor"}, {"role": "user", "text": "Thanks!. I did find some papers (e.g., PMID 35524564), but am not sure how to apply the methods described in the paper using Galaxy.\nRegards,\ncjain"}, {"role": "system", "text": "Hi @cjain\nfrom a quick look, public Galaxy servers have tools used in the paper: bowtie1 (Map with Bowtie for Illumina), cutadapt, trimming tool from fastx (Trim sequences)., SAMtools, bedtools.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi. I recently tried to replicate the results of a Galaxy tutorial for variant analysis using usegalaxy.eu server.\nBut when almost half of the analysis was done, I came to know that some tools used in the tutorial are only found at usegalaxy.org server. In fact all the analysis was done on usegalaxy.org server. Now I am doing again all the analysis on usegalaxy.org.\nIs there any way that I can copy all the completed steps of usegalaxy.eu on usegalaxy.org, so that I don\u2019t have to run again all the steps, coz it is taking so much time for me.\nCan please someone guide me about this problem? Or is there is any way I can \u201ctransfer\u201d (use) the tools found on usegalaxy.org to usegalaxy.eu server?\nWaiting for your kind responses,\nRegards,\n~Huzaifa"}, {"role": "system", "text": "Hi @hhuziii\nSeems your topic post was missed by the community. In case you still need help, or someone else wants more clarification about your questions later on (are likely things other new Galaxy users are interested in) \u2026 replies are below. Thanks!\n\nQuestion 1: Why are not all tools in each GTN tutorial hosted at all public Galaxy servers? How can I find out which server to use, per tutorial?\nHelp 1: Servers host different tools. You can check to see which public Galaxy servers are good choices per tutorial. These can change over time, and a server may actually host the tools but not be listed because the tool version was updated (meaning, the tool can be used but perhaps not exactly as described in the tutorial). How to use an updated tool is usually possible to figure out \u2013 but if you are not sure, can always ask \u2013 here (new topic) or even better at the GTN Gitter chat: https://gitter.im/Galaxy-Training-Network/Lobby (asking there may even get the tutorial updated).\nNotes: The particular tutorial you reference has a dedicated Docker image and isn\u2019t technically/exactly supported by any public Galaxy server \u2013 but you may still be able to use it at some of them.\nScreenshots and more info below:\n\n\n\n hhuziii:\n\nBut when almost half of the analysis was done, I came to know that some tools used in the tutorial are only found at usegalaxy.org server. In fact all the analysis was done on usegalaxy.org server. Now I am doing again all the analysis on usegalaxy.org.\n\n\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances globe icon menu  and review the Public Galaxy  server choices. If a tutorial is supported by a pre-configured Galaxy Docker  training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nScreenshot of the Hands-on for the tutorial you mention (the \u201cglobe\u201d icon menu is also here, for each tutorial):\ngtn-tutorial-available-public-galaxy-servers-within-hands-on1910\u00d71402 375 KB\n\nQuestion 2: Can I personally move data from one Galaxy server to another? Including public Galaxy servers?\nHelp 2: Yes. Datasets, Histories, Workflows can all be transferred directly (or downloaded to a file then uploaded to a public/private Galaxy again). For Datasets, click on the \u201cdisk\u201d icon in the expanded dataset, copy the URL, and paste that into the Upload tool at the other server. For entire Histories, create a History archive (option in the History menu \u2013 accessed from gear icon at the top right in the \u201cAnalyze data\u201d view) \u2013 once done, copy the link and paste it into the User > Histories \u201cImport from file\u201d button\u2019s function. For Workflows, go to Workflows, click on the workflow\u2019s menu and choose Share \u2013 on the next screen under Export, click on the Download button to generate a URL \u2013 paste that URL into the other server under Workflow > Import.\nNotes: On some \u201cfrom\u201d servers, Datasets may need to be in a History in a \u201cShared by URL\u201d state \u2013 and be sure to also check the box to share included datasets!. History archives always require the History (and included datasets) to be in a \u201cShared by URL\u201d state (do this before generating the archive). Histories \u2013 the History may need to be in a \u201cShared by URL\u201d state (another option in the History Menu) \u2013 be sure to also check the box to share included datasets!\n\n\n\n hhuziii:\n\nIs there any way that I can copy all the completed steps of usegalaxy.eu on usegalaxy.org, so that I don\u2019t have to run again all the steps, coz it is taking so much time for me.\nCan please someone guide me about this problem?\n\n\n\nQuestion 3: Can I personally move tools from one public Galaxy server to another?\nHelp 3: No but you can request that tools are installed at other servers. Or set up your own Galaxy and install any tools you want (since you\u2019ll be the administrator!). The tools at any public Galaxy server are determined by the server administrators.\nNotes: What is the tool not available at usegalaxy.eu? Include the full name/version from the top of the tool form. The tool may be no longer be supported, have an updated version, or be replaced. Or, maybe it has simply not been added yet. We can let you know and ask the usegalaxy.eu administrators if some tool install seems needed/possible \u2013 but no promises, they make the final decision.\n\n\n\n hhuziii:\n\nOr is there is any way I can \u201ctransfer\u201d (use) the tools found on usegalaxy.org to usegalaxy.eu server?\n\n\n\nThere are other ways to transfer/move data between Galaxy servers or to download data as a backup. I\u2019ve added a few more tags to your post with much more Q&A help that covers a broad range of use cases around this.\nWe also just added in new features that touch on these functions \u2013 so all of the above will go into a new FAQ after the next release and older FAQs will be updated. Tutorials may also be updated or added.\nBest!"}], [{"role": "user", "text": "I have problems with FastQ interlacer,\nthis work is taking too long to be completed"}, {"role": "system", "text": "Hello @jaqueap\nPlease see the advice in this prior Q&A. It would apply for most public Galaxy servers, not just https://usegalaxy.org.\n\n  \n    \n    \n    Queue time for Trinity usegalaxy.org support\n  \n  \n    Hello @ngwj \nQueue times can vary by time of day, day of week, the tool, and how many other people are running that same tool at the same time. The best strategy is to start jobs up then allow them to run. Avoid deleting and rerunning in hopes of jobs running faster, as that only puts your job back at the end of the queue, extending wait time. \nIf there are server issues, this is where to check: https://galaxyproject.statuspage.io/. Right now there are some issues with Support & Test Servers, bu\u2026\n  \n\n\nIf you still need to contact the administrators of a different public Galaxy server, contact information is usually on the home page of the server itself or noted in their directory listing here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\nThanks!"}], [{"role": "user", "text": "Hello:\nI would like to filter a set of reads that were aligned using Bowtie in a few different ways. 1. Filter reads that map to a certain class of genes (e.g., CDs or tRNAs). 2. Filter reads that contain a single mismatch. 3. Filter reads that contain a specific mismatch (e.g. T to C). What tools would be suitable to perform these operations?\nThank you!"}, {"role": "system", "text": "Dear @cjain,\n(1) You can use bedtools intersect (bedtools Intersect intervals find overlapping intervals in various ways) it works with bam files. You would need to provide an annotation for your CDS or tRNAs.\n(2) You can use BAM filter Removes reads from a BAM file based on criteria\n(3) This is actually not that straightforward (to my knowledge) and would need samtools (see this post Extracting Reads Containing A Specific Variant From A Bam File). The question that comes immediately to my mind is, why you want to filter reads with a specific mismatch? Because if you want to investigate specific variants and count how many reads support which variant, then this goes into variant callling (see here Galaxy Training!).\nBest wishes,\nFlorian"}], [{"role": "user", "text": "\nimage1225\u00d7385 4.32 KB\n\nDe novo transcriptome assembly, annotation, and differential expression analysis\nAuthors: Anthony Bretaudeau Erwan Corre. I am following this protocol for my sample, after trimmomatic  while using trinity de nova RNA transcriptome tool its running for  1 day and it show error how do I ratify the error"}, {"role": "system", "text": "I\u2019m having the same issue.\nI was wondering to assembly a de novo transcriptome with trinity mixing pair and single end data, following the instructions from the developer and after a few minutes trinity stops and return same issue.\nI hope this works soon.\nCheers!"}, {"role": "system", "text": "Hi @dev and @Agustin_Baricalla,\ncould you try to run the analysis in the usegalaxy.eu server?\nRegards"}], [{"role": "user", "text": "Galaxy738-D)_Plot_Cladogram_on_data_7371200\u00d7900 233 KB Dear All,\nMy Lefse cladogram has an issue. I have too many taxa in the legend and I cannot see all of them. It is like the legend has been cut cause the plot dimension is too small. If you see on the right, my legend stops at a9 Tyzeerella, so I ma missing all taxa till c8, so around 20 taxa missing. Do you know which parameter I should change to increase the dimension of my cladogram plot and make possible to see all taxa in legend? Thanks very much."}, {"role": "system", "text": "Hi @VinceBilly\nAdjusting the text size has helped other people to resolve this type of issue.\nI added a tag to this topic \u201clefse\u201d that points to the prior Q&A. Review and see if that helps. This is a good one to start with: Pictures of lefse on line has incomplete display\nThanks!"}], [{"role": "user", "text": "Hi all,\nI am currently doing RNA-seq analysis for tomato. I have managed to map my trimmed reads using both RNA STAR and HISAT2. However, I have failed to get past featureCounts because it\u2019s return an error every time I try. I have also tried mapping with two different reference genomes (Index of /genomes/all/GCF/000/188/115/GCF_000188115.2_SL2.40 and Index of /ftp/tomato_genome/Heinz1706/annotation/ITAG2.3_release/) but all in vain. Could I be getting the reference genome files wrong? Or what could be the problem with my analysis?\nKindly help."}, {"role": "system", "text": "Hi @oscar.mitalo,\ncould you share your Galaxy history with me? My email is .\nRegards"}, {"role": "user", "text": "Hello @gallardoalba, I am afraid I erased all the history and then started afresh.\nI will share the new history in the case I encounter the same problem again.\nThank you"}, {"role": "user", "text": "Hello @gallardoalba I have shared my history with you. Got the same error like before."}, {"role": "system", "text": "Hi @oscar.mitalo,\nthis is the problem:\n\nScreenshot from 2022-04-21 15-18-57727\u00d7115 7.53 KB\n\nIn order to fix it you can use the tool Text transformation with the following expression: s/Parent/parent/g on the GFF file. Then you should use the new file as input for FeatureCounts.\nRegards"}], [{"role": "user", "text": "Hey there!\nI converted FASTQ files to FASTA with the \u201cFASTQ to FASTA\u201d converter (on galaxy) as well as with the \u201cany2fasta\u201d tool I installed on my computer in order to run the \u201cABRicate\u201d tool on it. But after the conversion ABRicate doesn\u2019t give results. I\u2019m sure that ABRicate should give results because when running MEGAHIT bfore running ABRIcate it also works. But the process is supposed to work without MEGAHIT. Do you have any idea, why the other conversion do not work?\nThanks in advance!"}, {"role": "system", "text": "Hi @MelonaT,\ncould you share your Galaxy history with me? I\u2019ll have a look at it. My email is .\nRegards"}, {"role": "user", "text": "thanks, I shared it, the tests I were running basically start from job 112, where I uploaded a fasta file that I converted with any2fasta from fastq"}, {"role": "system", "text": "Thanks @MelonaT,\nI\u2019m running a test; my feeling is that the problem is related to some not allowed characters present in the headers. I\u2019ll let you know the result.\nRegards"}, {"role": "user", "text": "thank you so much!"}, {"role": "system", "text": "Hi @MelonaT,\nthe problems is not related with the headers. It seems that ABRicate is not able to identify resistant genes when the raw reads are provided as input; it requires to assembly the reads into contigs (e.g. as you did with MEGAHIT) as previous step.\nRegards"}], [{"role": "user", "text": "Hi all,\nI deleted the unnecessary files and histories, purged them as well but it still shows that I used 250 GB storage, which is not accurate. I trimmed my files but could not align due to this problem. Please help!!!\nThank you"}, {"role": "system", "text": "Hi @meisner\nLook under User \u2192 Histories. Open the advanced search and set \u201cstatus = all\u201d. The size of each history is the amount each is using.\nIf any of the histories are only deleted, they need to be permanently deleted (purged) to free up quota space. To find all stray deleted datasets across histories, or entire deleted histories, go to User \u2192 Preferences \u2192 Storage Dashboard and use the functions.\nAfter purging, go back to the top level page for the Storage Dashboard and click on the \u201crefresh\u201d button to recalculate the usage. You could also log out then back in again to refresh."}], [{"role": "user", "text": "Hello, since Monday I have been trying to upload data to galaxy europe. My data is 2.2 GB in total size, and it is compound by two files each one of 1.1GB (they`re compressed). The warning that came out from galaxy is: Request Entity Too Large (413) (this happens wether I tried to upload one file of 1.1 GB or two). How can I solve this problem?"}, {"role": "system", "text": "Hi Natalia,\nthere is a limit of 1GB file size uploading through the website.\nYou can use the FTP service for larger datasets.\nPlease follow the instructions at Galaxy Europe | FTP service instructions"}], [{"role": "user", "text": "Hello everyone! I\u2019m a newer in bioinformatic and galaxy - sorry \nI trying to use RNA STAR tool. I have full human transcriptom (RNAseq) in 8files (with R1 and R2 reads). So I have to get names of genes and their characteristics (pathogenic etc) in the end for clinicians.\nI want to create vcf file for 1-snpsift variant type and 2-snpsift annotate SNPs from dbsnp. And after this I wanted download this vcf file (for ClinVar annotation etc).\nI dont need the Trimmomatic so I was trying to do next:\nFastq\u2013>RNA-STAR(hg38)\u2013>Samtoolsmerge(for mapped.bam)\u2014>FreeBayes\nBut FreeBayes show me the error:\n\n{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 101 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 101, \u201ctype\u201d: \u201cexit_code\u201d }\nJob API ID: bbd44e69cb8906b5339b2fad2d9562d3\n\nPlease explain me what I do wrong and which tools I need to use for my purpose?"}, {"role": "system", "text": "Hi @Sofi\n\n\n\n Sofi:\n\nI dont need the Trimmomatic so I was trying to do next:\nFastq\u2013>RNA-STAR(hg38)\u2013>Samtoolsmerge(for mapped.bam)\u2014>FreeBayes\n\n\nYou should at least run FastQCand probably Fastq Info on your read data to make sure there is not a content/format problem.\nThese prior topics may also be helpful: Search results for 'FreeBayes order:likes' - Galaxy Community Help\n\n\n\n Sofi:\n\nBut FreeBayes show me the error:\n\n{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 101 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 101, \u201ctype\u201d: \u201cexit_code\u201d }\n\n\n\nDoes the Dataset Details view contain any other informative messages? If not, check the prior topics for common help with this protocol, along with the tutorials below.\nIf you need more help, please post back a share link to the history containing this error. Please leave all inputs and outputs undeleted, along with the outputs from the QA checks. Sharing your History\n\nTutorials\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: NGS data logistics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Variant Analysis\n\n  Genetic differences (variants) between healthy and diseas...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi, My understanding from other posts is that the bowtie2 \u201cFatal error: Exit code 1 ()\u201d is likely an issue with FASTQ inputs; I have run these paired-end FASTQ files through bowtie2 before without issue, but this time I processed reads with fastp before bowtie2 so I am confident the error has something to do with fastp output. How do I figure out with is wrong with fastp output and how do I correct it?\nThanks,\nJustin"}, {"role": "system", "text": "Can you send an error report in to us using the \u201cbug\u201d icon on the failed history item? This will help us track down the specific problem."}, {"role": "user", "text": "Hi Nate, I did last night. I\u2019ve copied the info from the confirmation email below. Unfortunately, I\u2019m under time pressure and my disk quota was full so I made the choice to abandon the fastp route, cleared my disk space deleting the inputs from this job (I\u2019ve left the failed job logs though), and am in the process of reloading the data now to go straight into bowtie2 without preprocessing. Does fastp output generally work as input for bowtie2? I know I\u2019ve made it harder for you to help by deleting the fastp data, but I wasn\u2019t sure when/if I\u2019d get a response. Thanks for any insights you can offer.\nJustin\nGalaxy Tool Error Report\nfrom  https://usegalaxy.org/\nError Localization\n\n\n\n\nDataset\n44215977 (bbd44e69cb8906b5a210fd6557c6d72b)\n\n\n\n\nHistory\n2917310 (387a03a1eca44028)\n\n\nFailed Job\n2320: Bowtie2 on data 2240 and data 2239: alignments (bbd44e69cb8906b5712e07adcbf51859)\n\n\n\nUser Provided Information\nThe user \u2018craigj2@mymail.vcu.edu\u2019 provided the following information:\nMy understanding is that the \u201cFatal error: Exit code 1 ()\u201d is likely an issue with FASTQ inputs; I have run these FASTQ files through bowtie2 before without issue, but this time I processed reads with fastp before bowtie2 so I am confident the error has something to do with fastp output. How do I figure out with is wrong with fastp output and how do I correct it? Thanks, Justin\nDetailed Job Information\nJob environment and execution information is available at the job info page.\n\n\n\n\nJob ID\n30079910 (bbd44e69cb8906b5cf9cf002ecc44c77)\n\n\n\n\nTool ID\ntoolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.3+galaxy0\n\n\nTool Version\n2.3.4.3+galaxy0\n\n\nJob PID or DRM id\n24727836\n\n\nJob Tool Version\n/cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-5bee08a20f60a5597c4ecd54735d608dc6a44caf6f433cd52f23c80aa5a38d02/bin/bowtie2-align-s version 2.3.4.1 64-bit Built on default-df05fd51-3d07-4109-abba-6883676f3ae8 Mon Jun 25 23:07:08 UTC 2018 Compiler: gcc version 4.8.2 20140120 (Red Hat 4.8.2-15) (GCC) Options: -O3 -m64 -msse2 -funroll-loops -g3 -DBOOST_MATH_DISABLE_FLOAT128 -m64 -fPIC -std=c++98 -DPOPCNT_CAPABILITY -DWITH_TBB -DNO_SPINLOCK -DWITH_QUEUELOCK=1 Sizeof {int, long, long long, void*, size_t, off_t}: {4, 8, 8, 8, 8, 8}\n\n\n\nJob Execution and Failure Information\nCommand Line\nset -o | grep -q pipefail && set -o pipefail; ln -s \u2018/galaxy-repl/main/files/044/211/dataset_44211834.dat\u2019 input_f.fastq.gz && ln -s \u2018/galaxy-repl/main/files/044/211/dataset_44211835.dat\u2019 input_r.fastq.gz && bowtie2 -p {GALAXY_SLOTS:-4} -x '/galaxy/data/hg38/hg38female/bowtie2_index/hg38female' -1 'input_f.fastq.gz' -2 'input_r.fastq.gz' -I 0 -X 1000 --fr --dovetail --rg-id \"INPUT_30_S18_L001_001.fastq\" --skip 0 --qupto 3000000000 --trim5 0 --trim3 0 --phred33 -N 1 -L 11 -i 'S,1,1.15' --n-ceil 'L,8,0' --dpad 15 --gbar 1 --no-1mm-upfront --end-to-end --score-min 'L,-0.6,-0.6' --mp '6,2' --np 1 --rdg 5,3 --rfg 5,3 --seed 0 2> '/galaxy-repl/main/files/044/215/dataset_44215978.dat' | samtools sort -@{GALAXY_SLOTS:-2} -O bam -o \u2018/galaxy-repl/main/files/044/215/dataset_44215977.dat\u2019\nstderr\nstdout\nJob Information\nNone\nJob Traceback\nNone"}, {"role": "system", "text": "Thanks for pasting your error report, for some reason I haven\u2019t received it via email. I checked the job and it\u2019s very odd, there\u2019s no output from Galaxy\u2019s job scripts or the tool at all, and it only ran on the cluster for 6 seconds. It\u2019s too late now but if the same thing happens again in the future, please give the job another try and report the error.\nRegarding fastp -> bowtie2, I don\u2019t know as I\u2019m not familiar with the science side of things, but someone else will hopefully provide some insight on that."}], [{"role": "user", "text": "Dear Galaxy team,\nWe are interested in deploying and running our own Galaxy server at University Computing Centre in Zagreb. Our users expressed the desire for it, so we were hoping that you can assist us in this matter.\nWe are providing resources for all Croatian scientists so deployment should be done in scalable manner. We tested CloudMan/CloudLaunch but that one is discontinued now: https://galaxyproject.org/blog/2021-10-sunsetting-cloudlaunch. We already have a cloud infrastructure based on OpenStack. Could you please recommend what would be the optimal way of deploying Galaxy on top of such infrastructure?\nWe would prefer to have a single Galaxy instance where users would be authenticated with their national AAI credentials. Ideally users would spawn their own instances through such interface, similar to JupyterLab infrastructure. Is this an option with Galaxy?\nFrom performance side, how does Galaxy perform with its tools against regular job running in terminal? Eg. bwa-mem2 aligning or processing large GWAS datasets?"}, {"role": "system", "text": "Hi @hrzolix!\nThe European Galaxy server is running on OpenStack and we provide terraform and ansible roles to deploy and configure everything on GitHub. This stack is used in Belgium, Norway, France, Estonia and Italy to deploy their national Galaxy instances.\nWe are also offering Admin training and you are invited to visit us in Freiburg to learn more about it.\nCheers,\nBjoern"}, {"role": "system", "text": "\n\n\n hrzolix:\n\nWe would prefer to have a single Galaxy instance where users would be authenticated with their national AAI credentials. Ideally users would spawn their own instances through such interface, similar to JupyterLab infrastructure. Is this an option with Galaxy?\n\n\nSpawning a single-user instance for each user on demand is the more challenging part. Such architecture is used e.g. in the Anvil project (https://anvilproject.org/) and lots of its code and configuration is public, but there is no cookie-cutter ready for it afaik.\n\n\n\n hrzolix:\n\nFrom performance side, how does Galaxy perform with its tools against regular job running in terminal? Eg. bwa-mem2 aligning or processing large GWAS datasets?\n\n\nIn the end Galaxy runs jobs on the compute cluster almost the same way as if you ran it directly \u2013 just with more convenience, tracked provenance etc. Performance-wise running a tool inside or outside of Galaxy should be close to equal."}, {"role": "user", "text": "Hi @bjoern.gruening\nThank you for the info you provided, will look into it.\nAdmin training sounds interesting. What is the procedure, that is, how does that look like(duration etc.)?\nThanks,\nMarko"}, {"role": "system", "text": "HI Marko,\nthis really depends. Usually its one week hands-on. But we also had a few days online, or a week-long online training. We also had one month stay in Freiburg and set up your server with us as part of an exchange project.\nPlease also search here for \u201cAdmin\u201d: Galaxy Event Horizon - Galaxy Community Hub\nCheers,\nBjoern"}], [{"role": "user", "text": "Hi all,\nIs it possible to download the exact mm10 reference file used by galaxy directly into my computer? I\u2019m trying to run a variant caller called MuTect and the error I am getting is that the mm10 file I am supplying and my aligned files (which i aligned on galaxy using BowTie and the built-in mm10 database) do not have the same order of contigs (ERROR MESSAGE: Input files reads and reference have incompatible contigs: Order of contigs differences, which is unsafe). I figured out that a few chromosomes are out of order, which means the mm10 on my computer isn\u2019t exactly the same as the mm10 used by galaxy. So the easiest way to fix this error message is to get the exact mm10 file used by galaxy and supply it to MuTect instead of re-aligning everything to the current mm10 file I have on my computer.\nThanks."}, {"role": "system", "text": "Reference file for what tool? E.g. hisat2 is here: http://datacache.galaxyproject.org/managed/hisat2_index/mm10/\nYou can browse others at http://datacache.galaxyproject.org/managed/ and http://datacache.galaxyproject.org/indexes/mm10/"}], [{"role": "user", "text": "Hi,\nTrying to upload dataset from 7 accessions in SRA using the \u2018(faster) download and extract reads in fastq format from NCBI RSA\u2019 tool. The only thing that is returned are 2 empty collections. The pasted entry file that I used for input nicely shows 7 ERR accesions, 1 per line. Both single-end data and paired-end data , however, returned empty. Anybody idea what went wrong?\nHenk"}, {"role": "system", "text": "Hi @Henk,\ncould provide me some of the ERR accessions that failed?\nRegards"}, {"role": "user", "text": "Hi,\nHere are some of the accessions that I tried\nSRR8307998\nSRR8248681\nERR3053529"}, {"role": "system", "text": "Hi @Henk,\nI tried to download those entries, and it failed with SRR8307998.  I\u2019ll notify it.\nRegards."}, {"role": "user", "text": "I\u2019m using the usegalaxy.org server, but noticed that it also didn\u2019t go well with the usegalaxy.eu server. What is remarkable, however, that I was able to download the datasets if I did it via the individual downloads using \u201cSSR accession\u201d  in the select input type dropdown menu."}, {"role": "system", "text": "Hi @Henk\nPlease try again now. NCBI SRA itself was very busy over the last week and we also noticed odd job failures for accessions that should have processed fine at both servers. Those tests passed for us yesterday.\nA new test history where your single example accession + a list input containing your three example accessions also ran correctly later yesterday: Galaxy | Europe | Accessible History | SRR8307998\nThanks for reporting the problem and please let us know if you get another odd error (please specify the server and accession, per error). Both servers and all accessions should perform the same, but details often help with troubleshooting.\ncc @gallardoalba"}], [{"role": "user", "text": "Hi,\nI am analysing the RNAseq data of the mosquitoe Aedes aegypti. I need to map my reads using the reference genome AaegL5.0_GCA_002204515.1.\nIn the RNA STAR tab, there is no A.aegypti ref genome and it was mentioned below \u201cIf your genome of interest is not listed, contact the Galaxy team (\u2013genomeDir)\u201d. I couldn\u2019t find a way to do it.\nany advice? how can I solve this? Is there a way to place the ref genome in here."}, {"role": "system", "text": "Hi @Ouns87\nThe UseGalaxy.eu may have this genome indexed already, so check there first.\nOr, the fastest way to incorporate this reference genome at the AU or ORG servers is to use it as a custom reference genome. That can be promoted to a custom build to create a custom database \u201cmetadata\u201d key that can be assigned to datasets as needed. The input is a simple fasta file. Make sure to get the matching reference annotation at the same time to avoid problems later on.\nThis prior Q&A has links to the \u201chow-to\u201d. You can click on the tags to find more examples in different contexts. Custom genome + custom build: How to use a genome that is not natively indexed at the server you are working at - #2 by jennaj\nThe AU server admins can also comment, but as far as I know, new reference genomes will be indexed sometime later this year for all three UseGalaxy.* servers. ping @igor"}, {"role": "system", "text": "Hi @Ouns87\nas @jennaj suggested, use a custom genome. Upload the genome sequence in fasta or fasta.gz format and during alignment step change source of genome to \u201cfrom history\u201d. Mapping tools such as HiSAT2 and RNA_STAR will index the reference genome in background and map reads.\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you @jennaj and @igor. I checked AU and ORG servers but they didn\u2019t contain the reference genome from Aedes aegypti.\nSo, first I treat the downloaded genome based on \u201cHow to use Custom Reference Genomes? tutorial\u201d. Then I use it the normalized genome as a reference. right?"}, {"role": "system", "text": "Hi @Ouns87\nthe tutorial is somewhat old. I believe ftp is not supported anymore. Very often the reference genomes come in \u201cready to use\u201d form. I never used NormalizeFasta, but I guess it would not hurt. Make sure NormalizeFasta does not change the sequence names. The gene annotation file must use the same sequence names, as the reference genome. Try mapping and read counting steps on one sample first, to make sure it works for you. Upload the reference genome and gene annotations using URLs (paste URLs into Galaxy Upload menu >Paste/Fetch Data tab), examine the files using preview (eye icon) and try mapping with, say, HiSAT2 followed by read counting using featureCounts.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi!\nI\u2019m trying to assemble and polish my sequences from MinION.\nFirst I used Flye to assemble, then I used Minimap2, I set the configuration of the alignment parameters to the preset option \u201cOxford Nanopore all-vs-all overlap mapping\u2026\u201d and as I don\u2019t have a reference genome, I used the same reads (my fasta file I got from Flye) as reference genome and as dataset. Then I converted the BAM output  to SAM with \u201cBAM-to-SAM\u201d.\nAfter that, I used Racon and I always got the same error: \u201cempty overlap set!\u201d. To use Racon I use three inputs: my raw sequence in fastq, the overlaps I got from Minimap2 in SAM and the output I got from Flye in fasta.\nI tried to use the ovelaps in paf. format but It didn\u2019t work either.\nDoes anyone know why I always get the same error?\nThank you.\n"}, {"role": "system", "text": "Hi @Balcala,\nyou wrote: I used the same reads (my fasta file I got from Flye) as reference genome and as dataset.\nRacon description says: Racon takes as input only three files: contigs in FASTA/FASTQ format, reads in FASTA/FASTQ format and overlaps/alignments between the reads and the contigs in SAM format.\nOutput of Flye is a file with contigs, not reads. It seems you need the original reads, an alignment (reads mapped to contigs) and contigs.\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you, @igor,\nI\u2019ve tried Minimap2 with reads mapped to contigs and I\u2019ve used that file in SAM format in Racon but It keeps saying \u201cerror: empty overlap set!\u201d."}, {"role": "system", "text": "Hi @Balcala\nI assembled a genome from nanopore reads using Flye, mapped nanopore reads to the draft assembly using minimap2, converted BAM to SAM and completed Racon on the draft assembly, nanopore reads and SAM alignment. I don\u2019t know if it does make sense or not, because Flye has \u2018built-in\u2019 polishing step, but I cannot reproduce the error.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi, @igor,\nyou were right, I mapped reads to contigs in Minimap2.\nThe problem that when I used Flye I applied the option \u201cPerform metagenomic assembly\u201d, now I\u2019ve tried turning that option false and applying what you told me and It worked.\nThank you so much."}], [{"role": "user", "text": "Hi\nI tried run Trinity on server of Galaxy.org and it suddenly finished with error message below.\n\u201cThis tools currently disabled, please see Galaxy Help site at help.galaxyproject.org for assistance.\u201d\nAnd to see the detail of error, I opened the window.\n\nScreenshot from 2022-06-21 15-48-591920\u00d71080 302 KB\n\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2.\nThe used file type was fastqsanger.gz and the tetrameter was like the second picture.\n\nScreenshot from 2022-06-21 15-51-341920\u00d71080 342 KB\n\nDo anyone have a solution for this problem?\nThanks"}, {"role": "system", "text": "During the last ten days, I have also experienced the same issue."}, {"role": "system", "text": "Hi @Pedro_Luis_Ramos-Gon and @tiredpenguinD,\nI\u2019ll ask the system admins about that. Thanks for the report!"}, {"role": "system", "text": "We are experiencing some memory issues with Trinity; until the problem is corrected, I suggest you to use rnaSPAdes.\nRegards"}], [{"role": "user", "text": "Hi,\nThe tool I am developing runs without any error on command line. I also check the return code ($?). Within Galaxy the process generates all the outputs. But, there is an error that is raised and renders the results with a red background instead of a green one. Strangely enough, I can view the results by clicking on the eye. I have no clue since. The only strange thing is that the stderr starts with \u201cFatal error:\u201d and continues with all the log messages (more than one hundred lines). Any tip?\nSamuel"}, {"role": "system", "text": "Quick clarification questions:\n\nAre you using Planemo for tool development? https://planemo.readthedocs.io\n\nAnd just as a guess, this could be a metadata error. Check if the output datatype is a mismatch for actual data content. What are those datatypes? A single output dataset or multiple?\n"}, {"role": "user", "text": "Thanks for your tips. Finally I found the error, which was a tar error at some stage of the process.\nWhat was very misleading to me is that the process reached its end whereas the error was in the middle of the whole process and that the \u201cFatal error:\u201d was written at the beginning of the stderr. Moreover I also get some syntax errors with a green background as result (simple example below). Hopefully it\u2019s solved.\n"}, {"role": "user", "text": "Thanks for your tips. While I solved an intermittent error, the problem is still present. Finally I discovered that Galaxy adds a \u2018Fatal Error:\u2019 line at the beginning of the stderr if there is \u2018ERROR\u2019 or \u2018error\u2019 somewhere in the stderr of the tool. Try it yourself with the following code in a simple tool.\nHope this help.\n<command detect_errors=\"aggressive\">\n<![CDATA[\necho << EOF 1>&2\n[15:34:56] error: blablabla\n[15:34:56] error: bliblibli\nEOF\necho Done!\n]]>\n</command>"}], [{"role": "user", "text": "Hello,\nI\u2019m having trouble installing packages to RStudio on galaxy.eu.\nI would like to install SNPRelate  and its dependent package gdsfmt. I tried following the instructions in RStudio to use conda install : conda install -c bioconda bioconductor-gdsfmt but got the following error : Error: unexpected symbol in \u201cconda install\u201d\nNot sure what I\u2019m doing wrong here.\nI also tried installing the package using biocManager: BiocManager::install(\"gdsfmt\")\nbut that led to the following error:\n/bin/sh: 1: x86_64-conda-linux-gnu-c++: not found\nmake: *** [/opt/miniconda/lib/R/etc/Makeconf:177: R_CoreArray.o] Error 127\nERROR: compilation failed for package \u2018gdsfmt\u2019\n\nremoving \u2018/opt/miniconda/lib/R/library/gdsfmt\u2019\n\nI appreciate any advice on how to install these packages that I need for my analysis.\nThanks!"}, {"role": "system", "text": "May I know if you have solved this problem? I am encountering the same problem now. Thanks!"}, {"role": "system", "text": "Same here. Would be nice to get an answer. I have that issue for several packages, such as zen4R."}, {"role": "user", "text": "I unfortunately never got an answer so decided to do this on my local version of RStudio."}], [{"role": "user", "text": "\nUntitled11.png1098\u00d7532 43.9 KB\n\nIn my history you can see what i mean. So they are all the same right '/? i dont thing they have diffrence with each other"}, {"role": "system", "text": "What does the input collection look like?"}, {"role": "user", "text": "there are 44  paired end collection fastq data."}, {"role": "system", "text": "@amir The job was submitted with that data in a single collection? If so, then yes this would be an odd result.\nFew questions for clarification:\n\nAre you certain the job was only submitted once?\nWas this done using the tool directly in the User > Analyse Data view. Or, are you using a workflow?\n\nGalaxy EU https://usegalaxy.eu administrator: ping @bjoern.gruening"}, {"role": "user", "text": "1 : yes im sure\n2 : No im not used any workflow"}, {"role": "system", "text": "@amir Has this happened again since first reported? It seems like a cluster glitch.\nThe EU server will rerun failed jobs a few times automatically. Maybe some part of that was a transient problem. If persistent still now, we can ping that server\u2019s admins again to report the problem."}], [{"role": "user", "text": "I can import a single workflow, but I need to import multiple workflows from shared workflow list to my workflow list.\nNB: I know how to import single workflow."}, {"role": "system", "text": "Hi @khairul0026, that is not supported in the graphical interface. What is your use-case?\nIf you really need to import a very large number, so that it isn\u2019t convenient to click multiple times, and you know Python, you can use BioBlend:\nfor workflow_id in workflow_ids:\n    gi.workflows.import_shared_workflow(workflow_id)\n\nBest wishes,\nSimon"}], [{"role": "user", "text": "I have noticed that many new reference genomes have appeared in RNA STAR. Would be great if Dog ((UU_Cfam_GSD_1.0/canFam4) Mar.2020) could be added to this collection, then I can complete all my mappings with the same pipeline.\nThanks in advance for your help!"}, {"role": "system", "text": "Hi @Phil\nThe EU team will be back on their tomorrow, and can comment about what they can do now. ping @wm75 @gallardoalba\nThe alternative is to get the genome fasta from the UCSC downloads area into Galaxy, then use it as a custom reference genome. The EU server can usually process the largest custom genomes of the three UseGalaxy* servers. But let\u2019s see what they state as the best way to proceed."}, {"role": "system", "text": "@Phil we would be ok with installing the genome, but are you sure you want canFam4? UCSC lists canFam6 by now, and judging by release date this should be identical to https://www.ncbi.nlm.nih.gov/nuccore/CM000001.4/.\nupdate: yes, checksums between UCSC canFam6 and the nuccore record are identical."}, {"role": "user", "text": "Hi,\nYes, I think that canFam6 would probably best then. Both seem well enough annotated for RefSeq and UCSC display etc.\nThanks for your help!\nPhil"}, {"role": "system", "text": "Hi @Phil,\ncanFam6 from UCSC is now available for various tools including STAR on usegalaxy.eu.\nPlease test and let us know if everything works as expected.\nThanks,\nWolfgang"}], [{"role": "user", "text": "Hello Galaxy Community,\nIm doing a RNASeq Analysis and having problems with the tool featureCounts. I will give a short overview of what Ive done so far and what error I get at the end.\n\nI checked the quality of my rnaseq files with FASTQC tool\nSince I Work with Pseudomonas I downloaded the genome reference (fasta) and annotated genome (gtf) from Pseudomonas Genome Database\n2b. I unpacked the files since they are in .x.gz format\nI used the tool hisat2 for the alignment of reads to the mentioned genome reference (fasta)\nI tried to link the aligned counts (hisat2 files) to genes with the annotated genome (gtf) and the tool featureCounts\nThe job was cancelled and I got following error code: \"Fatal error: Exit code 255 (). ERROR: no features were loaded in format GTF. The annotation format can be specified by the \u2018-F\u2019 option, and the required feature type can be specified by the \u2018-t\u2019 option\u2026\nThe porgram has to terminate. \"\nThe error code mentions something with format GTF. I double checked and the annotated reference genome was in the correct format. Maybe I did something along the line that I was not supposed to do.\nIm relatively new to galaxy and I would very much appreciate your help.\n\nCheers,\nMarcel"}, {"role": "system", "text": "Hi Marcel,\nI met the same problem as you. Did you figure it out?\nThanks,\nQing"}, {"role": "system", "text": "Hi @Marcel and @q_thistle,\nthe problem is that by default featureCounts set exon as feature filter (Advanced options/GFF feature type filter), but this feature doesn\u2019t appear in the Pseudomonas annotation file (third column). You need to specify CDS as feature type in featureCounts.\nRegards"}], [{"role": "user", "text": "I am using https://usegalaxy.org/\nI performed bwa-mem on a dataset, and then flagstat on the output dataset to check results. The flagstat output is also a dataset, in which I can review the results of each file individually. Is there an easy way to output flagstat to just one file, and look at the results of the entire dataset combined? Is there a better way to be able to visualize the mapping?\nAlexa"}, {"role": "system", "text": "Hi @AlexaDean,\nyou could merge your BAM files by using the Merge BAM Files tool. It requires previously to sort the alignments by coordinates, which can be done with the SortSam tool.\nRegards"}, {"role": "user", "text": "Hi, thanks for your reply!\nI used SortSam fine, but received an error with merging (attached here), I assume because this is RNAseq data and there are duplicates?\nI really just want to visualize/check that my bwa-mem mapping step on this RNAseq data is okay before moving forward to counting, if you have any other advice I would appreciate it!\nScreen Shot 2021-01-20 at 11.27.36 AM1508\u00d7366 38.1 KB\nAlexa"}, {"role": "system", "text": "Hi @AlexaDean,\nyou can use samtools stats for generating the statistics, and then aggregate the reports in an unique file by using Multiqc.\nRegards"}, {"role": "user", "text": "Thank you that worked perfectly!\nI appreciate it!"}], [{"role": "user", "text": "Hello,\nI used Unicycler on Galaxy in the past but with sequences from Illumina only.\nThis time I am using it with long reads obtained from a MinION.\nI tried to run it twice but the two times I got the same error message:\nRemote job server indicated a problem running or monitoring this job.\nIs it a problem with the server or is it a problem with my data?\nThanks for help,\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nIt is difficult to know from the information given. I am going to assume that you are actually working at Galaxy Main https://usegalaxy.org since that is how your post is labeled (and the error message is one that we report for this tool). Check the URL for where you are working and write back if it is different (and clarify).\nI\u2019m running quick tests to see if Unicycler is working at all (there could be a server-side issue). Both with and without long reads, just to make sure incorporating long reads at all isn\u2019t a distinct problem for some reason (shouldn\u2019t be, and would be a new bug).\nPlease also run two tests:\n\nRun just the short reads, without the long reads.\nRun the original job again that includes both.\n\nIf both fail, the data could be too large to run OR there could be a server issue. I\u2019ll be able to let you know if a server issue is a factor once my test completes.\nIf the first succeeds, and the second fails, there could be an input problem with the long reads OR including them now creates a job that is too large to run OR there is a tool bug involving long reads (my tests will catch that, too).\nIf both now succeed, this was a transient server issue that resolved itself.\nMore feedback after my tests complete. Please also post back what happens with your test runs. We can follow up from there.\nThanks for reporting the problem!"}, {"role": "system", "text": "Update:\nUnicycler is executing fine under all conditions with smaller test cases (including and not including long reads).\nIf your job failed again, start by double-checking the format and content of your inputs. Be sure to not skip read QA steps. Once done and if and the job is still too large to execute, then your choices are to 1) test running with different parameters 2) consider downsampling the inputs and/or 3) moving to your own Galaxy server where more resources can be allocated.\nI added some tags to your post that will help you to find prior Q&A that covers these choices with more details."}, {"role": "system", "text": "\n  \n    \n    \n    run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210 usegalaxy.org support\n  \n  \n    run Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n  \n\n"}], [{"role": "user", "text": "Hello Recently the old instance of galaxy that we host has went through some heavy utilization and crashed the server and database several times. I am currently stuck at looking at several upload jobs for one user that will not error out or go away even after a reboot.\nUnder \u201cAdmin\u201d Manage jobs I see all their jobs listed but there\u2019s no check box for me to select it and submit it for stopping. Is there a way to do this in the terminal of the server?\nthank you"}, {"role": "system", "text": "Hi @glam02,\nThere is a really neat Galaxy Admin utility called gxadmin which can help you out. It contains a bunch of querys and scripts that can look at jobs, fail jobs manually etc.\nYou can get it from: https://github.com/usegalaxy-eu/gxadmin\nThe particular query you want to run is: gxadmin mutate fail-job <job_id> where <job_id> is the job number of the stalled jobs in your database.\nThere is a little bit of setup required, mostly so that gxadmin knows how to talk to your database.\nI hope this helps.\nSimon."}], [{"role": "user", "text": "Hello.I am currently doing project in exome sequencing. My data is paired sample and when i do fastqc i get 4 outputs(one for forward and another for reverse i.e., raw data and webpage).\n\nimage1366\u00d7768 116 KB\n\nwhen i perform multiqc all files should be combined and it should show all the samples but it is showing only forward and reverse. I  need your help.\n\nimage1366\u00d7768 114 KB\n"}, {"role": "system", "text": "Have you seen this page already? Think it is a similar question.\nhttps://help.galaxyproject.org/t/multiqc-not-working-correctly/7386/2"}, {"role": "user", "text": "Hi @gbbio\nI already saw the page but i don\u2019t understand what is meant by \u201cflatten collection\u201d as i am a student who  is currently studying about galaxy server and NGS techniques . Please let me know the steps for flatten collection and also where to find that option. Please help me out."}, {"role": "system", "text": "\n\n\n Thara_Dharanidhar:\n\nI already saw the page but i don\u2019t understand what is meant by \u201cflatten collection\u201d\n\n\nHi @Thara_Dharanidhar\n\u201cFlatten collection\u201d is a Collection Operation tool.\nFind the tool at UseGalaxy.eu here: Galaxy | Europe\nFind it at most public Galaxy servers by either navigating to the tool panel section \u201cCollection Operations\u201d or by searching tool panel with the keyword \u201cflatten\u201d.\nGeneral process:\n\nRun the original read collection through the tool \u201cFlatten collection\u201d\nRun FastQC on the \u201cflattened\u201d read collection\nRun MultiQC on the FastQC outputs\nUse the original read collection for other downstream tools.\n\n\nNote: The original and flattened read collections both represent the same data content. The only difference is the technical formatting.\n\nTutorials for Collection Operations tools:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Search Tutorials\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi, I\u2019m currently analyzing a dataset produced from DESeq2 in Galaxy. I have two factors, each with three factor levels:\nFactor 1: Hour (0, 5, 24 hrs) Factor 2: Tissue (A, B, C)\nThe summary plots and results files only compare two factor levels at a time, i.e. 24 hrs vs 0 hrs, and tissue C vs tissue A. How can I get the results file for other comparisons, such as 5 hrs vs 24 hrs, or tissue B vs tissue A? I know in R, using the \u201ccontrast\u201d argument can do this. Is this possible in Galaxy?\nThanks in advance!"}, {"role": "system", "text": "Hi @julsies\nTo directly use the contrast function, try edgeR or limma instead.\nDeSeq2 has this function (partially) batched in the Galaxy wrapped tool version. The parameter is Output all levels vs all levels of primary factor (use when you have >2 levels for primary factor). That said, this probably isn\u2019t what you want, but you can compare it.\nHope that helps!"}], [{"role": "user", "text": "Hello everyone,\nI am creating a workflow with a tool that receives text as input.\n \n        \n\nThese options allow me to give text as input inside the workflow creation. I am trying to change that with\n\u201cSimple inputs used for workflow logic\u201d in order to provide the text input before running the workflow and not inside the workflow creation. I change the tool configuration like this:\n \n        \n\nThis way does not allow me to connect the 2 tools inside the workflow.\n\nAm I doing something wrong?\nThanks a lot!"}, {"role": "system", "text": "Hi @tkarabatsis,\nyou need to click on the \u201cAdd connection to module\u201d button on the right hand side:\n\nA new input will appear then in the workflow editor to which you can connect your parameter."}, {"role": "user", "text": "Hello mvdbeek,\nthanks a lot for the quick reply!\nI changed the workflow with the way you told me and now it looks like this:\n\nimage.png1240\u00d7662 46.2 KB\n\nI try to run the workflow, I add the values from here:\n\nbut when I click Run Workflow, I get this error:\nMay 23 15:09:33 thanasis3 run.sh[5648]: galaxy.workflow.run ERROR 2019-05-23 15:09:33,103 Failed to schedule Workflow[id=42,name=Unnamed workflow], problem occurred on WorkflowStep[index=0,type=parameter_input].\nMay 23 15:09:33 thanasis3 run.sh[5648]: Traceback (most recent call last):\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 190, in invoke\nMay 23 15:09:33 thanasis3 run.sh[5648]:     incomplete_or_none = self._invoke_step(workflow_invocation_step)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 266, in _invoke_step\nMay 23 15:09:33 thanasis3 run.sh[5648]:     use_cached_job=self.workflow_invocation.use_cached_job)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/modules.py\u201d, line 713, in execute\nMay 23 15:09:33 thanasis3 run.sh[5648]:     progress.set_outputs_for_input(invocation_step, step_outputs)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 397, in set_outputs_for_input\nMay 23 15:09:33 thanasis3 run.sh[5648]:     self.set_step_outputs(invocation_step, outputs)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 428, in set_step_outputs\nMay 23 15:09:33 thanasis3 run.sh[5648]:     output=output,\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 432, in _record_workflow_output\nMay 23 15:09:33 thanasis3 run.sh[5648]:     self.workflow_invocation.add_output(workflow_output, step, output)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/model/init.py\u201d, line 4443, in add_output\nMay 23 15:09:33 thanasis3 run.sh[5648]:     if output_object.history_content_type == \u201cdataset\u201d:\nMay 23 15:09:33 thanasis3 run.sh[5648]: AttributeError: \u2018unicode\u2019 object has no attribute \u2018history_content_type\u2019\nMay 23 15:09:33 thanasis3 run.sh[5648]: galaxy.workflow.run ERROR 2019-05-23 15:09:33,103 Failed to execute scheduled workflow.\nMay 23 15:09:33 thanasis3 run.sh[5648]: Traceback (most recent call last):\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 83, in __invoke\nMay 23 15:09:33 thanasis3 run.sh[5648]:     outputs = invoker.invoke()\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 190, in invoke\nMay 23 15:09:33 thanasis3 run.sh[5648]:     incomplete_or_none = self._invoke_step(workflow_invocation_step)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 266, in _invoke_step\nMay 23 15:09:33 thanasis3 run.sh[5648]:     use_cached_job=self.workflow_invocation.use_cached_job)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/modules.py\u201d, line 713, in execute\nMay 23 15:09:33 thanasis3 run.sh[5648]:     progress.set_outputs_for_input(invocation_step, step_outputs)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 397, in set_outputs_for_input\nMay 23 15:09:33 thanasis3 run.sh[5648]:     self.set_step_outputs(invocation_step, outputs)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 428, in set_step_outputs\nMay 23 15:09:33 thanasis3 run.sh[5648]:     output=output,\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/workflow/run.py\u201d, line 432, in _record_workflow_output\nMay 23 15:09:33 thanasis3 run.sh[5648]:     self.workflow_invocation.add_output(workflow_output, step, output)\nMay 23 15:09:33 thanasis3 run.sh[5648]:   File \u201c/srv/executor/lib/galaxy/model/init.py\u201d, line 4443, in add_output\nMay 23 15:09:33 thanasis3 run.sh[5648]:     if output_object.history_content_type == \u201cdataset\u201d:\nMay 23 15:09:33 thanasis3 run.sh[5648]: AttributeError: \u2018unicode\u2019 object has no attribute \u2018history_content_type\u2019\nAny ideas?"}, {"role": "system", "text": "Yeah, that\u2019s been fixed in https://github.com/galaxyproject/galaxy/pull/7799, I can open a PR to backport this."}], [{"role": "user", "text": "Hey all,\nI think there is a problem in Galaxy. I am trying to feed some data into my workflow but it doesn\u2019t work. Let me explain. So I have a collection of paired-end data (fastq-dump). The following options in HISAT2 are: Single-end, Paired-end, Paired-end Dataset Collection, Paired-end data from single interleaved dataset.\nI chose Paired-end Dataset Collection in HISAT2 to build my workflow\n\nCapture 4.PNG944\u00d7471 14.2 KB\n\nBut I can\u2019t connect them. It says: Incompatible collection type(s) for attachment.\nHowever, outside the workflow, I don\u2019t have a problem uploading the fastq-dump in HISAT2 with the Paired-end Dataset Collection setting. Why this discrepancy?Is it because of the fastq-dump?\nAlso, is there any difference like this: (instead of choosing Paired-end Dataset Collection)\n\nCapture 5.PNG991\u00d7533 15 KB\n\nAnyway, one more point: feeding collection into HISAT2 workflow is almost impossible.\nSituation 1:\n\nCapture.PNG953\u00d7504 39 KB\n\nMy paired-end collection is not seen\n\nCapture 2.PNG1460\u00d7551 34.8 KB\n\nSituation 2:\n\ncapture 6.PNG1024\u00d7438 42.4 KB\n\nSame. It won\u2019t let me select my data.\nThe only way that I found so far to have an workflow with HISAT2 was to choose Paired-end data from single interleaved dataset. But this means that I have to use Fastq-Interlacer on my raw samples before aligning them which takes so much time. I decided to give up on that.\nI should add that my workflow is not really complex. I have the input --> HISAT2 --> Filter SAM/BAM tool --> Merging BAMs. (The last step is not essential. It may or may not be there for now at least).\nSorry for the long post. I hope is understandable and is making sense."}, {"role": "system", "text": "\n\n\n Sammy:\n\nBut I can\u2019t connect them. It says: Incompatible collection type(s) for attachment.\n\n\nThen that is probably true. Make sure you set the input collection to either \u201cpaired\u201d or \u201clist:paired\u201d, not \u201clist\u201d on the right hand side\n\nScreen Shot 2019-05-20 at 09.51.21.png1370\u00d7296 38.4 KB\n"}], [{"role": "user", "text": "Hi, I am doing RNA-seq analysis, the pipeline is as follows: upload data > FastQC > Trimmomatic > HISAT2 > Feature counts > Deseq 2 > Annotate Deseq 2 output tables. The reference I used for annotation is \u201cmm10. refgene.gtf.\u201d However, the output shows just NA, like picture showed below, what is the problem?\n\nimage1472\u00d7245 27.7 KB\n\nThanks for help\uff01"}, {"role": "system", "text": "Dear @Xiangdong,\nHave you used just one sample (replicate) for DESeq 2? DESeq2 does not calculate p-values (signifiance) if you just use one replicate.\nKind regards,\nFlorian"}], [{"role": "user", "text": "Hi,\nI am following the galaxy with ansible installation tutorial (Galaxy Installation with Ansible) but fail when running the playbook (Ubunutu 22.04.2). It stops at the step \u201crun miniconda installer\u201d and prints the following error that I don\u2019t understand. What can I do?\nfatal: [localhost]: FAILED! => changed=true \n  cmd:\n  - /bin/sh\n  - /tmp/ansible-miniconda-installer.rnn9nixs.sh\n  - -b\n  - -p\n  - /srv/galaxy/var/dependencies/_conda\n  delta: '0:00:01.715882'\n  end: '2023-08-08 08:05:18.684320'\n  msg: non-zero return code\n  rc: 1\n  start: '2023-08-08 08:05:16.968438'\n  stderr: |2-\n...\nstderr_lines: <omitted>\n  stdout: |-\n    PREFIX=/srv/galaxy/var/dependencies/_conda\n    Unpacking payload ...\n  stdout_lines: <omitted>\n\n\nimage5112\u00d71736 637 KB"}, {"role": "system", "text": "\n\n\n Markus_Konkol:\n\nGalaxy Installation with Ansible\n\n\nHi @Markus_Konkol\nI don\u2019t recognize what this is except for some potential permissions problem.\nScroll down in this section of the tutorial to the last step (step 10). What content was created so far? In particular, does the content of this file appear to match what you expect? You could post that back here for more context.\n/srv/galaxy/config/galaxy.yml\n\nAnd, let\u2019s cross post over to the Admin chat. They may reply here or there, and feel free to join the chat.  You're invited to talk on Matrix"}, {"role": "user", "text": "Hi @jennaj\nI checked step 10:\nYes, Galaxy has been deployed under /srv/galaxy/server and the configuration file is in /srv/galaxy/config/galaxy.yml. I took some screenshots to show the setup and will also refer to this thread in the admin chat.\nimage1920\u00d72080 307 KB\nimage752\u00d7710 31 KB\nimage1692\u00d7218 26.4 KB\nimage1810\u00d7452 46.4 KB"}, {"role": "user", "text": "Or does anyone know how to get a more meaningful error message? The output doesn\u2019t state what is actually going wrong\u2026"}, {"role": "user", "text": "In case anyone else has a simlar issues: This PR solved the issue for me: Proposed changes to job-destinations and ansible-galaxy admin tutorials to prevent ansible-playbook errors. by Edmontosaurus \u00b7 Pull Request #4308 \u00b7 galaxyproject/training-material \u00b7 GitHub\nShould result in the following output:\nimage2090\u00d7810 147 KB"}], [{"role": "user", "text": "Dear Galaxy Community,\nI have recently doing LefSe analysis on line. Now i am facing a problem. The output picture of the lefse is incomplete. To avoid such problem, do I need to set some parameters in LefSe or anything else?\n\nThanks for your kindly help."}, {"role": "system", "text": "Hi - This may sound simple, but the graphic may be too large to display in the middle panel of the default Galaxy \u201cAnalyze Data\u201d tab\u2019s view.\nWould you try displaying the browser window larger on your computer or hiding the right and/or left panel (there is an arrow at the bottom on each side to toggle show/hide) to see if that resolves the issue?"}, {"role": "user", "text": "Hi. Thank you for your kindly reply. I tried to increase the value of \u201cwidth of the plot\u201d in \u201cSet some graphical options to personalize the output\u201d. This could work. Thanks again."}, {"role": "system", "text": "Hi-I meet a similar problem. There are over 100 abbreviating the labels in fact. However in the picture (both online picture and downloaded picture) of cladogram, there are only about 20 abbreviated labels in the legend. How can I get a unbroken legend? Or where can I find the taxonomic name of every abbreviated labels?\nThanks for your kindly help.\n\nimage919\u00d7669 207 KB\n"}, {"role": "user", "text": "Sorry, I didn\u2019t get many labels as you. I adjusted \u201cRation of the horizontal space to be given to the right side (for adjusting for label legend)\u201d in \u201cSet some graphical options to personalize the output\u201d, and got a whole figure."}], [{"role": "user", "text": "Hi all,\nI\u2019m trying to run DESeq2 with Sailfish files. I have two factors (three files for each factor) and I\u2019m running the program with my own gene mapping file. I have been getting the below error:\nError in read.table(tx2gene, header = hasHeader) :\nmore columns than column names\nCalls: get_deseq_dataset -> read.table\nHas anyone seen this? I\u2019m not quite sure what the problem is."}, {"role": "system", "text": "Hi @ramarro,\nthe DESeq2 tool in Galaxy can work with count files that may or may not have a header line.\nIf your file starts with\nName\tLength\tEffectiveLength\tTPM\tNumReads\n\nyou need to select Files have header? -> Yes.\nFor data from sailfish you need to select\nChoice of Input data -> TPM values\nProgram used to generate TPMs > Sailfish\nand then you need to provide the GTF/GFF file or Tabular file with Transcript-ID to Gene-ID mapping.\nThis should be the same file you used as input to sailfish.\nFinally note that the successor software to Sailfish is Salmon, which is also available in Galaxy."}], [{"role": "user", "text": "Hi! I am working with my organization to set up a Linux virtual machine (RedHat) to run a local Galaxy instance. Our IT department will make sure the required Python is installed. This VM will run behind a firewall and they\u2019d need to whitelist any access to the outside world. In order to install the Galaxy instance and tools, I was wondering which sites I would need to request to be white listed. I have Github and the toolshed site on my list. Do you happen to know which other sites I\u2019ll need to include? Thank you so much and I\u2019m sorry if this is a silly question."}, {"role": "system", "text": "Hi @phemarajata\nThis is a good question, and you\u2019ll have some decisions to make.\nGithub and the ToolShed are good places to start (for administration).\nPublic download sites:\nYou will probably also want to allow download access to any of the Get Data tools that you plan to enable. Also keep in mind that people often download data from NCBI, SRA, UCSC, and similar sites using the Upload tool with URLs. And if you plan to use Data Managers, wherever they are sourcing data would need to be accessible.\nPublic upload sites:\nYou\u2019ll also need to consider if you plan on allowing data to be displayed or used externally.  Examples: viewing data in genome browsers hosted publically or moving data to public Galaxy servers. This wouldn\u2019t be recommended if the data needs to be protected for privacy and related reasons.\nHope that helps! I\u2019ve also asked our admin team to review and make recommendations, so you may get another reply "}, {"role": "system", "text": "As @jennaj said, that list is quite large and may change without warning in a way that is beyond our control. Are you sure your organization needs to whitelist IPs / URLs, or just ports ? The ports would be standard HTTP and HTTPS for the most part. Some tools may require FTP or more exotic protocols, but that\u2019s not common.\nIf you do need to whitelist IPs or URLs it might be easiest if you set the VM up with internet access, then restrict it. Installing repositories from the tool shed also requires access to anaconda.org if you are planning to use Conda dependencies, if you\u2019re planning to use docker or singularity for dependencies these will require access to https://quay.io/."}, {"role": "system", "text": "In addition to what Jen and Marius said, you\u2019ll also need access to the Galaxy Python Wheels server (wheels.galaxyproject.org), PyPI (pypi.org), and the NPM registry (registry.npmjs.org)."}, {"role": "user", "text": "Thank you all so much! This has been really helpful. I am planning to use the ARTIC workflow on Galaxy to process SARS-CoV-2 data so I had to also include the Galaxy project site, auspice, nextstrain/nextclade, GISAID/Epicov, ENA, and the NCBI sites. I am sure I\u2019ll run into more problems with the firewall but this is a great start."}], [{"role": "user", "text": "Hello.\nI am trying to login and get data using \u2018usegalaxy.org\u2019.\nHowever, I was not able to login and get message as  \u2018Uncaught exception in exposed API method:\u2019\nAnd the downloaded data does not run and kept on gray block for an hour now.\nCould you help me with this issue?"}, {"role": "system", "text": "We\u2019ve had an unexpected downtime, details: https://status.galaxyproject.org/incidents/f48lrj68k4hb. It should be ok now, sorry for the inconvenience."}], [{"role": "user", "text": "Hi,\nI am the developer of Pharokka and am trying to use Pharokka v 1.2.1 on the EU server (the adding of this to the toolshed was extremely helpfully handled by Paul Zierep! thanks Paul!)\nWhen I go to run Pharokka, there is no Database available to run - would you be able to add it?\nGeorge"}, {"role": "system", "text": "\n\n\n gbouras13:\n\nWhen I go to run Pharokka, there is no Database available to run - would you be able to add it?\n\n\nHi @gbouras13\nDo you want to add indexes for this tool form option? built-in pharokka DB\nWhen wrapping the tool, did you also create a Data Manager to create/add indexes (by an admin)? If not, then try contacting who was helping you before (private), or the EU admins at their Lobby chat here (public) https://matrix.to/#/#usegalaxy.eu:matrix.org. Indexes can be server specific, and not all involve a DM, but that is how to start."}, {"role": "system", "text": "Hello!  I am also trying to use pharokka on this server (Galaxy | Europe), but cannot because it requires selecting a database, but there is none to select.\nBumping this thread as it would be extremely helpful to be able to use this tool.  Thank you."}, {"role": "system", "text": "You might have more luck by asking at their matrix chat. Or, maybe the developer will post back what is happening so far with an update, ping @gbouras13"}, {"role": "user", "text": "Looks like it works now  @juliambrosman (someone in the background fixed it so thanks Galaxy support)!\nI\u2019m not responsible for the Galaxy installation, but I am happy to help you install it locally in future if you run into troubles."}], [{"role": "user", "text": "I tried to install some tools and got this error below. Not sure why since a month ago I could still install tools. Galaxy version 20.05 on Ubuntu.\nraise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://toolshed.g2.bx.psu.edu/api/repositories/get_repository_revision_install_info?name=deeptools_plot_pca&owner=bgruening&changeset_revision=69264da0ce77\nCan someone please help? Thank you!"}, {"role": "system", "text": "Dear phemarajata,\nI have also been unable to install the tools in my local Galaxy using MacOS. Every time I click on \u201cInstall\u201d, an error message appears. Were you able to solve this issue?\nThank you,\nRobson"}, {"role": "user", "text": "Hi, this was a problem with the toolshed and they fixed it pretty fast. Hope this worked out for you too."}], [{"role": "user", "text": "Hi,\nwe have a tool which creates many (>100) output files, and would like to achieve a single history item (or whatever you would call it) from the tool with the list of output files with one download button.\nWe have managed to configure the tool to create a list (dataset) but in addition we get all the 100 history items, one per output file in addition. See the screenshot.\n\nimage_2021_05_12T11_48_21_775Z277\u00d7901 23 KB\n\nThe relevant lines in  the tool are\n <collection name=\"output_MC\" type=\"list\" label=\"Conventional Analysis for MC\">\n              <filter>input == \"MC\"</filter>\n             <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/MC/\" />\n</collection>\n\nHow to only get the List called \u201cConventional Analysis for MC\u201d in the screenshot, and not all the rest of the hist.MC. items in History?\nHelp much appreciated!\nThanks,\nMaiken"}, {"role": "system", "text": "Hi @maikenp,\nI believe that due to the inner logic of Galaxy collections every member of such a collection has to be represented as an independent dataset in the history. This is usually transparent to the user because the datasets are given the \u201chidden\u201d flag. I suspect the tool you\u2019ve made is not attaching such flag - check out the visible attribute in the discover docs: Galaxy Tool XML File \u2014 Galaxy Project 21.01 documentation\nedit: actually I think you can just drop the <filter> tag, that is probably duplicating your outputs."}, {"role": "user", "text": "We will try, thanks.\nThe full tool wrapper looks like this by the way:\n<tool id=\"ConventionalAnalysis\" name=\"Conventional Analysis\">\n<description></description>\n<requirements>\n<requirement type=\"package\" version=\"1.0.0\">fys5555_py3</requirement>\n</requirements>\n<command>\n<![CDATA[ \npython '/storage/software/src/FYS5555/Conventional_Analysis/CodeExample/ru nSelector.py' '$input';\n]]>\n</command>\n<inputs>\n<param name=\"input\" type=\"select\" label=\"Select Specific for above selection\" help=\"\">\n       <option value=\"Data\" selected=\"true\">Data</option>\n       <option value=\"MC\">MC</option>\n</param>\n</inputs>\n<outputs>\n\t\t<data name=\"output_Data\" from_work_dir=\"Histograms/Data/hist.Data.2016.root\" >\n\t  <filter>input == \"Data\"</filter>\n </data>\n <collection name=\"output_MC\" type=\"list\" label=\"Conventional Analysis for MC\">\n\t  <filter>input == \"MC\"</filter>\n\t <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/MC/\" />\n</collection>\n</outputs>\n</tool>"}, {"role": "user", "text": "Hi again.\nThanks very much for you help. Removing the filter tag worked beautifully. And now the separate output files are hidden by default.\nThis is what we ended up with:\n<tool id=\"ConventionalAnalysis\" name=\"Conventional Analysis\">\n<description></description>\n<requirements>\n<requirement type=\"package\" version=\"1.0.0\">fys5555_py3</requirement>\n</requirements>\n<command>\n <![CDATA[ \n python '/storage/software/src/FYS5555/Conventional_Analysis/CodeExample/runSelector.py' '$input';\n  ]]>\n</command>\n<inputs>\n<param name=\"input\" type=\"select\" label=\"Select Specific for above selection\" help=\"\">\n      <option value=\"Data\" selected=\"true\">Data</option>\n      <option value=\"MC\">MC</option>\n</param>\n</inputs>\n<outputs>\n\t <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/\" recurse=\"true\" visible=\"false\" />\n </collection>\n </outputs>\n</tool>"}, {"role": "system", "text": "Hi @maikenp,\n\n\n\n maikenp:\n\n<outputs>\n\t <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/\" recurse=\"true\" visible=\"false\" />\n </collection>\n\n\n\nThat is not a valid output section and the tool would fail to load at this point.\nIt should probably be\n<outputs>\n     </collection>\n         <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/\" recurse=\"true\" visible=\"false\" />\n     </collection>\n</outputs>\n\nYour initial tool looks correct and the filter is supposed to work.\nvisible=\"false\" is not needed, that is the default setting, although it doesn\u2019t hurt adding.\nI\u2019d strongly suggest linting and testing tools with planemo"}], [{"role": "user", "text": "Hi all,\nI am currently analysing data acquired from RNA Sequence from a specific cell type (microglia) from mouse brain. I have performed differential expression gene analysis by using DESeq2 between my control and my treatment. I would like now to perform some type of gene set enrichment analysis. According to the tutorials I can either perform fgsea or egsea.\n\nAre there any other tools that I can use?\nIf I want to use fgsea:\n\n\nwhat rank can is it better to use? I was thinking to use either the simple logFC or the signed fold change (ranked by foldFC multiply with the p values). However for the second option I don\u2019t know how to do it without R\n-Moreover, I am only working with only a cell type, will I be able to use the mouse hallmark set, or any other online available set?\n\n\nI have similar question also for the egsea tool. Working only with a cell type, can I use the MSigDB gene set which is available in the tool?\n"}, {"role": "system", "text": "Hi @des_b\nMaybe these tutorials will help?\nhttps://training.galaxyproject.org/training-material/search?query=fgsea\nYou can use R in Galaxy, these cover how:\nhttps://training.galaxyproject.org/training-material/search?query=rstudio\nFor your questions \u2192 the single cell tutorials and transcriptomics tutorials have some overlap. You can certainly mix methods: QA and sorting/counting steps are distinct from higher levels of data reduction as far as I know. The complex indexes you mentions are used for both. Maybe try a few different ways and compare if you are not sure if some index is a fit for your data. The species involved, and not the sequencing methods, will matter after a certain point.\nHope that helps!"}], [{"role": "user", "text": "Hello,\nI am a new Galaxy user doing my first RNA-seq analysis. I had my data in a collection and aligned it using HISAT2. This generated a collection titled \u201cHISAT2 on \u2026: aligned reads (BAM)\u201d with a list that I then attempted to enter into MultiQC, but the HISAT2 collection does not appear in the option (individual HISAT2 files for each sample can be selected). When I ran HISAT2 tool I had these parameters for output:\n\n\n\n\nOutput alignment summary in a more machine-friendly style.\nTrue\n\n\n\n\nPrint alignment summary to a file.\nFalse\n\n\n\nDo I need a printed alignment summary in order to be able to run MultiQC?\nI would really appreciate any help on this topic, thanks!"}, {"role": "system", "text": "Does it appear as a collection?"}, {"role": "user", "text": "Individual HISAT2 results that are hidden appear under multiple datasets, but when I select dataset collections the HISAT2 collection containing a list of all the samples does not appear."}, {"role": "system", "text": "I am having the same problem: When I select HiSat2 as the analysis that produced the output files, my HiSat2 files are not selectable options to run the MultiQC."}, {"role": "system", "text": "Hello @rmwade1786 @ypo @StefaniaMontesinos\nPlease see this prior Q&A:\n\n\n\nHisat2 BAM file output does not include stats -- Not a default output but is an option on the tool form\n\n\nTo output statistics directly from HISAT2 , that output option needs to be set to \u201cYes\u201d on the tool form. Find it nested under the section \u201cSummary Options\u201d. This would allow you to use MultiQC to compare multiple BAM results together in a single report.\n\n\n\nGalaxy has had some interface style updates since the original post, but the optional output toggle is still in the same place on the tool form, and will look like this:\n\nhisat2-summary-output-toggle-for-multiqc892\u00d7212 17.2 KB\n\nHope that helps everyone!"}], [{"role": "user", "text": "Hi everyone,\nI\u2019m trying to install Galaxy local on a server with Ubuntu 18.04.4 and python 3.10.2 .\nFollowing the steps on the Get Galaxy page, I successfully cloned the git repository. Now the problem is that when I try to run galaxy for the first time using sh run.sh (or sudo sh run.sh) I get the following error:\nCreating Python virtual environment for Galaxy: .venv\nusing Python: python3\nTo avoid this, use the --no-create-venv flag or set $GALAXY_VIRTUAL_ENV to an\nexisting environment before starting Galaxy.\nFetching https://files.pythonhosted.org/packages/source/v/virtualenv/virtualenv-16.7.9.tar.gz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 5001k  100 5001k    0     0  25.3M      0 --:--:-- --:--:-- --:--:-- 25.3M\nVerifying /tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9.tar.gz checksum is 0d62c70883c0342d59c11d0ddac0d954d0431321a41ab20851facf2b222598f3\nUsing base prefix '/usr'\nNew python executable in /home/andreaf/galaxy/.venv/bin/python3\nAlso creating executable in /home/andreaf/galaxy/.venv/bin/python\nTraceback (most recent call last):\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 2634, in <module>\n    main()\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 860, in main\n    create_environment(\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1162, in create_environment\n    install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages=site_packages, clear=clear, symlink=symlink)\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1721, in install_python\n    fix_local_scheme(home_dir, symlink)\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1807, in fix_local_scheme\n    if sysconfig._get_default_scheme() == \"posix_local\":\nAttributeError: module 'sysconfig' has no attribute '_get_default_scheme'. Did you mean: 'get_default_scheme'?\n\nHow can I fix it?\nThank you for the help"}, {"role": "system", "text": "seems a python version problem check the issue at https://github.com/pypa/pip/issues/8273#issuecomment-870399892\nIf you want to install galaxy properly on a local server I suggest you to follow the installation through Ansible Galaxy Installation with Ansible .\nAlternatively, Galaxy is also available as Docker see Docker Hub"}, {"role": "user", "text": "Thank you, I\u2019m trying with ansible but I#M having problems with the requirements.yml file:\nUsing it as on the document page results in the following error:\nERROR! Failed to parse the requirements yml at '/home/andreaf/galaxy/requirements.yml' with the following error:\nmapping values are not allowed in this context\n  in \"b'/home/andreaf/galaxy/requirements.yml'\", line 4, column 6\n\nThen removing the spaces after \u201c:\u201d yamllint runs fine, but then when using the command  ansible-galaxy install -p roles -r requirements.yml I et the following error:\nERROR! Unexpected Exception, this is probably a bug: 'str' object has no attribute 'keys'\n\nThat\u2019s my yml file content:\n---\n++ b/requirements.yml\n@@ -0,0 +1,14 @@\n- src:galaxyproject.galaxy\n  version:0.9.16\n- src:galaxyproject.nginx\n  version:0.7.0\n- src:galaxyproject.postgresql\n  version:1.0.3\n- src:natefoo.postgresql_objects\n  version:1.1\n- src:geerlingguy.pip\n  version:2.0.0\n- src:uchida.miniconda\n  version:0.3.0\n- src:usegalaxy_eu.certbot\n  version:0.1.5\n"}, {"role": "system", "text": "I think you have some problem in the requirement.yml\n++ b/requirements.yml\n@@ -0,0 +1,14 @@\n\nthey are not part of the file, check the Ansible documentation at Installing content \u2014 Ansible Documentation for more information about requirement files."}, {"role": "user", "text": "Ok, was just following the guide in the first link and they say to put that code in the requirements.yml file so I don\u2019t get what\u2019s wrong\u2026 Should I use something like\n---\nroles:\n - src: galaxyproject.galaxy\n   version: 0.9.16\n - src: galaxyproject.nginx\n   version: 0.7.0\n - src: galaxyproject.postgresql\n   version: 1.0.3\n - src: natefoo.postgresql_objects\n   version: 1.1\n - src: geerlingguy.pip\n   version: 2.0.0\n - src: uchida.miniconda\n   version: 0.3.0\n - src: usegalaxy_eu.certbot\n   version: 0.1.5\n"}], [{"role": "user", "text": "I found a website called tavaxy which claims that it\u2019s platform can convert taverna workflows to galaxy but all the post and the website are pretty old(from 2011 and 2012) and I couldn\u2019t find a way to open the platform.\nany updates on this?\ndoes Taverna to galaxy is possible? and how would that work?"}, {"role": "system", "text": "I haven\u2019t used Tavaxy, but the format of Galaxy workflows is backwards compatible, so if Tavaxy can convert taverna workflows into Galaxy workflows it should work. The paper on Tavaxy is here, I assume they will talk about how it is done: https://doi.org/10.1186/1471-2105-13-77 Also the manual: http://tavaxy.org/#ui-tabs-3"}, {"role": "system", "text": "Hello - Tavaxy (http://www.tavaxy.org/) was developed by Moustafa Ghanem when he was at the Center for Informatics Sciences, Nile University, Giza, Egypt, in around 2011-12. In 2013 the work (and the dept) was disrupted by political events in Egypt. In 2014 Moustafa tragically died and AFAIK work ceased.\nThe Taverna team are concentrating their efforts on the The Common Workflow Language as an interoperability platform for workflow systems. Galaxy is part of that initiative. http://www.commonwl.org"}], [{"role": "user", "text": "Hi colleagues\nUnfortunately, I have tried several times to use your website  ( https://main.g2.bx.psu.edu/). However, no respond. Is there any temporarily problem?\nRegards\nAdnan"}, {"role": "system", "text": "Hi @Adnan,\nplease try https://usegalaxy.org directly. It\u2019s working for me. You can see the status of the 3 big usegalaxy.* instances here: https://status.galaxyproject.org/\nHope that helps,\nBjoern"}, {"role": "user", "text": "Hi Bjoern,\nFirst of all, thanks for your attention and responding my inquire. I have followed your advice, but unfortunately no respond obtained. Do you think the reason of concurring this with me is due to  high usage of the galaxy website worldwide or because the internet service in my country (Iraq) not good enough to open the  website? Thanks again\nRegards\nAdnan\n\u202b\u0641\u064a \u0627\u0644\u0623\u062d\u062f\u060c 18 \u0623\u063a\u0633\u0637\u0633 2019 \u0641\u064a 1:19 \u0635 \u062a\u0645\u062a \u0643\u062a\u0627\u0628\u0629 \u0645\u0627 \u064a\u0644\u064a \u0628\u0648\u0627\u0633\u0637\u0629 \u202abgruening via Galaxy Community Help\u202c\u200f <\u202agalaxy@discoursemail.com\u202c\u200f>:\u202c"}, {"role": "system", "text": "Hi Adnan,\ncan it be that your country is blocking the access of this website? Can you try if you reach usegalaxy.eu?\nCiao,\nBjoern"}], [{"role": "user", "text": "Hi, when I tried to extract MAF blocks from a set of regions of the human  2013 GRCh38/hg38 assembly, the message \u201cNo options available\u201d popped out under \u2018Choose species\u2019. This operation works on the 2009 assembly GRCh37/hg19. However, if I send MAF alignments from the UCSC Table Browser using the 2013 GRCh38/hg38 assembly, I can actually join them and choose species! But, these are whole alignments, not blocks, so it doesn\u2019t help getting MAF blocks of specific regions, ie exons.\nIt would be great if the MAF blocks from the GRCh38/hg38 assembly could be extracted directly from genome coordinates of that assembly. I hope this can be fixed soon.\nThanks!"}, {"role": "system", "text": "Hi @ccasola\nTrue, the hg38 MAF data is not pre-indexed at Galaxy Main https://usegalaxy.org. Only hg19. This might be updated in the future \u2013 but it won\u2019t be soon and may not happen at all. Part of a larger project and there are some challenging technical/development items to address first.\nThe Galaxy EU https://usegalaxy.eu server does not have either. They may be able to add in hg38 (they use a different data organization structure \u2013 for now \u2013 soon-ish all usegalaxy.* servers will have data synced up). But some of the same issues may present at EU (specifically, the new Data Manager might be required first for them to add this index in). Ping @hxr @bjoern.gruening\nWhat you can do: Load the MAF data from UCSC into your account at either server or wherever you are working. It will be very large, will come in chunks (per chromosome) that need to be combined, should be sourced NOT from the UCSC Table Browser but their Downloads area (Conservation). Warning: you might run into quota space problems due to the size of the data. Give it a try first, and if you run into space problems, explain where you are working in a reply and we can follow up. Both of those servers above can temporarily grant extra quota for special purposes like this \u2013 IF and when actually needed. The process if different for both. You must be an academic for extra quota at usegalaxy.org. I\u2019m not certain if that is required for usegalaxy.eu but we can sort that out.\nOther public servers, in particular, Galaxy AU https://usegalaxy.org.au, already has a larger default quota allocation, so you could decide to do your work there. If an Australian national, the default quota is larger and there are other resources available. I also didn\u2019t check to see if they have hg38 MAFs pre-indexed for these tools already, but you could (create an account if you don\u2019t have one, load any bed dataset (test size/few lines is enough), assign to the genome hg38 \u201cdatabase\u201d, and see what options for the tools show up).\nAlternatives include setting up your own Galaxy with sufficient processing and space resources. Indexing for these particular tools is non-trivial line-command work and a Data Manager for these tools does not exist yet. Unless you are willing to try older scripts, troubleshoot, etc \u2013 just load the MAF data into your server and use it from a history. It could be put into a Data Library so that copies in histories do not consume extra space each time you want to work with it. Data cannot be used in analysis directly from a Data Library \u2013 it needs to be in a history or pre-indexed correctly on the server first.\nWe realize this is much more complicated than it should be, but those are the current options. Ask if you have questions about any of this - I didn\u2019t add in a bunch of links for options that you do not intend to try (yet).\nThanks!"}, {"role": "system", "text": "\n\n\n ccasola:\n\nBut, these are whole alignments, not blocks, so it doesn\u2019t help getting MAF blocks of specific regions, ie exons.\n\n\nI forgot to address this. In order to get \u201cblocks\u201d (putatively exons) a BED12 dataset is needed. Or, BED6 data that already represents exons. These can be extracted from the UCSC table browser from existing tracks. If you just have a BED dataset that you created yourself, and it represents an entire transcript, try the tool **Stitch MAF blocks**  given a set of genomic intervals (Galaxy Version 1.0.1). Some experimenting with tools in this group will be needed since we don\u2019t know what data you have now.\nHow the UCSC Table Browser handles data/blocks is different than using the MAF tools in Galaxy. The options are not the same, which is why the Galaxy tools exist. You could contact UCSC at their Google forum and see what options they have. Warning, this will probably include line-command work and you will not have the analysis tracking/histories/reproducibility that Galaxy provides. But how to do this is your choice."}, {"role": "system", "text": "See updated topic here: Attempting to use Stitch MAF blocks tool, but lack of documentation"}], [{"role": "user", "text": "Hi All,\nI would like to use the HyPhy-aBSREL tool to run selection tests on several genes. Is it possible to include an option to set the \u201cmultiple-hits\u201d option (not currently available) and set the \u201c\u2013branches\u201d to labeled in addition to the current options?\nThanks\nVInny"}, {"role": "system", "text": "Vinny,\nSupport for that feature is in this pull request, and will be in galaxy wrapper version 2.5.17+galaxy1 and later."}, {"role": "user", "text": "Great! Is there an ETA?\nBest,\nVinny\nVincent J. Lynch, Ph.D.\nAssistant Professor\nDepartment of Biological Sciences\nUniversity at Buffalo, SUNY\n551 Cooke Hall\nBuffalo NY, 14260\nvjlynch@buffalo.edu\n\u201c\u2026 the most useful thing we can do is to repudiate, and so we repudiate\u201d \u2013 I. Turgenev, Fathers and Sons (1862)\n\"There is a grandeur in this view of life, with its several powers,\nhaving been originally breathed into a few forms or into one; and that\nwhilst this planet has gone on cycling according to the fixed laws of\ngravity, from so simple a beginning endless forms most beautiful and most\nwonderful have been, and are being, evolved.\u201d \u2013 C. Darwin, On The Origin of Species (1859)"}, {"role": "system", "text": "I don\u2019t have any direct control over usegalaxy.eu or other public servers, but the new wrapper is merged and I\u2019ll be updating usegalaxy.org with it as soon as it\u2019s deployed to the toolsheds, which should only be another 15-20 minutes. If you\u2019re running a local or cloud galaxy, you\u2019ll be able to update your installed version in the same time."}], [{"role": "user", "text": "Do we have any platform or workflow for analyzing single cell  atac-seq data?\nthank you"}, {"role": "system", "text": "Hi @Yuliang_Wang,\nthere is a fantastic training on ATAC-seq analysis in the Galaxy Training Platform.\nRegards"}], [{"role": "user", "text": "Hello\nAm a new user for this platform.\nI have a paired end sequences samples from two groups of soils (farm B and farm C), each group has a sub-set of five samples B1,B2, B3, B4 and B5 for farm B and C1, C2, C3, C4, C5 for farm C. Is their a workflow show how to:\n\nset the sub-set of each group as forward and reverse read?\nmerge the two groups , so it will be easy to manipulate with my data.\nmake groups that the Galaxy know which sub-set is belong to which group.\n\nI will be appreciate your help"}, {"role": "system", "text": "Hi @kebabiness_over9000,\nI recommend you to have a look at this tutorials:\n\nRule Based Uploader\nRule Based Uploader: Advanced\n\nIt explain how to organize your input datasets in different groups.\nRegards"}], [{"role": "user", "text": "Hi team, is your \u201cmain tool shed\u201d down at the moment? I\u2019ve tried the \u201cInstall new tools\u201d button from the Admin panel of 3x different Galaxy instances (all in different OS\u2019s and remote locations) and they all timeout at \u201cWaiting for http://toolshed.g2.bx.psu.edu\u2026\u201d\nNothing to be found in terms of scheduled maintenance/outages at:\nhttps://galaxyproject.org/main/notices/\nhttps://galaxyproject.org/news/\ngalaxy-dev Digests Vol 149, Issues 9-11"}, {"role": "user", "text": "Marius is on the case https://gitter.im/galaxyproject/Lobby?at=5c04feaa48d021555ae38f27"}, {"role": "system", "text": "The machine that hosts both our toolsheds is down, we will get it all up asap. Thanks for reporting."}, {"role": "system", "text": "toolshed is back up now: https://toolshed.g2.bx.psu.edu/"}], [{"role": "user", "text": "Hi, I could not display the full taxon name of the legend labels on left side of graph while drawing the plot cladogram in LEFse analysis. Some of the indicator/legend labels are as follows. How can I do this?\n"}, {"role": "system", "text": "Hi @Jen\nYou might need to adjust the display settings. I added a tag to your post that points to much prior Q&A about different ways people have done that in the past. Sometimes the solution involves changing tool settings and sometimes changing something as simple as using a smaller browser font.\nMaybe start here then review others? Pictures of lefse on line has incomplete display\nHope that helps!"}, {"role": "user", "text": "Hi @jennaj, Thank you for your kindly reply. Unfortunately, I couldn\u2019t. When drawing the plot of LEFse results, I was displaying the full name of the taxon by entering \u201c-1\u201d in the \u201cNumber of label levels to be displayed (-1 means all)\u201d part. However, while drawing the plot cladogram, I could not display the full taxon name of the indicator labels on left side of graph "}, {"role": "system", "text": "You are working at the public Galaxy server in the quoted text below, correct?\nThey have custom versions of the Lefse tools, so contacting them is the best way to report potential bugs. Or, you can just try a different setting and see what happens. It seems your input choice on the C) Plot LEfSe Results (Galaxy Version 1.0) tool form should be valid as described within the form, but then down in the help section below it is described differently (?). I\u2019m guessing that the second is the correct usage since that seems to match the type of output you are actually getting in the result.\nTool form advanced setting:\n\nNumber of label levels to be displayed (-1 means all)\n\nTool form help section:\n\nText and label options\n\nNumber of label levels to be displayed: when dealing with hierarchical features this option sets the level to be displayed in the labels (-1 means the last level only, 1 means the highest level, 2 means the first two levels and so on).\n\n\nHuttenhower Galaxy server info. You can do both \u2013 modify the usage yourself for a quicker result but also report the conflict in the instruction to them. Maybe the usage changed at some point and they missed changing the instructions in both places.\n\n\n\nProblems with Plot Cladogram -- http://huttenhower.sph.harvard.edu/galaxy/\n\n\nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server.\nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers).\nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).\n\nContact: Huttenhower Lab \n\n\nHelp/tutorials: (also linked from the server\u2019s home page, with example data available):\n\nlefse \u2013 The Huttenhower Lab \nlefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHub \n\n\n"}], [{"role": "user", "text": "I ran Trinity for RNA seq assembly yesterday. It has been gone for nearly 24 hours but is still operational. Could you please tell me if Trinity is operational in galaxy Euorpe?"}, {"role": "system", "text": "Hi @Md_Atiqur_Rahaman,\nrun time depends on amount of data,  quality and resources allocated for the job,  and some Trinity assembly take long time. Also, sometimes Galaxy does not refresh history panel, so completed jobs are displayed  as running. Click at Galaxy Europe banner in the top left corner to refresh the web page and refresh the history. I don\u2019t know the current wall time for Galaxy Europe, but in they we very generous in the past. You may also consider alternative tools, like rnaSPAdes.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello,\nI recently upgraded our local Galaxy instance from 21.09 to 22.05 and I noticed two things:\n\n\nAs recommended in the documentation, I switched from supervisor to systemd using the sample config file provided: however, when Galaxy is started this way, tools give errors such as:\njobs_directory/052/52678/galaxy_52678.sh: line 9: mkdir: command not found\nas if the PATH was not right\u2026\nIf I start Galaxy manually from the virtualenv using galaxyctl start, no such error appears.\n\n\nI had set up Galaxy reports on \u201c/reports\u201d, but could not find a way to start it again on a prefix in the new version. Gravity states it handles Galaxy Reports, but it does not with the (modified) sample config file, and if I start it using run_reports.sh, it still only listens on \u201chost:9001/\u201d, not \u201chost:9001/reports\u201d. Setting galaxy_url_prefix does not change anything.\n\n\nAm I missing something?"}, {"role": "system", "text": "Hi @ppouchin\nIt is recommended to upgrade Galaxy in the order of releases. Did you first upgrade to 22.01, then 22.05? Releases \u2014 Galaxy Project 22.05.1 documentation\nIf not, the developers will likely ask for you to try that first. But let\u2019s see if they have advice now. I posted your question to the Matrix admin chat. They may reply here or there, and feel free to join the chat. You're invited to talk on Matrix"}], [{"role": "user", "text": "I\u2019m having an issue with mapping using HISAT2 and Bowtie 2. I am trying to map my genome to a reference genome. Both programs run, but the BAM files only show is QNAME and all the other components of the table are blank. I have already gone through trimming and QC and the quality scores are all above 30.\nThis is example of an error in mapping stats:\nWarning: skipping mate #2 of read \u2018V300007992L3C001R0141268281/2\u2019 because it was < 2 characters long Warning: skipping mate #1 of read \u2018V300007992L3C001R020207830/1\u2019 because length (1) <= # seed mismatches"}, {"role": "system", "text": "Hi \u2013\nFor the mapping stats, scroll down the page to find the report. The warnings at the top are about reads that are too short to map with the initial seeding criteria. You could filter out very short reads by length when doing other QA steps with Trimmomatic or choose to ignore them \u2013 they are not contributing to a failure (the runs are successful).\nThere does seem to be a performance problem at the Galaxy Main https://usegalaxy.org server that just started. I\u2019m checking with our administrator to find out what is going on. Expanding/viewing datasets is just one symptom. More feedback very soon & thanks for reporting the issue!"}, {"role": "system", "text": "Haven\u2019t heard back from the administrator yet but I was able to expand/view datasets again. There might have been a very heavy load on the server for a short window.\nTry viewing your BAM results again if you want. The data is intact and can be used as input to downstream tools, even if the \u201cview\u201d function is a bit slow right now."}, {"role": "user", "text": "Thank you. The run are successful, but all the other columns except QNAME are blank. So, even though it is running successfully. There is a error which is causing the other columns to be blank."}, {"role": "system", "text": "This is when viewing the BAM dataset, correct? Only the first line will display immediately. BAMs are compressed data. Viewing these is a special function in Galaxy and it takes some time to load the data up in the application. I was able to look at one of your smaller results and it took a few minutes. The first line loaded first, then the header (fairly large from the custom genome), then finally the data lines after scrolling down the page.\nTo view or work with the content in an uncompressed format, convert with BAM-to-SAM. To generate statistics on a BAM dataset, there are many tools \u2013 search the tool panel with the keyword \u201cbam\u201d and/or review the tools under the group SAM/BAM."}], [{"role": "user", "text": "I\u2019m trying to use the \u2018qiime2 tools import\u2019 for SampleData[SequencesWithQuality] with \u2018Single Lane Per Sample Single End Fastq Directory Format\u2019.\nI think I\u2019ve correctly build the manifest and metada files. But I keep getting the same persistent error regarding the element \u2018name\u2019.\nThis parameter asks me for \u201cFilename to import the data as. Must match regex: .+_.+_L[0-9][0-9][0-9]_R[12]_001.fastq.gz\u201d but it doesn\u2019t matter which name I try it always seems to fail, so Galaxy doesn\u2019t even execute my job.\nI\u2019ve also tried to look up RegEx matchers to know if my created name would be suitable but these don\u2019t seem to be helpful.\nCan you help me in this matter? I\u2019m not really a coder hahaha but I really want to understand this.\np.s. everything runs perfectly when i\u2019m using QIIME2 on command line, the problem seems to be nested in galaxy\u2026 (which I prefer because buttons)"}, {"role": "system", "text": "Update:\nThe import function has more discussion cross posted over at the EU Matrix chat \u2192 You're invited to talk on Matrix\n\nWelcome, @sashaphex\nThe \u201celement identifiers\u201d are the names of the files inside the collection. When creating a collection, the file extensions are usually removed \u2013 that is why the extra option to add them back in is also listed.\nTools in the Collection Operations tool group can be used for renaming element identifiers. Is that what you are doing?\nExample for using the tool when run in Galaxy. Note that the naming is more specific than command line (technical reasons\u2026). If sample ids are included in any other inputs, the naming should be consistent.\n\nSampleIdIndexedSingleEndPerSampleDirFmt\nSingle-end reads in fastq.gz files where base filename is the sample id\nThe full file name, minus the extension (.fastq.gz) is the sample id.\n\n\nsample-42_S1_L001_R1_001.fastq.gz is sample-42_S1_L001_R1_001\n\n\n\n\nPlease post back a few of the element identifiers names if you would like more help. You could also post a shared history link. Troubleshooting errors"}, {"role": "user", "text": "Hey @jennaj\nThanks for such a quick response.\nMy problem lies here:\n\nimage1102\u00d7787 25.2 KB\n\nI can\u2019t seem to progress after this step because I can\u2019t even execute my job\u2026\nDo you know how I could solve this or maybe if there is a way to work around it? Maybe the collection method was you mentioned?\np.s. thank you for the access to the EU Matrix chat, I will also check there for insight"}, {"role": "user", "text": "Meanwhile in the Matrix people already helped me with this problem.\nSo for the data \u201cARA.SOIL.1fastq.gz\u201d, the name \u201cARA_S1_L001_R1_001.fastq.gz\u201d doesn\u2019t create an error.\nHope this helps anyone having the same problem."}], [{"role": "user", "text": "Hello, for several years I have been using Galaxy to host my bigwig files, which I then visualize in the UCSC genome browser. Just recently, I have found that Galaxy doesn\u2019t allow this feature anymore. To elaborate: Typically, I would upload my bigwig file to Galaxy, and then I could simply click the bigwig file in my history to open a small box which displayed an option that said (maybe with different wording) \u201cview in UCSC.\u201d Clicking this option would immediately take me to the UCSC genome browser, with my bigwig file being displayed as browser tracks. This was great, and was exactly what I wanted. But in recent weeks, this option to \u201cview in UCSC\u201d has disappeared from my bigwig files being hosted on Galaxy. There is no longer a clickable link that says \u201cview in UCSC.\u201d Could you help me understand what is going on? Is this feature totally gone from Galaxy? As in, do I need to find another host for bigwig files which I want to view in the UCSC genome browser? Or is this feature still available in Galaxy, but just re-worked in a way that I haven\u2019t realized? To try an alternative approach, I clicked the \u201ccopy link\u201d icon for my bigwig on Galaxy, and tried pasting that link when I manually upload a custom track to the UCSC genome browser, but this approach is also not working (it prompts an error from UCSC saying that they could not open the file being specified by the link which I copied from Galaxy for my particular bigwig). It seems strange that Galaxy would still be okay with hosting our files, but not be okay with making those files (a bigwig file, in my case) viewable/accessible to other sites like the UCSC genome browser."}, {"role": "system", "text": "Hello!\nYep, you guessed it \u2013 it\u2019s still in Galaxy but accessed a different way.  We\u2019ve consolidated all the visualization options to a single interface, and if you click on the visualization icon for the dataset in question:\n\nAnd then in the middle panel at the top, this should be what you\u2019re looking for:\n\nimage732\u00d7416 25.9 KB\n\nLet me know if this isn\u2019t working correctly for you and I can dig deeper.  Thanks!"}], [{"role": "user", "text": "Hello,\nI would like to request adding the latest tomato genome assembly to Galaxy Main.\nThe latest genome assembly is called \u201cSL4.0\u201d and also S_lycopersicum_Feb_2019.\nHere is the sequence from the tomato genome Web site:\nftp://ftp.solgenomics.net/tomato_genome/assembly/build_4.00/S_lycopersicum_chromosomes.4.00.fa.gz\nMany thanks,\nAnn Loraine (IGB project)"}, {"role": "system", "text": "This is to reinforce the request made by Ann. Please add the SL4.0 version of tomato genome to Galaxy!"}, {"role": "system", "text": "Hi,\nsorry for the late reply. We install this Tomato genome on usegalaxy.eu\nCheers,\nBjoern"}], [{"role": "user", "text": "Hello Galaxy community, I am new to Galaxy and had a question while developing my local instance and running my files through the local instance.\nWhat I have tried so far:\nSo I created a local Galaxy instance with default configurations, and after becoming an admin following the guidelines in this tutorial: Get Galaxy I created a simple workflow by following this tutorial while I am still an admin in my local instance Create and share workflows in Galaxy - YouTube\nand wanted to share this with my end user as a service for RNA-Seq analysis keeping these rules in mind Ten Simple Rules for Setting up a Galaxy Instance as a Service\nChallenge:\nBut once I log out of the web instance in which I was the admin and log back in as a non admin user into the same local instance I created, I am not able to see the workflow I created. I guess I was assuming that once the workflow is created, anyone who has access to the local instance can just access the workflow and run their files through it.\nTo summarize:\nI am interested in sharing this workflow with my end user so that they can simply open up this galaxy instance, input their files and the files run through the galaxy instance which gives them the desired output in the end! How do I accomplish this?\nThank you in advance!\n-Aditya Natu"}, {"role": "system", "text": "Hi @adityanatu\nTry this:\n\nLog into your account with the workflow.\nShare the workflow with all other users working on that same Galaxy server by publishing it.\nPublishing creates a URL that can be shared with others, but also places the shared object directly under \u201cShared Data > Workflows\u201d.All users working on that same Galaxy server will have access, and they can import and use the workflow in their own accounts.\nAny account can \u201cpublish\u201d into Shared Data: 1) Histories (and the Datasets inside of them), 2) Workflows, 3) Visualizations, 4) Pages.\nAny account can also \u201cshare\u201d work. Directly with the other person\u2019s account email address, or by generating a share link URL and giving that URL to them (however you communicate: text, slack, email, etc)\nOnly admin accounts can add data into Shared Data: 1) Data Libraries. It is a different way of organizing commonly used data and can save you some storage space since data in a Data Library imported by a user is just a linked clone of the original dataset.\nWorkflows do not consume any space at all in any account in terms of quota usage, but these files of course do consume a tiny amount of space on disk. That said, Datasets and Histories are much larger and matter more in terms of managing the overall disk size of your instance.\n\nFAQs: Galaxy Support\n\nSharing and Publishing your work\nData Privacy Features\n\nTutorials that include share functions: Galaxy Training!\nWebinars. The first and fourth will probably be of the most immediate use for you: Galaxy-ELIXIR webinars series: Advanced Features | ELIXIR\nNote: The FAQs and Tutorials show a slightly older version of Galaxy (the application changes fast!), but you should still be able to find the \u201cShare or Publish\u201d form for any Galaxy object and share/set permissions how you would prefer to. In practical terms, you might want to not grant editing permissions the workflows if you want the users to import and run the workflow exactly how it is published. The Webinar series was new this month, and includes operations using the most current Galaxy version.\nHope that helps!"}], [{"role": "user", "text": "this invoked workflow has been stuck at what should\u2019ve been a relatively short step. It\u2019s been \u201crunning\u201d for a few weeks. Ran this on usegalaxy.eu\nJob API ID:\t11ac94870d0bb33a1b57076a9eb8139e"}, {"role": "system", "text": "Hi @thepineapplepirate\nMaybe try again now if this hasn\u2019t completed?\nThe next troubleshooting step would be to try opening the workflow in the editor. Do you see any technical warnings that need to be addressed?\nIf all of that looks okay, what happens if you run the same tutorial workflow on the sample data provided in the tutorial?"}, {"role": "user", "text": "Hi Jenna,\nAppreciate the response . There are no technical issues with the workflow that I\u2019m aware, and they would usually lead to an error and subsequent job failure if they\u2019re any. This is sort of a unique issue, where it\u2019s just stuck in a \u201crunning\u201d state at a particular step, that should otherwise run in a handful of minutes or less. I\u2019m also running this on the sample data. Wondering if the admins can look under the hood of that job ID and see what\u2019s actually happening. Thanks!"}, {"role": "system", "text": "Hi @thepineapplepirate\nI hope things are resolved by now. If not, please try reaching out to this dedicated chat usegalaxy-eu/Lobby - Gitter.\nRelated Workflow invocation scheduling failed - Galaxy EU"}], [{"role": "user", "text": "We just finished the upgrade to Galaxy 20.01 (from 19.09) and everything works well, except that all uploaded files default to the data type of ZIP. I\u2019ve tested CSV, TSV, FASTA, TXT, and a couple more, and they all have \u201cformat: zip\u201d once uploaded/processed and in my history.\nI can, of course, manually change each item\u2019s format using the \u201cedit attributes\u201d option, but this is not desirable.\nAny ideas why this may be happening? Thanks in advance."}, {"role": "system", "text": "Can you share an example of such a dataset as you try to upload it ? Note that zip archives should not ever be auto-selected on a normal release of 20.01, we have introduced a zip sniffer only in Galaxy release 20.05. Do you have a custom datatype_conf.xml file or a zip datatype installed from the tool shed ?"}], [{"role": "user", "text": "Hi,\nI\u2019m the administrator for a local Galaxy instance for a few users on a dedicated server in my lab  [v19.05, Apache proxy, Linux Debian 10]\nFor some time now, we\u2019ve run into an annoying and mystifying issue :\nwhen anyone (myself included) tries to login, the site cycles back to the welcome page, without taking authentication into account\nHowever, it remains possible to:\n\nhave a wrong user/password combination rejected\nreset a user\u2019s password\nregister a new user\n\nBesides, AFAIK, nothing has changed on the server ! (Issue started before recent Debian upgrade from 10.2 to 10.4)\nAnd I cannot find any relevant error in the logs (Galaxy, Apache or PostgreSQL) \nAny suggestion/possible test would be appreciated\u2026\nThanks in advance\nPS : Of course, no amount of service restart(s), or even server reboot, helped"}, {"role": "system", "text": "Think this is a known thing of version 19.05 after logging try to refresh the page (think this was the solution, can\u2019t remember it so well).\n\n  \n    \n    \n    Apache Redirect at logout \n  \n  \n    Hello, \nI am working on a galaxy server that uses apache and PAM auth for user login/logout. However, we are encountering an issue that when a user logs out they are redirected to the login page but url includes the redirect location twice. \ngalaxyurl /root/login?redirect=%2Flogin \nWhen the user tries to login from this page it is unsuccessful and it redirects them to the correct login url which is just /login and the user is able to login. \nI am struggling to locate where this behavior is comin\u2026\n  \n\n\n\n  \n\n      github.com/galaxyproject/galaxy\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      \"logout\" generates an incorrect redirect URL when deployed behind Apache or Ngnix\n    \n\n    \n      \n        opened 11:31PM - 27 Jan 20 UTC\n      \n\n        \n          closed 03:49PM - 18 Mar 21 UTC\n        \n\n      \n        \n          \n          jennaj\n        \n      \n    \n\n    \n        \n          kind/bug\n        \n        \n          area/UI-UX\n        \n    \n  \n\n\n  \n    See discussion here: https://help.galaxyproject.org/t/apache-redirect-at-logout/\u20262830\n\nIt seems to be at least as old as 19.05 and still present in 19.09.\n\nBug or a configuration problem?\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Hi,\nThanks for your suggestion (and sorry about the delay).\nUnfortunately, URLs look OK and refreshing does not change anything.\nIn the meantime, I upgraded our Galaxy instance to release_20.01. Still impossible to login (!)\nThat\u2019s getting\u2026 frustrating"}, {"role": "user", "text": "OK, some follow-up\u2026\nAfter somme digging using Firefox developer tools, I noticed that, when redirected to the home page from the login form, there was an additional warning at the head of the web console, about the \u00ab galaxysession \u00bb cookie\u2026 (pointing to https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite)\nFrom there, view-source:https://(\u2026)/galaxy/user/login yields\n{\"err_msg\": \"No session token set, denying request.\"}\nSince the issue was present on all the recent browser I tried (Firefox, Safari and Edge), on a hunch, I tried with an older one, namely Firefox ESR\nAnd this time I was able to log to Galaxy at last! \nSo there might be something wrong with session cookie handling in Galaxy\u2026\nShould I file a bug report?"}, {"role": "user", "text": "OK, final update :\nThe issue was eventually resolved when I was able to use \u201cproper\u201d SSL certificates for Apache (i.e. instead of self-signed ones), that were supplied by my University\u2019s IT department.\nNo more login issues, including with the latest browser versions \nHope this might help someone else\u2026"}], [{"role": "user", "text": "Limited by the resources of our own platform, we accepted the suggestions to use the usegalaxy.org.\nBut when we using the usegalaxy.org, some tools are unreachable. So we upload the tools needed by us to the Repositories, but it still can\u2019t be used in usegalaxy.org.\nMay I ask how the tools uploaded by myself can be included in the public tools shed? Thanks!"}, {"role": "system", "text": "Galaxy administrator chooses what tools to install in their Galaxies. The Main Galaxy at usegalaxy.org does not have all the tools from the Toolshed installed, rather it only has a curated set of tools most useful to the community."}], [{"role": "user", "text": "Hello everyone!\nI assembled raw and trimmed reads (Illumina PE150) using metaSPAdes and then clustered into bins with MaxBin2. Now I want to annotate these bins by using Prokka but I\u2019ve got this error:\nArgument \u201c1.7.8\u201d isn\u2019t numeric in numeric lt (<) at /usr/local/bin/prokka line 259.\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/759/40759167/_job_tmp -Xmx28g -Xms256m\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/7\nI\u2019ve uploaded the bins obtained from MaxBin2 in fasta format.\nCould anyone help me with that?\n\nerror661\u00d7653 87.2 KB\n\nThank you in advance!"}, {"role": "system", "text": "Update:\nThe tool is now running correctly. Please try again.\nThank you to everyone who reported the problem \n\nHi @mjordanr\nThe tool has a new known configuration issue specific to UseGalaxy.org. The fix is in progress but not completely done yet. Tracking ticket: Prokka 1.14.6+galaxy1 = missing dependency \u00b7 Issue #354 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub\nMeanwhile, please run the tool at a different public Galaxy server. Both of these are good choices and host a toolset similar to the US server: UseGalaxy.eu or UseGalaxy.org.au.\nThanks for reporting the problem!"}], [{"role": "user", "text": "Hi,\nI\u2019ve been successfully using galaxy.auth.providers.ldap_ad with auto account creation for over a year, and today it started throwing an error. There is a chance this is the first new user to log in since I upgraded to release_19.05.  Here is the log output:\ngalaxy.auth.providers.ldap_ad DEBUG 2019-09-26 16:03:06,064 LDAP authentication successful\ngalaxy.auth.util DEBUG 2019-09-26 16:03:06,071 Email: someone@example.com, auto-register with username: someone\ngalaxy.web.framework.decorators ERROR 2019-09-26 16:03:06,097 Uncaught exception in exposed API method:\nTraceback (most recent call last):\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/web/framework/decorators.py\", line 282, in decorator\n    rval = func(self, trans, *args, **kwargs)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 116, in login\n    return self.__validate_login(trans, payload, **kwd)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 137, in __validate_login\n    message, user = self.__autoregistration(trans, login, password)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 93, in __autoregistration\n    user = self.user_manager.create(email=email, username=username, password=\"\")\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/managers/users.py\", line 99, in create\n    self.session().flush()\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py\", line 162, in do\n    return getattr(self.registry(), name)(*args, **kwargs)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2446, in flush\n    self._flush(objects)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2584, in _flush\n    transaction.rollback(_capture_exception=True)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\", line 67, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2544, in _flush\n    flush_context.execute()\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 416, in execute\n    rec.execute(self)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 583, in execute\n    uow,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 245, in save_obj\n    insert,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 1116, in _emit_insert_statements\n    statement, params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 980, in execute\n    return meth(self, multiparams, params)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 273, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1099, in _execute_clauseelement\n    distilled_params,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1174, in _execute_context\n    e, util.text_type(statement), parameters, None, None\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1458, in _handle_dbapi_exception\n    util.raise_from_cause(sqlalchemy_exception, exc_info)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py\", line 296, in raise_from_cause\n    reraise(type(exception), exception, tb=exc_tb, cause=cause)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1171, in _execute_context\n    context = constructor(dialect, self, conn, *args)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 719, in _init_compiled\n    for key in compiled_params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 719, in <genexpr>\n    for key in compiled_params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/type_api.py\", line 1194, in process\n    return process_param(value, dialect)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/model/custom_types.py\", line 373, in process_bind_param\n    value = value[0:self.impl.length]\nStatementError: (exceptions.TypeError) '_hashlib.HASH' object has no attribute '__getitem__' [SQL: u'INSERT INTO galaxy_user (create_time, update_time, email, username, password, last_password_change, external, form_values_id, deleted, purged, disk_usage, active, activation_token) VALUES (%(create_time)s, %(update_time)s, %(email)s, %(username)s, %(password)s, %(last_password_change)s, %(external)s, %(form_values_id)s, %(deleted)s, %(purged)s, %(disk_usage)s, %(active)s, %(activation_token)s) RETURNING galaxy_user.id'] [parameters: [{'username': 'someone', 'disk_usage': None, 'form_values_id': None, 'deleted': False, 'email': 'someone@example.com', 'last_password_change': datetime.datetime(2019, 9, 26, 16, 3, 6, 92741), 'external': False, 'active': True, 'password': <sha1 HASH object @ 0x7f9950dda620>, 'purged': False, 'activation_token': None}]]\n\nPlease let me know if you know what broke. I checked the github issue tracker but didn\u2019t see anything that seemed related.\nthanks,\n-Will"}, {"role": "system", "text": "Welcome @wholtz\nI\u2019ve pinged the developers at Gitter to see if anyone recognizes the issue. Please feel free to chat there, too. https://gitter.im/galaxyproject/Lobby?at=5d9382df47de0a719d445dbe\nI also found a similar error message reported yesterday:\n\nobject has no attribute \u2018getitem\u2019\n\nBUT with respect to a completely different use case. No idea if related. There will be feedback there as well: https://github.com/galaxyproject/galaxy/issues/8680.\nIf there is no actionable feedback after a few more days, then this should be reported as a new issue at https://github.com/galaxyproject/galaxy/. Be clear and include as many details as you can, including what you posted above. You might also want to cross-link the two (put the Galaxy help link in the Github issue ticket, and then post that Github ticket back here. That way if others run into this issue, or a similar issue, now or later, all will be connected.\nThanks!\nadmin edit: correction Galaxy releases all support Python 2, as will the upcoming 19.09 release. Python 3 is not recommended quite yet."}, {"role": "system", "text": "Hi Will,\nI think I have a hunch. Are you using use_pbkdf2: false in config/galaxy.yml ?"}, {"role": "user", "text": "Hi @nsoranzo,\nYes, I did have use_pbkdf2: false and removing this setting did allow autoregistration to start working again. Given how disconnected autoregistration is from FTP server encryption settings, I\u2019m quite impressed with your ability to nail this on your first attempt!\nI\u2019m no longer making use of the FTP server functionality, so my issue is resolved.\nthanks much!\n-Will"}, {"role": "system", "text": "@wholtz Thanks for the compliment, your report and the traceback were very helpful in tracking the issue down. It should be fixed by https://github.com/galaxyproject/galaxy/pull/8739 , let us know if you want to have this backported to 19.05."}], [{"role": "user", "text": "Hola,\nWe got a problem with \tvelvet toolshed.g2.bx.psu.edu/repos/devteam/velvet/velveth/1.2.10.0\nThe tool standard error is \u201cError running velveth Killed\u201d\nWe use a virtual machine with 4go RAM for the moment.\nPlease help\nThanks"}, {"role": "user", "text": "We upgrade to 24 gigas Ram. Same problem"}, {"role": "system", "text": "Hi - That tool wrapper is several years old with no update plans, so it is problematic to recommend that it be used (at all).\nIf you still want to try, start by checking if the install is correct and if the tool is really running out of memory with a down-sampled dataset. \u201cKilled\u201d in a job error usually means the job failed for server-side reasons \u2013 running out of memory can be one underlying reason. Your server logs might have more details.\nI would usually also recommend running the tool test for the repository or even using that same test data through the GUI to test basic functionality before diving into data/input/parameter solutions. However, this tool wrapper version doesn\u2019t have one (related to being older).\nIt is probably better to use a different Velvet wrapper. This one was updated 2018-10 and the author is an admin for the http://usegalaxy.org.au server. It does include a test case, which should help narrow down what is going wrong (if/when something does the first time you install and execute a job). Or run the test first to confirm the install (best option). https://toolshed.g2.bx.psu.edu/view/simon-gladman/velvetoptimiser/37d88f41c810"}, {"role": "system", "text": "Update: If still having problems with Velvet, try an alternative assembler:\n\n\n\nDisappear Velvet tool\n\n\nWe recommend Unicycler for small genome assembly.\nGTN Tutorial: http://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html\n\n"}], [{"role": "user", "text": "Hi all,\nI\u2019m using Galaxy to get terminator regions for all genes in a genome and I\u2019m using the \u2018get flanks\u2019 tool.\nQuestion: does get flanks define the 3\u2019 end of the gene on the sense strand as the \u2018Around end\u2019 regardless of the direction of transcription? So if the gene is transcribed from the antisense strand, get flanks would be taking the flanking region around the transcription start site\u2026?\nI hope this makes sense, but let me know otherwise.\nThanks,\nDavid"}, {"role": "system", "text": "Cross-posted on biostars: Question about the 'get flanks' tool in Galaxy"}, {"role": "user", "text": "Thanks Ram"}, {"role": "system", "text": "Hi @DWJM\nIf you include the strand in the BED6 input, the tool will extract upstream/downstream correctly. For any BED dataset (Genome Browser FAQ), the coordinate numbers are always with respect to the forward strand but which bases those represent depend on the strand assignment.\nScroll down on the tool form for the examples: both are extracting the same coordinates on different strands as output. Those are based on different stranded inputs combined with some same parameters and some different parameters set on the form.\nIf you are not sure or want to check settings against your data: try running the tool on a small sample that includes both strands and visualize the results in IGV.\nThanks @RamRS for linking to two posts together "}], [{"role": "user", "text": "Hi.  It\u2019s been a couple years since I used Galaxy regularly so I\u2019m a bit rusty.  I have just uploaded a custom genomic scaffold file in fasta format to my history and I need to know how to process the file so that I can query it looking for promoter regions upstream of two common genes (cytoplasmic actin, polyubiquitin) using the coding sequence from closely related species.  Thanks for any help!"}, {"role": "system", "text": "Dear @duncan-d,\nYou would probably need to do a genome assembly. Galaxy has a big section regarding this topic Galaxy Training!. Maybe start with the introduction tutorial: An Introduction to Genome Assembly.\nIf you have your contigs, you can investigate your promotors. If it is a genome that is well studied, then you could use annotated promotors from a database. If you look for de-novo promotors or have a less prominent genome (e.g., procaryotes), then you could look for typical promotor sequences of your organism.\nHave a good day and best wishes,\nFlorian"}, {"role": "user", "text": "Hi Florian\nThanks for the quick response! What I have is an unannotated, assembled genome from another lab. The genome is Schistocerca americana (American locust), which has a very large genome full of repetitive DNA. There are 1700 sequence contigs in a fasta file. I\u2019ve managed to use the NCBI discontinuous megablast to find the polyubiquitin gene using Drosophila coding sequence, but of course it doesn\u2019t give me the upstream sequence data. I tried opening the file in IGV but it tells me that I\u2019m missing a .fai index file and it can\u2019t recapitulate that file. I emailed the lab asking for the original .fai file but haven\u2019t heard back yet. Is there a way to regenerate that index file with just the fasta file I have?\nDianne\n(redacted contact info for privacy)"}, {"role": "system", "text": "Hi @duncan-d\n\n\n\n duncan-d:\n\nI tried opening the file in IGV but it tells me that I\u2019m missing a .fai index file and it can\u2019t recapitulate that file.\n\n\nGalaxy creates fa.fai indexes at runtime (when needed).\nYou can also create the index directly and have it show up as a dataset in your history.\nThe help in this topic covers all the use cases (any fasta, not just from Unicycler): Opening Unicycler assemblies with IGV local\nHope that helps "}], [{"role": "user", "text": "Hi Galaxy team,\nI need to include new variable modifications which are absent in the list. Is it possible to add these modifications to Galaxy? Like in the MaxQuant configuration tab?\nWith best regards,\nArman"}, {"role": "system", "text": "Hi @Arman_Kulyyassov,\ndo you mean to modify some tools in order to include additional options?\nRegards"}, {"role": "user", "text": "Thank you very much for your reply.\nI downloaded MaxQuant (version  1.6.6.0)  to my desktop computer and included two new variable modifications Glyceroyl-Lys (composition H(4) O C(3)) and PhosphoGlyceroyl-Lys (composition H(5) O(6) C(3) P) on the configuration tab. Do you have the same options to include new protein modifications on MaxQuant in Galaxy?\n\nimage1920\u00d71080 235 KB\n"}, {"role": "system", "text": "Yes,\nin that case, you need to open a Pull Request in order to include those modifications in this file tools-galaxyp/macros.xml at master \u00b7 galaxyproteomics/tools-galaxyp \u00b7 GitHub. Let me know if you need any additional help.\nRegards"}], [{"role": "user", "text": "Why is this error happening:\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/bin/extract_aln_ends.py\", line 102, in <module>\n    alns_bedpe = alns.bam_to_bed(bedpe=True, mate1=True, ed=True)\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/bedtool.py\", line 806, in decorated\n    result = method(self, *args, **kwargs)\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/bedtool.py\", line 337, in wrapped\n    decode_output=decode_output,\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/helpers.py\", line 356, in call_bedtools\n    raise BEDToolsError(subprocess.list2cmdline(cmds), stderr)\npybedtools.helpers.BEDToolsError: \nCommand was:\n\n\tbedtools bamtobed -mate1 -i /data/jwd/main/038/753/38753581/tmp/tmpahcdouwa/fixedmates.bam -ed -bedpe\n\nError message was:\nThe edit distance tag (NM) was not found in the BAM file.  Please disable -ed.  Exiting\n"}, {"role": "system", "text": "Hi @biostaring,\nwhich tool have you used to generate the BAM file? The problem is that it doesn\u2019t include the edit distance tag (NM).\nRegards"}, {"role": "user", "text": "Hi, the BAM file is from after UMI-tools deduplicate"}, {"role": "user", "text": "What would cause the BAM file to not have the distance tag?"}, {"role": "system", "text": "It depends on the tool used for mapping the samples; e.g. in order to include the NM tag when using RNA STAR, it should be specified in the BAM output format option.\n\nSelection_001881\u00d7432 66.5 KB\n"}], [{"role": "user", "text": "Apologies if this is too chatty, not an actual problem, just an anomaly, and from experience anomalies are often signalling a hidden problem.\nBasically, my local Galaxy installation (runs within my University firewall) installation processes far faster than when I run the equivalent on a faster stand alone machine (native applications). Worries me as to why? I doubt there will be a specific answer?\nSo running 20Gb FASTQ files through a workflow that concatenates across lanes then Hisat2>samtools sort/view>stringtie\u2026\nOn my Galaxy server (ubuntu on an iMac Intel i3 4-core 24GB RAM) takes about 12hrs.\nOn my much newer iMac (Big Sur) Intel i9 8-core (16 virtual) 48GB RAM this takes days.  As far as I can see the command line options are the same except that the sever version ADDs FASTQ groomer and FastQC.\nAny thoughts? It seems the Samsort bundled within Galaxy\u2019s Hisat2 >bam is the place where they are so different.\nKindest Regards\nRichard"}, {"role": "system", "text": "You can see the specific runtime of every tool in the workflow by clicking on tie [i] icon \u2013 that is  where I\u2019d start to see what the difference is. FASTQ groomer is also a slow tool afaik and ti is unclear why only one invocation would run it but not the other \u2192 this indicates the workflows are not the same."}, {"role": "user", "text": "Thanks Marten, yes well I have copied the commands and options from Galaxy exactly (except Groomer) to my Mac, \u2014the only thing I can\u2019t see is how many processors it uses because it is \"-@{GALAXY_SLOTS:-1} (which means I haven\u2019t yet checked). But all else the same. I am wondering if the Mac binaries for Samtools are just not as efficient as those in the Galaxy linux deployment?  That said I have used two different versions; bioconda and homebrew.  So if there are no hidden galaxy optimisation steps, and groomer does not make subsequent samsorting run faster it looks like this is an issue for me to resolve with Samsort support?\n\u2026ooh did spot another subtle difference will look at this.  Galaxy runs samtools sort with default processor and pipes to samsort view with @ pocessors.  My Mac was @ for both. Will change this and try again! Thanks"}, {"role": "system", "text": "I guess I misunderstood, I thought you are comparing workflow invocation amongst two Galaxy installations.\nBut in this case it seems we are actually debugging your iMac software configuration that is mimicking the Galaxy workflow\u2019s dependency stack. This is possibly significantly more complex. I\u2019d start with validating the results \u2013 are they the same?\nIf they\u2019re not I\u2019d probably install a Galaxy instance on your iMac and duplicate the workflow invocation properly. This will be rather easy compared to building the same dependency stack because in Galaxy each step is executed in an isolated environment \u2013 i.e. you can have different versions of the same dependency invoked during the workflow.\nControlling the number of resources with GALAXY_SLOTS has also possible significant effect on the speed of computation."}], [{"role": "user", "text": "So I\u2019m trying to wrap tools into galaxy using planemo, however I\u2019m getting variations of this error: \u201cFatal error: Exit code 1 ()\nFATAL(lofreq_call.c|main_call:1293): Cowardly refusing to overwrite file \u2018/mnt/galaxy/files/000/208/dataset_208081.dat\u2019. Exiting\u2026\u201d\nthe .dat is the output eg \u201clofreq call --ref \u201c/mnt/galaxy/files/000/208/dataset_208056.dat\u201d \u201c/mnt/galaxy/files/000/208/dataset_208052.dat\u201d -o \u201c/mnt/galaxy/files/000/208/dataset_208081.dat\u201d\u201d\nhere is the XML https://gist.github.com/Micropathology/be429cade2224f3107317c1e5b2d521a\nIs there something about handling outputs that I\u2019m missing?"}, {"role": "user", "text": "I\u2019ve found the error code in Lofreq_call\n\"    if (vcf_out && 0 != strcmp(vcf_out, \u201c-\u201d)) {\nif (file_exists(vcf_out)) {\nif (! force_overwrite) {\nLOG_FATAL(\u201cCowardly refusing to overwrite file \u2018%s\u2019. Exiting\u2026\\n\u201d, vcf_out);\nreturn 1;\n} else {\n/* filter will fail if we don\u2019t remove now */\nunlink(vcf_out);\"\nIs it normal for a tool to overwrite an empty output that galaxy has generated?"}, {"role": "system", "text": "i know it\u2019s a work-around but have you tried to give a name to the output and say to galaxy to take it from the working dir? for example:\nas command :\n\n<![CDATA[ lofreq indelqual --dindel --ref \"$input1\" \"$input2\" --out foo_bar.bam ]]>\n\n in the output block\n <data  format=\"bam\" name=\"output\" from_work_dir=\"foo_bar.bam\"/>"}], [{"role": "user", "text": "I have a local instance of Galaxy installed. I found out that the jobs_directory file is 71.6 GB. It has a file named \u201c/galaxy/database/jobs_directory/000/119/working/splitby.nameOrCount.dat.extra.temp\u201d which is 68.6 GB alone. The workflows I use in galaxy only takes about 6.7 GB. What is the function of this file? Will deleting it cause any problems?"}, {"role": "system", "text": "Is this from mothur? Are you using mothur?\nIt looks like it comes from this tool but I am not sure:\n\n  \n      github.com\n  \n  \n    galaxyproject/tools-iuc/blob/master/tools/mothur/cluster.split.xml\n<tool profile=\"16.07\" id=\"mothur_cluster_split\" name=\"Cluster.split\" version=\"@WRAPPER_VERSION@.0\">\n    <description>Assign sequences to OTUs and split large matrices</description>\n    <macros>\n        <import>macros.xml</import>\n    </macros>\n    <expand macro=\"requirements\">\n        <requirement type=\"package\" version=\"2.6.0\">vsearch</requirement>\n    </expand>\n    <expand macro=\"stdio\"/>\n    <expand macro=\"version_command\"/>\n    <command><![CDATA[\n@SHELL_OPTIONS@\n\n## create symlinks to input datasets\n#if $splitby.splitmethod == \"distance\":\n    ln -s '$splitby.matrix.dist' splitby.matrix.dist.dat &&\n    ln -s '$splitby.matrix.nameOrCount' splitby.matrix.nameOrCount.dat &&\n#elif $splitby.splitmethod == \"classify\":\n    ln -s '$splitby.dist' splitby.dist.dat &&\n    ln -s '$splitby.nameOrCount' splitby.nameOrCount.dat &&\n\n\n  This file has been truncated. show original\n\n  \n  \n    \n    \n  \n  \n\n\nI guess if you did your analyses and saved the output that you need outside galaxy it is save to remove it. So if you are in a hurry I would say remove it. And if you are not wait for a developer or some one with more expertise to give a better answer."}, {"role": "system", "text": "If your cleanup policy in the galaxy config is not \u2018always\u2019 then the jobs_directory will slowly grow.\nIf you need to have your cleanup policy set to anything else then you will need to use something like tmpwatch to clean up the folder. This can be automated with cron.\nThere is also this issue that causes the jobs_directory to grow quickly.\nIf you are running out of space while running a workflow, I recommend enabling the \u201cOutput cleanup\u201d option in tools that it makes sense for the workflow."}], [{"role": "user", "text": "Hi,\nI am at the step of using the tophat tool for aligning my rna seq reads to the genome. I have paired end reads. The tophat tool asks me for \u201cMean Inner Distance between Mate Pairs\u201d. How do I calculate this? I know my sequence length is 101bp. I do not know anything more. Do I need to ask the sequencing company for this information?"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI,\none approach to infer the sequence length can be to perform a preliminary alignment with bowtie2 by using a subsample of your dataset (you can generate it with the seqtk_sample tool). The alignment output includes the ISIZE field (ninth column), the inferred insert size. By substracting the lenght of the mate reads from the mean ISIZE value you can infer the inner distance between mate pairs. Negative values would indicate that both reads overlap (that is, inner distance between mate pairs = 0).\nRegards."}, {"role": "user", "text": "Thank you for the reply. Can I use the default value of 50 as another option?"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI, I recommend you to test different values (e.g. 100, 50 and 0), and evaluate the alignments in order to infer the optimal parameter value.\nRegards"}], [{"role": "user", "text": "In the last two weeks the tool join two datasets has been upgraded from version 2.1.2. to 2.1.3 and now I can\u2019t get the tool to run.it keeps failing with the error Traceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nIt also now says that This tool requires galaxy-util (Version 19.9.0). Click here for more information.\nCan any one please advise as to what I need to do please to get this tool to work.  Many thanks"}, {"role": "system", "text": "Hey Keeley,\ni have the same problem. The python script join.py is on the Galaxy server ??  so they should fix the bug. I would be happy if the import: from galaxy.util import stringify_dictionary_keys works soon."}, {"role": "system", "text": "Dear Galaxy Community, I would be happy if you would fix the bug."}, {"role": "system", "text": "@Starmoel007 @KeeleyB\nThank you for reporting the problem!\nThis tool is undergoing a correction as are a few other text manipulation tools. Please see this prior post linked below for details. I am going to close this topic out to keep ongoing Q&A all on the same topic.\n\n  \n    \n    \n    Specify column to group by -- Join two datasets -- Fixed \n  \n  \n    I am following along with a tutorial given on the coursera genomic data science course. It involved first joining two datasets on a column column to produce another data set. Then it involves grouping the resultant dataset on a column. In the video it shows that there is a drop down list of column names from the dataset being grouped to pick from. No such drop down appears. When I tried typing the column name in from the dataset it seems to have ignored it when executing the task with an error i\u2026\n  \n\n\nThanks!"}], [{"role": "user", "text": "Hello,\nI have several jobs waiting to run in RNA STAR for already days. Is there any trouble with this tool?\nThanks"}, {"role": "system", "text": "Hi @mjbarrero,\ncould you confirm me that you are using the usegalaxy.org server?\nRegards"}, {"role": "user", "text": "Yes, I am using usegalaxy.org"}, {"role": "system", "text": "I seem to have the same problem but with other tools. My data is stalled in Trimmomatic, BWA and Bowtie2 tools (the input data is not connected, I do mapping on other files than those subjected to teimming in Trimommatic).\nI use galaxy.org and it is an entire day already. I understand there is some server overload (?) but i just wanted to know if there is any info available, about the duration of the problems, as I need the output of this analysis ASAP."}, {"role": "system", "text": "There were some delays in handling larger jobs in the past 12 hours or so, but everything should be moving now. Sorry for the trouble - if your jobs still have not started, please post your usegalaxy.org username, or if you prefer to keep it private, email to galaxy-bugs@galaxyproject.org, and I can check the state of your jobs."}], [{"role": "user", "text": "what is the differences between this two tools, and which one i should use for pseudoalignment.\nafter alignment which tool can i use for normalization?"}, {"role": "system", "text": "Hi @azzahrae,\nyou should use Kallisto quant; Kallisto pseudo runs only the pseudoalignment step and is meant for usage in single-cell RNA-seq. If you want to perform a differential expression analysis you do not need to normalize it . You can feed DESeq2 with the output generated by Kallisto.\nRegards"}], [{"role": "user", "text": "I\u2019m trying to do alignment using trimmed, paired (by Trimmomatic) R1 and R2 fastq files. I\u2019m using appropriate reference file (.fa). While executing the HISAT2 operation I\u2019m getting the following error.\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nI don\u2019t understand what the issue is about. Please help me to solve this issue."}, {"role": "system", "text": "Hi @atsathe\nWe communicated about this via email already, but for others reading:\nThis was a transient cluster error at UseGalaxy.org that is now corrected. Any jobs that failed with this error should be rerun.\nThanks!"}], [{"role": "user", "text": "Hi,\nI tried to upload data in tabular file. It began to be load and after few seconds it stops and I have a message error \u201cInternal Server Error (500)\u201d.\nCould you please tell me how to fix it?\nThank you very much and have a good day,\nBest regards,\nKathy"}, {"role": "system", "text": "Hi! Have you soved the problem yet?"}, {"role": "system", "text": "Hello, I meet the same question with you, Do you have solve it ?"}, {"role": "system", "text": "4 posts were split to a new topic: Tabular input for Lefse at Huttenhower Galaxy"}, {"role": "system", "text": "Hi all, @Kang @wlh1 @kathy @dj1234321\nIf the failures were at UseGalaxy.org, it seems to be working now and maybe there was a transient problem that resolved itself. I just tested uploading with the different methods with a broad range of datatypes, specifically: \u201cChoose local file\u201d (file on the user\u2019s computer),  \u201cChoose remote files\u201d (file from a public URL), and \u201cPaste/Fetch data\u201d (file uploaded via FTP first , and, from a hosted directory). All were successful.\nPlease try again now if you haven\u2019t already. If problems persists, please explain with more details about:\n\nWhat exactly is going wrong, including which upload method was used\nFile datatype if different from the original question\u2019s tabular file\nWhere this is happening (server URL)\n\nThanks for reporting the problem and hopefully it is resolved for everyone now "}], [{"role": "user", "text": "I can\u2019t get featureCounts to complete on my GVL/Galaxy instance running on AWS. If I download the same HISAT2 output and use the same built-in reference (hg19) on the public server, the job completes just fine. Any thoughts?"}, {"role": "system", "text": "Hi @mitchellgc, Can you provide some more information on what happens. What specific error message (if any) do you get. Can you also provide the Galaxy server logs? Also, the size of the instance that you launched, and any other pertinent information would be useful."}, {"role": "user", "text": "If I select View Error Logs in the admin console on Galaxy, it says \u201cNo Errors Available.\u201d Unfortunately, the console won\u2019t launch from GVL (503 Service Temporarily Unavailable), so I can\u2019t get any other logs. The server is running on Amazon elastic cloud.\nThe featureCounts job ran until it finally timed out with the error that it had taken linger than the specified time."}, {"role": "system", "text": "So just to verify, the upload issue has been resolved? Was that related to the size of the node?\nRegarding the console not launching, are you able to access the cloudman console?\n\nimage1920\u00d71063 220 KB\n\nIf so, you can edit the Galaxy app settings (3 dot icon on the Galaxy app panel), and specify a different timeout for the k8s_walltime_limit in job_conf.yml.\n\nimage1019\u00d7666 73.9 KB\n\nYou can try bumping that up to a larger number."}], [{"role": "user", "text": "Hello, I tried to process my data set with the help of the information and tutorials in Clustering 3K PBMCs with Scanpy, but every time I got an error when using the Import Anndata and loom tool.\nI fixed the defects that I found myself and edited my gene files\nAnd now I have uploaded the data in the Import Anndata and loom tool.\nIt is possible for you to take a look at my history and tell me if there is a problem with the files or options selected in the Import Annadata and loom tool.\nI must say that the processing of this data is very important and essential for me and I will be very happy if you help me.\nHistory link:\nhttps://usegalaxy.eu/u/gh.maaryaam1996/h/unnamed-history-1"}, {"role": "system", "text": "Hi @maryam-gh99\nEverything looks Ok from a quick review.\nIf the current job fails, try removing the header line from the tabular dataset \u201cnew 2.txt\u201d, then a rerun.\nPs: When you delete datasets, others cannot see them in the shared history view. So, if you want the best feedback, please leave all inputs and outputs undeleted."}], [{"role": "user", "text": "I have some very basic questions regarding usegalaxy.eu\n\nCan I somehow set up new tools on my own on usegalaxy.eu? For example MetaMaps?\nCan I upload large databases? (Up to more than several hundred GB)\nIf yes, what is the entry point to set up a new tool?\nIf no, I there any other service providing this options or do I have to set up my own machine to do this?\n\nThanks you!"}, {"role": "system", "text": "1.+ 3. No, but you can ask the admin, for a trusted and popular tool I think you have a good chance. Admin of every instance has special interface and API to install tools.\n2. Most public instances have storage quota to ensure fair use. Hundreds of GB won\u2019t fit in most public Galaxy instances I know.\n4. Setting up Galaxy is convenient (http://getgalaxy.org/), but given the size of data you want to process and possible analyses you can run keep in mind you will need the corresponding hardware."}], [{"role": "user", "text": "Would it be possible for galaxy to include  Camelus dromedarius reference genome in built in index ?"}, {"role": "system", "text": "Hi @sagar_khulape\nFor now:\nUse the custom genome method. See Custom genome + custom build: How to use a genome that is not natively indexed at the server you are working at\nFor later (later this year, or early next)\nWe can likely index the genome at the public servers. We will need to know the exact assembly, and it should be from a stable public source with a version. A link is usually the best way to specify that.\nHope that helps!"}, {"role": "user", "text": "Thank you for the response\nThe Camelus dromedarius genome can be found at Camelus dromedarius genome assembly CamDro3 - NCBI - NLM\nThe list for genome sequence for assembly as -\nNC_044511.1, NC_044512.1, NC_044513.1, NC_044514.1, NC_044515.1, NC_044516.1, NC_044517.1, NC_044518.1, NC_044519.1, NC_044520.1, NC_044521.1, NC_044522.1, NC_044523.1, NC_044524.1, NC_044525.1, NC_044526.1, NC_044527.1, NC_044528.1, NC_044529.1, NC_044530.1, NC_044531.1, NC_044532.1, NC_044533.1, NC_044534.1, NC_044535.1, NC_044536.1, NC_044537.1, NC_044538.1, NC_044539.1, NC_044540.1, NC_044541.1, NC_044542.1, NC_044543.1, NC_044544.1, NC_044545.1, NC_044546.1, NC_044547.1, NC_009849.1\nCould you help out to  include said genome as index for mapping in galaxy"}, {"role": "system", "text": "Ok, great, thanks for posting back.\nUCSC hosts many of the genomes as well, so that is probably where we will get it. That is where you could get it as well to avoid having to merge chromosome file together. https://hgdownload.soe.ucsc.edu/hubs/UCSC_GI.assemblyHubList.txt\n\nGCF_000803125.2\tCamDro3\tCamelus dromedarius\tArabian camel (Drom800 2019 refseq)\t9838\tmammals\n\nI\u2019ll mark this for updates later. Thanks!"}], [{"role": "user", "text": "This problem applies to both web-based galaxy and 19.05 local.\nWhat\u2019s the current way to rename output based on input name?\ncurrent workflow: input database -> Trim Galore! (paired-end)\nI tried to rename output as #{input1}_trimmed, #{input_name}_trimmed, #{input1}_trimmed, and for all of them I got _trimmed as new file name. What should I put inside { }?\nhttps://galaxyproject.org/learn/advanced-workflow/variables/\nSince I am trying to assemble about 100 sequences, default naming does not allow me to keep track of them."}, {"role": "system", "text": "Hi @ZhuYi_Wang\nYou may have figured this out already \u2013 different tools name the \u201cvalid input variables\u201d in different ways. The workflow editor prompts with the proper variable(s), per tool.\nTwo screenshots as examples. The first is the tool you are using:\n\nworkflow-rename-example1950\u00d7608 68.2 KB\n\n\nworkflow-rename-example2822\u00d7568 56.6 KB\n\nHope that helps!"}], [{"role": "user", "text": "Hi,\nI deleted some histories permanently, but didn\u2019t give back the storage space. What can I do? Thanks!"}, {"role": "system", "text": "Welcome, @Declan0305!\nPlease try logging out of Galaxy then log back in. This will reset the quota usage quicker.\nEventually, the server will update if you stay logged in, but that is a scheduled task that can sometimes take longer when the server is busy.\nFAQ: * The account usage quota seems incorrect\nThanks!"}], [{"role": "user", "text": "Hi\nI have been attempting to identify polymorphic DNA sites according to this workflow: https://usegalaxy.org/u/coursera/p/genomic-data-science-with-galaxyidentify-polymorphic-sites.\nI am trying to use Freebayes to identify polymorphic sites from the SAM file that I cleaned up. However, I keep getting an \u201cunable to find FASTA index\u201d error irrespective of the version I use. Any feedback would be helpful. Thanks!\n\nScreen Shot 2020-03-29 at 12.07.16 PM2560\u00d71600 554 KB\n \nScreen Shot 2020-03-29 at 12.05.51 PM2560\u00d71600 565 KB\n \nScreen Shot 2020-03-29 at 12.15.11 PM2560\u00d71600 697 KB\n"}, {"role": "system", "text": "Hi and welcome,\none suggestion first: it is way more helpful if, for reporting errors, you click on that little bug icon on the failed datset and copy/paste the complete error message you find there, instead of posting screenshots.\nNow for your actual issue: the error is not about a missing FASTA index, but about specific entries in there. In other words, one or more of the chromosome names listed in your SAM/BAM inputs could not be found in the reference genome you\u2019ve been trying to call variants against.\nFor your dataset 31 that makes lots of sense since Group1 is most definitely not a chromosome identifier anywhere. For dataset 32, the name is cut off in the screenshot, but it is up to you anyway to check whether this is a valid chromosome name in your analysis.\nBest wishes,\nWolfgang"}], [{"role": "user", "text": "Hello\nI used Import Annadata and loom to process my single cell data\nMy genes file included 28 thousand gene symbols and ensemble IDs\nNow, although the output of this tool is green, the number of genes is considered zero\nAnd in the matrix he created, my genes are not there\nwhat is the reason?"}, {"role": "system", "text": "Hi @maryam-gh99\n\n\n\n maryam-gh99:\n\nMy genes file included 28 thousand gene symbols and ensemble IDs\nNow, although the output of this tool is green, the number of genes is considered zero\nAnd in the matrix he created, my genes are not there\n\n\nSome tools do just output nothing if there are technical \u201cmismatches\u201d.\nItems to check for:\n\nUnexpected content/format in your input datasets versus the tool form parameters (usually involves datatype issues).\nDifferences in ID formats contained in different input datasets and/or server indexed reference data (important \u201ckeys\u201d like gene names).\nThe tool form may be set up to look for specific attributes that are missing from the inputs (example: loom inputs have several default keys set for data parsing-- does your data contain those same keys?).\n\nAnd, there is a version of the Import Anndata and loom tool at that should be avoided. AnnData Import/ AnnData Manipulate not working?.\nSingle cell tutorials are here:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you need more help, please share more details. Considering sharing your history \u2013 as this tool processes several different input/output types and those details matter. Leave any inputs and outputs undeleted, and note the output dataset number that is unexpectedly empty.\n\n\ntroubleshooting-errors \u2013 covers how to review logs for any job, not only red/failed\n\nunderstanding-input-error-messages \u2013 covers the same top-level technical data problems we would check for at this forum\n\nsharing-your-history \u2013 public is best (more people can help) but if you would rather share this privately, state so and we can set up a direct message thread to use\n"}], [{"role": "user", "text": "Hi,\nI am going through the tutorials of DNA methylation analysis and once I have selected the bwa-meth for alignment. There is no option available to select the refence genome. I am also attaching a screeshot.\n\ngalaxy_problem1336\u00d7522 47.4 KB\n"}, {"role": "system", "text": "Hi @ssusmani,\ncould you try to run the tutorial in usegalaxy.eu? Alternatively, you can download the hg38 genome from the NCBI server and upload it to Galaxy; it that way you would be able to use it  from the history.\nRegards"}, {"role": "user", "text": "Thank you. It has been resolved. I was using wrong server usegalaxy.org"}], [{"role": "user", "text": "I am following galaxy 101 tutorial.\nWhich exon had the highest number of repeats? How many repeats were there?\n\n\n\n\nchr22\n19150024\n19150283\nENST00000086933.2_cds_2_0_chr22_19150025_r\n0\n-\n\n\n\n\nchr22\n27796762\n27800543\nENST00000302326.5_cds_1_0_chr22_27796763_r\n0\n-\n\n\nchr22\n50603840\n50605064\nENST00000329492.6_cds_4_0_chr22_50603841_f\n0\n+\n\n\nchr22\n17119390\n17121127\nENST00000331437.4_cds_0_0_chr22_17119391_r\n0\n-\n\n\nchr22\n19766388\n19766867\nENST00000332710.8_cds_8_0_chr22_19766389_f\n0\n+\n\n\n\nHow many repeats were there?\n3\nIs the answer above is correct? I have shared my history.\nhttps://usegalaxy.org/u/ashgourd/h/galaxy101-continued-25mar22\nAppreciate your help. Thanks."}, {"role": "system", "text": "Hi @Jayanthi_Ramprakash\nThe dataset posted represent just the \u201cexons\u201d.\nWhat tutorial are you following? The answer is probably in dataset 20, not dataset 21, and the tutorial should include the answer. But that\u2019s just a guess \u2013 it looks like you may have missed a step, creating empty datasets in the middle of the history.\nThis is a different version (from GTN) for comparison: Galaxy 101"}], [{"role": "user", "text": "Hello!\nThis is a follow-up question to the topic I had asked last week here:\nAs per the guidelines by @jennaj in that topic, I started with a fresh install of Galaxy and enabled access to reference genomes and indexes via CVMFS. I can now access reference genomes and indexes for almost all tools except that twobit does not show the hg38 genome (for the tool \u201cTranslate BED transcripts\u201d) in the dropdown options.\nI looked at data tables and looked in to the twobit folder and I see hg19 only for human plus the number of options are also quite less (~47 entries). Is this related to the CVMFS cache size (set at 500)? Should I be seeing a lot more options in the twobit folder?\nThank you"}, {"role": "system", "text": "\n\n\n prao123:\n\nTranslate\n\n\nHi @prao123\nThe human hg38 is missing, along with several others. The hg19 duplicates were also cleared up in the hosted version. Legacy ticket. We\u2019ll be addressing missing indexes over the next year.\nChoices:\n\nuse a translation tool not dependent on this index\nuse a Data Manager to add it directly for your server locally. Make sure to choose the hg38 dbkey when doing that.\n"}, {"role": "user", "text": "Hello @jennaj\nFor pursuing the second option where I use the data manager to add the hg38 index, should I follow the order?\n\nSam fasta index builder\nPicard fasta index builder\nthen\n2bit index builder\n\nShould I source the genome from UCSC?\nI just want to confirm that it will not interfere with any of the indexes sourced from CVMFS? Are there any additional things I should keep in mind before I start the indexing?\nThanks!"}, {"role": "system", "text": "Just run the extra 2bit indexer. Choose the existing hg38 database as the source."}], [{"role": "user", "text": "Help! I\u2019m running a local dev instance of Galaxy purely to write a wrapper for an in-house R script and I have a failed job preventing Galaxy from loading at all, at least I think that\u2019s the issue:\n\ngalaxy.jobs.handler ERROR 2021-01-14 13:54:24,285 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] failure running job 621\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 408, in __handle_waiting_jobs\nself.dispatcher.put(self.job_wrappers.pop(job.id))\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 1007, in put\nself.job_runners[runner_name].put(job_wrapper)\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 152, in put\njob_wrapper.enqueue()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1456, in enqueue\nself._set_object_store_ids(job)\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1478, in _set_object_store_ids\nobject_store_populator.set_object_store_id(dataset)\nFile \u201clib/galaxy/objectstore/init.py\u201d, line 1082, in set_object_store_id\ndata.dataset.object_store_id = self.object_store_id\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018dataset\u2019\ngalaxy.jobs.mapper DEBUG 2021-01-14 13:54:24,305 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2021-01-14 13:54:24,318 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Dispatching to local runner\ngalaxy.jobs DEBUG 2021-01-14 13:54:24,464 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Persisting job destination (destination id: local)\ngalaxy.jobs.handler ERROR 2021-01-14 13:54:24,473 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] failure running job 622\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 408, in __handle_waiting_jobs\nself.dispatcher.put(self.job_wrappers.pop(job.id))\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 1007, in put\nself.job_runners[runner_name].put(job_wrapper)\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 152, in put\njob_wrapper.enqueue()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1456, in enqueue\nself._set_object_store_ids(job)\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1478, in _set_object_store_ids\nobject_store_populator.set_object_store_id(dataset)\nFile \u201clib/galaxy/objectstore/init.py\u201d, line 1082, in set_object_store_id\ndata.dataset.object_store_id = self.object_store_id\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018dataset\u2019\n\nAs recommended here: Killing jobs or dead processes after a database crash on galaxy - #3 by Slugger70, I tried installing and running gxadmin but of course I haven\u2019t set up a postgresql db for my dev instance, so I get this failure:\n\ngxadmin mutate fail-job 621\n/usr/bin/gxadmin: line 258: psql: command not found\n\nI am completely stuck. Do I just need to install psql even though Galaxy is using SQLite?  We\u2019re going to have actual professionals help with Galaxy admin in about a month or two but I\u2019d really like to use my local instance before then.  Any recommendations would be great.  I\u2019m this close to gnawing off my own leg to get out of the Galaxy trap (i.e. clean reinstall)."}, {"role": "system", "text": "Have you tried using the Jobs tab under the Admin panel?"}, {"role": "user", "text": "Alas, the job does not show up. When I load the Galaxy website I get this error: \u201cAn error occurred while updating information with the server. Please contact a Galaxy administrator if the problem persists.\u201d  In general, no information at all shows up, even though I am logged in as admin (history is missing too.)"}, {"role": "system", "text": "Hey @KeelyDulmage I think when we suggested gxadmin to you, we didn\u2019t know you were running with sqlite!  This makes a lot of sense.\nI think for that specific error you\u2019ll need to run the sql in your sqlite3 database potentially.\nSomething like\nsqlite3 universe.sqlite # Or wherever your DB is\nupdate job set state='failed' where id = 621;\n"}], [{"role": "user", "text": "Hi, I\u2019m new on processing raw data from RNA-seq experiments but my recent project requieres me to get it on.\nSo, I\u2019m using the 12 runs from the ENSEMBL accession number: E-MTAB-4929;\nI made the trimming using Cutadapt, also I had to figure out if the reads were stranded or not, so I used Infer Experiment for that and they are paired-end unstranded.\nFor the alignment I used RNA-STAR, I downloaded the reference genome from Genome NCBI (Glycine max (soybean)) as well as the annotation file in gff version 3 format.\nMy QC, made with MultiQC, from the alignment seem to me OK, here is a ss:\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.47.011088\u00d71052 99.2 KB\n\nThe problem is when I run the featureCounts;\nmy input files are the BAM files from the alignment and the anotation file gff version 3 of the Glycine max genome.\nFor the extra info., I followed the tutorial: Reference-based RNA-Seq data analysis\nI made the QC on the summary file [one output file of featureCounts] and I obtained this:\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.47.441248\u00d7972 79.8 KB\n\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.48.111248\u00d7972 57.3 KB\n\nI\u2019m getting less than 50% of the total fragments mapped and I don\u2019t know why :S\n\u00bfCan someone please help me?"}, {"role": "system", "text": "Dear Nati2208,\nThe results of featurecounts does not show you the mapping statistics it shows you the counting done by feature count. That is to say, You have 50% of the reads that assign to at least two or more features in your annotation file, e.g., two or more transcripts of your gene, or two or more exons because they map to exon-intron boundaries.\nI hope I could answer your questions.\nBest wishes,\nFlorian"}, {"role": "user", "text": "Thank you Flow,\nI didn\u2019t know, thank you for correcting me.\nIs something I can do to have more assigned counts?"}, {"role": "system", "text": "Dear Nati2208,\nYou can select in featureCount under advanced options different modes for ambiguitiy Allow reads to map to multiple features (.e.g, -O) together with the option Largest overlap to Yes. Test it and see what will happen.\nBest wishes,\nFlorian"}], [{"role": "user", "text": "Hi team,\nI\u2019m trying to download my Galaxy histories to free up room for more analysis. However when I select \u2018Export History to File\u2019 and get a link to use with wget or curl (On MacOS and I\u2019ve attempted both) it only downloads an HTML file instead of a tar.gz file of the datasets in the history.\nThe file looks like this:\n ==> export_archive?id=XXXXXXXXXX <==\n<!DOCTYPE HTML>\n<html>\n<!\u2013base.mako\u2013>\nHope that this makes sense,\nBest,\nTim.\nadmin edit: remove archive ID"}, {"role": "system", "text": "Hello,\nTry downloading the archive from the web page directly. Curl/wget will only work for datasets, with collection datasets requiring that your Galaxy API key is included.\nReference FAQ for anyone else reading: * Downloading Data\nThanks!"}], [{"role": "user", "text": "I\u2019m not getting the stats when running my Hisat2 alignment, I\u2019ve already followed the suggestion of checking the stdout and stderr sections but there is nothing there either. The statistics of bam.iobio tell me I have a good quality alignment, though. Can I trust those results or what should I do?"}, {"role": "system", "text": "Hi @MarioG\nhttps://bam.iobio.io is a nice graphical summary tool for BAM/SAM alignment stats.\nTo output statistics directly from HISAT2, that output option needs to be set to \u201cYes\u201d on the tool form. Find it nested under the section \u201cSummary Options\u201d. This would allow you to use MultiQC to compare multiple BAM results together in a single report.\nhisat2-output-alignment-stats1466\u00d7370 30.4 KB\nThere are also other ways to generate alignment statistics \u2013 review tools under the group SAM/BAM for the choices available at usegalaxy.eu.\nIf you plan to go forward with differential expression analysis, after generating counts (example: Featurecounts or HT-seq count), those results can be examined with scientific content metrics with the tool QualiMap Counts QC.\nThanks!"}, {"role": "user", "text": "Those options are actually set to \u201cYes\u201d already, but the summary seems to be empty (0 bytes) and although the alignment is completed, it keeps sending the following error messages: Error, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of \u2018int\u2019\n(ERR): hisat2-align died with signal 6 (ABRT)\n[bam_sort_core] merging from 6 files and 1 in-memory blocks\u2026\nHere a picture: \nI\u2019m concerned this might have to do with the fact that I\u2019m using files that have been previously trimmed. I used Headcrop on Trimmomatic to trimm off the first 10 beses as they didn\u2019t look good on the FastQC report (per base sequence content). On the other hand, when I use files that haven\u2019t been trimmed off, HISAT2 doesn\u2019t send any warnings and it actually shows the statistics. What do you think? I don\u2019t feel confident about which files I should use to do the alignment.\nThank you so much for the recommended tools, Jennifer. We\u2019ll definitely try them as soon as we move forward on the analysis."}, {"role": "system", "text": "Hi @MarioG\nApologies for the delay.\nThe message about reads not being paired is just a warning. It will not cause a mapping job using HISAT2 to fail.\nIf you haven\u2019t tried a rerun yet, do that now, this may have been a cluster or server issue now resolved.\nIf the jobs failed again, then you are actually not getting any hits for some reason. Maybe the reads are too short \u2013 the parameters are too strict \u2013 or the wrong database was mapped against? This tool is designed to find \u201cexact matches\u201d when all default settings are used but the details can be adjusted under \u201cAdvanced settings\u201d.\nHope you worked this out!"}], [{"role": "user", "text": "Hello,\nI had a question about the default settings for HISAT2 on Galaxy, and whether soft-clipping is applied? Normally HISAT2 applies soft-clipping but I\u2019m unsure of whether this is the case on Galaxy. As I haven\u2019t trimmed for adaptor sequences, I am wanting to ensure that soft-clipping is applied. I can see for HISAT2, if you select \u2018Advanced Settings\u2019 you can choose whether soft-clipping is applied under Scoring Options, does this mean by default (e.g. not using any advanced options) it is not applied?\nThanks in advance."}, {"role": "system", "text": "Hi @clarissa\nI believe HiSAT2 uses soft clipping with the default settings, but please check any BAM file. Soft clipping is recorded in CIGAR (column 6) with S character. It is very common, especially if you not trimmed for adapter.\nHope that helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Thanks for your reply @igor. I have checked the CIGAR column and most of them end in the character \u2018M\u2019, however some of them also end in \u2018S\u2019. Does that mean I can assume soft clipping was used?"}, {"role": "system", "text": "Hi @clarissa\nyes, presence of S means soft clipping was used. It is used only for reads with soft clipped nucleotides. M means match, well, match or mis-match, to be precise. You can check description of CIGAR string in SAM/BAM documentation.\nKind regards,\nIgor"}], [{"role": "user", "text": "Starting last night, we had a series of failures of blastx.  These included with two new subject dbs, and with one we had used successfully previously.  When we try to run blastx, the message is \" Job submission failed\"  and also \u201cThe server could not complete the request. Please contact the Galaxy Team if this error persists. Action requires account activation.\u201d\nIs the blast server functioning?"}, {"role": "system", "text": "Dear tom_abrams_lab,\nThe tool blastx should run on usegalaxy.eu and usegalaxy.org. Your question misses a bit of context and could involve other problems. First try to re-run your job. If the problems still persist, then please make an error report (click on the bug icon), if you are working in an official Galaxy instance.\nBest wishes,\nFlorian"}, {"role": "user", "text": "Hi Florian,\nI will try to provide more information.  This problem appeared on Saturday with a student working with me from another location - she\u2019s at Penn State.  We have been using Galaxy since the beginning of April, and as far as I\u2019m aware, neither of us had set up accounts.  Uploaded files and histories remain accessible in the browser.  When I ran the blastx that she had tried, I got the same error as she did.  When I tried to view the error message, it seemed that I needed to be logged in under an account, so I set up an account.  I got the error message even for a blastx that we had run before.  When I clicked to get more info, I got the full set of blastx parameters, which I would be happy to forward - if I could figure out attachments.\nBecause the fasta files had been uploaded before the account was set up, and because it seemed that perhaps a problem had developed with these assembly files, I deleted all the files in the history, and then began to upload them.  I was unable to complete the uploads and received an error that the account hadn\u2019t been activated.  I went to the activation email, and now got an error message that the link was not valid, and that perhaps the account was already active.\nI have started the process again by setting up another account on my iPad.  I know we are not supposed to use multiple accounts, but this one, which I would prefer to use, is not functional.\nI appreciate any guidance you can provide.  Thanks much."}, {"role": "system", "text": "Hey tom_abrams_lab,\nAn error with a tool should not be linked to a problem with your account. If you an active account try to resubmit your job again for blastx. If another error pops up then please bake a bug report. For that, click on the failed job, and then click on the bug-icon. It will then automatically generate an error report for you that can be send to the Galaxy community.\nCheers,\nFlorian"}, {"role": "system", "text": "Hi @tom_abrams_lab\nHi Tom, we talked about this over email but I am posted back the highlights for others that may run into similar problems.\n\n\n\n tom_abrams_lab:\n\nAction requires account activation.\n\n\nCreate an account and log into it to run compute-intensive tools (User > Register). FAQ Registering Accounts\n\n\n\n tom_abrams_lab:\n\nI was unable to complete the uploads and received an error that the account hadn\u2019t been activated. I went to the activation email, and now got an error message that the link was not valid, and that perhaps the account was already active.\n\n\nAccounts have an activation step to confirm the registered email address is valid and belongs to you. Activation links are sent via email and are active for about six hours. Resend if needed using the password reset function (User > Login).\nAs you noticed, activation emails can be sorted into Spam or Trash folders depending on email filters, so if others run into the same problem check in those locations or search your email account with the keyword \u201cgalaxy\u201d if it seems to be missing.\nIf you never get an activation email (can take a few hours to arrive, depending on how busy the server is), your email service may be rejecting the email. We\u2019ve seen more of this occurring with institutional email services over the last year as updated security was implemented. Contact your institution\u2019s administrators to request that emails from the @lists.galaxyproject.org and @galaxyproject.org domains are accepted. Or, use a public email service instead. Please delete any accounts that will no longer be used (User > Preferences) to avoid multiple account issues. Account quotas at most public Galaxy servers are one account per person per public Galaxy server. Meaning, it is fine to have a single account at each but you shouldn\u2019t have more than one at any particular server. FAQ Registering Accounts\n\nMore help about using custom fasta datasets indexed with makeblastdb\nFormatting of the fasta dataset matters. The usual custom genome format rules apply Datatypes, plus BLAST has more options for parsing line/identifier content than other mapping tools\n\n\nProblems with format usually do not show up until the database index created from the original fasta is mapped against. Meaning, the job output from makeblastdb may appear to be successful (green), but the index is corrupted, and running blastn, blastx, etc will fail when mapping against that index. The stderr will report that the index cannot be found. The error will start with text like this and means that you need to correct the fasta format and rebuild the index.\n\nstderr\nBLAST Database error: No alias or index file found for \u2026 (more details specific to your data)\n\n\n\nRemove description line content. Tool: NormalizeFasta using the option to \u201cremove content after the first whitespace\u201d. Wrap the sequence length to a value between 40-80 bases for the best results with most tools, including BLAST.\n\nnormalize_fasta_custom_genome1180\u00d7558 44.7 KB\n\n\n\nThe fasta identifiers can include alphanumeric characters (a-z, A-Z, 0-9), underscores (_), dots (.), and dashes (-). If you want to keep the description content, replace other characters with one of those. The goal is to produce sequence identifiers that are all one \u201cword\u201d with no whitespace (tabs, spaces) or other characters that BLAST cannot parse. Avoid very long identifiers (over ~30 characters), or expect problems.\nSearch the tool panel with the \u201creplace\u201d keyword to find all tool options. Replace Text in entire line should work with most data.\nAlternatively, a tool like Text transformation with sed can also be used within Galaxy. For example, a command like this will replace all commas with a dash in the entire file.\ns/,/-/g\n\n\nPipes (|) can be Ok, but the fasta data containing these characters are best sourced from NCBI and the option Parse the sequence identifiers should be used with the makeblastdb tool when creating the database index to map against.\n\nmakeblastdb-header936\u00d7108 9.48 KB\n\n\u2026 other options, then find this option and set it to \"Yes:\n\nmakeblastdb_NCBI_parse_identifiers1204\u00d7148 14.9 KB\n\n\n\n\nThanks for reporting the error, glad this is working for you now, and hope these tips help others "}], [{"role": "user", "text": "Dear All,\nI am new to Galaxy. I want to use Galaxy to teach students in 90 mins how to analyze cancer NGS data in an easy way without the need to install command line tools or use R (which I am familiar with).\nIs there any way to use Galaxy for that purpose? Any recommended tutorials or workflow that can help?\nI appreciate your response.\nBest Wishes,\nMohamed"}, {"role": "system", "text": "Have a look at https://training.galaxyproject.org/"}, {"role": "system", "text": "As @marten mentioned already, https://training.galaxyproject.org is the way to go and if you need training resources for an entire classroom you could apply for https://galaxyproject.eu/tiaas."}, {"role": "system", "text": "There is, currently, no perfect training material tailor-made for cancer data analysis. If what you have in mind, when you say cancer NGS data, is exome sequencing data to call somatic variants from, then I\u2019m currently working on such a tutorial, and it should be finished and get included at https://training.galaxyproject.org/ within the next 2 weeks or so. Until then, https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/exome-seq/tutorial.html may be the closest to what you\u2019re actually looking for."}, {"role": "system", "text": "Update: The somatic variants tutorial I mentioned before is available now at https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/somatic-variants/tutorial.html.\nWorking through it will take substantially longer than the 90 mins you gave as your requirement, but if you run all the steps before the actual variant calling prior to your exercises with your students, and have them start from BAM datasets of mapped and postprocessed sequencing reads, you may be able to just squeeze it into your time frame."}], [{"role": "user", "text": "ive look for Compute tool in toolshed but i cant find it.\nis it deleted in new version of galaxy or just the name,  changed ?"}, {"role": "system", "text": "Could you please be specific in what tool you are looking for?"}, {"role": "system", "text": "Hi Amir,\nThe compute tool can be found under column_maker in the Toolshed."}], [{"role": "user", "text": "Hi all,\nI wonder if there is any option to use just one file from composite dataset (e.g. pbed) without downloading it if I want to use this file in automatic workflow."}, {"role": "system", "text": "Hi @Alex_Shap\nI don\u2019t think this is what you are asking about, but it is what we can help with just in case.\n\nGalaxy has a couple of file format conversion tools that accept pbed as an input. Searching the tool panel with that datatype can find those.\nOptions to convert any datatype will also be available under the  \u2192 Edit Attributes \u2192 Datatype tab if the format conversion is a direct action eg doesn\u2019t involve parameter choices or other datasets.\nIf you are not sure what datatype(s) a tool accepts: start up an empty history then load up the tool form. The expected datatypes will be listed out in each input section of the tool form.\n\nI\u2019m not sure what this part means. Are you trying to input the data to a tool in a Galaxy workflow? Or, trying to make the data available by URL to input/visualize at some remote website? Or, want to manipulate the data directly with other tools (text manipulations, and similar).\n\n\n\n Alex_Shap:\n\nif I want to use this file in automatic workflow\n\n\nIf the above does not help, would you please explain a bit more about what you are trying to do? I\u2019m guessing that this is followup for the prior question Galaxy Plink doesn't accept pbed composite dataset when uploaded from local disk. I asked for more details but those were not sent in, and greatly helps. Galaxy hosts 1000s of tools and 100s of datatypes, so need to narrow down the use case a bit please "}, {"role": "user", "text": "Thank you very much for your answers,\n\nAs far as my previous topic has been closed (Galaxy Plink doesn\u2019t accept pbed composite dataset when uploaded from local disk), I will reply here. I addressed that issue to the developer of the galaxy plink tool on GitHub. He advised to look at files in working directory. So we found the reason of the problem:\nWe found that the error occures because Galaxy assigns files\u2019 names the in composite dataset not the way Plink tool expects. When upload composite pbed dataset to Galaxy we get the following files: \u201cComposite Dataset.bim\u201d, \u201cComposite Dataset.bed\u201d, \u201cComposite Dataset.fam\u201d. And the Plink tool expects theese names: \u201cRgeneticsData.bim\u201d, \u201cRgeneticsData.bed\u201d, \u201cRgeneticsData.fam\u201d.\nWhen we manually changed files\u2019 names to the expected ones on our server, Plink accepted the dataset and worked correctly. But there is no option to change files\u2019 names in the composite dataset using Galaxy\u2019s GUI, so ordinary users with no admin rights are not able work with pbed composite dataset when it is uploaded from local disc.\nThis is my reply about this topic. My aim is to make a workflow using plink. However I have a couple of steps in my workflow where I must process .bim files directly as text files (.bim is a part of the pbed dataset). The Galaxy version of plink is somewhat restricted in functionality so it cannot produce just .bim file - only the whole dataset (no analogue of the command-line-plink option --make-just-bim). So I wonder if there are any options to break pbed dataset into separate files. It seems to me that I will have to make a custom tool to extract .bim file from pbed.\n"}, {"role": "system", "text": "Ok, thanks for explaining. All is very helpful.\nI also found issue tickets about this, and one is yours? That is the best way to work with the developers. This forum is mostly about helping to solve tool usage issues for end users  Hopefully the details you have explained can be modeled differently in later updates.\n\nUsing one of pbed (plink) files without downloading it \u00b7 Issue #5289 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\n__DATA_FETCH__ doesn't create metadata at upload for composite datatype pbed \u00b7 Issue #14050 \u00b7 galaxyproject/galaxy \u00b7 GitHub\n\ncc @bernt-matthias"}, {"role": "user", "text": "Thank you for the prompt replies.\nThe fastest way to solve my problems was to create custom galaxy tools - 1) for renaming files in composite dataset and 2) for extraction of files from composite dataset. It seemed to be not so easy at first but I succeeded in the end."}], [{"role": "user", "text": "Hello, I am completely new to bioinformatics but was hoping to use Salmon in Galaxy to analyze gene-level expression in a time-series RNA-seq dataset. Based on the parameters I set and files I have included (transcriptome FASTA and annotation GTF) I am able to get quantification of both transcripts and genes. I was wondering if there is a way to obtain information on the mapping rate? I know that alignment methods such as HISAT2 or STAR produce alignment rates to assess how well the read mapped to the genome and subsequent assignment rates when counting with a method such as featureCounts. Is there a similar output for mapping rate to assess how well my pair-end reads were assigned to a transcript/gene? Thank you in advance!"}, {"role": "system", "text": "Hi @PaulaB\nSetting this option on the tool form to yes will output the mapping results: Write Mappings to Bam File\nThen summarize the mappings with a tool in the SAM/BAM tool group like: Samtools stats generate statistics for BAM dataset"}, {"role": "user", "text": "Hi @jennaj, thank you so much for the quick reply, this is very helpful! Looking at the samtools output I see that reads mapped: 14647458 and reads unmapped: 14635722. Does this suggest that my mapping rate is ~50%? Additionally the value for reads mapped and paired is 11736. Is it correct that this number refers to the paired-end reads that mapped and this value is included in the total reads mapped? Given these results is it safe to assume that the counts Salmon is generating (quantification and gene quantification results) are based on everything that mapped (and includes reads that mapped either on their own or as a part of a paired-end fragment)? Apologies for if these are very basic questions. I have included a part of the samtools output below for reference. Thank you again for the very helpful input!\nSN\traw total sequences:\t29283180\nSN\tfiltered sequences:\t0\nSN\tsequences:\t29283180\nSN\tis sorted:\t1\nSN\t1st fragments:\t14641590\nSN\tlast fragments:\t14641590\nSN\treads mapped:\t14647458\nSN\treads mapped and paired:\t11736\t# paired-end technology bit set + both mates mapped\nSN\treads unmapped:\t14635722\nSN\treads properly paired:\t11736\t# proper-pair bit set\nSN\treads paired:\t29283180\t# paired-end technology bit set\nSN\treads duplicated:\t0\t# PCR or optical duplicate bit set\nSN\treads MQ0:\t0\t# mapped and MQ=0\nSN\treads QC failed:\t0\nSN\tnon-primary alignments:\t196568514\nSN\ttotal length:\t1479526038\t# ignores clipping\nSN\ttotal first fragment length:\t739701308\t# ignores clipping\nSN\ttotal last fragment length:\t739824730\t# ignores clipping\nSN\tbases mapped:\t740503012\t# ignores clipping\nSN\tbases mapped (cigar):\t740502931\t# more accurate\nSN\tbases trimmed:\t0\nSN\tbases duplicated:\t0\nSN\tmismatches:\t0\t# from NM fields\nSN\terror rate:\t0.000000e+00\t# mismatches / bases mapped (cigar)\nSN\taverage length:\t50\nSN\taverage first fragment length:\t51\nSN\taverage last fragment length:\t51\nSN\tmaximum length:\t51\nSN\tmaximum first fragment length:\t51\nSN\tmaximum last fragment length:\t51\nSN\taverage quality:\t255.0\nSN\tinsert size average:\t411.8\nSN\tinsert size standard deviation:\t249.1\nSN\tinward oriented pairs:\t5859\nSN\toutward oriented pairs:\t9\nSN\tpairs with other orientation:\t0\nSN\tpairs on different chromosomes:\t0\nSN\tpercentage of properly paired reads (%):\t0.0"}, {"role": "system", "text": "\n\n\n PaulaB:\n\nGiven these results is it safe to assume that the counts Salmon is generating (quantification and gene quantification results) are based on everything that mapped (and includes reads that mapped either on their own or as a part of a paired-end fragment)?\n\n\nYes, this is all true as far as I know when using the default settings (mostly \u2013 coverage of a read against a transcript must be at least >= 31 bases to count).\nTo make the counting even more permissive, you could toggle either or both of these parameters to yes: \u201cAllow Dovetail\u201d or \u201cRecover Orphans\u201d.\nTo make what is counted up by Salmon stricter (less permissive), toggle one or more of the other parameters on the form to yes.\nGlad the prior info helped, and hope this does too "}], [{"role": "user", "text": "Hi All, I used Kallisto to generate transcript abundance files for RNASeq data and am trying to run it through DESeq2. My input is 4 .tsv files as well as a .gene_trans_map file, and the following error message is generated:\nNote: importing  abundance.h5  is typically faster than  abundance.tsv  reading in files with read.delim (install \u2018readr\u2019 package for speed up) 1 2 3 4 removing duplicated transcript rows from tx2gene Error in  $<-.data.frame ( *tmp* , \u201cTXNAME\u201d, value = character(0)) : replacement has 0 rows, data has 510059 Calls: get_deseq_dataset -> <- -> <-.data.frame\nI have tried changing a variety of parameters, including setting header to false, and I have also tried only using one dataset per factor level, and nothing is working to fix this error. I have also tried using a .gff3 file rather than a .gene_trans_map file, and that also is not making a difference. Any advice would be much appreciated!\nThank you."}, {"role": "system", "text": "Welcome, @Sarah_Perkins\nI see your bug report sent in from Galaxy Main https://usegalaxy.org.\nThe problem is that your transcript fasta dataset identifiers do not match up with the transcript-to-gene mapping dataset.\nThe fasta (data 14) and TMP (data 356 + 357 + 358 + 362) transcript identifiers are formatted as:\nTR100009|c0_g1_i1|m.500685\n\nThe tabular transcript-tab-gene dataset (data 347) has two problems: 1) truncated transcript and gene identifier formats that do not match the TMP inputs, plus 2) it looks as if the order might be reversed (gene-tab-transcript)? As long as the genes are named consistently, it doesn\u2019t matter what they are, but the transcript names do need to match the other inputs.\nTR1|c0_g1\tTR1|c0_g1_i1\n\nIn short, primary IDs need to be in the same exact format across all inputs or tools cannot match data up. This is true for any tool, not just DESeq2.\nThe gff3 dataset is missing a header line: ##gff-version 3. That is why it was given the more generic gff datatype during Upload, and why the tool form does not recognize it as a valid input.\nAlso, replicates are required with DESeq2 in Galaxy.\nFAQ: https://galaxyproject.org/support/\n\nExtended Help for Differential Expression Analysis Tools\n\nHope that helps!"}], [{"role": "user", "text": "I am trying to extract BCR data from single (B) cell RNA sequencing data (SmartSeq, Nextera, HiSeq4000) to compare clonotypes between two samples. I\u2019m particularly interested in comparing % germline mutation to look at affinity maturation.\nThese are the settings I have used:\n\nimage435\u00d7718 19.7 KB\n\nI have tried running one of my samples (= 81 paired reads, i.e. 81 B cells) with MiXCR with the shotgun analyze approach for non-enriched libraries, and it seems to run without any errors but all the clonotype tables are empty.\n\nThe report for the same sample shows this in the final section which makes it look like contigs have been generated. Could you please advise how to get these to export?\n"}, {"role": "system", "text": "I\u2019m having the same issue here. Have you had any luck since posting this?"}, {"role": "user", "text": "Unfortunately not in Galaxy - my bioinformatician collaborator ended up running MiXCR himself."}, {"role": "system", "text": "Hi @carolyn_nielsen,\nsorry for the late response; I asked @OrangeOwl23 to share their datasets with me in order to reproduce the problem. I\u2019ll try to fix it as soon as possible.\nRegards"}, {"role": "system", "text": "\n\n\n carolyn_nielsen:\n\nSmartSeq\n\n\nHi, also MiXCR now supports SmartSeq2 protocol out of the box (and other single-cell protocols). There is a single command that only requires to set the species and provide input files:\nmixcr analyze smartseq2-vdj-full-length \\\n    -s hsa \\\n    161014_SN163_0729_AH3VW7BCXY_L1_{{CELL0ROW:a}}-{{CELL0COL:a}}_R1.fastq.gz \\\n    161014_SN163_0729_AH3VW7BCXY_L1_{{CELL0ROW:a}}-{{CELL0COL:a}}_R2.fastq.gz \\\n    result\n\nYou can read more on that here:\n  \n      \n\n      mixcr.com\n  \n\n  \n    \n\nBuilt-in presets - Docs @ MiLaboratories\n\n  MiXCR: ultimate software platform for analysis of Next-Generation Sequencing (NGS) data for immune profiling.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAlso, let me know if you need any help running the software.\nDisclaimer: I am one of the developers of MiXCR"}], [{"role": "user", "text": "Hi\nI have been using Unicycler for assemblies and have been following this tutorial:\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Unicycler Assembly\n\nDNA sequence data has become an indispensable tool for Mo...\n\n\n  \n  \n    \n    \n  \n  \n\n\nHowever, when I try to open my assembly using \u201cdisplay with IGV local\u201d (figure 14 of the tutorial) I get an error message:\n\nScreen Shot 2019-12-12 at 4.35.44 PM824\u00d7293 20.1 KB\n\nIt seems to say that the fasta file is available but the fai file is missing.\nI do not have jobs still running, all my assemblies are showing as green, so I don\u2019t see why it says \u201cadditional datasets require to be be generated\u201d.\nDid I miss a step when trying to use IGV local?\nThanks for help\nVanina"}, {"role": "system", "text": "Hi @vanina.guernier\nAn index for IGV is being created at this preparatory step. This is another \u201cjob\u201d, distinct from the assembly or other jobs run in the History. Your Unicyler job created a fasta output (without an index). Display in IGV requires an index, which is what is happening at this step.\nWhen \u201cDataset Status\u201d states \u201cnew\u201d, then that input (a fa.fai index in your case) is still being generated.\nWhen \u201cReady\u201d for both are true, then the data can be visualized.\nHow long this prep step takes depends on the size/content of the original dataset and how busy the server you are working at is."}, {"role": "user", "text": "Hi\nSo if I understand properly, the only way for me to know if both files are available is to check regularly using the IGV local button?\nI thought that an analysis showed in green meant that it was all done, but then that\u2019s not true for index files?\nI\u2019ll just wait then, Thanks"}, {"role": "system", "text": "The assembly is done, but moving the data into any 3rd party external application can require some reformatting or the creation of additional files that the other application is expecting \u2013 or they can require none \u2013 it depends on the external application.\nIn this case (IGV), a fasta index is required for new genomes. This would be true even if you decided to load the fasta directly into IGV as a new genome (instead of loading it directly from Galaxy). The \u201cGalaxy-to-Local IGV\u201d data web-based loading for fasta data is a convenience feature in Galaxy that skips the need for you to create the fasta index as a distinct step, download the fasta and fasta index, then load both into IGV directly.\nTo avoid this step, you could pre-load your assembly as a new genome in IGV. Then promote the assembly in Galaxy to a \u201cCustom Build\u201d. Use the same exact \u201cdatabase\u201d aka \u201cdbkey\u201d name in both places, and assign that \u201cdatabase\u201d to datasets you wish to visualize in IGV against that new genome/assembly.\nAny fasta file can be used directly as a \u201cCustom Genome\u201d with tools wrapped in Galaxy natively, without any fasta or tool-specific indexing (that is done at job runtime, when needed). But, sometimes promoting a fasta/Custom genome to a \u201cCustom Build\u201d is helpful for other reasons \u2013 there are tools in Galaxy that also require an assigned \u201cdatabase\u201d aka \u201cdbkey\u201d.\nHow to create/use a Custom Genome/Build in Galaxy:\n\nPreparing and using a Custom Reference Genome or Build\n\nIGV is a different application. The methods to create a \u201cNew Genome\u201d in your own IGV are below. The preparations steps will require that you index the fasta with samtools as part of that process. Or you can create the fasta index in Galaxy and download that along with the fasta.\n\n\nIGV: https://software.broadinstitute.org/software/igv/LoadGenome\n\n\nCreating a fasta index in Galaxy that can be downloaded as a distinct dataset: click into the pencil-icon for a fasta dataset to reach the Edit Attributes forms. On the tab for \u201cConvert\u201d, pick the option to create an index:\n\n\n\ncreate-fasta-index-in-galaxy1444\u00d7468 34.8 KB\n\n\n\nNote: Certain other datatypes already have an attached index. An example is BAM data. The download icon for BAM data will contain two datasets: the .bam (mapping results) and the .bam.bai (index).\n\nWhether or not to spend the extra effort to directly index your fasta in IGV as a new genome and create a Custom Build in Galaxy so that you can assign that custom \u201cdatabase\u201d to other datasets is up to you. That will somewhat depend on how often you plan to use it as a reference genome for other analyses.\nHope that helps to explain the options "}], [{"role": "user", "text": "Hello today i was using HiSAT2 to fo a mapping of my fastq paired-end secuences to the reference genome of my organism (Hydractinia symbiolongicarpus) and the software returned this error:\nTotal time for call to driver() for forward index: 00:07:18\n(ERR): mkfifo(/tmp/39861.inpipe1) failed.\nExiting now \u2026\nsamtools sort: failed to read header from \u201c-\u201d\n[main_samview] fail to read the header from \u201c-\u201d.\nI dont know exactly what it means. This is the link to the history im working on:\n  \n\n      usegalaxy.org\n  \n\n  \n    \n\nGalaxy\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "Hi @Nicolas_Romero_Villa\nI successfully tested HiSAT2 with a custom genome. I don\u2019t know why your jobs have failed. I converted your genome and reads into plain text formats, fasta and fastqsanger, and completed a test job with 1k PE reads. It is possible that the reads were compressed in non-standard option, and HiSAT2 cannot unpack the data. However, I tested only 1k reads, so there might be an issue with the data. It is very unusual situation.\nJust in case, procedure for conversion of formats: click at Edit Attributes (pencil icon) > in the middle window switch to Convert tab and select an appropriate option from the pull-down menu.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear all,\nHope you all do well in Galaxy.\nI have a problem of getting shared histories from the other Galaxy users. Her account showed the histories have been shared, but it shows 0 in the \"History shared with you\" item in my account.\nI am a little confused about this, Looking forward to suggestions.\n\nimage1273\u00d7224 11.6 KB\n\nThanks a lot.\nTing"}, {"role": "system", "text": "Hi Ting!\nJust to make sure, check the following:\n\nDid the other person share the history (with you only) using the exact same email account you have registered in Galaxy?\nAre you both in the same Galaxy server?\nMaybe the other person shared the history via link? In that case, you will need the link that the other person can provide.\n\nHope this helps!\nDavid"}, {"role": "user", "text": "Hi David,\nMany thanks for your suggestions.\nThe person who tried to share history, I think we are both in the same Galaxy server.\nBecause I can share my histories with her using the same account, but when she tried to share the histories with me, it showed my account is not a valid Galaxy user.\nTing"}, {"role": "system", "text": "@Ting\nWe\u2019ve been discussing this over email for the last week or so.\n\n\n\n Ting:\n\nI think we are both in the same Galaxy server.\nBecause I can share my histories with her using the same account, but when she tried to share the histories with me, it showed my account is not a valid Galaxy user\n\n\nMany people have one account at several public Galaxy servers. Accounts on different servers are different accounts, even though they may use the same email address for registration. You really need to confirm the server is the same for both of you for this function to work.\nThe advice that @davelopez offers are the same as what I have shared. If you want to share a History by email, you will need to:\n\nConfirm that both people have an active account on the same exact Galaxy server. How to know which server(s) are being used? Compare the URLs.\nConfirm that the correct email address is used \u2013 account email addresses are case sensitive. How to check your email address? Look under User > Preferences.\n\nPlus we discussed how to generate/import a history archive (to transfer histories between Galaxy servers). Most of that information was already in this forum and I added more details based on our conversion earlier today:\n\n  \n    \n    \n    transfer history from my account on one Galaxy server to another Galaxy server usegalaxy.org support\n  \n  \n    If the server is still up and public, or you have downloaded the history archive, then moving the history to another server is possible. \nYou state that the accounts are on different Galaxy servers. This is why the share by email didn\u2019t work. To share with another user by email, both accounts must be on the same server. All accounts on the same Galaxy server will have distinct registered email addresses. \nGalaxy FAQs: \nOptions for Data Sharing \nLoading/Downloading Data \nTips: \n\nMake sure the his\u2026\n  \n\n"}], [{"role": "user", "text": "An error occurred while running the tool data manager gatk picard index builder (the only version of the manager 0.0.1) - I was just trying to build the indexes for the GATK tools, and I didn\u2019t succed\n\nScreenshot from 2022-12-18 19-58-58875\u00d7410 25.3 KB\n\n(galaxy 22.05 was installed locally on a ubuntu server)\nDoes anyone can help me?"}, {"role": "system", "text": "Hi @apolitics\nThis data manager has not been functional for some time now, was based on an earlier version of GATK (2), included a tool dependency model that is not supported anymore, and the tools it was creating indexes for, and this DM itself, are all considered deprecated. None are hosted at any of the public servers for these same reasons."}], [{"role": "user", "text": "Hi all,\nI\u2019m following along the RNA-Seq reads to counts tutorial with my own data and am up to the Generating a QC Summary Report step.\nI\u2019ve followed the instructions (imported the QC report workflow, downloaded the RefSeq BED file) but in the workflow I cannot seem to select my BAM files - it only gives me the option to select MultiQC files.\nAny help would be appreciated! Thanks"}, {"role": "system", "text": "It is a bit hard to debug for me so I hope someone else can give a better answer . But if I go to the workflow page after getting the workflow I see this:\n\nimage1292\u00d7123 4.88 KB\n\nAnd after I click \u201crun workflow\u201d I see this:\n\nimage1309\u00d7292 11.1 KB\n\nDo you see the same? Did you used the url to import the workflow?"}, {"role": "user", "text": "Hi, thanks for your reply\nYes I see the same there, however my issue is I can\u2019t select any BAM files to enter, it will only let me chose MultiQC files in the drop-down menu\n\nimage1323\u00d7414 22.4 KB\n"}, {"role": "system", "text": "\n\n\n EmWhat:\n\nhowever my issue is I can\u2019t select any BAM files to enter\n\n\nHi - are you still having this problem?\nIf so, some items to check:\n\nThe BAM inputs should be in a dataset collection.\nThat collection needs to be in the same history where the workflow is launched (the \u201cactive\u201d history).\nAnd, this workflow is run after the completion of the other workflow associated with this tutorial.\n\nIf you want to share a link to your tutorial history back, we can help to test more. FAQ sharing-your-history"}], [{"role": "user", "text": "Hi all,\nI have setup a local Galaxy server on a headless ubuntu server follow this website.\nLocally (http://localhost:8080), everything works fine. I can upload data en install tools. Currently, I am trying to make the Galaxy server available through reverse proxy using this website. The Galaxy server is reachable through https://galaxy.mydomain.com but now if I want to upload data or install tools nothing happens.\nI have tried proxying using Nginx Proxy Manager and by just using nginx but nothings works.\nAny ideas?\nThanks in advance!\nErik"}, {"role": "system", "text": "Hi @zmeel\nResources. It looks like you already found the second, but maybe not the first?\n\n\nTutorials https://training.galaxyproject.org/training-material/topics/admin/\n\n\nDocs Galaxy Documentation \u2014 Galaxy Project 22.05.1 documentation\n\n"}, {"role": "system", "text": "Have you already checked the logs? You need to add the following line to the configuration file:\nerror_log /var/log/nginx/error.log;\nYou can choose any path you want, and you need to restart nginx after changing the configuration.\n  \n      \n\n      docs.nginx.com\n  \n\n  \n    \n\nConfiguring Logging | NGINX Plus\n\n  Capture detailed information about errors and request processing in log files, either locally or via syslog.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nEDIT:\nOr do you see something in the logs of galaxy?"}, {"role": "user", "text": "I have reinstalled Galaxy and now use the reverse proxy (Apache) of my Synology NAS but still no upload. Then I checked the console of my browser and saw the following errors:\nhttpStack.js?acdc:96 Mixed Content: The page at 'https://galaxy.mydomain.com/' was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint 'http://galaxy.mydomain.com/api/upload/resumable_upload/0ef7bc2e94a1471392af53da8de178a7'. This request has been blocked; the content must be served over HTTPS.\nFailed because: Error: tus: failed to resume upload, caused by [object ProgressEvent], originated from request (method: HEAD, url: http://galaxy.mydomain.com/api/upload/resumable_upload/0ef7bc2e94a1471392af53da8de178a7, response code: n/a, response text: n/a, request id: n/a)`\n`, will retry in 10 seconds\n\nAt least I have an error now\u2026"}, {"role": "user", "text": "Okay. Fixed this. It was the same as this issue:\nproxy_set_header X-Forwarded-Proto $scheme; causes Chrome to block the upload request. The solution was amazingly simple: proxy_set_header X-Forwarded-Proto \"hppts\";"}], [{"role": "user", "text": "We\u2019re in the middle of setting up a new Galaxy server (18.09) and currently installing tools - mixed ToolShed and locally developed code.\nQuite a lot of our tools are currently failing with error:\nConda dependency seemingly installed but failed to build job environment.\nThis includes BCFTools, GATK (3.x), and others.  Looking through the manage dependencies panel, the ones with issues often have multiple dependency requirements, with Conda as the resolver and separate environment paths listed. Other, working tools with multiple requirements via conda give the same mulled-v1-HASH environment path for all of them. Now, we don\u2019t have the mulled beta enabled right now, and I\u2019m not sure we ever did, so I don\u2019t know how these were sourced. We also aren\u2019t caching the dependencies, although maybe we need to.\nHas anyone seen this behavior, or have any troubleshooting suggestions? Is there a latency factor here - could it fail due to time taken to build the env across filesystems?"}, {"role": "system", "text": "Hi @dkolbe\nIf a tool requires multiple dependencies the only we to make this work using Conda is to install all dependencies into a new environment. mulled-v1-<HASH> is a hash of the combination of package names and versions. This is not configurable, and this is what you want to prefer and see for all mutli-dependency tools.\nOther options that concern mulled are about building/fetching containers using a similar scheme, so likely don\u2019t concern your use case.\nToolShed installed packages have been deprecated a long time ago because they are difficult to install and maintain. All recent tools from should be installable via Conda.\nMixing of ToolShed packages and Conda packages is possible in principle, but likely won\u2019t work well.\nIf you encounter cases where this doesn\u2019t work you can try uninstalling the ToolShed pckages. To do this go to Manage dependencies in the admin panel, select the tool you are interested in and hit Install dependencies via Conda.\nIf you encounter tools that just won\u2019t install via Conda I would suggest contacting the authors and ask them for an update. Unfortunately we won\u2019t update GATK 3 due to the licensing issue. GATK4 had been in works this summer, I hope this will materialize soon."}, {"role": "user", "text": "Thanks @mvdbeek.  Does that mean I should enable the mulled beta so that the combined environments get built? Or do I need to build them by hand? If they should be created automatically, for some reason that\u2019s not happening.\nP.S. - when I said ToolShed, I just meant obtaining them via toolshed, they\u2019re all actually fulfilled by conda, it looks like."}, {"role": "system", "text": "No, it\u2019s all fine. Make sure all your tools that have multiple requirements are using a Conda environment that starts with mulled-v1-. This should happen automatically, but can be very slow. The installation progress is logged in your log file. If for some reason this doesn\u2019t happen you can trigger the installation in the Manage dependencies menu. Have an eye on the logs when you do this, you\u2019ll see the commands that will be executed."}], [{"role": "user", "text": "When running sh run.sh, the following error occurs:\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f42e08aa0b8>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/adal/\nERROR: Could not find a version that satisfies the requirement adal==1.2.5 (from versions: none)\nERROR: No matching distribution found for adal==1.2.5 \n\nI tried to instal adal version 1.2.5 to galaxy environment but the error remained.\nThe OS I\u2019m using is WSL. Could this be the root of the problem?"}, {"role": "system", "text": "I see you posted this question twice, I guess it was a mistake, can you delete the one that you dont want? \nThanks!\nIt could very be a WSL thing, I dont know much about it but it could be a firewall isseu.\n  \n\n      stackoverflow.com\n  \n\n  \n      \n    \n  \n\n\n  I can't use pip on WSL Ubuntu\n\n\n\n  python, flask, pip\n\n\n\n  asked by\n  \n  \n    EnDelt64\n  \n  on 09:05AM - 05 May 19 UTC\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Sorry, my bad. The duplicate has been removed"}, {"role": "user", "text": "A strange thing have happened as I restarted my computer - now it works like a charm and has no errors. Prior to that, I started to try few solutions and made few actions:\n\nActivated hyper-v via power shell as described here: Enable Hyper-V on Windows 10 | Microsoft Docs\n\nEnabled Virtual Machine Platform via: Settings \u2192 Apps ->Programs and features \u2192 Turn windows features on or of\n\nThese steps (including the restart of the computer) might have no influence on resolving this problem so I am not sure if we can call it as a solution.\nYet, the problem somehow has been solved."}], [{"role": "user", "text": "Hi I\u2019m doing an RNA-seq analysis and I\u2019m having problems identifying some of the genes in the output of the DESeq2 tool because the list of genes obtained are in Entrez id, for context the pipeline used was Fastp > HISAT2 > feature counts > DESeq2. I used g: profiler and annotateMyIDs to identify the genes by the Entrez id, but there are still some that appear as NA in the list, so I tried to identify them manually by searching the Entrez id in NCBI and then took the coordinates from there and looking if there was a match in my samples using the visualizer UCSC and IGB, in parallel some of the genes in NCBI appears as updated and have another Entrez id associated to a gene, that gene is already identified in the list that I have, the question is can I keep the gene that is so far in the list and discard the obsolete one that had been updated?. Other than that can I discard the ones that haven\u2019t been updated and remains as NA?"}, {"role": "system", "text": "Hi @kizzy\nThis prior Q&A is similar to your question: Should i change de EntrezID of my genes if it has changed in NCBI?\nYou could use the UCSC\u2019s GTF annotation instead (incorporate it at the first step you incorporate the other annotation). Where to find it plus some related tips: RNA STAR error on trimmomtaic files - #6 by jennaj\nWhether to keep data that doesn\u2019t have any known annotation yet (NA) is your choice. If the research goal is to discover novel features, then you would probably want to keep those. If you are not hunting for novel features, then you can restrict the analysis to known features upstream during mapping.\nHISAT2 has an option to only report hits with known features: Advanced options >> Spliced alignment options\n\nGTF file with known splice sites (input the UCSC annotation)\nTranscriptome assembly reporting (set to: \u201cReport only those alignments within known transcripts\u201d)\n\nHope that helps! "}], [{"role": "user", "text": "Hi all,\nI want to add a reference genome for RNA-seq analysis to use on HISAT2 and FeatureCounts.\nGenome assembly ARS-UI_Ramb_v2.0\n  \n      \n\n      NCBI\n  \n\n  \n    \n\nOvis aries genome assembly ARS-UI_Ramb_v2.0\n\n  None - Ovis aries (sheep)\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nHow do I go about doing this and what file formats do the they need to be for each program?\nThanks in advance."}, {"role": "system", "text": "Hi @Maxmus\nUpload FASTA file into your history and while composing HiSAT2 job, change source of genome from built-in to \u2018in your history\u2019.\nHope that helps.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi @igor ,\nThanks for your reply. Yes the FASTA file works to align to the genome using HISAT2, however, this does not work when using \u2018FeatureCounts\u2019. I have tried changing the file format to GTF but that doesn\u2019t count any genes. Do I need to download an annotation file or build a custom genome to conduct FeatureCounts?\nMany thanks,\nMax"}, {"role": "system", "text": "Hi @Maxmus yes, you need a gene annotation file. Usually annotation files are available from the same source, as genome assembly. During setup of featureCounts job change Gene annotation file to \u2018in your history\u2019. Make sure the gene annotation is for the assembly used for read mapping. Inspect the gene annotation file and double check that values used for read counting are present in the file. By default, reads are counted against exons and aggregated for gene_id attribute - see Advanced options in featureCounts.\nKind regards,\nIgor"}], [{"role": "user", "text": "Good morning. It is my first time using Galaxy/hutlab. I have uploaded a  text tab-delimited file and it apears in my history. I am trying to use it to perform the first step of LEfSe (A. Format data for LEfSe) but i am not able to select any file as the drop-down menu does not work and doest not appear when I click on the first option. I can not select either which rows have to be used as class or subject (I am not able to select anything in the items on the red circles). Should I do anything else before this step appart from uploading my text tab-delimited file?\nThanks.\n\nLEfSe960\u00d7720 254 KB\n"}, {"role": "system", "text": "You need to change the datatype of your txt file. You can do that by clicking on the pencil icon of the file. After that a new window will appear, on top of that window you will find some tabs, choose the datatype tab."}], [{"role": "user", "text": "Hello,\nI have a Galaxy 19.09 installation I need to upgrade to 20.05.  It is running on Ubuntu 18.04.2 LTS.  I also want to update it to use Python3 (currently running with Python 2.7).  I was generally following the procedure from \u201chttps://galaxyproject.org/admin/maintenance/\u201d under section \" General Update Procedures\" but I\u2019m worried this may be outdated.  Below I\u2019ve listed my planned steps.  I\u2019m hoping someone more knowledgeable can let me know if I\u2019m following the wrong instructions.\nProcedure:\n\nclone existing prod instance (version 19.09) using tar/gzip\nclone PostgreSQL database using \u201cCREATE DATABASE galaxy_prod2 WITH TEMPLATE galaxy OWNER galaxy;\u201d\nupdate galaxy.yml for cloned installation and change database_connection parameter to use new database called galaxy_prod2.\nNext upgrade to 20.01 so I can step my way to latest version:    \u201cgit fetch origin && git checkout release_20.01 && git pull --ff-only origin release_20.01\u201d\nthen follow instructions on page \u201chttps://galaxyproject.org/admin/maintenance/\u201d under section \" General Update Procedures\".\nI\u2019m especially unclear whether I need to execute steps #3, #4, and #8.\nassuming that all works, then use git to get latest version 20.05 and follow same instructions.\nupgrade from Python 2.7 to Python 3\n\nI tried upgrading Python first but ran into trouble so I assumed I had to upgrade Galaxy first to latest version.  I followed these steps for the Python upgrade before attempting to upgrade Galaxy.\nI followed these instructions:\n(https://docs.galaxyproject.org/en/master/admin/python.html)\n\nI removed .venv from base galaxy install directory\nFollowed Step #3:  set this in .bash_profile and in the current window:  export GALAXY_PYTHON=/usr/bin/python3\ncreated symbolic link to python3 and added at beginning of path\nverified new correct version using \u201cpython --version\u201d <-- now shows 3.6.8\nfollowed step #4 and issued this: \u201crm -rf /path/to/galaxy/database/compiled_templates/\u201d <-- updated for my env\nstarted galaxy:  now when Galaxy started it rebuilt .venv from Python 2.7 instead of 3.6.8.\n\nObviously the Python upgrade did not work.  Something under Galaxy is holding onto 2.7 and I\u2019m not sure what.\nThanks in advance for any help or advice."}, {"role": "system", "text": "Hello\nI\u2019ve just updated from 19.09 to 20.05.\nRegarding Python2.7 to Python3? I\u2019ve followed the steps here:\nhttps://docs.galaxyproject.org/en/release_20.01/admin/python.html\nI\u2019ve also launched this command:\nfind . -name '*.pyc' -delete\nIt will remove all \u201cold\u201d .py compiled under Python2.7\nI think it will avoid errors such like:\nbad magic number in 'galaxy.web.stack'\nFred"}, {"role": "user", "text": "Thank you Fred for the thoughful reply.  I compared those Python upgrade instructions with mine and they are the same but I did not try to remove all *.pyc code first.  I did execute this which might do the same thing \u201crm -rf /path/to/galaxy/database/compiled_templates/\u201d.\nFor the Galaxy upgrade did you first upgrade to 20.01 or did you go directly to 20.05?  If you first went to 20.01, after you downloaded the 20.01 release from github did you run \u201c./scripts/common_startup.sh\u201d, then manage_db.sh?\nI think those are the correct next steps according to my instructions.\nCheers."}, {"role": "system", "text": "find . -name '*.pyc' -delete : I launched this on the root install dir of galaxy. So it will remove every .pyc file in lib/* . I think it should be launched.\nand I\u2019ve upgraded from 19.05 to 20.05 without any other step, directly! "}], [{"role": "user", "text": "I have about 12 samples I aligned with mm39. All samples had good quality reads and all paired, as trimmomatic gave me empty files for unpaired reads. Strangely, two of them had 0% alignment with the genome. I blasted a few reads and they seem to match mus musculus or other rodents. I\u2019m using HISAT2 on galaxy. Is it a bug?"}, {"role": "system", "text": "Hi @Diogo_de_Moraes1\nTry a rerun with the data that didn\u2019t align, just to eliminate server/cluster issues from being a factor.\nAs you do that, double-check that the correct target reference genome is selected on the tool form. The most current version of the mouse genome is GRCm38 (mm10, sourced from UCSC). Most mapping tools, including HISAT2, will only have built-in indexes for mm9 or mm10 available (not the earlier genome builds).\nSometimes the wrong genome is selected. HISAT2 is designed to align reads to the same species they were sourced from. BLAST+ is different, and allows for cross-species mappings to be reported.\nmouse-genomes984\u00d7216 22.2 KB\nIf that doesn\u2019t work, and the reads are of high quality (tool: FastQC) then something else is going on. Perhaps a sample mixup, or the inputs (forward/reverse) were not entered correctly on the form, or possibly the read content doesn\u2019t meet the minimum mapping criteria set on the HISAT2 tool form.\nExample of the last case: too much QA (trimming) can generate truncated reads that won\u2019t meet the minimum default mapping criteria. You\u2019ll need to investigate then make changes depending on what you uncover about the content.\nHope that helps!"}], [{"role": "user", "text": "I would appreciate help by someone regarding \u2018flye\u2019 tool in galaxy to perform assembly of Pacbio data in bam format,  step by step guide will be better. i tried it, using SRA accession number from NCBI but it gives error.\nthank you!"}, {"role": "system", "text": "Hi @ZAYNE\nAssembly tutorials and workflows are available here.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Assembly\n\n  DNA sequence data has become an indispensable tool for Mo...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThe training site can be navigated by topic or searched with keywords like the datatype or tool name. This is probably your best match.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Genome assembly using PacBio data\n\n  DNA sequence data has become an indispensable tool for Mo...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n\n\n ZAYNE:\n\nassembly of Pacbio data in bam format\n\n\nInput Datatype\n\n\nFlye requires input reads in a fastq or fasta format.\nStart up a new empty history, then load up a tool\u2019s form to review what the exact supported input datatypes are (file content/format + \u201cdatatype\u201d metadata).\n\nNCBI SRA\n\nGet the reads from SRA using a tool like Faster Download and Extract Reads in FASTQ.\nAvoid attempting to retrieve reads in BAM format with the tool Download and Extract Reads in BAM. From my experiences, that one rarely works correctly due to time/data limits set at NCBI. The query can either error (red) or output truncated results (putatively \u201csuccessful\u201d, but not actually usable). Confusing, and better to skip using it anyway since very few tools accept read inputs in that format.\n\nRead data in BAM format already?\n\nA tool like SamToFastq can be used to extract the reads.\nIf the BAM was a mapping job result, keep in mind that some reads might be output more than once.\n\nHope that helps!"}], [{"role": "user", "text": "Hi, I\u2019m trying to upload a large .bam file to my galaxy account using galaxy upload. I did those steps but still getting errors that I\u2019m not able to fix. Please help!\n$ pip install galaxy-upload\n$ python3 -m venv ~/galaxy-upload\n$ . ~/galaxy-upload/bin/activate\n$ pip install galaxy-upload\n(galaxy-upload) (base) TS261@PHS024050 outs % galaxy-upload --url https://usegalaxy.org \\ \u2013api-key cae08ba0d4323e26716c58de5cb3c4 --history-name \u2018RNA\u2019 -i \\ \u2013file-name possorted_genome_bam.bam\nUsage: galaxy-upload [OPTIONS] [PATH]\u2026\nTry \u2018galaxy-upload --help\u2019 for help.\nError: Missing option \u2018\u2013api-key\u2019."}, {"role": "system", "text": "I dont see a pip error so I think your question is not correctly formulated.\nIt looks like galaxy-upload is installed but your command is not correct.\nBased on your command it should be something like:\ngalaxy-upload --url https://usegalaxy.org --api-key cae08ba0d4323e26716c58de5cb3c4 --history-name 'RNA' possorted_genome_bam.bam\nYou have \u2013api-key instead of --api-key"}, {"role": "user", "text": "Hi, Thank you for your reply!\nI fixed the command but still getting the error and not able to upload the bam file to my galaxy account. Do you have any suggestions? I\u2019m not sure what I\u2019m doing wrong. This is what I\u2019m getting:\n(galaxy-upload) (base) TS261@PHS024050 ~ % galaxy-upload --url https://usegalaxy.org \\ \n\u2013api-key bfdf58ba226efa18da654f47224b2e4c --history-name \u2018RNA\u2019 -i \\ \u2013file-name possorted_genome_bam.bam\nUsage: galaxy-upload [OPTIONS] [PATH]\u2026\nTry \u2018galaxy-upload --help\u2019 for help.\nError: Missing option \u2018\u2013api-key\u2019."}, {"role": "system", "text": "You are showing the same command as your orignal question. Besides the api key the use of --file-name is also not correct. Look at this galaxy-upload \u2014 galaxy-upload 1.0.0 documentation\nYou could also try to upload your file with galaxy itself on the webinterface.\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: A short introduction to Galaxy\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I am trying to use Diamond alignment tool for short sequences against a protein database. As the title suggests, when I click on the \u201cSelect a reference database\u201d there are only options for the NCBI NR and RefSeq. Underneath is the message \u201cIf your database of interest is not listed, contact your Galaxy admin\u201d.\nSince I do not know how to contact Galaxy admin, I am submitting here. I hope someone can help or at least alert someone if this is something that can be done.\nThanks in advance!"}, {"role": "system", "text": "Hi @mateoc,\nwhich Galaxy instance are you using?\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nI am working on usegalaxy.eu if that\u2019s what you are asking.\nRegards"}, {"role": "system", "text": "Thanks, I have informed the system administrator about that.\nRegards"}, {"role": "system", "text": "Hi @mateoc,\nthe database is now available.\nRegards"}], [{"role": "user", "text": "Hi all.\nAfter having used DESeq2, my results show up as nucleotides (per NCBI: Search NCBI databases - NLM) instead of genes.\nDo you know what causes this and how I could find out the correct gene IDs?"}, {"role": "system", "text": "Hi @Iisa\nThe URL you shared is a link to a search function with no content.\nPlease share more details about the inputs and result. The description so far seems very odd - so this will be best done by generating and posting a share link to the working history. You can post that back here publicly, or ask for a moderator to start up a direct message chat to post that in instead.\nLet\u2019s start there, thanks "}, {"role": "user", "text": "Hi @jennaj,\nright, I just wanted to show which site I had used to look for the IDs.\nI would appreciate a direct message chat with a moderator.\nThank you. "}, {"role": "system", "text": "Hi @Iisa\nThe content of the gene_id and transcript_id fields of the reference annotation GTF are what Featurecounts uses to generate the counts. Mapped hits at the \u201ctranscript_id\u201d level (aka transcript coordinates) are summarized per \u201cgene_id\u201d in that output. Then, those gene_id level counts are interpreted by DESeq2 for differential expression.\nFor this data, the annotation was predicted using an automated annotation pipeline developed by NCBI. To review the general stats for the annotation, and the pipeline details, go to the assembly page here ASM2155985v1 - Genome - Assembly - NCBI and expand the section \u201cComment\u201d\n\nThe annotation was added by the NCBI Prokaryotic Genome Annotation Pipeline (PGAP). Information about PGAP can be found here: NCBI Prokaryotic Genome Annotation Pipeline\n\nExample line from the public GTF\nNZ_CP058222.1\tProtein Homology\tCDS\t1\t1401\t.\t+\t0\tgene_id \u201cHW370_RS00005\u201d; transcript_id \u201cunassigned_transcript_1\u201d; gbkey \u201cCDS\u201d; gene \u201cdnaA\u201d; inference \u201cCOORDINATES: similar to AA sequence:RefSeq:NP_418157.1\u201d; locus_tag \u201cHW370_RS00005\u201d; product \u201cchromosomal replication initiator protein DnaA\u201d; protein_id \u201cWP_000059111.1\u201d; transl_table \u201c11\u201d; exon_number \u201c1\u201d;\nWhat those values represent\n\nNZ_CP058222.1 == chromosome identifier, pairs with the genome fasta on the title line > value (everything before the first whitespace) and the third column in BAMs generated via mapping against the fasta. Those BAMs were not in the shared history but that\u2019s OK \u2013 are the BAMs you input to Featurecounts (maybe in another history, or locally on your computer, or some other online tool)\ngene \u201cdnaA\u201d == the gene the pipeline guessed as the closest valid match. This is what you could lookup, example: https://www.ncbi.nlm.nih.gov/gene/?term=dnaA\n\ngene_id \u201cHW370_RS00005\u201d and locus_tag \u201cHW370_RS00005\u201d == both are the same value. This is a distinct name, presumably since more than one locus could be assigned to the same \u201cgene\u201d via the pipeline\u2019s \u201cinference\u201d algorithm. Technically, \u201cgene_id\u201d must be unique in any single assembly\u2019s annotation \u2013 most tools will error or produce weird results if there are any duplications.\n\nYou could transform the GTF to a tabular format with gffread then use Join two Datasets side by side on a specified field to map gene_id/locus_tag \u2192 gene in batch. Tutorial with these and more manipulations: Data Manipulation Olympics.\nYou can also search the tool panel with the keyword \u201cgtf\u201d to find more tools to manipulate or extract (or extract and simplify) any other attribute data you are interested in mapping the gene_id/locus_tag value to (the first column in the DESeq2 output table). Example: you want to map to the \"inference \u201cCOORDINATES: similar to AA sequence:RefSeq:NP_418157.1\"\u201d you would need to strip all the extra content until just the NP value remains to use that key in a search to get the associated protein fasta.\nHope that helps!"}], [{"role": "user", "text": "Suppose I have a nested directory structure, something like data > si_n > (si_n.cell, si_n.param) with multiple values of n. Is there a way to upload this folder into a Galaxy collection with the same nested structure?"}, {"role": "system", "text": "I think if this is available for your respective instance, FTP upload can do this for you.\nBut FTP upload has been decommissioned for usegalaxy.org, so this won\u2019t work there. Does anybody have any suggestions for how to do this without FTP available? Is there some kind of --recursive functionality for the galaxy-upload tool?"}, {"role": "system", "text": "\n\n\n dlaehnemann:\n\nIs there some kind of --recursive functionality for the galaxy-upload tool?\n\n\nI\u2019m not exactly sure and can\u2019t test right now due to the UseGalaxy.org server being down for maintenance today.\nOnce back up \u2013 I would test to see if the upload string will accept standard unix commands, something like one of these.\nPer n\n\n\u2026/data/*n*\n\nPer file extension\n\n\u2026/data/*.cell\n\u2026/data/*.param\n\nThis would probably fail due to the . and .. and any other dot files getting picked up.\n\n\u2026/data/*\n\nLet us know what you think. That tool can be updated if there is a use case we missed on the first pass. We liked FTP too but had to change out for alternative methods due to updated security requirements on our academic services."}, {"role": "system", "text": "What\u2019s working currently is:\nfind <top-level-folder> -type f -exec galaxy-upload --url https://usegalaxy.org --api-key $API_KEY --history-id <id_of_history_to_upload_to> --file-name \"{}\" \"{}\" \\;\nThis will upload all files in <top-level-folder> as individual datasets to the history with ID <id_of_history_to_upload_to>.\nOnce the data is there you can select all of it and choose Build Collection from Rules. Since the above upload command preserves paths into subfolders, you can use them with the Rule builder to structure your collection like you want.\nI leave it up to @jennaj to explain individual steps of this in detail. For the rule-based uploader I also recommend Rule Based Uploader and Rule Based Uploader: Advanced.\n@nate maybe galaxy-upload could make this more convenient than the current hack?"}, {"role": "system", "text": "Good suggestion, I\u2019ve created an issue for it."}], [{"role": "user", "text": "Hello,\nThe plots generated by Deseq2 and EdgeR, is there a way to get the images of them either as SVG or TIFF, or anything aside from a PDF?\nIt\u2019s very difficult when trying to use them for a manuscript etc if the only output is a pdf that you need to screenshot.\nIf anyone has a way around this please let me know."}, {"role": "system", "text": "Hello,\nI\u2019ll make a ticket for the developers for this and other tools to see what we can do about PDF outputs. Seems like a useful transformation and/or optional output format for exactly the reasons you explain.\nMeanwhile, try using a free conversion program. I found several with a google search with \u201cpdf to svg converter free\u201d. I cannot personally recommend any of these but some look to be reputable and worth a try.\nThanks!"}, {"role": "system", "text": "Ticket here, please feel free to upvote or comment on it more: https://github.com/galaxyproject/tools-iuc/issues/2477"}, {"role": "system", "text": "If you\u2019re a Mac user you could use Preview to convert PDF to TIFF."}], [{"role": "user", "text": "I understand how to share histories. But can multiple members of a team share and work on a history (and not just view it)?"}, {"role": "system", "text": "Hi @herricjb\nother users can import a shared history and edit it, not just view.\nI am not aware about multi-user access to a history through the web interface, but you can share the user name and password with your colleagues, so they can access your account, not just one history.\nKind regards,\nIgor"}, {"role": "system", "text": "\n\n\n igor:\n\nbut you can share the user name and password with your colleagues, so they can access your account,\n\n\nImportant Warning \u2013 directly sharing accounts would cause administrative issues at UseGalaxy.org and UseGalaxy.eu. Everyone should only use and log into their own single account. If someone already knows your password, update it and keep it private to protect your own data.\nSomeone else using your account could do anything you could do \u2013 up to and including deleting the entire account. This is usually not \u201cun-do-able\u201d.\nThe collaborative working model involves sharing functions. Share your history, others can review and/or import to work with it, then share back. You can generate a link and share that, or directly share with other people also working at the same server (by email address).\n\n\n#can-i-create-multiple-galaxy-accounts\n\n\n#sharing-your-history\n\n\n#finding-and-working-with-histories-shared-with-me\n\n\n#how-to-set-data-privacy-features\n\n\nAt some public servers you can just keep on adding data to that history with the share link or the history that is directly shared. This is continuous sharing. UseGalaxy.org is organized like this by default and can be customized.\n\n\nAt some public servers this will mean that you need to re-share to capture new datasets or adjust your account User \u2192 Preferences \u2192 Set Dataset Permissions for New Histories to allow for continuous sharing. UseGalaxy.eu is organized like this by default, but you can customize.\n\n\nAnd some public servers offer bonus resources for regional affiliations, making multiple account usage unlikely. I believe that UseGalaxy.org.au is organized this way, and therefore sharing accounts is not restricted (?). @igor can correct me or fill in the details.\n\n"}], [{"role": "user", "text": "Hello! I am an undergrad and am in desperate need of any assistance, as my mentor is not familiar with bioinformatics!\nI am using Galaxy to do differential gene expression RNA sequencing and am following Galaxy\u2019s tutorial (Reference-based RNA-Seq data analysis). I have 12 pairs of pair-end samples in FASTQ format of HeLa cells. I did QC and did not do cutadapt as one online source stated that it was not necessary if I would be using STAR to map my sequences. (Do you think that this was a smart decision?)\nRight now, I am trying to use STAR to map my sequences to the Homo_sapiens.GRCh38.109.gtf file, but I keep getting the following error: \u201cFatal INPUT FILE error, no valid exon lines in the GTF file: /jetstream2/scratch/main/jobs/49599174/inputs/dataset_b28934b5-2b19-\u201d I used a file from Ensemble and retried with one from UCSC, but both gave the same error. What should I do for my next step?\nThank you so much in advance for your help! I really appreciate it!! "}, {"role": "system", "text": "Hi @EriKoy\npopular aligners use soft clipping (ignore unmappable nucleotides at reads\u2019 start and end). You\u2019ll see it in CIGAR string, eg 12S50M means 12 soft clipped nucleotides at alignment start followed by 50 matches. For additional information check specification of SAM format. You can check role of adapters and compare counts from original and trimmed reads. What important: treat all samples in the same way.\nWe do see elevated rate of errors when RNA_STAR is used with gene annotations. Gapped aligners can map reads across splice sites without gene annotations.\nPersonally, I prefer two step approach, mapping and reads counting. Some tools have built-in gene models for popular organisms 1: RNA-Seq reads to counts\nUsually I import annotations, so I know what is used.\nRNA_STAR provides very limited control over read counting, while featureCounts and htseq-count allow selection of attributes and features.\nIt is hard to say why the job failed without checking the annotation file. The annotation and the reference genome should be for the same version of genome assembly, with identical chromosome/contig names, for example, chr1, Chr1 and 1 might be considered as three different text strings/names.\nMaybe try HiSAT2/featureCounts approach described in the tutorial above on one sample to see if it works for you.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I keep getting the following error when trying to run LDA effect size (lefse):\n\u201cTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 435, in \nfeats = numerical_values(feats,params[\u2018norm_v\u2019])\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 174, in numerical_values\nfeats[k] = [float(val) for val in v]\nValueError: could not convert string to float: MN3207_VT1303\u201d\nAny help would be greatly apperciated!\nBest regards,\nCollin"}, {"role": "system", "text": "hello @mgcollin19 ,\nseems that you have installed the tool from the test-toolshed, try to install the same tool from the, more stable and controlled, toolshed."}, {"role": "user", "text": "Thank you for your help! I was actually using the website based version. @pietro would you be able to provide me with a link for the more stable version you are referencing?\nBest regards,\nCollin"}, {"role": "user", "text": "@pietro to follow up on my previous question is the more stable version you referenced website based or do I have to access it another way?\nThank you,\nCollin"}, {"role": "system", "text": "to install the same tool from the main toolshed, if you are the admin of the instance, you can follow the steps present in this tutorial https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/#open-the-tool-shed.\nlink to lefte in toolshed link to lefte in toolshed.\nThen, having never used it, I don\u2019t know if it will work but it is always better to install the tools from the main toolshed."}], [{"role": "user", "text": "As a new user, the Filter List documentation was confusing/unclear.  The documentation states that the function will:\n\ntake an input list and a text file with identifiers to filter the list with\n\nThis led me to believe that partial identifiers could be used, such as \u201cRep1\u201d matching all data sets in the list with \u201cRep1\u201d in their names.  I found, however, that the filter function will only work if entire file names are used.\nIf the intended behavior is to only match complete file names and not support partial matches, I think the documentation would be more clear if it used \u201cfile names\u201d instead of \u201cidentifiers\u201d.\nIf the intended behavior is to support partial matches, then I think there may be a bug as I was not able to get partial matches using either a plain string or a proper regular expression as identifiers in my text file."}, {"role": "system", "text": "Thanks, you are correct. The Filter List tool only works with complete identifiers.\nThe coded that implements Filter List is in class FilterFromFileTool in the galaxy.tools module and reading that code it shows that a \u201cfilter match\u201d occurs when an identifier from the input collection matches an identifier from the input list of identifiers. I.e. complete match, not substring match.\nThe documentation is in the tool XML file. Do you have a suggestion for better wording?"}, {"role": "user", "text": "I would probably change\n\nThis tool will take an input list and a text file with identifiers to\nfilter the list with. It will build two new lists - one \u201cfiltered\u201d to\ncontain only the supplied identifiers and one of the discarded elements.\n\nTo something like\n\nThis tool will take an input list and a text file with the names of list members to split the list by.  It will build two new lists - one \u201cfiltered\u201d to contain only the supplied names and one of the discarded elements.\n\nAnother potential way to reduce confusion might be to call the method Split List instead of filter list, since a filter usually implies some sort of pattern matching while splitting does not.\nAnother option might be to tweak the interface for the method.  You could show all the file/element names in the list in a pane and allow the user to select the ones they want to keep (just like a batch file selection field) instead of using a text file.  This would make the method easier to use and eliminate potential sources of confusion."}, {"role": "system", "text": "I understand your confusion and have opened a PR (https://github.com/galaxyproject/galaxy/pull/7127) to update the help text. I think changing the text is better than changing the tool name.\nI think it is best to keep to using a text file so that this can be part of a workflow without requiring user intervention."}], [{"role": "user", "text": "I\u2019m using workflows for the first time.  I fed 59 datasets into the workflow.  Each dataset has multiple #hashtags associated with it.  Once I started the workflow, I noticed that the datasets were being split into groups and processed in independent histories.  I did select the \u201csend the results to a new history\u201d option, but anticipated only 1 new history being generated - this has generated >10 new histories.  Is this a result of selecting that option, or is it being caused by something inherent with the #hashtags associated with the datasets.  It does appear to have grouped the datasets based on the tags, as this was not the order that the samples were listed in the original history.\nI\u2019m trying to delete all of these histories on the \u201cuser history\u201d page by selecting them all and clicking \u201cdelete permanently\u201d, but the browser just seems to freeze up and I get an error: \u201cGrid refresh failed\u201d.  Not sure how to clean this mess up x_x\n"}, {"role": "system", "text": "Ok, this is unexpected as far as I know.\nWould you please save back a copy of history with the input datasets in it and the workflow that created this? Copying datasets does not use up more quota. Then generate a share link to both and send that to me in a direct message. I\u2019d like to try to reproduce what happened and share this with the developers. This is odd enough to flush out what is happening soon. We are finalizing the next release.\nFor the rest of the data you want to purge, go ahead and purge histories in batch (under the history menu\u2019s gear icon pick \u201cSaved Histories\u201d). The view are you using is more compute intensive \u2013 the other view is just a list.\nEither way, purging many histories at once will overload the server a bit (why the grid fresh message come up). Wait for that message to come up (important, the action is being processed) then click on the top left \u201cGalaxy\u201d name/icon to reload the browser. The server will refresh and you can do other operations, including purging more histories. The database will catch up as a process that runs in the background.\nThanks and I\u2019ll watch for your share links. Thanks!"}, {"role": "user", "text": "Unfortunately I deleted the histories before getting your reply.  However, I did find that re-running the workflow while selecting no for \u201csend the results to a new history\u201d prevented any new histories from being created.  So, the strange result was definitely related to that option within the workflow.  I\u2019ll share my input samples and workflow with you, and maybe you can try running it on your end to see if it re-populates the error?"}, {"role": "system", "text": "\n\n\n JVGen:\n\nI\u2019ll share my input samples and workflow with you, and maybe you can try running it on your end to see if it re-populates the error?\n\n\nYes, that is exactly what we need. Your original output (multiple histories) shouldn\u2019t be needed. The test will be to see if/how that can be reproduced with the \u201csend to new history\u201d workflow runtime option. Then to follow-up with a remedy.\nThank you again for reporting the problem and sharing the test case!"}, {"role": "system", "text": "Ok, turns out this is expected behavior. Splitting into \u201cone history per input\u201d was a function put in place before dataset collections/tags were introduced. This works fine for people that have ~2-5 inputs. Not so good for 50+. We might update the workflow execution help text to make this type of \u201csend to new history\u201d output result more clear.\nSo, you\u2019ll need to use dataset collections, or stick with individual dataset inputs and be Ok with all the history outputs. These are all named the same, but there isn\u2019t a clear way to do that better. The new history name is specified by the user. There is no way to know how many inputs before the workflow executes \u2013 and the user entering a new name for each would be tedious anyway.\nI hadn\u2019t involved a workflow like this before (or it was long ago\u2026) so didn\u2019t recognize what was going on. But @dannon confirmed this is all working correctly.\nHope that helps to explain what is going on!"}], [{"role": "user", "text": "Dear Galaxy users,\nFirst, thanks in advance for your help,\nI am trying to get a de novo assembly from RNA-Seq data using Trintiy. I have just one sample that was sequenced in the Illumina platform with more or less 40 M paired-end reads 150 bp (strand-specific). I have trimmed those reads with Trimmomatic and checked the quality before and after that, and the reads have good quality. I am getting the following error:\nExecution resulted in the following messages:\nFatal error: Exit code 255 ()\nTool generated the following standard error:\nFATAL:   could not open image /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all/mulled-v2-db9b22af087bf11ca8136acc57516d3e3b7cbe14:6b4c1eb9563d366a9f4ff35670f4f430d7306d78-0: failed to retrieve path for /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all/mulled-v2-db9b22af087bf11ca8136acc57516d3e3b7cbe14:6b4c1eb9563d366a9f4ff35670f4f430d7306d78-0: lstat /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all: no such file or directory\nGalaxy job runner generated the following standard error:\nFailed to initialize root file catalog (16 - file catalog failure)\nCould you help me with this, please?\nBest wishes,\nJose"}, {"role": "system", "text": "Hi @Jose_Manuel_Latorre\nWe are looking into reported problems with this version of Trinity \u2192 Trinity de novo assembly of RNA-Seq data(Galaxy Version 2.15.1+galaxy0), and a correction is pending.\nFor now, try this:\n\nAt least one rerun. Why? To eliminate transient server issues.\nTry using a prior version of Trinity. Any of the last three versions work for me Ok at UseGalaxy.org. Changing the tool version\n\nThanks for reporting the problem!"}], [{"role": "user", "text": "I am doing the From Genes to Peaks tutorial and I am getting stuck\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: From peaks to genes\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWhen I do the Intersect of promoter regions with peaks regions, I get a \u201cAn error occurred setting the metadata for this dataset\u201d message. Inside the attributes, the info says: \u201c/corral4/main/jobs/040/816/40816980/galaxy_40816980.sh: line 123: galaxy-set-metadata: command not found\u201d\nDoes anyone know what\u2019s going on?"}, {"role": "system", "text": "Hi @pashadag\nThe output generated was malformed for some reason. It was possibly empty.\nTry this:\n\n\nFirst, try a rerun for that job. The \u201cdouble-circle\u201d icon within the output dataset will bring up the original tool form, and it can be submitted again to exactly reproduce the original run. The goal is to eliminate a technical problem server-side. For any error or odd result at least one rerun is a good idea (not just when running tutorials).\n\n\nIf the same error comes up, check which inputs are being selected on the tool form. Sometimes these get mixed up.\n\n\nIf the correct input datasets were selected, in the right order, maybe one of those datasets has a problem.\n\n\n\nBoth should have the datatype assigned as bed. You can expand the dataset to view the datatype assigned, number of lines, and other details. Click on the \u201ceye icon\u201d to view the dataset contents \u2013 the format should match the pictures in the tutorial (not exactly \u2013 but the content should be similar).\nYou can confirm that the data is actually in bed format by redetecting the datatype. Try this for both: click on the pencil icon to reach the Edit Attributes form, go to the tab \u201cDatatypes\u201d, then click on the button for \u201cAuto-detect\u201d. The result should be bed for both \u2013 if not, something went wrong in a prior step and you\u2019ll need to back up and maybe redo a step or two.\n\nIf you are still stuck after those checks, please generate a share link to your tutorial history and post that back here. Try to not delete any of the input or output datasets. This is how: Galaxy Training!. Be aware that this will be publicly shared \u2013 but if the history only contains the tutorial data, it shouldn\u2019t matter and more people at this forum will be able to help you.\nLet\u2019s start there "}, {"role": "user", "text": "Hi jennaj,\nThank you for helping! I tried the steps you outlined but it didn\u2019t seem to change things. Here\u2019s a link to my shared history:\nhttps://usegalaxy.org/u/pashadag/h/another-attempt\nThank you for your help,\nPaul"}, {"role": "system", "text": "Update March 8, 2022\nThe tools in this suite are corrected at UseGalaxy.org. Please rerun any prior failures. The fix was a missing dependency added back at our clusters, so the tool version didn\u2019t change.\nThanks to all who reported it!\n\nThanks for sharing the history. The tool itself is problematic, along with a few related tools. It will take at least a few days to investigate the root cause.\nPlease run the tutorial at either UseGalaxy.eu or UseGalaxy.org.au instead for now.\nRelated recent Q&A (someone else ran into the same error today) Get flanks - error setting the metadata"}], [{"role": "user", "text": "Hello,\nI have been using usegalaxy.org (the main browser) since a few months and when I tried to log in today, it shows  \"This account has been marked deleted, contact your local Galaxy administrator to restore the account. Contact: galaxy-bugs@galaxyproject.org\"\nI really need my galaxy account as I have a lot of important data on it. I\u2019m sure this is a mistake. Kindly recover/ restore my account as soon as possible.\nMy email address on Galaxy: chegg0022@gmail.com\nAwaiting your reply."}, {"role": "system", "text": "This was probably not a mistake. I recommend reading through Accounts deleted from Galaxy Main to enforce posted Terms and Conditions"}], [{"role": "user", "text": "Hi,\nI\u2019m attempting to install a local version of Galaxy on a centos VM.  In order to get admin rights to the instance, it said I have to have a registered account.  I had one under one email, but the local galaxy install will not authenticate it.  It authenticates fine when I login to the main public galaxy site.\nI then registered a new account under a different email address (yeah I know, but it\u2019s broken so what am I supposed to do).  I made that account an admin, then I deleted the original account.  Then I tried to re-create the account, or just login, in the local instance but no luck as it says my email is already registered.  I tried logging in as a registered user on this local instance, tried creating a new account, and also tried to create a user from the admin interface and none of them worked at all.\nHow does this actually work?  When a user authenticates to a 127.0.0.1 loopback based local instance, is it actually checking somehow against your main registration server?  How do I have local users and also have an admin user?  How does any of this work?  If it\u2019s documented clearly somewhere, please advise as I searched the help and faqs and came up empty.\nAt this point I\u2019m just trying to test this thing and see if I can actually get it running on a VM for a user, I\u2019m not a genomics guy I\u2019m a sysadmin.\nFrom what I can tell, once a person registers an email address with you centrally, you can never use that email again on a local user install.  So even though it\u2019s local it phones home to mama to see if the user exists?  It just keeps saying that the account exists already, but won\u2019t actually authenticate to it because the password doesn\u2019t work.   So how do local users even get created and managed?  It requires an email address no matter what.  This is for my local install of galaxy not y\u2019all\u2019s system.\nI\u2019m seeing some suggested topics to browse and will try those but any help or links or suggestions at all will be greatly appreciated.\nThanks -"}, {"role": "user", "text": "Update - reinstalling galaxy on a different machine, I was able to re-register my account.  It also has admin privilege.\nFrom what I can tell, the local galaxy install keeps the user account and password hash local in the database, but does check the email address against the global galaxy system to see if the user is registered with galaxy?  This would explain why I was getting errors trying to re-register since it probably had a local db entry for my userid that the system didn\u2019t clean up, even after I deleted and purged the account.\nWould still like to understand how the admin piece works - that actually checks an email registration entry on you all\u2019s central public galaxy system?"}, {"role": "system", "text": "Hi crh\nJust to confirm, a local installation does NOT \u201cphone home to mama\u201d\nRegards, Hans-Rudolf"}, {"role": "system", "text": "There is no primary \u201cGalaxy\u201d instance. Various institutions provide compute resources managed by their own deployment of the Galaxy software. If you are thinking of \u201cusegalaxy.org\u201d, that is only one of many public instances of Galaxy. It just happens to be maintained by one of the same institutions that develops Galaxy. Galaxy does not attempt to connect to any other instance, with a few exceptions, and it must be explicitly configured to do so in those instances.\nTo give a user account within an instance of Galaxy admin privileges, you must add that users email address to the list of admins in your Galaxy config: https://docs.galaxyproject.org/en/master/admin/config.html#admin-users"}, {"role": "user", "text": "Thank you both for your replies.  I understand now, I had been under the impression that an admin had to be a \u201cregistered user\u201d and that the \u201cregistered user\u201d had to be registered centrally with the usegalaxy.org site in order to access the tool installs, etc.  Rather, it seems that an admin user just has to be registered locally on that galaxy instance, and added to the config file\u2019s admin list - which completely makes sense."}], [{"role": "user", "text": "As already discussed in github, when sending a galaxy tool error report, the TO and FROM address are the same.\nThough it is possible to set an email_from param in galaxy.yml, it is assigned to error_email_to when the report is sent.\n\n  \n      github.com\n  \n  \n    galaxyproject/galaxy/blob/0fa3ffa01b0d2c2ec542343e064156af94fa50c6/lib/galaxy/tools/errors.py#L243\n\n        return self._send_report(user, email=email, message=message, **kwd)\n\n\n\n\nclass EmailErrorReporter(ErrorReporter):\n    def _send_report(self, user, email=None, message=None, **kwd):\n        smtp_server = self.app.config.smtp_server\n        assert smtp_server, ValueError(\"Mail is not configured for this Galaxy instance\")\n        to_address = self.app.config.error_email_to\n        assert to_address, ValueError(\"Error reporting has been disabled for this Galaxy instance\")\n\n\n        frm = to_address\n        # Check email a bit\n        email = email or ''\n        email = email.strip()\n        parts = email.split()\n        if len(parts) == 1 and len(email) > 0 and self._can_access_dataset(user):\n            to = to_address + \", \" + email\n        else:\n            to = to_address\n        subject = \"Galaxy tool error report from %s\" % email\n        try:\n\n\n\n  \n  \n    \n    \n  \n  \n\n\nI wander why such a behavior. Can anyone tell me ?\nIn my case, I have a sender address imposed by the mail server but it is not linked to a real email box. I need to give a different email address as recipient."}, {"role": "system", "text": "Hello @eancelet\nThis is to prevent \u201cemail spoofing\u201d as described in the GitHub ticket you referenced.\nIf you want to ask the developers about this again, you can post to that question for an updated rationale.\nThanks!"}, {"role": "user", "text": "Thanks @jennaj for your response.\nSorry, I don\u2019t know this subject very well. Is it to prevent \u201cemail spoofing\u201d from Galaxy admins or from malicious persons throughout the internet ?"}, {"role": "system", "text": "@eancelet\nThe idea is to ensure that a bug report is sent in by the actual person who has control over that registered account.\nNow, bug reports themselves are fairly sparse: the error message and a few clues about what is going wrong. Only the account owner themselves or an administrator on that server would be able to make use of that content (for practical reasons like troubleshooting)."}], [{"role": "user", "text": "I find the left panel scrollbar of https://usegalaxy.org/ was hidden on my Google Chrome and Firefox 71 browser. My PC screen resolution: 1366 x 768\uff0cand I use Windows 7 x64.\nWhen will this problem be fixed\uff1f\n\ngalaxy-srollbar1366\u00d7726 186 KB\n"}, {"role": "user", "text": "Waiting for help."}, {"role": "system", "text": "Hi @istevenshen\nThe tool panel scroll bar itself is only activated once the tool panel is \u201chovered over\u201d with mouse/trackpad pointer and is scrolled. It will go away fast unless you keep the pointer near the scroll bar area and keep using it.\nOn a Mac, all three panel\u2019s scroll bars work this way (left tool, middle, and right history). From your screenshot, it appears that two of those three panels are persistent, or have longer display timing on a PC using Chrome.\nYou can see the same behavior here at Galaxy Help (hosted by a 3rd party software, in this case Discourse) when typing in a post/reply in the GUI editor window or reviewing posts. The scroll will display when used then hovered over. Once you hover over somewhere else \u2013 or stop scrolling \u2013 the scroll bar is not displayed anymore. The \u201cshow\u201d toggle timing is a little longer here than in Galaxy for me (Mac OSX), but it could just be due to OS/Browser version factors.\nThat said, I wouldn\u2019t expect two of the three scrolls on this view to ever be \u201cpersistent\u201d. This seems like something worth asking the Galaxy developers about. Ticket, that includes a link to a discussion at the Chrome developer website about recent scroll bar changes (not Galaxy related, but web-browser related). https://github.com/galaxyproject/galaxy/issues/9187\nI did a quick web search and that discussion about Chrome happened to show up first, plus it is recent and relevant. Your own web search would almost certainly find even more discussion about OS + Browser functionality differences.\nVery minor display differences between various OS and web browser versions are expected but should not impact basic usability. You can still scroll directly in the tool panel when using Galaxy, correct? And the scroll bar displays when in use?\nIf you cannot scroll at all (mouse or two fingers on a trackpad), that would be a different novel issue that has not been reported before (for Galaxy). Test to see if that is reproducible at other sites (including here) to find out if this is a Galaxy or OS/Browser issue\n\nIf reproducible (no scrolling at other sites), that is a browser or computer/OS or usage issue, not Galaxy.\nIf not reproducible at other sites, that would point to a Galaxy UX design problem we don\u2019t know about yet.\n\nThanks!"}, {"role": "user", "text": "Thank you very much for your detailed reply. Actually I can scroll directly in the tool panel when using Galaxy\uff0cit just doesn\u2019t look good on the page."}, {"role": "user", "text": "The real reason is that I changed the browser\u2019s default font size to small. When I changed the font in my browser to  Medium(Recommended) , everything was fine."}], [{"role": "user", "text": "I get the following error when trying to load any multiple sequence alignment on .org\n"}, {"role": "system", "text": "Hi @Peter_van_Heusden,\ncould you provide me with some additional details about how to reproduce this error?\nRegards"}, {"role": "user", "text": "Simple to reproduce - upload a FASTA format MSA, go to Visualize this data, click Multiple Sequence Alignment viewer and the error occurs. This is on usegalaxy.org, doesn\u2019t happen on e.g. usegalaxy.org.au."}, {"role": "system", "text": "Update Jan 30, 2023\nThis was fixed. If a new problem shows up, please open a new topic and include the link to this one for context. Thanks!\n\n\nHI @Peter_van_Heusden\nWe know of several visualization tools that have this same technical problem.\nPrimary issue ticket. When that closes out, expect the fix to show up at usegalaxy.org and test.galaxyproject.org soon after.\nAll UseGalaxy.* servers are impacted.\nLet\u2019s keep this topic open until fully resolved with testing in the wild \ncc @dannon"}], [{"role": "user", "text": "I\u2019ve been trying to run the circos tool on galaxy for the past couple of days however my jobs have all been reaching an error. This is not my first time using the application and attempting to rerun previous runs that  have worked in the past results in the same error message."}, {"role": "system", "text": "Hi @Vignesh_P, you need to provide more information before we can help you.\n\nDid you submit a bug report?\nWhat\u2019s the error message? What does it say?\nCan you share your history so we can see what you\u2019re doing?\n"}, {"role": "user", "text": "Here is the history https://usegalaxy.org/u/vnkp/h/unnamed-history-2\nand this is the resulting error message.\nerror1600\u00d7755 85 KB"}, {"role": "system", "text": "Hi @Vignesh_P, you\u2019re using an older version of the Circos Galaxy wrapper which has known issues. Please use a newer version like the one on UseGalaxy.eu. Try this one: https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/circos/circos/0.69.8+galaxy7\nAdditionally, perhaps the admins can update circos on .org? @jennaj"}], [{"role": "user", "text": "Hi,\nRecently, I tried use galaxy bowtie2 tool to mapping chip-seq data. However, some chip-seq data downloaded from SRA has been reported a warning as followed:\n##################################\nWarning: skipping read \u2018Run2_FC2_20100512_Sample1B_1279_21_112_F3/1\u2019 because length (0) <= # seed mismatches (0)\nWarning: skipping read \u2018Run2_FC2_20100512_Sample1B_1279_21_112_F3/1\u2019 because it was < 2 characters long\nWarning: skipping read 'Run2_FC1_2010\n################\nI am curious about this warnings? Is it fine to go on next step (samtools) or need to map the genome again with bowtie2? And how to solve this problem?\nThanks\nBest"}, {"role": "system", "text": "It appears that some of the reads are very short. This may or may not indicate a quality problem and could impact your decision to use this particular data. It could also just be how the data was already trimmed and/or filtered (or not). If you did some trimming on the data yourself, consider examining a sample of the reads before and after, to confirm the appropriate settings were used.\nToo-short reads will fall out during mapping. This isn\u2019t a problem to worry about unless 1) a significant portion of the data is very short/unusable or 2) a mapping job fails for memory/resource reasons \u2013 then filter out short reads that would never pass the initial mapping criteria to reduce the size of the job.\nIn short, run FastQC and review the results.\n\n\nGalaxy Tutorials >> Start here: NGS logistics - this is an introduction to Galaxy\u2019s functionality for the analysis of Next Generation Sequencing data.\n\nFastQC Help >> Start here for how to interpret the report\n"}, {"role": "user", "text": "Thanks for your help.\nIn fact, fastqc reported these data have the average length (50bp). So I have done the trimming and filter the low quality and min length (<30). However, it reported the same warnings. According to your advise, this warning would not affect the followed steps. So, I can continue to next step, am I understanding right?\nThanks"}, {"role": "system", "text": "\n\n\n skytguuu:\n\nmin length (<30)\n\n\nDid you retain the fastq reads over 30 bases or under? Double check the settings used for filtering. Bowtie2 is reporting a read found that is 0 bases long. My guess is that you probably intended to filter for >=30 bases?"}], [{"role": "user", "text": "Hi,\nI constructed a pipeline like \u2018fastp> RNA STAR> featureCounts> DESeq2\u2019, and exploited the built-in genome of each of RNA STAR and featureCounts.\nThe built-in genome of RNA STAR and featureCounts must be different. Will there be any problem with this use?\n(I don\u2019t know, but I was told that the same genome should be used for mapping and counting)."}, {"role": "system", "text": "Hi @spring\n\n\n\n spring:\n\nThe built-in genome of RNA STAR and featureCounts must be different. Will there be any problem with this use?\n\n\nYes, this will be a problem, and it may not be obvious or easy to detect. The Deseq may not technically fail but instead produce putatively correct \u201cgreen\u201d dataset results that have scientific content issues.\nWhat is your use-case/goal?"}, {"role": "user", "text": "Thank you for answer.\nI have constructed the above pipeline to analyze the differentially expressed genes of the control and treatment group.\nThen in order to use the same reference genome, should I use Comprehensive gene annotation (CHR) at the top of \u2018GTF/GFF3 files\u2019 as input of featureCounts and Transcript sequences (CHR) at the top of \u2018Fasta files\u2019 as input of RNA STAR at GENCODE - Human Release 38 ?"}, {"role": "system", "text": "Hi @spring\nGencode is a good source for reference annotation for hg38. Be sure to format the data correctly after loading it into Galaxy \u2013 specifically for this source, that means removing the header lines.\nThen, use the built-in hg38 reference genome when mapping your reads.\nFAQs: Galaxy Support\n\nDatatypes\nHelp for Differential Expression Analysis\n\nTutorials: Galaxy Training!\n\nChoose the topic Transcriptomics\n\nI also added some tags to your post that link to prior Q&A around this type of analysis. You can also try a search with tool names, as that sometimes find more exact examples of troubleshooting/use-cases.\nThanks!"}], [{"role": "user", "text": "For the past few days I have been trying to use Cufflinks for my RNA seq analysis but I have been repeatedly getting this error:\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nThis started happening a few days ago and up until then I had no issues running the tool at all.  My inputs are BAM file obtained from HISAT2 alignment and gtf file for mm10 reference annotation.  I noticed that the assembled transcripts gtf file still outputted from Cufflinks despite the error, so I tried inputting these assembled transcripts into Cuffmerge (along with some successfully obtained assembled transcript gtf\u2019s before this error started recurring), but Cuffmerge also resulted in an error:\n[Fri Aug 28 23:06:48 2020] Preparing output location output/\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 580, in \nsys.exit(main())\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 538, in main\ngtf_input_files = test_input_files(transfrag_list_file)\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 268, in test_input_files\ng = open(line,\u201cr\u201d)\nIOError: [Errno 2] No such file or directory: \u2018input_1 input_2 input_3 input_4 input_5\u2019\nI have been able to successfully use this tool without any problems up until now.  Any solutions would be appreciated; I\u2019m still relatively new to Galaxy."}, {"role": "system", "text": "Hi, it\u2019s been the same for me but with Stringtie\nFrom their error: slurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nI found that people outside of Galaxy Project can have problems with slurmstepd, so there\u2019s probably something wrong inside GP rather than with your jobs\nI\u2019ll write them a report and see if they know what\u2019s happening."}, {"role": "system", "text": "@maxininfa @jdyee\nThanks for reporting the problems! We are reviewing. More feedback soon.\nUpdate:\nThe issue with Stringtie has been confirmed and isolated. Details: https://gitter.im/galaxyproject/dev?at=5f4eb3e449148b41c98a2b6c\nThe issue with Cufflinks appears to be similar but that is not confirmed. Cufflinks is a deprecated tool and is no longer supported. However, you can certainly try the same options below and see what results.\nNote: This part of the error message is just a log message about rerouted resources and isn\u2019t related to the bug uncovered related to Pulsar/Jetstream/Stringtie. Even successful jobs can report it in a log message. These two tools, plus others.\n\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\n\nFor Stringtie, our administrator @nate will be re-routing all jobs for this tool to the cluster where it works while resolving the root bug. Not sure that is done or not yet (testing in progress), but you can try two ways to get the tool run successfully at the cluster where it works right now \u2013 you don\u2019t need to wait for us to fix it the problem or to confirm the temporary configuration change. You can try both ways at the same time to get this done quicker (and purge one of the results to recover disk space should both work).\n\n\nRerun again using defaults for the \u201cJob Resource Parameters\u201d. This is probably what you are doing now \u2013 your two matched jobs were sent to different clusters, and one is problematic for this tool (Jetstream). If this is successful, then you can keep using this method. Is simpler, and once this bug is fixed, you\u2019ll want Galaxy to choose the appropriate cluster for most use cases (this tool and others). Jobs are scheduled to clusters based on data/tool factors and cluster node availability at any particular time.\nScreenshot:\nallow-galaxy-to-choose-the-cluster1366\u00d7128 5.75 KB\n\n\nRerun again, choosing the cluster to send the job to. Under \u201cJob Resource Parameters\u201d set the option to  \u201cGalaxy cluster (default)\u201d. The problem is at the \u201cJetstream\u201d cluster \u2013 and this action specifically avoids it.\nScreenshot:\nchoose-the-cluster1362\u00d7582 40.8 KB\n\n"}, {"role": "system", "text": "Related: BWA and BWA-MEM also impacted. See: BWA-MEM Index can't be found\nThe workaround is the same \u2013 target the \u201cGalaxy cluster (default)\u201d and avoid \u201cJetstream\u201d for these tools."}, {"role": "system", "text": "This was fixed as of September 22, sorry for not updating here. Please let us know if you\u2019re still encountering issues."}], [{"role": "user", "text": "\nimage1273\u00d7375 13 KB\n"}, {"role": "system", "text": "Hi @Jayanthi_Ramprakash,\nI have informed the sysadmins; please, use the usegalaxy.eu instance meanwhile.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nusegalaxy.eu doesn\u2019t have repeatmasker tool but it has repeatmodeler. Tutorial is based on repeatmasker!!!\nRegards"}, {"role": "system", "text": "HI @Jayanthi_Ramprakash,\nyes, that tool is available in usegalaxy.eu, but there\u2019s a problem in the tool browser, so it is not easy to find it: repeatmasker.\nRegards"}], [{"role": "user", "text": "Hello\nWhat is the best tool for deduplication for my data?\nlets say i have more than 50% duplication in my reads, how should i remove them.\nI saw some tool but im not sure which one is best for my analysis.\nThanks"}, {"role": "system", "text": "Hi @amir\nMapping the reads, marking duplicates in BAM results, then removing or ignoring them is one way. Samtools and Picard have tools to mark and/or remove duplicates.\nVariant analysis tutorials have example workflows. For other protocols, search with the keyword \u201cduplicates\u201d or review the protocols themselves as many include QA/QC steps for addressing PCR duplicates. https://training.galaxyproject.org/\nYou can also search the tool panel \u2013 some tool suites have tools specific to the analysis goals, and not all are incorporated into a GTN tutorial (external documentation is usually linked on the tool form).\nThe question is pretty broad. If you have some specific analysis goal in mind, and can\u2019t find help in the above resources, describe what you are doing more. Read protocol, planned analysis tools/goal, anything else special about your data (how the 50% duplicate rate was determined? if done already)."}, {"role": "user", "text": "I am working on Small RNA seq (miRNA) and when I discard GC and duplication, My counts are dropping very much.\nSpecially when i correct GC percentage this will happening.\nIn miRNAs should i even do anything with GC and duplication level or let it be ?\nBest."}, {"role": "system", "text": "Hi @amir,\nin the case of miRNA, I suggest you go on with the analysis without considering the GC and duplication content for this reasons:\n\nmiRNA sequences usually contain overrepresented motifs that could interfere with the real duplication values (because in many cases they have been originated from duplications).\nThe miRNA GC content is quite variable, and depending of the organism, the miRNAs can be enriched in those nucleotides (Lam et al., Mishra et at.).\n\nWhat tool did you use to perform the alignment?\nRegards"}], [{"role": "user", "text": "Hi,\nThis is a complete beginner question and this probably belongs to the RTFM section. However, there is obviously a logic that I do not understand in galaxy xml.\nWhat I want to do (independently of whether or not you can do that with existing tools in galaxy), is to give a list of bed files to my tool, and output a tab like:\n#filename        #nb of lines\nfile1                  4645464\n...                      ......\nfilen                  346354345\n\nHere is my xml file:\n<tool id=\"bedcounts_summary\" name=\"Counts and summarizes line number for each bed file of a list\" version=\"0.1.0\">\n    <requirements>\n    </requirements>\n    <command>                                                                                                                                \n      bash $__tool_directory__/bedcounts_summary.sh #for $key in $bedlist.keys() # $key $bedlist[$key] #end for                              \n      >> $outputtab                                                                                                                          \n    </command>\n    <inputs>\n      <param type=\"data_collection\" format=\"bed\" name=\"bedlist\"/>\n    </inputs>\n    <outputs>\n      <data name=\"outputtab\" format=\"tabular\"/>\n    </outputs>\n    <help><![CDATA[                                                                                                                          \n        TODO: Fill in help.                                                                                                                  \n    ]]></help>\n</tool>\n\nHere is the bash script:\n#!/bin/bash                                                                                                                                  \n\nbedfilename=$1\nbedfile=$2\n\ncountslines=`wc -l < $bedfile`\n\necho -e \"$bedfilename\\t$countslines\"\n\nThis is returning only the first line. I managed to have all lines using a simple echo but that is not the point, obviously.\nIf you can point to my logic/programming error that would be very helpful!\nThanks!"}, {"role": "system", "text": "Do you want it to specifically be a tool?\nbgruening\u2019s awk tool in the main toolshed will readily do this.\nI have a more feature rich version that is pending a PR to that linked repo that you might be interested in."}, {"role": "user", "text": "yes I would like to understand why it is not working, not to leave it unsolved. I might end using awk indeed."}, {"role": "user", "text": "by the way, I was able to do the same thing using:\n\n\nLine/Word/Character count  of a dataset (Galaxy Version 1.0.0), taking care of not including the header\n\nCollapse Collection  into single dataset in order of the collection (Galaxy Version 4.0), enabling the option \u201cAppend file name\u201d\n\nBut if one see the mistake in the code above, it would be useful to know."}, {"role": "system", "text": "It is because you are passing your script two arguments per collection element but your script only takes two arguments.\nYou need to wrap the entire call to your script in the #for block.\nLook at the command in the Info section of the tools output. You will see your mistake."}], [{"role": "user", "text": "Preparing for an installation of Galaxy:\nI am asking with a simple multicore VM i mind. No cluster.\nHow will Galaxy handle multithreading? Can I define globally the amount of available CPUs somewhere and then the tools will use the availble CPUs. Or what is the strategy?"}, {"role": "system", "text": "Maybe this helps https://galaxyproject.org/admin/config/galaxy_slots/.\nFor a more detailed answer I think a developer need to help you. In the file config/job_conf.xml you can specify the cores. This \u201cvalue\u201d can be used in the XML wrapper by using the GALAXY_SLOTS variable.\nBut what I want to add and maybe this will partly answer your other questions as well. Keep in mind that in the basics galaxy is a \u201cwrapper\u201d, \u201cshell\u201d, \u201cUI\u201d around the command line. So if you execute a tool it just executing a command. (galaxy can do much more but for this is enough for your question).\nSo galaxy itself is not really handling the multithreading, trough galaxy you tell BWA for example how many threats it need to use and BWA handles the multithreading."}, {"role": "user", "text": "So the prepared tools (on the Tool Shed, assuming they are configured accordingly) will understand GALAXY_SLOTS and automatically use the appropriate number of cores?\nAs an example: Lets say I move from 8 to 16 cores and I just adjust GALAXA_SLOTS, most tools should switch to 16 cores?"}, {"role": "system", "text": "Uhm I am no expert but yes, but it depends how the tools are created but think most tools in the toolshed follow all the official guidelines.\nIn your case if you switch from 8 to 16 cores you only need to adjust config/job_conf.xml"}], [{"role": "user", "text": "Hi Wolfgang,\nI also got a error recently when I used MiModD Variant Calling tool in public Galaxy server (https://usegalaxy.org/) after aligned with Bowtie2.\nI tried to repeat this step (by \u2018Run Job Again\u2019) in a previous successful analysis case and got the same error. Could you give me any suggestions?\nThanks.\nError code:\n/corral4/main/jobs/051/303/51303573/tool_script.sh: line 10: mimodd: command not found.\nJob Information\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varcall/0.1.9\nCommand Line empty\nTool Standard Output empty\nTool Standard Error\n/corral4/main/jobs/051/303/51303573/tool_script.sh: line 10: mimodd: command not found\nTool Exit Code: 127\nJob Messages\n{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 127 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 127, \u201ctype\u201d: \u201cexit_code\u201d }"}, {"role": "system", "text": "Hi @WangNIE\nThanks for posting back the logs. The message \u201cmimodd: command not found\u201d could be a cluster configuration issue OR some input issue the tool doesn\u2019t understand how to handle.\nExamples: an empty or header-only BAM could be a trigger. A mismatched reference genome or poor alignment rates would probably produce something more informative. If that BAM was from Upload and not a job run in Galaxy, or from using a custom reference genome+build, more could be going on data-wise leading to the error.\nWould you please generate and share back a link to the history? Troubleshooting errors\nIf you are following a tutorial, or using tutorial data, please also link that back for context.\nLet\u2019s start there "}, {"role": "user", "text": "Hi Jennifer,\nThanks for your kind reply. My history link is (Galaxy). And I submited a bug report just now. Actually I tried re-running the MiModD Variant Calling (by \u2018Run Job Again\u2019) in a previous successful analysis history but got the same error (command not found).\nWolfgang said this maybe an issue with MiModD installation in usegalaxy.org.\nWarm regards,\nWang Nie"}, {"role": "system", "text": "Hi @WangNIE\nWe have confirmed the problem at the US server. Please try at EU or AU for now.\nConsolidating this this post, where we will post updates: MimodD extract variant sites\nThanks for the followup!"}], [{"role": "user", "text": "Hi everyone,\nI am new to Galaxy and I am facing a conda environment problem lately on the usegalaxy.org Main server.\nI am trying to map my data on the human genome using the RNA STAR tool but whenever I execute my jobs on the usegalaxy.org server, I have this error:\n\nFailed to activate conda environment! Error was:\n/jetstream/scratch0/main/jobs/41688358/command.sh: line 110: /cvmfs/main.galaxyproject.org/deps/_conda/bin/activate: No such file or directory\n\nand all of my files are empty.\nI tried to do the same mapping step with HISAT2 and TOPHAT tools but I still face the same error.\nHowever, I tried to follow the galaxy tutorial on RNA-seq analysis Reference-based RNA-Seq data analysis and I do not have that problem.\nCan someone tell me if I am doing something wrong or is it problem occurring within the server?\nThank you!"}, {"role": "system", "text": "It is possible that the server has a problem, perhaps the directory really does not exist"}, {"role": "system", "text": "Hi @MPG\nIf the tutorial data works, but your data does not, with the same tool \u2013 that indicates a problem with your data.\nQuestions:\n\nHave you tried a rerun yet? This error usually occurs for a very small fraction of jobs and is not reproducible (small cluster hiccup). But I\u2019ve also seen problematic inputs be the root reason when reproducible.\nIf a rerun also fails\u2026\n\nAre you running the most current version of the mapping tool(s)? Which? Please post back the tool names and versions for each, or better, try using the latest version(s) first, then post back which were used/failed.\nDoes a tool like FastQC execute properly against the fastq input datasets?\nHow are the read data organized? Single end or paired? Individual datasets, multiple datasets, or a dataset collection?\nAre you using a built-in indexed for the mapping (available in the drop-down menu on the tool form) or using a custom reference genome (fasta from the history)?\n\n\n\nLet\u2019s start there. The reruns/read QA you should first, since it might find uncover the problem. And if not, we\u2019ll ask you to share the history with those test runs in it, review the details, and try to help more."}, {"role": "user", "text": "Hi @jennaj,\nThank you for your answer.\n1. Have you tried a rerun yet? This error usually occurs for a very small fraction of jobs and is not reproducible (small cluster hiccup). But I\u2019ve also seen problematic inputs be the root reason when reproducible.\nYes, I tried to rerun the mapping but I still have errors.\n\nIf a rerun also fails\u2026\n\nAre you running the most current version of the mapping tool(s)? Which? Please post back the tool names and versions for each, or better, try using the latest version(s) first, then post back which were used/failed.\nI am using the tools available on the usegalaxy sever:\nRNA STAR Gapped-read mapper for RNA-seq data (Galaxy Version 2.7.8a+galaxy0)\nHISAT2 A fast and sensitive alignment program (Galaxy Version 2.2.1+galaxy0)\nTopHat Gapped-read mapper for RNA-seq data (Galaxy Version 2.1.1)\nThis is the error I get for each tool:\n\nCapture d\u2019e\u0301cran 2022-03-31 a\u0300 20.02.461874\u00d7198 69.6 KB\n\nDoes a tool like FastQC execute properly against the fastq input datasets?\nYes, I started with a FASTQC quality control step, trimmed my adapters with the trim galore! tool (output format: fastqsanger) then reran FASTQC/MultiQC to check if my files were ready to use for mapping and every step executed properly.\n\nCapture d\u2019e\u0301cran 2022-03-31 a\u0300 20.19.241920\u00d7910 87.5 KB\n\nHow are the read data organized? Single end or paired? Individual datasets, multiple datasets, or a dataset collection?\nI have paired-end reads and I select multiple datasets.\nAre you using a built-in indexed for the mapping (available in the drop-down menu on the tool form) or using a custom reference genome (fasta from the history)?\nYes, I am using a built-in reference genome (the human genome (hg38)) without a built-in gene model. As for the gene model, I specify .gtf file (GRCh38) downloaded from the ensemble database. Then, I specify the length of the genomic sequence around annotated junctions (ReadLength-1) and I execute my job.\n\nCapture d\u2019e\u0301cran 2022-03-31 a\u0300 20.11.051500\u00d71250 162 KB\n\n\nCapture d\u2019e\u0301cran 2022-03-31 a\u0300 20.11.321466\u00d71184 150 KB\n\nI just want to specify that the tutorial I am following treats a different genome (dm6) and I had no problem with the files.\nSo I do not know if I am using the wrong file formats or not the adequate tools but I hope someone can enlighten me on this matter.\nThank you."}, {"role": "system", "text": "Hi @MPG\nCheck these two items. The first may or may not be OK (worth checking), and the second is likely a content problem (mismatched chromosome names). Both might need to be adjusted.\n\n\n\n MPG:\n\nI have paired-end reads and I select multiple datasets.\n\n\nMake sure that the pairs are entered in the same order on the tool form, and that none of the ends of any pairs are omitted. You can click into the \u201ci\u201d Job Details page to see what was originally submitted to check (even for failed jobs). If you have many paired end datasets, using dataset collections would help: Search Tutorials\n\n\n\n MPG:\n\nYes, I am using a built-in reference genome (the human genome (hg38)) without a built-in gene model. As for the gene model, I specify .gtf file (GRCh38) downloaded from the ensemble database.\n\n\nThe reference genome was sourced from UCSC when we indexed it on the server.\nAll other inputs must use that same genome/build, including the same chromosome naming formats, or tools will either ignore the mismatched input or trigger a failure.\nThe reference annotation (gene model) was sourced directly by you\u2026 from Ensembl (?). They use a different chromosome name format than UCSC.  Where to get the annotation with UCSC chromosome names is in this prior Q&A:\n\n\n\nStringtie merge error\n\n\nThe hg38 reference annotation GTF is now best sourced from UCSC. Not from the Table Browser but from the Downloads area here . There are a few Gene tracks to choose from. You can review what each of those represents then upload the selected GTF to Galaxy with the Upload tool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\n\n\nNothing else pops out. So, please try using the UCSC GTF instead with rerun(s). If any still fail, please send in a bug report from one of the red error datasets, and include a link to this topic in the comments so we can link the two for context. Also, please leave all datasets undeleted (inputs + outputs, successful or failure) for review.\nThanks!"}], [{"role": "user", "text": "Hello,\nI attempted to input two WGS data sets (two different sets of reads) and a reference genome from NCBI into the BWA-MEM2 tool. The job failed and resulting in the following messages:\nExecution resulted in the following messages:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nLooking to launch executable \u201c/usr/local/bin/bwa-mem2.avx2\u201d, simd = .avx2\nLaunching executable \u201c/usr/local/bin/bwa-mem2.avx2\u201d\n[bwa_index] Pack FASTA\u2026 22.12 sec\n** Entering FMI_search*\ninit ticks = 173525096758\nref seq len = 5422419662\nbinary seq ticks = 119852397722\nAllocation of 40.40 GB for suffix_array failed.\nCurrent Allocation = 45.45 GB\nIf it helps troubleshoot this issue, I have included some information about the job and the parameters used below."}, {"role": "user", "text": "BWA-MEM2 on data 1, data 8, and data 5 (mapped reads in BAM format)\nDbkey: bosTau9\nFormat: bam\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2/2.2.1+galaxy0\nTool Parameters\nWill you select a reference genome from your history or use a built-in index?\nHistory\nUse the following dataset as the reference sequence\nData 5: fasta.gz, 1957 sequences, 801.4 MB, bosTau9\nSingle or Paired-end reads\nPaired\nSelect first set of reads\nData 8: fastqsanger.gz, 25.2GB, bosTau9\nSelect second set of reads\nData 1: fastqsanger.gz, 25.7 GB, bosTau9\nEnter mean, standard deviation, max, and min for insert lengths.\n150\nSet read groups information?\nDo not set\nSelect analysis mode\nIllumina\nBAM sorting mode\nSort by chromosomal coordinates\nJob Resource Parameters\nNo\nJob Metrics\nCores Allocated: 10\nMemory Allocated (MB) 29995\nJob Runtime (Wall Clock): 6 minutes\nmeminfo\nTotal System Memory: 28.6 GB\nTotal System Swap: 0 bytes"}, {"role": "system", "text": "Hi @edf393\nThe FAQs explain what you can do in general to continue working at a public Galaxy server. In short: use an indexed genome as the mapping target (bosTau8 instead) + split the query reads into a collection, run some QA, map, then merge the results.\nOr, you can consider setting up a private Galaxy server with scaled up resources.\nDetails:\n\nThe job failed for exceeding memory limits at the public Galaxy server FAQ.\nSpecifically:\n\nThe fastq datasets are too large \u2013 ~25 GB compressed per end. FAQ\n\nThe fasta custom genome is too large, plus has a formatting problem. FAQ\n\n\n\n"}], [{"role": "user", "text": "\nimage1104\u00d7666 24.1 KB\n\nI was trying to upload the main annotation GTF file (after downloading it to my computer from the GENCODE website) but I\u2019m getting this error. The upload happens normally for a few seconds but halts right after the 3% mark every time.\nHow to resolve this? I tried different browsers as well but the status displays the same error regardless."}, {"role": "system", "text": "Hi @sonakshi,\ncould you try to compress it into gzip format before uploading it? It will reduce the size considerably.\nRegards"}, {"role": "system", "text": "I have the same problem. I used the files which a few months ago worked perfectly and they are not uploading now. I also tried to use Galaxy Australia and the files upload normally - but the server does not contain the required tools, thus I have still not solved my problem\u2026 any suggestions on what is happening?"}, {"role": "system", "text": "@sonakshi Uploading through the website are limited to files not bigger than 1 GB. For larger files, you can use our FTP service. All the details here Galaxy Europe | FTP Configuration"}], [{"role": "user", "text": "The Tetrahymena thermophila isn\u2019t indexed. Is this possible? It can be found at\nhttp://ciliate.org/index.php/home/downloads"}, {"role": "system", "text": "Hi,\nwhich index are you talking about? A simple fasta index or an index for a specific tool?\nGenerally, the situation with T. thermophila is a bit complicated by the fact that it has a Mic and a Mac genome and you would likely want a different one of those depending on the question you\u2019d like to answer.\nNote however that you can always upload any reference genome in FASTA format to your history, and most tools will be able to generate required indices on the fly from that.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hello! I had an account under the email redacted for couple of years, and I had no issue with logging into my account. But now it is asking me to re-enter the password upon login, which I didn\u2019t remember the password. When I clicked \u201creset password\u201d, it was showing \u201cFailed to produce password reset token. User not found\u201d and did not allow me to reset the password. I\u2019m very confused with this error and hope to get some help finding my account and history back. Thank you for your time."}, {"role": "system", "text": "Hi @Shuangshuang_Chen\nDoes this explain the problem? Can I create multiple Galaxy accounts?\nIt could also be that you are looking for your account at a different server than where it was located. Accounts are server-specific. Check your email history for the initial registration email. That would have the: server URL, administrative contact information, terms of service. Contacting those administrators is how to get private help for account issues.\nI\u2019m going to redact your email address for privacy reasons."}, {"role": "user", "text": "Hi. Thank you very much for your information. But I only had one account at the point where it got deleted then I created a second account. I\u2019ve been always using the same server on my laptop. I\u2019m going to email the support email in the registration email. Do you have any other advice? It would be great to find my original account back. Thank you for your time."}], [{"role": "user", "text": "the jobs shows it\u2019s been waiting to run for two days ,what\u2019s wrong with this tool? please help me !"}, {"role": "system", "text": "Hey arronwang,\nGalaxy was currently under maintenance (two days). The cluster accepted jobs, but it had limited resources, and thus many jobs stayed in the queue. Your job should run by now, if this is not the case, then please resubmit your job."}], [{"role": "user", "text": "Hello. Why do readings outside of genes appear in an RNAseq experiment in IGV? The readings are supposed to match exons, right?\nThank you"}, {"role": "system", "text": "It is possible to have reads align to regions of the genome not specifically targeted by the library construction/sequencing method.\nBAM datasets can filtered to reduce spurious hits, for example, by mapping quality, concordantly aligned paired reads. And can be ignored in some protocols as they are excluded by the analysis tools in various ways.\nSome Galaxy tutorials that might be helpful if you are just getting started: https://galaxyproject.github.io/training-material/topics/introduction/\nWith all here, including those that cover RNA-seq:\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy server choices. If a tutorial is supported by a pre-configured Galaxy Docker training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n"}, {"role": "user", "text": "Yes, but what is the cause of these alignments in the intergenic regions.\nThank you."}, {"role": "system", "text": "At least two possibilities:\n\n\nThese are real RNA molecules that were in your library. poly-a selection in RNA-seq is not 100% efficient, so these could be ncRNAs, or just spurious transcription\n\n\nAlignment error: alignment is also not perfect, some reads may be placed in intergenic regions incorrectly.\n\n\nThis is typically not something to worry about. Downstream, if you assemble transcripts or quantify using an existing transcriptome, these will be ignored."}], [{"role": "user", "text": "Hello,\nI have been unable to connect to the network using FTP. I am using FileZilla and am getting these messages when I try to connect.\n|Status:      |Resolving address of ftp.usegalaxy.eu|\n|Status:      |Connecting to 132.230.223.103:21\u2026|\n|Status:      |Connection attempt failed with ENETUNREACH - Network unreachable.|\n|Error:        |Could not connect to server|\nI am using ftp.galaxy.eu for the Host name and using my login credentials. I\u2019ve also tried setting Port to 21.\nI have been able to connect to the usegalaxy.org network by selecting \u201crequire implicit FTP over TLS\u201d in settings.\nI appreciate any help!"}, {"role": "system", "text": "\n  \n      \n\n      galaxyproject.eu\n  \n\n  \n    \n\nFTP service instructions\n\n  The European Galaxy Instance\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt is ftp.usegalaxy.eu\nEDIT:\nSorry, I see now that you indeed used the correct adress in the error message you shared. I only looked at this part:\n\nI am using ftp.galaxy.eu  for the Host name and using my login credentials. I\u2019ve also tried setting Port to 21.\n"}, {"role": "system", "text": "@ryzhu Today, we are under maintenance and all our services, except for the front page, are off.\nTry again tomorrow"}, {"role": "user", "text": "Hi,\nI\u2019ve tried connecting again today and it still hasn\u2019t been working. I am still getting the error: ENETUNREACH - Network unreachable. Is there something else that could be the issue?\nThanks."}, {"role": "system", "text": "The FTP service is still off because the maintenance is still ongoing.\nWe don\u2019t have, at the moment, a time estimation when it will finish.\nCheck regularly the usegalaxy.eu homepage, we are posting updates there."}], [{"role": "user", "text": "Hi there,\nI am trying to integrate galaxy docker image on my platform and I am having issues with how galaxy docker image resolves GALAXY_URL.\nThe thing is next:\nWhen we make request to the galaxy, request flows through few proxies, and the last one, let\u2019s call it Proxy Z is on another domain, let\u2019s call it z.proxy.com. Galaxy container is running behind that proxy, thus GALAXY_URL is pointing to z.proxy.com\nI would like to know if it\u2019s possible to configure GALAXY_URL to point to our organizational public domain, instead of the proxy\u2019s.\nWe are using at the moment latest stable galaxy docker image 20.09 Docker running 20.09 galaxy version"}, {"role": "system", "text": "Hi @heisenberg\nDoes this documentation help? #Galaxy-behind-proxy\nIf not, please consider asking a question at the dedicated chat bgruening/docker-galaxy-stable - Gitter. You can include a link to this thread in your post for context. One item to clarify is the goal a bit more \u2013 do you want to change the base URL that users see/access or the underlying API configuration? I\u2019m guessing the first but please confirm."}, {"role": "user", "text": "Hi @jennaj\nI\u2019ve already checked the documentation and there\u2019s nothing about how GALAXY_URL variable is resolved. This is the variable that is used for generating response links in Get Data section, and it\u2019s also used when fetching welcome page.\nfor ex. this is where GALAXY_URL is used, when openning UCSC Archaea table browser in Get Data section:\n\nScreenshot 2022-12-16 at 11.43.381920\u00d71014 212 KB\n\nI\u2019ll write in chat you\u2019ve provided, thanks!"}, {"role": "system", "text": "heya, adding some info to this:\nit looks like GALAXY_URL is built from the request\u2019s HOST header field; if i change my etc/hosts file and navigate to galaxy with the added host, then use the UCSC Get Data tool, the request looks like:\nGET\nscheme: http\nhost: somedomain.bork:8080 <==== my addition to /etc/hosts\nfilename: /tool_runner/data_source_redirect\ntool_id: ucsc_table_direct_archaea1\nAddress: 127.0.0.1:8080\n\n\u2026and the response looks like:\nHTTP/1.1 302 Found\nServer: nginx/1.14.0 (Ubuntu)\nDate: Wed, 04 Jan 2023 21:31:38 GMT\nContent-Type: text/html; charset=UTF-8\nContent-Length: 591\nConnection: keep-alive\nx-frame-options: SAMEORIGIN\nLocation: http://archaea.ucsc.edu/cgi-bin/hgTables?GALAXY_URL=http%3A//somedomain.bork%3A8080/tool_runner&tool_id=ucsc_table_direct_archaea1&sendToGalaxy=1&hgta_compressType=none&hgta_outputType=bed\n\n(note the GALAXY_URL=http%3A//somedomain.bork%3A8080/)\nare you navigating from the desired URL in your browser, but getting the downstream proxy URL when redirected?\nhere\u2019s some more random info in case it\u2019s useful:\nusing the \u2018Get Data \u2192 UCSC Archaea table browser\u2019 follows this link:\n\n$URL/tool_runner/data_source_redirect?tool_id=ucsc_table_direct_archaea1\n\n\u2026which uses this chunk of logic to check the galaxy tool:\n\ngalaxy/ucsc_tablebrowser_archaea.xml at release_20.09 \u00b7 galaxyproject/galaxy \u00b7 GitHub\n\nregards,\n\nLuke\n"}], [{"role": "user", "text": "Hi there! I have reached my disk quota, and therefore am trying to purge my history that is using up all of the storage. However, when I click \u201cpurge\u201d the screen freezes and does not purge the history. Any suggestions on how to fix this?\nThanks!"}, {"role": "system", "text": "Hi @jradfor8\nYou are working at UseGalaxy.org, correct? The function is working correctly (just tested it to be certain).\n\n\n\n jradfor8:\n\nHowever, when I click \u201cpurge\u201d the screen freezes and does not purge the history\n\n\nThe screen may fade out in the middle panel under the User > Histories view when large amounts of data are permanently deleted-purged. Allow that process to complete before navigating away.\nYou may also need to then log out and back in again to reset the updated quota usage. The server will eventually catch up but that triggers the quota usage recalculation to run quicker.\nNote: If a history is in a shared state, you may not be able to purge it. Unshare, then purge.\nFAQ: Account quotas\nThanks!"}], [{"role": "user", "text": "Dear admin,\nI cannot access my Galaxy account for the past week, whenever I tried to login to my account, the system states that it fails to submit email and requires me to contact the administrator. I also tried resetting the password but failed as well.\nI further attempted to register a new account using a different email, unfortunately no verification email was sent to me and log-in access was not granted as the system states I have not activated my account.\nAs I am using the Galaxy system for my university assignment, I need to prove that it was a system error with some evidence such as an email reply.\nKindly advise me on this matter. Thank you for your attention."}, {"role": "system", "text": "Hi @Steffi_Chan\nTo help, we need to know which Galaxy server are you working at. Please post back the server URL."}, {"role": "user", "text": "Hi @jennaj,\nThank you for your prompt reply. My Galaxy server is http://galaxy.rhc.ac.ir"}, {"role": "system", "text": "@Steffi_Chan I\u2019m afraid you need to contact the admins of this particular server. Please have a look here: http://galaxy.rhc.ac.ir/static/contact_us.html"}], [{"role": "user", "text": "Hi. I am doing an RNA-seq study and I want to group in a single file all the counting data of my six samples obtained with HTseq, to be able to visualize them all together. Is there any tool in Galaxy to do this? Thank you."}, {"role": "system", "text": "Hi @julieta, I haven\u2019t run htseq-count in a while, but I think this will work.\nThe Setup\nhtseq-count produces a two column output. The first column is the feature name (from the GFF) and the second is the overlap count.  This is shown for all features, even if there are 0 overlaps.\nWhere to Look\nFor questions where you have multiple datasets, with a common column, and you want to unite them into a single file, start with these two toolboxes in the tool panel (on the left): Text Manipulation and Join, Subtract and Group.\nWhat tio do (I think)\nThe first thing with promise under Text Manipulation is Multi-Join, and I think his will work. Pick one of your htseq-count outputs as the File to Join and then select the rest in the Add additional file list.\nCommon key column is 1\nColumn with values to preserve is 1 and 2\nAdd header line to the output file is yes\nInput files contain a header line is yes (mine did)\nYou can then use the Text Manipulation \u2192 Cut operator to rearrange the columns."}], [{"role": "user", "text": "I obtained the transcript count file using StringTie and now I am trying to run the count file on DeSeq2 using galaxy server. I am getting the following error.\n\nThe design matrix has the same number of samples and coefficients to fit, so estimation of dispersion is not possible. Treating samples\nas replicates was deprecated in v1.20 and no longer supported since v1.22.\n\nI don\u2019t know how to fix this error, please help!"}, {"role": "system", "text": "Hi @vedantbharti,\nhow many replicates do you have for each condition? it indicates that it is not possible to perform any statistical analysis."}, {"role": "user", "text": "I have 1 transcript count file for each factor level. I have 2 factor levels. One wild type and another with one particular gene knocked out. Please help!"}, {"role": "system", "text": "HI @vedantbharti,\nyou need to provide more than one replicate, otherwise the statistical analysis is not possible.\nRegards"}], [{"role": "user", "text": "Hi Jenna,\nI am having the exact same error as posted. I Have an abundance of fast5 Files that return the same result.  Was there an eventual solution to this formatting issue? Would be more than happy to share my history if this helps?\nThanks!\nJesse"}, {"role": "system", "text": "\n\n\nNanopore data upload, format/datatype, and unexpected results from tools\n\n\nThese tools are working at UseGalaxy.org  using the tool\u2019s test data (raw data URL link ). Review to see if/how your input dataset differs and make adjustments as needed. The input was directly loaded by URL with the datatype \u201cautodetected\u201d. Galaxy | Accessible History | nanopore tar upload test \n\n\nHi @jessemartin\nI just reran the two test tools in this history, and both are still functional. Maybe that helps?"}, {"role": "user", "text": "Hi Jenna,\nI think I am having a similar problem as the original post. I have uploaded fast5 files that have autodetected as \u201cH5\u201d format, however when I run them through  both of the tools I get either \u201c1 line or 0 bytes\u201d\nI imported the trial data for the tools and that when processed through the tools it works just fine but I cant seem to successfully extra reads from my own files. The original post said there was some formatting issues, could this also be the problem with my data?\nThanks!\nJesse"}, {"role": "system", "text": "\n\n\n jessemartin:\n\nThe original post said there was some formatting issues, could this also be the problem with my data?\n\n\nYes, this is likely a problem for you as well if tools cannot read your files.\nIf the data is compressed, then that compression format should be included on the dataset name.\nIf Galaxy did not guess the correct datatype (including the compressed versus uncompressed state), then one of these should work:\n\nUpload the data and set the datatype to match\nConsider loading uncompressed data to avoid any technical mismatches between the utilities Galaxy uses versus your own computer (or wherever the original source data was created).\nConvert to fastq, then Upload the reads to Galaxy for analysis.\n\nKeep in mind that data can be truncated or corrupted when moving it around. This is fatal for a compressed file, and the full original file is what tools will be able to interpret (in Galaxy or otherwise). Utilities from the format developer: GitHub - nanoporetech/ont_fast5_api: Oxford Nanopore Technologies fast5 API software\nTroubleshooting the above is what the other person had to do, too.\nOnce you have reads, then these tutorials can help.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Single Cell\n\n  Training material and practicals for all kinds of single ...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I am trying to follow the MD simulation with GROMACS tutorial (Running molecular dynamics simulations using GROMACS) on a PDB file, 2XQW, which contains 3 protein chains. However, when I try to run the initial GROMACS set up tool, I get an error that the job cannot run:\n\nimage492\u00d7518 30.7 KB\n\nThe full error says \u201cThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource.\u201d\nIs there a way to follow this tutorial on a PDB file containing more than 1 protein chain?"}, {"role": "system", "text": "Hi @christos-efthymiou, can you submit a bug report (using the bug icon on the failed dataset?)\nThere should also be a log file created by the job which should provide more information."}, {"role": "user", "text": "Hi @simonbray, I have submitted a bug report. I am also placing several screenshots of information about the error below:\n\nimage1006\u00d7803 63.1 KB\n\n\nimage985\u00d7630 54 KB\n\n\nimage990\u00d7701 52 KB\n\n\nimage1000\u00d7788 54.3 KB\n\n\nimage986\u00d7397 21 KB\n"}, {"role": "user", "text": "@simonbray I also wanted to add that I have tried this again with separate PDB files with only a single protein chain and the same error occurs."}, {"role": "system", "text": "@christos-efthymiou thanks for the bug report, I replied by email.\n\nI also wanted to add that I have tried this again with separate PDB files with only a single protein chain and the same error occurs.\n\nThe reason would be that the error is not due to the multiple chains, but unexpected atom naming in the original PDB file."}], [{"role": "user", "text": "Usegalaxy.org\nMissing tools for untargeted LCMS interactive tutorial.\nThe list below was copied from the tutorial list of tools for processing demo data.\nI found msnbase_readmsdata but did not have permission to install into Galaxy\nmsnbase_readmsdata\nxcms plot chromatogram\nxcms findChromPeaks (xcmsSet)\nxcms findChromPeaks Merger\nxcms groupChromPeaks (group)\nxcms adjustRtime (retcor)\nxcms fillChromPeaks (fillPeaks)\nCAMERA.annotate\nxcms process history\nI\u2019m requesting that above list be installed into Galaxy or a custom set of the above tools made available to me.\nRob Kleps"}, {"role": "system", "text": "Hello Rob, you can find the tools here: https://usegalaxy.eu/."}], [{"role": "user", "text": "Hi, How to a request more space on Galaxy? (apart from deleting files as i go along). Would be great to have around >250gb space or more (happy to pay towards this too). Thanks"}, {"role": "system", "text": "Hi @mohammed\nwhat Galaxy server do you use?\nMake sure you purge deleted files in every history. Deleted but not purged files count towards the quota. History menu (the cog wheel icon at the top of the history panel) > Purge Deleted Datasets.\nKind regards,\nIgor"}, {"role": "user", "text": "Hi Igor,\nI am using: https://usegalaxy.org/\nYes, I deleted and purged them but I was hoping I could get more that 250gb if possible?\nThanks\n(remainder admin redacted for privacy)"}, {"role": "system", "text": "Hi @mohammed\nI am not admin on this Galaxy server. I hope @jennaj can answer your question about the quota increase. The forum is for Galaxy community covering several Galaxy servers, hence the question.\nKind regards,\nIgor"}, {"role": "system", "text": "\n\n\n mohammed:\n\nWould be great to have around >250gb space or more (happy to pay towards this too). Thanks\n\n\nHi @mohammed\nThis FAQ explains some semi-advanced strategies for managing data on public servers. All of this is GUI based, but Galaxy also has an API if you want to interact that way.\nHow can I reduce quota usage while still retaining prior work\nAnd this FAQ explains options for setting up a server for larger ongoing work, including academic/scientist friendly grant-based and pay-for-use options with low/GUI-based to no administrative set up on your part. That FAQ also points to all choices: some versions can use your own local/institutional resources, others involve commercial resources, and you could do a mix (process smaller jobs locally + send larger jobs off to regional academic cloud clusters).\nGalaxy choices\nIt is fine to keep a single account on any or all of the public free services (for public/sharing purposes, or any other reason), even when working at your own private server. One account on each gives you access to distinct resources: data storage quota, hosted tools, indexed reference data.\nIf you just need an extra 250 GB \u2013 all of the UseGalaxy.* servers offer about that or more as a default for everyone, so that is at least 1 TB of free storage before including the other resources at the remainder of public sites (138 total as of today). Moving data between servers is easy and common.\nHope that helps!"}], [{"role": "user", "text": "I have been trying to do some differential expression studies with my dataset\u2019s, i have been feeding just a single replicate followed by settings as blind dispersion method ! yet the process fails each time i select generate SQLite while not selecting it and running the tools generates tabular files !"}, {"role": "system", "text": "Hello @Hart95\nThe tool has been deprecated for a couple of years. The creation of an SQLite database was one of the known issues that was never resolved (and won\u2019t be).\nTicket with more details: https://github.com/galaxyproject/usegalaxy-playbook/issues/50\nWe strongly recommend moving to use current (and supported) differential expression tools and methods. Find tutorials that explain usage under the \u201cTranscriptomics\u201d topic from the GTN.\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy  server choices. If a tutorial is supported by a pre-configured Galaxy Docker  training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nThanks!"}, {"role": "user", "text": "Can you provide me some details for a workflow which is suitable for DE studies and can be handy with customized genomes ! Thanks for the help !"}, {"role": "system", "text": "The tutorial themselves include exact tools and associated workflows. Each tutorial starts out with an overview of the analysis concepts, then describes what parts will be specifically covered by the examples, and ends with a list of reference publications. Not every possible path is covered in detail (of course!), but combined, those resources should help you to 1) get oriented and 2) make choices that are best for your own research goals.\nPlease give them a review, it will be worth your time  Plus you can review prior Q&A at this forum. I added the tag \u201ctranscriptomics\u201d to your post. Click on it to find similar Q&A directly, or just do a search to find even more as you progress. Many have links to FAQs and related help that can help if you run into trouble (usage, etc).\nThanks!"}], [{"role": "user", "text": "I have install galaxy locally.\nThe programs/tools  are running well , however the output cannot be displayed in the center.\nThe folllowing is displayed for the fastqc result\n\nimage1834\u00d7905 237 KB\n"}, {"role": "system", "text": "Because of security reasons you need to whitelist tools first before you can see the HTML output. You can do that on the admin page (Admin > Manage allowlist ). Here you can see a list of all your tools, select fastqc and add it to the \u201callowlist\u201d. Hope this helps."}], [{"role": "user", "text": "Hi,\nWe would like to establish a local Galaxy server that resembles the current public main Galaxy server (https://usegalaxy.org/) as much as possible, and we would like to accomplish this by using the Ansible and other Galaxy team recommended approach.  Unfortunately, we have some confusions about those instructions.  Could someone kindly post a list of key steps that we should follow?\nMany thanks!\nUSC Libraries Bioinfo Service."}, {"role": "system", "text": "Linked post:\n\n  \n    \n    \n    Galaxy and Ansible \n  \n  \n    Hi Marten, \nThanks much for the reply!  We have decided to try \u201cclone\u201d the current public main Galaxy server from the ground up using your \u201cOfficial\u201d Ansible approach.  If \nall possible, could you give us a couple of key links of the resources for us to follow? \nBest! \nUSC Libraries Bioinformatics Service\n  \n\n"}, {"role": "system", "text": "Please have a look here (Jan 2019, day 1, has an \u201cIntro to Ansible\u201d section):\n\n  \n      \n      GitHub\n  \n  \n    \n\ngalaxyproject/admin-training\n\nGalaxy Admin Training. Contribute to galaxyproject/admin-training development by creating an account on GitHub.\n\n\n  \n  \n    \n    \n  \n  \n\n"}, {"role": "system", "text": "We do publish our playbooks online at e.g. https://github.com/galaxyproject/usegalaxy-playbook/ for usegalaxy.org. However, I am aware that this is quite complex, even for people already familiar with Ansible. The most relevant bits - specifically, the Galaxy config - are all in the galaxyservers group vars as well as the stack, update, and config playbooks. There is a fair amount of extra stuff in those playbooks that most people will not need though, since we deploy Galaxy to CVMFS.\nBeginning with Galaxy 19.05, you will be able to load our shed tool config files directly from CVMFS without needing to also point your Galaxy at our install database (these are both configured in the Galaxy config defined in the group vars linked above), so you can still install tools locally from the Tool Shed without issues.\nRoughly, for the Galaxy setup only:\n\nCreate a group variables file for your Galaxy server and choose the options from galaxy_config_hash that are most relevant to you (but place them in the variable galaxy_config as that is the variable name used by galaxyproject.galaxy)\nSet galaxy_layout and related directory variables as described in the galaxyproject.galaxy documentation.\nCreate a simple playbook as shown in the documentation (see also the training materials linked by Jen).\nCreate an Ansible hosts file containing the connection information for your Galaxy server.\nansible-galaxy -p roles install galaxyproject.galaxy\nansible-playbook -i hosts playbook.yml\n"}], [{"role": "user", "text": "i want to use kallisto. my fragments length are 80 and SD is 8.\nbut it came with the error containing this : This job was terminated because it used more memory than it was allocated.\nPlease click the bug icon to report this problem if you need help.\nwhat should i do ?"}, {"role": "system", "text": "Hi @amir!\nThis type of error can indicate that one of two distinct problems is going on:\n\n\nThere is a problem with one or more of the input datasets. Check format, metadata assignments (datatype, database), and overall content.\n\nThe job really is too large to run at the public server with the given inputs and tool options. Note: the memory used to process jobs is different than the available \u201cmemory\u201d you may have still available in your account\u2019s quota allocation.\n\nFAQ with more details about how to check inputs and what your options are if the job really is too large to run at the public server (exceeds computational resources).\n\nMy job ended with an error. What can I do?\n\nIf you cannot figure out the problem, let us know and we can follow up from there. We\u2019ll probably need to look at your history, so please do not delete the inputs, error jobs, and any tests you did yourself to verify that the inputs are correctly formatted/labeled.\nThanks!"}, {"role": "user", "text": "yes i will be appreciated if tell me whats the problem.\nbut my email i used in galaxy is admin-redacted not the one im using here."}, {"role": "system", "text": "Ok, I\u2019ll look and see if can find the history/error job. Am going to remove your email address from the public thread to keep it private. Next time you can send me a direct message here with private info "}, {"role": "system", "text": "Ok, that was fast.\nUsing the complete hg38 reference genome will not work. You need to find or create a fasta file for a transcriptome to use for the input option \u201cFASTA reference transcriptome\u201d."}], [{"role": "user", "text": "Hello!\nI was trying to run multiple (3 at this time) workflows in galaxy and upload data for another 6 workflow invocations. Everything went fine (not exactly, but not with workflow of question), but then i saw that one of my workflow runs has totally vanished! I am 146% sure it was running (moreover, it had been running for more than one hour). But there are no errors in web interface nor in the log!\ntransgen@transgen-4:~/galaxy$ cat galaxy.log | grep -Pv 'Failed to activate conda environment|VERBOSITY=ERROR|HTTP/1.1\" 200' | grep -Pi 'HTTP/1.1\"|excep|error' --color=always | tail -n 20\ngalaxy.jobs DEBUG 2021-07-15 17:21:03,823 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1237', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:03,975 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:03,984 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1232\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.jobs DEBUG 2021-07-15 17:21:10,706 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1252', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:10,821 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:10,866 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1249\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.jobs DEBUG 2021-07-15 17:21:12,526 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1254', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:12,778 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:12,794 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1253\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:14,697 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:14,701 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1255\ngalaxy.exceptions.ObjectNotFound: No such object found.\n172.16.2.70 - - [15/Jul/2021:17:22:43 +0300] \"POST /api/workflows/1cd8e2f6b131e891/invocations HTTP/1.1\" 400 - \"http://172.16.2.200:8080/workflows/run?id=1cd8e2f6b131e891\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:17:34:21 +0300] \"POST /api/workflows/1cd8e2f6b131e891/invocations HTTP/1.1\" 400 - \"http://172.16.2.200:8080/workflows/run?id=1cd8e2f6b131e891\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:17:42:24 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:18:01:17 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:18:16:02 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n\nSo the last error message was at 18:16:02, but the run disappeared at about 19:15."}, {"role": "user", "text": "I think it is a web interface bug cos this run appeared again, but another run disappeared."}, {"role": "system", "text": "Hi @wormball\nMaybe upgrade to 21.05?\n\n\n\nWorkflow submission failed\n\n\nIt might help to update to the latest commit on the 21.05 branch.\n\n\nOtherwise, this just looks like your server is not quite keeping up with database updates. But @mvdbeek can correct me."}, {"role": "system", "text": "Go to User \u2192 Workflow Invocation, it should be there."}], [{"role": "user", "text": "Hello,\nWe are upgrading Galaxy from Rel 16 to Rel 20 using the Docker deployment.\nNo matter what we do,  restart the container, update /config/tool_conf.xml with our apps in the order we want them and then  \u201csupervisorctl restart galaxy:\u201d,  Galaxy comes up with the applications,  but not in the order we want them.\nHow can we force it to \u201crespect\u201d the order we give it the apps?\nBelow,  the list of our apps in the order we want them.\nThanks!\nGeorge Weingart PhD\nHuttenhower Lab\nHarvard School of Public Health\n\nScreen Shot 2022-04-25 at 5.21.49 PM963\u00d7785 28 KB\n\n\nScreen Shot 2022-04-25 at 5.23.01 PM1254\u00d7794 208 KB\n"}, {"role": "system", "text": "Hi George\nNot sure whether this works using the Docker deployment:\nHave you tried deleting (or renaming, just to be on the safe side) the \u2018integrated_tool_panel.xml\u2019 file before restarting?\nRegards, Hans-Rudolf"}], [{"role": "user", "text": "Hello,\nIs there a tool in usegalaxy that can help me retrieve reads from a fasta file (8Gb) based on a list of read identifier names (~150)? The list of identifer names is in a text file but I can convert to fasta or something else. I\u2019d then run the selected reads through blast for ID.\nThanks for any help."}, {"role": "system", "text": "Hi @dacosta,\nin order to extract a set of reads based on a list of identifiers you can use the seq_filter_by_id tool. Previously you will need to set the text file datatype as tabular."}, {"role": "user", "text": "Thank you so much for your reply @gallardoalba. I might be mistaken but it looks like I\u2019d need to use this tool with python. Thanks in part to your reply, I was able to find the filter fasta tool which does the same thing but is an online usegalaxy tool."}, {"role": "system", "text": "Yes, you can find that tool in Galaxy as Filter sequences by ID (if you look for seq_filter_by_id in the tool search bar you will get the same result)."}], [{"role": "user", "text": "Hello.\nIt seems the bam file ( GIAB-Ashkenazim-Trio-hg19.gz) in the training session \u2018Calling variants in diploid systems\u2019\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Variant Analysis\n\n  Genetic differences (variants) between healthy and diseas...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nare not able to process for variants calling.\nI also used Samtools to convert this bam file to a sam file, and got the error:\n[W::bam_hdr_read] EOF marker is absent. The input is probably truncated\nAlso, the size of the bam file is extremely small, only 52kb"}, {"role": "system", "text": "Hi @wcq848677\nThe BAM is downsampled so it will run quickly when running through the tutorial.\nThis is a history that has that particular BAM loaded (size is 51.2 KB), and the content is as expected. I also converted Bam-to-Sam in that history, and that worked plus produced the right result. Galaxy | Accessible History | tutorial variants. When you run Freebayes, input the BAM, no need to convert to SAM. If you want to see the content of the BAM, it is small enough that Galaxy can produce the uncompressed \u201cSAM\u201d view (use the \u201ceye\u201d icon for the dataset).\nMaybe you got a corrupted version? Try pasting just this link into the Upload tool in Galaxy. Leave all the other settings at default. https://zenodo.org/record/60520/files/GIAB-Ashkenazim-Trio-hg19.gz\nYou can also make a copy of that history to work with it. I\u2019ll leave it shared for this week.\nTutorial: Calling variants in diploid systems\nNote: You marked this as being \u201cgalaxy-local\u201d and as if working at UseGalaxy.org.\n\nIf you are actually working at UseGalaxy.org, UseGalaxy.eu or UseGalaxy.org.au, this will work.\nIf you are trying to run the tutorial in a Local or Docker Galaxy on your own computer, other things may be going wrong, even if the downloaded BAM is the right size. Please explain a bit more about where you sourced Galaxy and what you have done to setup your server. Include details \u2013 is this the first dataset you have tried to load to that server? The first BAM dataset?\n\nHope that helps!"}], [{"role": "user", "text": "Hello,\nWe have occurred some problems when using the SortMeRNA with paired end samples. We suspect our RNA seq reads have ribosomal RNA contamination and try to filter them out. However, SortMeRNA reported an error (still green but the output file is 0kb) as shown below. We were wondering which part could be wrong. All parameters are default, and we tried the non-paired option which works perfectly, but the results cannot be used for further analysis like STAR.\nThank you so much for your time!\n\nimage1548\u00d7562 187 KB\n"}, {"role": "system", "text": "Hi @Pinhki\nWe discussed this over email but maybe this summary helps \n\nThe reference needs to be input to the tool (required). It was empty last time I checked, and the stderr in your screenshot has that same error message. Another error I reviewed included a reference but it was in the wrong format.\n\nUseGalaxy.org does not have any references for this tool natively indexed at this time.\n\nFor a successful run, you\u2019ll need to provide a properly formatted reference yourself from the history, or consider using the tool at UseGalaxy.eu (has some indexes).\n"}], [{"role": "user", "text": "Hello! If a workflow fails on a certain step, after fixing whatever issue causing it, what\u2019s the best way to resume that step and let the workflow continue? I don\u2019t mean to rerun that step (like using the rerun button on the UI), that will trigger a new invocation. We want to continue the original workflow invocation by restarting from the failed step and continue to the end of workflow, and have the result still associated with the original invocation. Thanks!"}, {"role": "system", "text": "When you go to re-run the step, it will give you an option to replace the dataset in the workflow and resume all dependent steps."}], [{"role": "user", "text": "Perhaps this is an easy one, but I haven\u2019t yet found the answer. Can anyone tell me if there is a Galaxy tool that will filter a fasta (specifically a protein fasta) to yield only the longest isoform per gene?\nBTW, I understand that the longest isoform may not be the best, but this is a way of getting a multi-fasta with one entry per gene.\nI have proteomes in fasta format downloaded from Uniprot and also NCBI. Something about the formatting of these fasta files is not letting me use the perl, python, and R suggestions I have found on various help forums. Some of them returned a file with almost nothing in it. Part of this difficulty is likely owing to my naivete for troubleshooting code.\nA solution on Galaxy would be good, and would also be invaluable to others who need this as part of their workflows.\nThanks for any tips!"}, {"role": "system", "text": "Hi @jaredbernard\nit is an interesting question. I am not aware about a dedicated tool on Galaxy for this purpose. Files from NCBI and UniProt might use different rules for sequence names, hence, require different processing.\nYou can do it in Galaxy. You need a tabular file with columns for gene name, transcript name, sequence length and protein sequence. The order of columns is irrelevant, and the file can have other columns. usegalaxy.* servers usually have tools for all steps needed, like getting sequence length, FASTA to tabular conversion, splitting text into smaller string.  The processing depends on sequence names. Once you get the tabular file, use Datamash:\ngroup by gene name\nPrint all fields from input file: set to Yes (default No)\nOperation to perform: maximum on column with sequence length\nThis operation selects a longest sequence per gene and print the entire line from the input tabular file.\nTabular file with longest sequences can be converted into FASTQ using Tabular-to-FASTA  converter.\nDevelop a workflow using a small subset of data, check the output, and use it with fill sized dataset.\nHere an example Galaxy | Australia | Accessible History | protein length\nHope this answers the question.\nKind regards,\nIgor"}], [{"role": "user", "text": "Is there any issue with DEseq2 ? I have been doing some RNA seq analysis for DE genes in treated samples with two replicate each for two conditions but it seems to take long time\u2026is it usual? Its my 1st time with the tool\u2026"}, {"role": "user", "text": "I have been running my analysis with 30 million reads of RNAseq data ! how much runtime does DEseq2 generally takes for this amount of data ?\nThanks Already ! Wish you all Good Health in the worst of times ! "}, {"role": "system", "text": "Hi @Hart95\nHas the job been queued for a long time (gray dataset)? That may be possible. The server is very busy. The best strategy is almost always to allow queued jobs to stay queued. If you delete and restart them, they just move back to the end of the queue again, extending the wait.\nNo job should actually be executing (yellow/peach dataset) for over 3 days \u2013 that would exceed the maximum \u201cwalltime\u201d (runtime) on the clusters at Galaxy Main https://usegalaxy.org.\nThe runtime for executing jobs varies not only by the number of reads, but the reference genome and parameters used (among other factors). If a job is already executing, allow it to complete unless you already know something is wrong with it (problematic inputs, settings, etc). Certain types of input problems can cause jobs to run out to their maximum memory or runtime usage \u2013 and that can be quick or long (few days, once not queued anymore) \u2013 then they eventually fail (red error result).\nHelp for using DE tools, including links to Tutorials, is here:\n\nExtended Help for Differential Expression Analysis Tools\n\nGeneral troubleshooting, including info about how to interpret jobs that fail for resources (memory or walltime):\n\nMy job ended with an error. What can I do?\n\n\n\n\n Hart95:\n\nThanks Already ! Wish you all Good Health in the worst of times !\n\n\nThank you! You too.\nLet us know if you need more help. Thanks!"}], [{"role": "user", "text": "Hi, I am using GATK4 mutect2 for variant calling. I need baserecalibrator for the preprocessing process, but I can\u2019t find it in Galaxy tools and Tool Shed.\nCan you tell me how to use baserecalibrator or a tool that can replace it?\nVariant Calling\nBase (Quality Score) Recalibration\nGATK4\nThanks."}, {"role": "system", "text": "Hi @bcg,\ncurrently BaseRecalibrator is not available in the public Galaxy servers. I recommend you use Freebayes for performing the variant calling.\nRegards"}], [{"role": "user", "text": "Hello Galaxy Community,\nI am trying to educate myself on this NGS thing. Have some questions regarding my data, paired-end RNA seq. So I have mouse tissue, from Knockout and Control animal, 4 biological replicates for each. They were prepared using TruSeq Kits Single Index. I read on the Illumina page (TruSeq Single Indexes) that for trimming, I only need these following sequence:\nRead 1\nAGATCGGAAGAGCACACGTCTGAACTCCAGTCA\nRead 2\nAGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\nQuestions:\n\nShould I use those 2 sequences for all my data sets?\nOr do I need only to trim the index sequence, like 6 bp? I know the index sequence for each sample. But it seems inconvenience if I do trimming for all my datasets at once\u2026\nThe aim of my sequencing is to know the differential gene expression from knockout vs. control as well as to identify splice variants. Is trimming still necessary? Because I read in some discussions for that aim, and if I align it with RNA STAR, trimming is not necessary anymore.\n\nI highly appreciate your  help \nCheers!"}, {"role": "system", "text": "Hi @zee_azizah\n\n\n\n zee_azizah:\n\nThey were prepared using TruSeq Kits Single Index.\n\n\nTry using either Trimmomatic or Cutadapt for trimming. Both contain this adaptor already indexed.\n\n\n\n zee_azizah:\n\nIs trimming still necessary?\n\n\nThis tutorial has more help about common QA steps. Some of these should be run even if you decide that the reads do not need trimming.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n zee_azizah:\n\nBut it seems inconvenience if I do trimming for all my datasets at once\u2026\n\n\nYou can run QA on all of your datasets in batch if using dataset collections and workflows. At a minimum, consider using collections as these help to keep data organized. Workflows are for faster and reproducible processing that you can later layer in (or adapt one of our workflows \u2013 most tutorials have at least one).\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n zee_azizah:\n\nThe aim of my sequencing is to know the differential gene expression from knockout vs. control as well as to identify splice variants.\n\n\nPlease see our tutorials for example workflows that do exactly this. Some cover QA steps, and explain why for each. You should at least run a tool like FastQC to make sure the read data is intact and fully uploaded.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThere is much Q&A at this forum about the tools you will be using. Search with keywords to find these: tool names, datatypes, error messages. I also added a few more tags to this post."}], [{"role": "user", "text": "Hi, I am running local Galaxy on my dedicated server with version 20.09. I got this problem when I try to run IReport tool with my dataset.\n\" \u2018base64\u2019 is not a text encoding; use codecs.encode() to handle arbitrary codecs\"\nThe weird thing is that I can run IReport in Galaxy from another my local server but with lower version (19.06). And here is the picture from \u201cView Error Logs\u201d sections.\n\nCapture1591\u00d7418 90.8 KB\n\nThanks for your attention to this question.\nI will be glad to have your responses.\nThank you!\nSincerely,\nPham Quang Huy."}, {"role": "system", "text": "Hi @Huy_Pham_Quang\nThat looks like potentially a python2/3 issue. I think the tool was designed for python 2, you might need to either run it explicitly in a py2 environment.\nI\u2019m sure we can get a new release made fixing that after the winter holidays.\nHope this helps"}, {"role": "user", "text": "Hi @hxr,\nI see, so I think I can fix py2/3 by myself.\nThanks a lot."}, {"role": "system", "text": "Resolved, see: Error in ireport - base64 is not a text encoding -- Solution: See built in workflow invocation reports instead"}], [{"role": "user", "text": "We have installed a custom galaxy instance and we would like to analyze 20 samples. However, each of these samples is spread over four lanes. For subsequent analysis we would like to merge them first.\nI am aware of several similar questions on this topic and they all point to the Text Manipulation: Concatenate datasets tail-to-head tool. But to do this for a large amount of samples is quite tedious. Is there a better way?\nI am quite proficient in the commandline, but I would really like it if the wetlab people could do this themselves."}, {"role": "system", "text": "Hi @BasH,\nan approach would be to build a dataset list by using the Operation on Multiple Datasets option.\n\nThen, you can use the Collapse collections into a single dataset in order to combine all of them into a single file dataset.\nRegards."}], [{"role": "user", "text": "Hi!\nusegalaxy.org is super slow today and I was wondering if there was a problem. My \u201cbamcoverage\u201d and \u201ccomputematrix\u201d do not progress since yesterday.\nBest"}, {"role": "system", "text": "Update 11/09/2022\nThe small backlog of queued jobs due to the prior issue has now been cleared. Jobs will queue and execute as usual.\nAny jobs that failed should be rerun at least once, starting now, to eliminate technical issues. Persistent failures can be reported in new topics. Remember to include enough information so the community can help: tool/version and error messages at a minimum, with a shared history link ideal.\n#troubleshooting-errors\n#understanding-input-error-messages\n#sharing-your-history\n\n\nUpdate 11/8\nThere was a small cluster issue and we are working to correct it. Leaving jobs queued is still recommended for the fastest processing. Our admins are tracking this, and updates as needed will continue to post back to this topic.\n\nHi @Roland_Marci\nThe server is up and processing work but is very busy https://status.galaxyproject.org/\nFAQ #understanding-job-statuses\nIf we have any updates, we\u2019ll post that back here. Meanwhile, please leave your queued jobs queued.\nThanks for reporting the issue!"}], [{"role": "user", "text": "I have the raw R1 and R2 files of a bacterial genome sequence that have been assembled into contigs using spades. How can I calculate the genome coverage / depth? Thanks."}, {"role": "system", "text": "Dear @dna,\nYou calculate the typical genome coverage with paired-end sequencing with the equation:\nC = (N \u00b7 L \u00b7 2) / G\nN = number of reads\nL = read length\nG = genome size\n2 = factor for the paired-end sequencing\nKind regards,\nFlorian"}], [{"role": "user", "text": "I realized LEfSe provides different results from identical data sets in which only feature names have been modified. Compare the outputs below: the number of significant feature differ by 1 and the LDA scores changed for some features:\n\nGalaxy117-C)_Plot_LEfSe_Results_on_data_1161050\u00d7847 73.8 KB\n \nGalaxy120-C)_Plot_LEfSe_Results_on_data_1191050\u00d7817 65.3 KB\n\nAnyone faced the same issue?"}, {"role": "system", "text": "Hi - yes I have also had this issue, but just assumed that I had made a copy-paste error. Have you sorted it out?\nLaura"}, {"role": "system", "text": "Hi, I have also faced this problem. Have you found why that happened?"}, {"role": "system", "text": "Hi @hlsw2001 @Laura_mason  @DrNezar\nThe answer is cross-posted here: the way microorganisms are named affect the final results of lefse\nIn short, Lefse is picky about identifier formats. Clean those up and these technical problems will likely resolve."}], [{"role": "user", "text": "Hi everyone\nAs for reference genome, when I load hg38, it sends me mm10. Is that a problem?\n\nFor hicBuildmatrix, how to set \"remove self ligation\",when I set this parameters, I get error \"hicBuildMatrix: error: argument --removeSelfLigation: expected one argument\"\n\nAnd how to configure homer in Galaxy?\n\nThank you"}, {"role": "system", "text": "I have the same problem for the remove self ligation option."}, {"role": "system", "text": "Hi,\nThanks for reporting this error. I noticed we have a small bug here, the value is always set to true at the moment, so it does not make a difference if you actively set it or not, it is always active. To bypass this error, just uncheck it and it should be fine. Only in the case you want to keep self ligations, you need to wait for the upcoming 3.7 release of HiCExplorer.\nReference genome: That\u2019s an issue for the Galaxy admin of your server. If you use usegalaxy.eu, please inform me and I will forward it to our admins.\nHomer: If Galaxy wrappers are existing, that\u2019s also an issue for the galaxy admins of your server to install it. If no wrappers are existing, maybe you can ask the developers of Homer to write any.\nBest,\nJoachim"}], [{"role": "user", "text": "Hi\nif there is a low MAPQ in my reads . someone tells me to Use  samtools view in.bam \"chr1:234-567\"  to explore the reads in the region of the gene.\nhe said : Your read counter might be refusing to count reads that do not align uniquely. this tool will let you count reads in the region, even if your read counter won\u2019t assign them to a gene.\nbut i dont know how do it in galaxy ? im be grateful if someone can help me .\nthis is our conversation link : https://www.biostars.org/p/381880/"}, {"role": "system", "text": "Hi,\ntwo things:\n\nto answer the question you are asking here. If you already know how to inspect your reads with IGV (as you\u2019re mentioning in your linked post on biostars), there isn\u2019t anything samtools view could tell you in addition. If you hover over any read of interest in IGV (depending on your setting you may also have to click on it), you should get a pop-up info window that also has the info about the read\u2019s MAPQ score.\nsamtools view is not directly available through Galaxy as a dedicated tool. To get the functionality you want you could combine the tools Slice Bam and BAM-to_SAM, where the first one allows you to extract the reads mapping to a particular region, but outputs them in binary format, while the second tool would turn that output into the same text format that you\u2019d get with samtools view as suggested to you.\nThere are lots of reasons why specific reads may not be counted by featureCounts and some of them can be controlled through the Options for paired-end reads and the Advanced options of the Galaxy tool. In particular, check these settings:\n\n\n\nExclude chimeric fragments: if set to True, then paired-end reads will only be counted if both mates map to the same chromosome\n\n\nCount multi-mapping reads/fragments: if disabled, then any reads mapping ambiguously to more than one location will not get counted\n\n\nMinimum mapping quality per read: if the aligner gave the mapping of the read a MAPQ score less than the threshold you set here, the read will not get counted.\nIn addition, the last two settings above can be somewhat confusing when you align with STAR because STAR uses a very simplified MAPQ scoring scheme, in which it assigns MAPQ 255 to every uniquely mapping read. Reads that map ambiguously to 2 locations get MAPQ 3, those with 3 or 4 mappings get MAPQ 1, and those with 5+ mappings get assigned MAPQ 0. This means that if you use STAR for alignment and go with the default Minimum mapping quality per read of 12 in featureCounts, you will, effectively, ignore all multimapping reads even if you enable Count multi-mapping reads/fragments.\n\n"}], [{"role": "user", "text": "Hi Martin,\nThank you so much for your reply!  We have since referenced the info you provided and updated our Galaxy to the 19.01 release.  It is weird that following the upgrade, while the\nfont/color theme of our Galaxy UI has been changed to the same of your public main Galaxy server, the Tool Pane layout/organization remains unchanged.  One of the possible reasons might be that we did not use the Ansible for the tool installation and update.\nSpeaking of the Ansible, we do have a very important question for you:\nWe realize that the Galaxy team is now recommending Ansible for setting up, managing and upgrading Galaxy.  Our question\nis if this Ansible approach is now Galaxy\u2019s team\u2019s official strategy going forward?   In late 2018, we tried the Galaxy Kickstart, and ran into several issues (ftp, postgresql, etc.).   We are unsure if the current Ansible approach has resolved those\nissue.\nWe would appreciate your comments!\nThanks again,\nUSC Libraries Bioinformatics Service"}, {"role": "system", "text": "\n\n\n nmlbio:\n\nis if this Ansible approach is now Galaxy\u2019s team\u2019s official strategy going forward?\n\n\nYes.\n\n\n\n nmlbio:\n\nWe are unsure if the current Ansible approach has resolved those issue.\n\n\nI do not know what issue you ran into, but be assured that usegalaxy.org, usegalaxy.eu, usegalaxy.org.au, and many other Galaxies are being deployed using Ansible."}], [{"role": "user", "text": "Hi there,\nI am trying to use FastTree to generate a tree to be visualized on Phandango.\nI have data of 4 E. coli isolates sequences. Three of them are on our own collection (natural strains) and the fourth was a reference genome from NCBI of E. coli K12 MG1665 (This strain was also part of the experiments, thus I wanted to include it on the tree)\nOriginally I had my own annotations with Prokka (from Linux) on the three strains. With the Gff3 files, I ran Roary followed by FastTree (nucleotide) on these three isolates and it worked (used core aligment and newick file from Roary)\nAs I wanted to expand the analysis and include more sequences (for a better understanding of the core genome). I started simple, adding the 4th sequence (MG1665). Running Prokka in Linux for this sequence was no longer an option for me (changed institutions), so I decided to rerun prokka on the four isolates, including MG1665.\nThen I ran Roary on those 4 strains and worked.\nWhen running FastTree (nucleotide) on them, I got the following error: \u201cThis job was terminated because it used more memory than it was allocated\u201d.\nSince they are only 4 isolates, I wondered if this was a memory problem (I run on the galaxy guest, not a cloud server).\nMaybe relevant\u2026 or not:\n\nThe three first strains were annotated from scaffolds, while the MG1665 was annotated from the chromosome sequence.\nThe Newick file from Roary for the 3 strains and 4 strains looks like this (P.S. I shortened the names of the three sequences on the last one)\n\n3 strains: (38.27_PROKKA_09102020.gff:1.843208323,39.62_PROKKA_09102020.gff:0.806511634,9.54_PROKKA_09082020.gff:0.000000005);\n4 strains:\n(39.62.gff:0.417292218,9.54.gff:0.138018002,(38.27.gff:0.253699967,MG1655.gff:0.218665394)1.000:5.990038221);\nFrom the Newick files, I see a big change, yet I don\u2019t know if it is caused by real differences in the sample or by the processing. Trying different possibilities is tricky, as Roary takes around 12h to complete every combination of strains that I am using.\nP.S. I am quite new to WGS analysis, so I apologize if I have done something stupid.\nThank you in advance."}, {"role": "system", "text": "Hi @Rebeca_Pallares,\nwhich Galaxy instance are you using? It would be nice if you could send an error report (bug icon), since it can provide us additional information about the error.\nRegards"}, {"role": "user", "text": "Hi Alba, thanks for reaching\nI was running the online tool (from Europe so I think European server)\nAs an update, I must say I tried to reproduce the Roary+FastTree that worked (rerunning same dataset) and FastTree is giving the same error. I think the problem is on the Roary output data though.\nBefore, Roary took really long to run and now it produces the analysis in a matter of minutes (yet results i.e. nhx file are really similar, although not exactly the same).\nFirst run (38.27_PROKKA_09102020.gff:1.843208323,39.62_PROKKA_09102020.gff:0.806511634,9.54_PROKKA_09082020.gff:0.000000005);\nNew run\n(38.27_PROKKA_09102020.gff:1.868025368,39.62_PROKKA_09102020.gff:0.804283884,9.54_PROKKA_09082020.gff:0.000000005);\nWhen I re-run FastTree on the output that worked, it still works but not on the new outcome.\nI am really puzzled about where can be the error, as for running Roary I am not tunning the settings.\nThanks for your assistance"}, {"role": "system", "text": "Could you share your history with me? I\u2019ll privide you my mail by DM.\nRegards"}, {"role": "user", "text": "Hi Cristobal,\nThank you for your message.\nI was first working in https://usegalaxy.org/ and I have tried again in https://usegalaxy.eu/\nIn the Europe based web FastTree worked, with new analysis and also with the files provided by https://usegalaxy.org/.\nThanks for your time and for the service galaxy provide.\nBest, Rebeca."}], [{"role": "user", "text": "Hi,\nI am very new to galaxy and with no knowledge in Bioinformatics. I\u2019m trying to analyze some scRNA seq data from Cryptococcus neoformans.\nI\u2019ve been trying (unsuccessfully) to use the GTF2GeneList tool to generate the filtered FASTA and transcript gene-map using a GTF file and FASTA file downloaded from the NCBI (https://www.ncbi.nlm.nih.gov/assembly/GCF_000149245.1/), for later running Alevin\nEvery time I run the tool the same error appears: Fatal error: Exit code 1 ().\nI\u2019d appreciate if you could tell me what\u2019s that meaning and if you have any suggestions.\nThanks a lot for the help.\nBest wishes,\nDiana T."}, {"role": "system", "text": "Welcome, @Diana_Patricia_Tamay !\nMaybe it won\u2019t work because you\u2019re using NCBI version:\n\nWhat it does\nGiven an Ensembl GTF file, it will extract all information on chromosomes, coordinates, and attributes provided at the specified feature level. Mitochondrial features can also be flagged.\n\nYou can download from Ensembl Fungi here"}], [{"role": "user", "text": "Dear Admin,\nI would like to add gene information (gene name, strand, transcript IDs, uniprot IDs etc) to the DESEQ2 output file generated by Galaxy (EU).\nPlease check the attached slide picture.\nIs there a way to create it using tools in galaxy or using R ?\n\nGALAXY QUESTION.png960x720 344 KB\n\n.\nThanks in advance.\nJames PC"}, {"role": "system", "text": "Dear James,\nYou can use https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deg_annotate/deg_annotate/1.0 for this  purpose. At the moment, it can add gene position, gene symbol and biotype from a GTF file but not uniprot, pathways and GO terms."}], [{"role": "user", "text": "Dear all,\nI can\u2019t upload my fastq.gz files for rna seq analysis since two days. Altogether, I have 12 files of which only the first two or three make the upload successfullyy, the rest fails.\nCould you please help me?\nBest and thanks in advance,\nB."}, {"role": "system", "text": "Hi Bernhard,\nA couple of questions to get started:\n\nIs this on a public Galaxy server? If so, which one?\nCan you share a screenshot, or any text that is being shown?\nHow big are your files?\n\nCheers"}, {"role": "user", "text": "Hi,\nthanks for your help. It worked with retrying the upload for a second or third time now\u2026?\nIt was on the public eu galaxy server, data size is between 1.5 and 2.5 gb.\nCheers"}, {"role": "system", "text": "We had an issue for a few days with a misconfigured upload node.\nWe fixed it and now all the upload tasks are going well.\nSorry for the inconvenience."}], [{"role": "user", "text": "Screenshot 2020-10-13 at 16.36.291686\u00d7264 28.7 KB\nWhy is this happening??? "}, {"role": "system", "text": "Hi @Wendi_B\nIf you are working at usegalaxy.org (not usegalaxy.eu \u2013 how this post was originally tagged).\nPlease see: UseGalaxy.org scheduled maintenance downtime October 13, 2020 \u2013 status and updates\nIf actually working at usegalaxy.eu, please let us know and we can follow up from there."}, {"role": "user", "text": "It was usegalaxy.eu, sadly"}, {"role": "system", "text": "@Wendi_B That Ok, we can recruit help from that server\u2019s administrators here.\nping @wm75 @bjoern.gruening could you help, please?\nMeanwhile, I would suggest that you try it again. Sometimes server issues are transient. The public Galaxy servers are all very large \u2013 some small fraction of \u201cactions\u201d will occasionally fail. A rerun is almost always the best first-pass troubleshooting strategy.\nFailed jobs at usegalaxy.eu are automatically rerun once by default for that very reason.\nHowever, this workflow functionality is a bit newer. So what is going wrong could be technical (Galaxy issue, server issue) or it could be related to the inputs or workflow itself. This is worth rerunning (you first) and looking into if reproducible (admins)."}, {"role": "user", "text": "Ahh yes, this morning it is playing ball! Thank you!"}], [{"role": "user", "text": "Any ideas as to why when I try to click the paired end (as collection) option from the drop down menu in the trimmomatic tool my paired end data collection doesnt appear as an option and only single ended is an option? Thanks in advance"}, {"role": "system", "text": "Welcome @Niamh\nThe Trimmomatic tool at https://usegalaxy.org is functioning correctly for all of the input \u201cselect\u201d options.\nExact tool version: Trimmomatic flexible read trimming tool for Illumina NGS data (Galaxy Version 0.38.0)\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nMaybe your paired-end dataset collection has some problem that needs to be corrected?\nThis history has examples of individual plus various data collection groupings for fastq data. Maybe it will help to compare your collection data structures with those in the shared history? https://usegalaxy.org/u/jen/h/test-trimmomatic-collection-paired-yes-no\nCollection tutorials: Redirecting\u2026. Also, many other tutorials at this site include data collection creation/usage. I\u2019ve also added a tag to your post that links to similar prior Q&A.\nThanks!"}, {"role": "user", "text": "Hi Jenna thanks for the reply, I think it must be my dataset or something I am missing. I have been told I\u2019m working with paired end reads which is indicative of the library prep methods used. But it appears to me when I upload them on galaxy I\u2019m working with read 1 of a pair. I thought they could be combined and I could split them but I don\u2019t think they are.\nThanks,\nNiamh."}, {"role": "system", "text": "Expand a few of your initially uploaded datasets and examine the content.\nSome things to check for:\n\nIs the \u201cdatatype\u201d for the fastq data set as fastqsanger or fastqsanger.gz? Or something else?\n\n\nGalaxy will autodetect fastqsanger for fastq data that is originally uncompressed and will uncompress then assign that datatype to fastq that is originally compressed.\nTo preserve compression, the datatype fastqsanger.gz can be assigned. But you should double check the data really is in that format (fastqsanger) AND is actually compressed.\n\n\nNow click on the \u201ceye\u201d to view the content:\n\n\nDo you see only forward reads?\nOr only reverse reads?\nOr are both forward and reverse reads combined (interleaved/interlaced) into the same single file?\n\nThat review should help to determine the data you have now, and how to get it formatted in the correct way so Galaxy can use it.\nThese FAQs cover the details: https://galaxyproject.org/support\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nReformatting fastq data loaded with NCBI SRA\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\n\nIf you get stuck, we can troubleshoot from here. Please describe the current assigned datatype and the data content itself. Screenshots can sometimes help. If more info is needed after that, we can keep following up until it is right."}], [{"role": "user", "text": "How can I incorporate an annotated bacterial genome for RNA sequencing analysis using galaxy?"}, {"role": "system", "text": "Hi @Luisa_Nieto,\ncould you provide some additional details about that question?\nRegards"}, {"role": "user", "text": "Yes, I am using bwa tool and the reference genome that I need to use is not included. I used the fasta format and the auto option for the algorithm for constructing the BWT index (as one of the options suggested). However, I am having difficulties with the featurecount tool downstream. Therefore, I would like to know if it is possible to follow the recommendation #1 given (to see if I can improve the downstream analysis):\n\" 1. Contact galaxy team using Help->Support link at the top of the interface and let us know that an index needs to be added\""}, {"role": "system", "text": "Hi @Luisa_Nieto\nThanks for explaining \nUse a custom genome fasta and optionally promote that to a custom build to create a \u201cdatabase\u201d metadata that be assigned to datasets. Some tools use the fasta directly and some tools require/use the \u201cdatabase\u201d metadata. Tip: avoid assigning an existing \u201cdatabase\u201d to any data that is associated with a custom genome fasta \u2013 instead create your own custom database key to avoid content conflicts that can be difficult to interpret.\nThe general flow is usually:\n\nUpload the genome fasta to Galaxy\nUpload the genome annotation to Galaxy (GTF is usually best)\nStandardize the format of both\n\nmake sure the \u201cchromosome\u201d IDs are an exact match between the fasta and GTF\nremove any extra content from the fasta > title line\nremove any # header lines from the GTF (and GFF3 would have at least one)\n\n\nCreate the custom build \u201cdatabase\u201d for the standardized fasta\nDone and ready to start an analysis project\n\nThese FAQs explain the \u201chow to\u201d\n\nreference-genomes\nadding-a-custom-database-build-dbkey\nworking-with-fasta-datasets\nworking-with-reference-annotation\n\nYou can also search this forum with keywords to find prior Q&A about troubleshooting problems. I also added a few tags that do the same. You\u2019ll notice that most of those involve format problems and link back to those FAQs or variations of them. Getting both of these reference files prepped and ready to use, at the very start and before starting the actual analysis, will make life happier \n\n\n\n Luisa_Nieto:\n\nHowever, I am having difficulties with the featurecount tool downstream.\n\n\nWe usually only index reference genomes and not reference annotation since the latter changes so frequently. The problem is likely with the current format of your fasta or GTF or both. The help above usually solves those kinds of errors. The custom genome functions works great for bacterial sized (or smaller) genomes, and sometimes is better than a native index. Why? It is easier to compare the IDs when both datasets are in a history (and not behind an index) and it is something you can do immediately without waiting for the public sites to be updated.\nSo, please give that a try, and if the errors persist, you can post back a shared history link that contains both reference datasets and the error data (inputs and outputs undeleted) and we can try to help more. That can be public as a reply, or ask for a private message chat to be started up and you can share the history URL in that.\nNote: Larger genomes (mammalian, some plants) do benefit from server-side indexes \u2013 and we would need a link to the data source for those. The associated annotation data would still be uploaded/standardized independently in the working history by you. Any annotation data that can be indexed on the server is probably already available (rare, and only for a few tools).\n\n\n\n\n Luisa_Nieto:\n\nYes, I am using bwa tool and the reference genome that I need to use is not included. I used the fasta format and the auto option for the algorithm for constructing the BWT index (as one of the options suggested). However, I am having difficulties with the featurecount tool downstream.\n\n\nThe tool choice could also be a problem \u2013 but not necessarily since you are working with a bacterial genome.\nGeneral usage:\n\n\nBWA is for mapping unspliced reads (WGS, ChIP-seq, others)\n\nHISAT2 and Bowtie2 are for mapping spliced reads (RNA-Seq)\n\nFeaturecounts is for counting up the abundance of transcriptome features from RNA-seq experiments (mostly).\n\nFor examples, including workflows, please see our tutorials here\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi,\nI\u2019m trying to install the mothur suite on my local Galaxy (newest version), but I\u2019m running into troubles. When I attempt to install it, my Galaxy stops working. Then I tried to install a single components through the repository, but it has troubles to install the mothur (1.39) dependency. In the terminal I\u2019m getting this message:\nPackageNotFoundError: Packages missing in current channels:\n\nmothur\n\nWe have searched for the packages in the following channels:\nPlease help me, I do not want to be mothurless any longer."}, {"role": "system", "text": "Hi, I asked for some feedback/help from the tool authors at the IUC Gitter chat, feel free to join the chat there, or they might get back here: https://gitter.im/galaxy-iuc/iuc?at=5c8a830fd3b35423cbbaa805\nJust to clarify a few items that will help with troubleshooting:\n\nAre you running the stable branch of the 19.01 Galaxy release?\nWas this a new install or an upgrade from a prior release? Which version?\nHave other tools installed correctly since the install/upgrade?\nAre you running Galaxy inside a virtualenv?\nAre you using the default dependency resolvers or have you made changes? A new 19.01 install has some improvements for dependency resolving with conda.\nMore about Dependency Resolvers in Galaxy.\n"}, {"role": "user", "text": "I was running Galaxy 19.04, but I tried it now with the stable branch of the 19.01 Galaxy release with the same result. Both of them were a new install. I\u2019ve tried to install other tools (with dependencies) and for them it worked.\nI\u2019m running Galaxy inside a virtualenv. I guess I use the default dependency resolvers, because I haven\u2019t changed the settings.\nI will check out gitter. Thank you very much for your help."}, {"role": "user", "text": "If someone has the same issue: I was trying to install it on my mac, while mothur is not available for macOS in Bioconda"}], [{"role": "user", "text": "Hellow, I see the MimodD read ailgnment tool in Galaxy in this example step 1: https://mimodd.readthedocs.io/en/latest/tutorial_example1.html#ex1-step0 . Now i can not find it in same way.\nThank you,\nYang"}, {"role": "system", "text": "Dear Yang,\nthe linked tutorial assumes a full local install of MiModD. Public severs like usegalaxy.org and usegalaxy.eu often choose not to offer MiModD\u2019s own read alignment tool and a couple of others as they would only be redundant with existing alternatives on these well-equipped servers.\nThe recommendation is to use bowtie2 for read alignment before using MiModD tools again for variant calling and linkage analysis.\nMake sure you read https://sourceforge.net/p/mimodd/wiki/MiModD%20on%20public%20Galaxy%20servers/ for usage hints of bowtie2 and SnpEff (for variant annotation) in conjunction with MiModD.\nGood luck with your analyses,\nWolfgang"}], [{"role": "user", "text": "Hi,\nI\u2019m trying to use text reformatting using awk (version 1.1.2) to keep aligned DNA fragments of a certain size range. I want to keep all fragments between size 1 - 120 bp. I\u2019ve used BAM to SAM on aligned files and then text reformatting using awk with the following instruction:\n\u2018 $9 <= 120 && $9 >= 1 || $9 >= -120 && $9 <= -1 \u2018\nHowever, I\u2019m only getting fragments between 100-120bp retained.\nI know I have fragments smaller than 100 bp as I see them in the SAM file and in other size distribution analyses. I tried removing the second portion of the code (negatives) and still get the same result. Am I missing something? Any help would be appreciated.\nThanks"}, {"role": "system", "text": "Hi @stealsh\nmaybe consider using Sample, Slice or Filter BAM on flags, fields, and tags using Sambamba with Filter expression template_length > -120 and template_length < 120. No need for BAM to SAM conversion.\nAs for the awk: maybe try something like $9 > -120 && $9 < 120 or ($9 > -120) && ($9 < 120).\nI am somewhat curious how you got mapped fragments of size 1.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi, I\u2019m writing a tool to concatenate output csv files from a custom Rmarkdown galaxy analysis tool and am having some issues.  I borrowed the concatenation code from the \u201cconcatenate_multiple_datasets\u201d tool, which works just fine to concatenate files.  The problem is, I really just want to append the concatenated dataset to a header line, but whenever I try to do this it looks like Galaxy decides that we\u2019re just going to concatenate file names to the header.\nHere\u2019s the code, where \u201csome stuff\u201d will eventually be a megalist of comma separated column IDs:\n    <command><![CDATA[\n  echo \"some stuff\" > $out_file1\n  cat\n  #for $file in $input\n      ${file}\n  #end for\n  >> $out_file1\n\n    ]]>\n</command>\n\nAnd here\u2019s the output:\nsome stuff cat /mnt/d/git_repos/Galaxy/galaxy/database/objects/c/f/d/dataset_cfda7470-0c6e-4ff1-8e63-1fd8a9bdffd0.dat /mnt/d/git_repos/Galaxy/galaxy/database/objects/a/3/9/dataset_a397895a-57b5-4963-828f-58414280225c.dat\n\nHere\u2019s what Galaxy says it\u2019s doing on the command line, which looks right to me?:\necho \u201csome stuff\u201d > /mnt/d/git_repos/Galaxy/galaxy/database/objects/4/e/6/dataset_4e657268-daa7-4a2a-9c37-c04d7fd4f221.dat\ncat /mnt/d/git_repos/Galaxy/galaxy/database/objects/c/f/d/dataset_cfda7470-0c6e-4ff1-8e63-1fd8a9bdffd0.dat /mnt/d/git_repos/Galaxy/galaxy/database/objects/a/3/9/dataset_a397895a-57b5-4963-828f-58414280225c.dat >> /mnt/d/git_repos/Galaxy/galaxy/database/objects/4/e/6/dataset_4e657268-daa7-4a2a-9c37-c04d7fd4f221.dat\nIf I leave out the echo \"some stuff\" > $out_file1  line everything just concatenates as normal.  I\u2019m a beginner at Cheetah and python so may have just missed something obvious (at least, I hope this is obvious).  Any advice would be appreciated!"}, {"role": "system", "text": "I strongly recommend you do not write a custom tool for this. You are creating a burden on yourself and anyone else that might use this tool to maintain it.\nYou can readily accomplish this task by simply pre-creating a dataset with just the header and adding it to the top of your collection of CSVs. The existing cat tool will then concatenate the header along with your csv data."}, {"role": "system", "text": "Hi @KeelyDulmage,\nyour immediate issue is a missing & between your shell commands.\nBeyond that I agree with @innovate-invent. If you write this just to understand how Galaxy tool development works that\u2019s perfectly fine, but otherwise don\u2019t reinvent the wheel "}], [{"role": "user", "text": "Hi,\nfirst of all, I would like to thank the people involved in running and supporting UseGalaxy.eu, the range and choices of tools available are really great and the platform usually runs very smoothly.\nI don\u2019t know how much work it represents, but I was wondering whether it would be possible to update the transcript quantifier \u201cSalmon\u201d to a more recent version (currently 0.14.0 versus 0.11.2 on usegalaxy.eu).\nI am referring to this tool:\n\n  \n      \n      GitHub\n  \n  \n    \n\nCOMBINE-lab/salmon\n\n\ud83d\udc1f \ud83c\udf63 \ud83c\udf71 Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment - COMBINE-lab/salmon\n\n\n  \n  \n    \n    \n  \n  \n\n\nIt has received a number of fixes/changes/improvements since 0.11.2:\n\n  \n      \n      GitHub\n  \n  \n    \n\nCOMBINE-lab/salmon\n\n\ud83d\udc1f \ud83c\udf63 \ud83c\udf71 Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment - COMBINE-lab/salmon\n\n\n  \n  \n    \n    \n  \n  \n\n\nThanks.\nlabnv40"}, {"role": "system", "text": "@bjoern.gruening  maintains the Galaxy wrapper for salmon, but I\u2019m not sure if we have anyone who can work on this currently."}, {"role": "user", "text": "Ok, no worries.\nThanks."}, {"role": "system", "text": "@labnv40 after our Galaxy Community Conference we will look at this. Its an important tool!"}], [{"role": "user", "text": "Hi, wanted to ask for some advice regarding RNAseq data analysis using Salmon.\nI was going to use this tool to align my RNAseq data to human reference genome and then obtain the read counts for downstream DESeq2 analysis. I obtained human reference genome data from ensembl (cDNA) in fa.gz format. However, I see that for the alignment, I need to have alignment files in .bam format. I also seem to need GTF files to map transcripts to genes?\nI am a little bit at a loss on how to proceed with the alignment. At the moment I have trimmed RNAseq data in fastsanger.gz format and that cDNA reference from ensembl, but I understand some intermediary step is needed?\nWould appreciate any advice on how to proceed - most Salmon tutorials I\u2019ve looked at were for count quantification or written for advanced users of bioconda etc., and the threads on this forum cover issues dowstream of the initial alignment steps. So at the moment I\u2019m struggling \nWould appreciate your help!"}, {"role": "system", "text": "Hi @Egle,\nSalmon works on a reference transcriptome. You don\u2019t need alignments: Salmon takes FASTQ files. Because of alternative splicing, you need a file with transcript to genes map. I had less issues with a tabular transcript-to-genes map file. Here is my old history with Salmon: Galaxy | Australia | Published History | Transcript quantification with Salmon 2018/06/08\nYou may also consider a traditional approach: mapping with HiSAT2 and read counting with featureCounts. GTN provides several detailed tutorials for this approach.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am relativerly new to Galaxy, and i am sorry if this is a very basic question. My main question is what is the intended way to keep permanent record of what has been done to what files in complicated analysis.\nI have run analysis and now is the time to delete the bulky files and make space for another analysis. Is the intended method for keeping record of tools and parameters just the \u201chistory\u201d? My history is relatively messy, because I tried several options of a tool before settling on final analysis. My history also contains intermediate steps of large files i had to delete for space reasons. I thought I could extract workflow from history and cleanup the abandoned branches and keep the workflow as record of the analysis. However, the intermediate deleted files/steps are not part of extracted workflow. Is it possible to extract workflow including deleted intermediate files/steps?"}, {"role": "system", "text": "Hi @BagiM,\nwhat is your goal for maintaining the stages of the analysis that are not part of the final workflow?\nRegards"}, {"role": "user", "text": "Thanks for the response. I probably wasn\u2019t very clear.\nThe analysis steps look like this:\nsample1_file1 + sample1_file2 >step1> sample1_file3 >step2> sample1_file4 >step3> sample1_file5\nrepeat steps 1-3 for all samples.\ncombine file5 of all samples into one large file6\nrun analysis on file6\nsteps 1-3 are simple operations like combining forward and reverse reads, trimming reads, converting fastq to fasta, adding prefix to read names, sampling reads. For space reasons once i got the file6 i needed to delete file3 and file5 for all samples. Then I run the final analysis on file6.\nIf I now try to extract workflow the intermediate steps resulting in file3 and file5 are not present and the workflow is not continuous string of steps."}, {"role": "system", "text": "Hi @BagiM\nPlease review this FAQ for help about managing data, freeing up quota space, and saving prior analysis work: Account quotas\nFor the extracted workflow problems, it sounds like you will need to directly edit the workflow.\nWorkflows are a record of the tools/actions used to create data. Workflows do not contain data themselves. If you deleted intermediate datasets, the tools/actions that created them might not have been extracted, or possibly may just have disconnected tools/actions within the workflow.\nTips:\n\nWorkflows can be created \u201cfrom scratch\u201d or extracted. Sometimes a bit of tuning in the workflow editor is needed when doing either until it runs as expected.\nYou can reconnect tools or add back any tools/actions that are missing from the workflow by editing it.\nIntermediate datasets can be deleted while executing a workflow to save space. This is a per-tool \u201caction\u201d. Tool actions listed in the far right panel of the workflow editor when a tool is selected from the canvas.\nIf you add or adjust workflow steps, you may need to disconnect/reconnect all the \u201cnoodles\u201d between tools again to reset the workflow\u2019s metadata. Reconnect starting from the inputs down through the analysis in the order of execution.\nOnce this workflow is working as expected, try running it again with the original inputs all in one new clean history. That way you can create a \u201chistory archive\u201d and download it, along with the associated workflow, for a complete record of your work. All of your datasets (that were not deleted while running the workflow!) will be inside the archive and can be used any way you want. You can even import that history archive + workflow into your own local Galaxy, to view the data in context, but I wouldn\u2019t try running the workflow locally unless you set up a more complicated local Galaxy, or decide to use a cloud Galaxy.\nIf you are an academic researcher, and need a bit more temporary quota space, most public Galaxy servers will grant that. Each handles requests a bit differently. Let us know if you cannot find where/how to request temporary quota space at the server you are working at (post back the server URL).\nPlease see FAQ above with more details about most of this, plus the tutorials below for how to extract/edit workflows.\n\nGTN tutorials that cover creating or editing a workflow are here. Scroll down past the analysis tutorials, the ones you will be most interested in are listed lower down with the \u201cworkflow\u201d search term.\nhttps://training.galaxyproject.org/training-material/search?query=workflow\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training\n\nCollection of tutorials developed and maintained by the w...\n\n\n  \n  \n    \n    \n  \n  \n\n\nHope that helps!"}], [{"role": "user", "text": "Hello,\nI was wondering if there is a way to add the \u201cfrogs\u201d tool into galaxy public server please ? Without creating an admin interface.\nhttps://toolshed.g2.bx.psu.edu/repository?repository_id=11a27cab761b5c8c&changeset_revision=834843ebe569"}, {"role": "system", "text": "Hello @Melissa_Hayes\nThis tool suite is not available for commercial use, so hosting on the public Galaxy servers is not possible at this time. FROGS: Trainings\nThe tool authors do keep the wrappers updated and have simplified installation instructions. A Docker Galaxy is probably the quickest way to get a server up and running as much is pre-configured eg utility tools and data."}], [{"role": "user", "text": "Hello,\nWe are attempting to install galaxy following the \u201cGalaxy Installation with Ansible\u201d tutorial (Galaxy Installation with Ansible).  Everything is working fine through the galaxy installation.  However, when we try to complete the systemd setup and execute \u201cansible-playbook -vvv galaxy.yml\u201d, we get the following error:\n\n<galaxy-dev.classe.cornell.edu> EXEC /bin/sh -c \u2018rm -f -r /home/galaxy/.ansible/tmp/ansible-tmp-1630271534.663397-13888-233149462200077/ > /dev/null 2>&1 && sleep 0\u2019\nThe full traceback is:\nTraceback (most recent call last):\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/template/init.py\u201d, line 1100, in do_template\nres = j2_concat(rf)\nFile \u201c\u201d, line 24, in root\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/jinja2/runtime.py\u201d, line 903, in _fail_with_undefined_error\nraise self._undefined_exception(self._undefined_message)\njinja2.exceptions.UndefinedError: \u2018__galaxy_user_group\u2019 is undefined\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/plugins/action/template.py\u201d, line 146, in run\nresultant = templar.do_template(template_data, preserve_trailing_newlines=True, escape_backslashes=False)\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/template/init.py\u201d, line 1137, in do_template\nraise AnsibleUndefinedVariable(e)\nansible.errors.AnsibleUndefinedVariable: \u2018__galaxy_user_group\u2019 is undefined\nfatal: [galaxy-dev.classe.cornell.edu]: FAILED! => {\n\u201cchanged\u201d: false,\n\u201cmsg\u201d: \u201cAnsibleUndefinedVariable: \u2018__galaxy_user_group\u2019 is undefined\u201d\n}\n\nAny help would be greatly appreciate.\nMany thanks,\nDevin"}, {"role": "system", "text": "Hey @Devin_Bougie\nwould you mind providing some extra information for us to help you?\n\nDo you set galaxy_manage_user?\nWhat layout method are you using?\nWhat version of ansible are you using?\n"}, {"role": "system", "text": "Specifically `__galaxy_user_group` is undefined if `galaxy_manage_paths` is not enabled \u00b7 Issue #137 \u00b7 galaxyproject/ansible-galaxy \u00b7 GitHub might be related to your issue"}, {"role": "user", "text": "Thank you, that does appear to be the issue as we do have galaxy_manage_paths disabled.  A workaround was to set __galaxy_user_group in group_vars/galaxyservers.yml."}], [{"role": "user", "text": "I am using a standalone instance of Galaxy.  I have downloaded a data set successfully and it is in my history.  I have another user locally, who is logged in successfully and who shows up in the admin panel as a user.  However, when (as user 1, who is an admin) I click on Edit Attributes and then go to the data set\u2019s Permissions tab, this other user does not appear in any of the drop down boxes when I try to assign permissions to them.  The only user that I see is myself, for both the \u201cmanage permissions\u201d and \u201caccess\u201d fields.\nIs there some sort of magical configuration ritual that I must perform, in order to make this dataset visible to other users?"}, {"role": "user", "text": "I should add that I also do not appear to be able to share histories with other users either, and the drop down box to assign permissions simply does not get a match on them or even recognize they are there."}, {"role": "system", "text": "Hi @crh\nPlease see these FAQs: Galaxy Support - Galaxy Community Hub\n\nSharing and Publishing your work\nData Privacy Features\n\nAlso, please see the prior Q&A at this site. I added the tag \u201cshare-or-publish\u201d.\nThese two posts will likely help the most:\n\n  \n    \n    \n    How to share or publish your work in Galaxy -- Datasets, Histories, Workflows, Visualizations, Pages \n  \n  \n    Welcome, @Hnos_Cooper! \nGood question and there wasn\u2019t a prior post that covered all the data sharing options in one place. We are going to use the opportunity to answer your specific question plus link together similar functions into a summary. \n\nUse the Share or Publish function to share any Galaxy Object (History, Workflow, Visualization, Page) with others. \nFAQs: https://galaxyproject.org/support/ \n\nSharing and Publishing your work\nData Privacy Features\n\nA history was shared with me, where c\u2026\n  \n\n\n\n  \n    \n    \n    Help needed regarding usegalaxy.eu and usegalaxy.org usegalaxy.eu support\n  \n  \n    Hi @hhuziii \nSeems your topic post was missed by the community. In case you still need help, or someone else wants more clarification about your questions later on (are likely things other new Galaxy users are interested in) \u2026 replies are below. Thanks! \n\nQuestion 1: Why are not all tools in each GTN tutorial hosted at all public Galaxy servers? How can I find out which server to use, per tutorial? \nHelp 1: Servers host different tools. You can check to see which public Galaxy servers are good c\u2026\n  \n\n"}, {"role": "user", "text": "Hi,\nFrom the docs I see:\n\u201cFor datasets you have created (or were given permissions) you can control what roles can access it. The interface is available on history datasets after clicking the \u2018pencil - Edit attributes\u2019 icon and then moving on the \u2018Permissions\u2019 tab. Watch animation. Adding additional roles to the \u2018access\u2019 permission along with your \u201cprivate role\u201d does not do what you may expect. Since roles are always logically ANDed together, only you will be able to access the dataset, since only you are a member of your \u201cprivate role\u201d.\u201d\nThis is what I am trying to do, actually, but when I try to add any users in that panel (datasets/edit in the URL) on the Permissions tab, the only user that is visible is me.\nI was able to figure out how to publish a history and then share it with other users, but it appears that the permissions options on actual datasets don\u2019t actually do anything, or can\u2019t be set?   What is the purpose of that dialog if other users can\u2019t be added to roles?  Is there a global config setting that turns this capability on or off?"}, {"role": "user", "text": "ok wait\nthat panel is talking about roles not users.  That\u2019s counterintuitive\u2026 anyway, I guess I figured it out, you have to create roles first and then assign them, rather than just assigning permissions to user emails directly.  sigh."}], [{"role": "user", "text": "Hi all,\nI have browsed for similar questions, found them, but did not get a working solution from it. Hence a repost, sorry.\nI have troubles logging into FTP to upload files.\n\nI can login to the website of usegalaxy.eu normally, using either username + password or email + password\nusing Filezilla, login fails using the following: host = usegalaxy.org, username = email, password = password, port = [empty], quickconnect\noutput:\n\nStatus:       Resolving address of usegalaxy.org\nStatus:       Connecting to 129.114.60.60:21\u2026\nStatus:       Connection established, waiting for welcome message\u2026\nStatus:       Initializing TLS\u2026\nStatus:       Verifying certificate\u2026\nStatus:       TLS connection established.\nCommand: USER [bleep]\nResponse:  331 Password required for [bleep]\nCommand: PASS *\nResponse:  530 Login incorrect.\nError:         Critical error: Could not connect to server\n\nsimilarly using lftp:\n\nlftp -u [bleep] usegalaxy.org\nput [file]\nput: [file]: Login failed: 530 Login incorrect.\nCan somebody explain why I can login to the website, but not the FTP? I am not over my data quota. Not sure how to proceed\u2026\nMany thanks for any help you can offer!"}, {"role": "system", "text": "Hi @leonvgurp\nI asked the EU admins at Gitter to find out status about FTP uploads.\nThe server URL you are using is for usegalaxy.org. Every public server has its own distinct FTP URL (when FTP is enabled, and it at EU as far as I know). So, you need to know the proper FTP URL for usegalaxy.eu, otherwise the connection will error the way you post it (mismatched account credentials and server URL) and the one I think is correct, when tested, had other problems (what is displayed in the Upload tool there). Specificaly: ftp.usegalaxy.eu\nWorkaround if you are in a hurry: Skip FTP and try uploading the files by browsing them in the Upload tool. The Upload tool at many public Galaxy servers, including EU, can handle files larger than 2 GB now.\nQuestion posted here. The EU admins may write back here or there, feel free to join in either place: https://gitter.im/usegalaxy-eu/Lobby?at=5e9a12192ff88975b42a64df\nping @bjoern.gruening @wm75"}, {"role": "user", "text": "Hello Jennifer, thanks for your clear answer! I was indeed able to login using ftp.usegalaxy.eu, but my ftp folder is empty, while I have a number of files in my account. does ftp uploaded data end up in a different location than data that was uploaded via the webserver? Again, many thanks for your help!"}, {"role": "system", "text": "@leonvgurp\nGlad you got connected! The FTP directory will be empty until you transfer data. Try with a small file as a test and you\u2019ll figure out how it works. Then once you understand, go ahead and move your larger data \nMore about FTP transfer to Upload data:\n\nUpload files to the target FTP location following the instructions here: https://galaxyproject.org/ftp-upload/\n\nFTP locations are account and server-specific. It is an intermediate data staging area. The FAQ above has the FTP URL for Galaxy Main (usegalaxy.org) included as an example. Other servers will usually note the FTP URL in the Upload tool (if FTP is enabled on that server).\nGalaxy EU https://usegalaxy.eu has server-specific help here: https://galaxyproject.eu/ftp/\n\nThe files stay there for 3 days (max) at usegalaxy.* servers. Other public servers may have different criteria \u2013 the home page may state how their site is configured, or you can try contacting them.\nMove the files from your FTP location to a history with the Upload tool within those 3 days. Data will load as a dataset, the same as with other Upload methods.\nOnce an FTP\u2019d file is added to a history as a dataset, or if you don\u2019t move it within 3 days, it will no longer be in that temporary FTP staging location.\nOnly files uploaded with FTP will ever be in your FTP location.\nFiles may start to show up in the FTP location listing before the transfer is completed! So be sure to not move data into a history before FTP is completed successfully, or you\u2019ll need to start over.\nYour FTP location is associated with your account. The same account as used originally to FTP transfer the data.\n\nContact information for all known public Galaxy servers is usually on the server\u2019s home page and/or in their directory listing here: https://galaxyproject.org/use/\nGeneral Galaxy questions and most usegalaxy* questions (that do not require privacy) can be asked at this forum. And if you cannot find contact information for other public Galaxy server(s) for some reason (for private or server-specific help), we can try to help locate it, based on the server\u2019s URL (you\u2019ll need to post that info with your question).\nThanks!"}], [{"role": "user", "text": "I am trying to develop a tool on my local galaxy.  Here are the commands and inputs:\n<command>\n    <![CDATA[\n        Rscript '${__tool_directory__}/correlationMatrix.R'\n            -f $expressionData\n            -m $method\n            -c $correlationMatrix              \n\n]]>\n\n<inputs>         \n    <param name=\"expressionData\" type=\"data\" format=\"tabular\" />\n\n    <param name=\"method\" type=\"select\" label=\"select one method\"/>\n            <option value=\"pearson\" selected=\"true\">Pearson(use default) </option>\n            <option value=\"spearman\">Spearman rank </option>\n            <option value=\"kendall\">kendall rank </option>\n     </param>\n\n\n</inputs> \n\nBut now galaxy show en error message: \u201cCurrent on-disk tool is not valid\u201d\nand no method can be selected.\nWhat\u2019s wrong?"}, {"role": "system", "text": "It means that your tool is not valid, and so you\u2019re seeing the last valid version.\nI\u2019d recommend running planemo lint on your tool, which should tell you what went wrong."}, {"role": "user", "text": "What is \" planemo lint\"? How can i run it?"}, {"role": "system", "text": "Have a look at https://planemo.readthedocs.io/en/latest/readme.html#quick-start"}], [{"role": "user", "text": "Hello\nWhat is the normalization method used in Deseq2 in galaxy.eu exactly ?\nI mean RPKM or TPM ?\nCould some one please explain .\\\nbest"}, {"role": "system", "text": "Hi Amir,\nTake a look at this post and the links that it has.\nHope this helps,\nDave C"}, {"role": "user", "text": "Hi\nThank you for your response.\nBut i have been searching into galaxy for more detail of normalization method used in deseq2 in galaxy and found this :\nDESeq2 normalization\n-Objective: Obtain comparable counts between samples, whatever the difference of their libraries sequencing depth\n-Accepted values: NGS counts (positive integers ; no missing values)\n(null values, will stay null after transformation)\n-Range of values: Input: [0;100.000] / Output: [0; 100.000]\n-Adapted for: Differential analysis\nBut i didnt understand it well.\nWhen they ask me what is was your normalization method in your analysis?? Still dont know what to answer\nReally need some help on this"}, {"role": "system", "text": "Hi @amir,\nthe normalization method used by Deseq2 is called DESeq2\u2019s median of ratios . This method relies on the calculation of size factors (s), whose objective is to render counts from different samples, which may have been sequenced to different depths, comparable. Herein lies the importance of normalization performed by Deseq2. Deseq2 normalized values are the result of dividing the raw counts by sample\u2019s normalization factor (size factor, s).\nYou can find an extended explanation here .\nRegards"}], [{"role": "user", "text": "Hi there,\nI am really new to this and I am trying to RNA Star some trimmed sequences (cutadapt) against an imported genome from NCBI. I imported using \u201cNCBI Accession Download\u201d.\nI have been following the \u201cHands-on transcriptomics tutorial\u201d however, when I multiQC the RNA Star log, the webpage output is over a page of this: \"MultiQC Report N4IgzgLghgTg+lANgSwOYDsC2BTdE4AOiA9hCAFwAEoRpcEAngdhZSAEaxyoxQEAWIADRswUTEWxhWAbRkgAwgAUADACYALAE4A7GrgBGEAF1jIkABMo0MNgjSqc0OnEsqIAKrpkARwCu2IgMlJh8zBbCbFbQsgYGGho6ABxaAKwAdCpmbADGxCQwrCAAxBoAzDrs7EYAviLOrkUAsmHYFpQQxCF+iBDIkpQkOciRltZQsmVqZRoqmdkgeQVFxTo57KnYOSB11CAuOM2t7Z0dxF2h6MFDI+bRE44zZfPmS8SF7sUAZjpQZanbXYNQ7uLyhAjhKidLpgfjvMh3caxFRJJIqDJZV75d4raoojSA+r7RqgrDHKikfjYQqImKObRopI6F65bEfNirL4qbkqHamLHoL5oVigZARdyQLhINBYXD4WgIth9CCINxsADKABUAIIAJSo2pQGBweEo6ryMCkowYiCg7BWlF12CgFmkrwIYAA7sgIDl+HA8n48GA4Lb2IEigA5PyYcMwSjEL6O52unY1GpAA=== {\u201cdecimalPoint_format\u201d: \u201c.\u201d, \u201cnum_datasets_plot_limit\u201d: 50, \u201csample_names_rename\u201d: , \u201cshow_hide_mode\u201d: , \u201cshow_hide_patterns\u201d: , \u201cshow_hide_regex\u201d: \"\nRather than the graphical output that is expected?\nAs I say, I am really new to this so any help would be greatly appreciated.\nThanks"}, {"role": "system", "text": "Hi @reltubycul,\nare you running the analysis in a public (e.g. usegalaxy.eu) or in a local Galaxy instance?\nRegards"}, {"role": "system", "text": "Hi @reltubycul\nIf this was happening at usegalaxy.org with MultiQC aggregate results from bioinformatics analyses into a single report (Galaxy Version 1.11+galaxy0), please try to view the HTML result again now. You won\u2019t need to rerun or do anything else special. I just adjusted the rendering (was missed during the prior version update).\nIf you were working at some other public Galaxy server, two choices: 1) post the server URL back here and we can try to help \u2013 or 2) contact that server\u2019s administrators directly and ask. They\u2019ll probably need the full tool name plus version (found at the top of a tool from).\nWhat to do if working at your own server (must be an admin): HTML is not rendered by default for security reasons (and data in that format cannot be Uploaded for similar reasons). But, HTML can be created by certain tools. To have that output rendered correctly, move the tool/version from the \u201cHTML Sanitized\u201d to the \u201cHTML Rendered\u201d list under Admin > Tool Management > Manage Allowlist.\nThanks for reporting the problem, and hope that helps! "}], [{"role": "user", "text": "Hi all,\nI got paired-end .fastq files from Genewiz Ampicon EZ service.\nThen I try to de-multiplex the files by the barcodes (in my primers) by the Barcode Splitter tool.\nHowever, that tool returned my .fastqsanger files instead of .fastq format.\nBecause I want to use CRISPResso2 (CRISPResso2) to analyze my result and CRISPRresso only recognize .fastq format. Does anybody have experience converting .fastqsanger to .fastq?\nI have tried Fastq Groomer tool and Fastq-sanger to Fastq Sequence Converter (http://sequenceconversion.bugaco.com/converter/biology/sequences/fastq-sanger_to_fastq.php)\nBut they dont seem to help, the resulting files still cannot be recognized by CRISPResso."}, {"role": "system", "text": "Hi @Woofung,\ncould you provide me the output of FastQC on your samples? Usually the extension doesn\u2019t provide so much information about the specific fastq encoding version. I think there should be some additional problem since the example sequences provided by the developers are fastqsanger (Sanger / Illumina 1.9).\nRegards"}, {"role": "user", "text": "\n\n\n gallardoalba:\n\nFastQC\n\n\nHi gallardoalba,\nThanks for the help.\nI am new to this field, and I am not sure if this is what you were asking for.\nI run the FastQC for the file I upload to split:\n\nThis is the split file that I received.\n"}, {"role": "system", "text": "Yes, your sequences are in fastqsanger format. You should be able to use them with CRISPesso2, since as I mentioned you previously, the test datasets provided by CRISPesso2 developers (e.g.nhej.r1.fastq.gz)  are fastqsanger too, despite having the fastq file extension.\n\nRegards"}], [{"role": "user", "text": "Good evening, I had started a Trimmomatic job on a paired end dataset at about 1:10 UTC. It\u2019s been nearly 3 hours now with the jobs stuck in queue. Are you currently experiencing a lot of traffic?\nAnother question I had was how many jobs I could run concurrently? I was going to queue FastQC as well, but I\u2019m unsure if queuing too many jobs might lead to an error. I apologize if the questions were a little basic, I\u2019ve just started using Galaxy. Thanks for your time!"}, {"role": "system", "text": "Hi @aramesh\nWere the jobs queued (gray in color) when you wrote in? That means they were staged and will process as resources become available. After 4 days, those likely completed. All public Galaxy servers are very busy, not just usegalaxy.org.\nAll compute-intensive jobs are queuing longer than usual due to the high load on the servers and clusters. Jobs queuing for several days would not be unusual, and even then only a subset of jobs may run at a time. Avoid deleting and rerunning \u2013 that will only place the jobs back at the end of the queue again, further extending wait time.\nUsing workflows is a good way to ensure work proceeds at the fastest possible rate. The entire analysis is queued up and will run as resources become available, without intervention.\nYou can also queue up jobs directly. Having many queued jobs is fine. The work will generally execute in the order submitted \u2013 but some tools may route to a different cluster than others and run out of order (unless dependent on some upstream job to run). So yes, there are some limits on the number of concurrent jobs anyone will have \u201cexecuting\u201d (actually running then completing, not just queued) but what tools those are can also be a factor.\nIf you have urgent work, setting up your own Galaxy server is an option, too.\nI added a few tags to your post that will link to other Q&A about all of this, including FAQs and resource links. But if you have a specific question that doesn\u2019t seem to be addressed yet or anything is unclear, we can try to help more/clarify directly here.\nThanks!"}], [{"role": "user", "text": "Hello\nI want to use the Inspect AnnData tool to generate a matrix from my data.\nThe annotation file that I give to this tool as input is about 26 GB and every time the output of this tool turns red.\nIs it because of the large amount of data? And if so, is there a way for me to get a matrix from my annotation data?\nThank you in advance for your advice"}, {"role": "system", "text": "Hi @maryam-gh99\nYou started to ask a question in this thread Problem using Import Anndata tool - #2 by jennaj but there wasn\u2019t any followup. I\u2019ll close that thread and we can continue here. We can\u2019t help without more details. Many things can contribute to job failures.\nIf you think the data might be corrupted, or too large to process (unlikely if you are working at UseGalaxy.eu), these are a few items to try:\n\n\nCheck to make sure that this very large file is intact. The Inspect AnnData tool can be used to collect general information as a sanity check.\n\n\nTry converting just parts of the file to a matrix, or other summaries, instead of all of it. The errors (if any) from this might inform more about what might be wrong.\n\n\nAfter running more checks, if you have an error that you don\u2019t understand, you can post that back with information from your checks so people can help you to troubleshoot. This can include job parameters, error messages, and (optionally) a shared history.\n\n\nMore help is in the single cell tutorials here, including more QA/QC checks:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi all,\nI\u2019m trying to run the goseq tool on my data.\nHowever, to do this the files need to be converted to genes with true/false for differential expression. The help manuel says to do this using the compute an expression on every row tool. However I do not find this in the left row with tools.\nHow can I access this tool?"}, {"role": "system", "text": "1 posts were split to a new topic: Calculate log10 with \u201cCompute an expression on every row\u201d"}, {"role": "system", "text": "Resolved: the tool is found in the Text Manipulation tool group at most public Galaxy servers."}], [{"role": "user", "text": "I am working on rna seq analysis and am trying to add a T/F significant differential gene expression column onto my DeSeq2 results for use in goseq. Compute is failing when it gets to rows with NA listed as the p-value because it\u2019s not converting the strings to floats, but even when I play with the error-handling options it is failing no matter what. I am using c7<0.05 as my function where c7 is the adj. p-value. I have already tried to Skip the Row & Produce an empty column to deal with the errors. TIA"}, {"role": "system", "text": "Hi @jbliss\nWhen choosing an Error Handling option for the Compute on rows tool, the last 2 of the 3 options go together.\nThat means if you are setting a non-default option for If an expression cannot be computed for a row, make sure that the option right above it Fail on references to non-existent columns is set to NO.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Reference-based RNA-Seq data analysis\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "THANK YOU!!! I knew it had to be something simple and I wasn\u2019t clicking all the right settings together. The Training Material is super helpful-- thank you so much for sharing!"}, {"role": "system", "text": "@jbliss glad to hear that the GTN was helpful.\n@jennaj just to clarify how the different error handling options are interconnected:\n\nthe most important option with conversion errors like this is Autodetect column types.\nConversion to the types Galaxy has recorded for all columns is otherwise the first thing the tool does, and this doesn\u2019t count yet as trying to compute the expression. So if this step fails, that\u2019s that.\nafter you\u2019ve disabled the above option, you will have to use an extended expression that does the type conversion itself, like float(c7).\nNow, in this case here, that attempt will also fail, but now the failure is part of computing the expression, and can be handled via If an expression cannot be computed for a row\n\n\nFail on references to non-existent columns does not matter in the case here because the column does exist, just its value cannot be converted to a float.\nYou\u2019re right, of course, that if a missing column is the issue, then you need to set this option to No and configure error handling at the same time.\n"}], [{"role": "user", "text": "I can\u2019t access usegalaxy.org page on safari as well as google chrome. It had been working perfectly fine yesterday. Can someone please help me out with the issue?"}, {"role": "system", "text": "\nThis page isn\u2019t working\nusegalaxy.org is currently unable to handle this request.\nHTTP ERROR 500\nMe too.\nIt\u2019s so sad.\nGOGO!POWER RANGER!"}, {"role": "system", "text": "Hi @ambarisha_samantaray @userlee7655\nThe database crashed for a few hours last night at UseGalaxy.org. All is up now. Please try to connect again if you haven\u2019t already.\nApologies for the confusion!\nStatuspage: https://galaxyproject.statuspage.io/"}], [{"role": "user", "text": "Hello,\nI am following a Galaxy tutorial \u2192 Reference-based RNA-Seq data analysis \u2013 and have run into an error. While using RNA STAR, the option to \u201cSelect reference genome\u201d drop down arrow is not functioning properly. Can you please advise. Attached is an image illustrating the issue.\nGalaxy_Assignment_Annotated2558\u00d71038 477 KB\nThanks,\nZ"}, {"role": "system", "text": "Hello @zwyoas\nThere were server issues yesterday at Galaxy Main https://usegalaxy.org leading to server-side errors (across tools), and more work has been occurring overnight and continues through now. Meaning: even jobs successfully submitted may fail for server-side reasons, or have an extended queue time then eventually fail. It is best to wait a bit for that process to complete before rerunning.\nWe are investigating whether the problem with RNA-Star not locating indexed reference genomes is related or not.\nMore feedback soon.\nThanks for reporting the problem!"}, {"role": "user", "text": "jennaj,\nThank you for the response.\nAfter searching for a workaround, the reference genomes can be accessed in RNA STAR versions 2.5.2b-0 and 2.6.0b-1, but not 2.7.2b. Hopefully this helps debugging the issue.\nCheers,\n~Z"}, {"role": "system", "text": "Update:\nThe most current version of the tool has more reference genomes added at UseGalaxy.org: RNA STAR Gapped-read mapper for RNA-seq data (Galaxy Version 2.7.5b)"}], [{"role": "user", "text": "I\u2019m trying to reset my https://usegalaxy.org password but the reset link I am sent  never leads to a page that would allow to change password but always lead to the login page Galaxy\nAny ideas what else I can try?\nthanks a lot"}, {"role": "system", "text": "Thanks for reporting this, I confirmed this as a bug and we are going to track it in password reset link does not work on usegalaxy.org \u00b7 Issue #15451 \u00b7 galaxyproject/galaxy \u00b7 GitHub and fix it asap."}], [{"role": "user", "text": "Currently using this tutorial as I am working with Arabidopsis Thaliana. I would assume the uploaded files of mature_miRNA , star_miRNA , and miRNA_stem-loop are all specific to A.Thaliana so no change is needed there. I ran my uploaded original sequencing up to the mapping and quantification using the tutorial\u2019s reference datasets. However, how could I proceed with differential expression when I don\u2019t have a control sequence to use?"}, {"role": "system", "text": "Hi @alio\n\n\n\n alio:\n\nI would assume the uploaded files of mature_miRNA , star_miRNA , and miRNA_stem-loop are all specific to A.Thaliana so no change is needed there.\n\n\nYes, the reference data is in full as far as I know.\n\n\n\n alio:\n\nI ran my uploaded original sequencing up to the mapping and quantification using the tutorial\u2019s reference datasets. However, how could I proceed with differential expression when I don\u2019t have a control sequence to use?\n\n\nThe DE analysis could compare the conditions you did generate. Two or more conditions, each with two or more replicates, are the minimum technical requirements. Three replicates per condition is a common minimum scientific requirement."}], [{"role": "user", "text": "Hi,\nI am trying to upload my own data, but I get the following error:\n\u201cWarning: Waiting for server to resume\u2026\u201d\nIs this normal? How long should I wait? If I want to cancel this upload how do I do it?"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI and @atschultz414,\nif you are using usegalaxy.org, it is probably due to heavy load on the server. I recommend you to try a different one, such as usegalaxy.eu or usegalaxy.org.au.\nRegards\n\n  \n    \n    \n    Error connecting to the server when uploading fastq.gz usegalaxy.org support\n  \n  \n    @alexgal & @etecle \nThe Galaxy Main https://usegalaxy.org public server is very, very busy. Some patience will be needed for all operations. \nThere is also a warning banner at the top of that server specifying certain tools that are running with reduced memory or are temporarily disabled (server resource-related). That banner will update and/or go away as status changes. \nFor those working at other public Galaxy servers, also experiencing delays, it is expected that most are also very busy. But \u2026\n  \n\n"}], [{"role": "user", "text": "Hi Galaxy Team!\nWe try to use RNAStar for analyzing our paired-end sequencing data. After building a list of dataset pairs, the platform returns the following message to us when we try to load RNAStar:\nLoading tool toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.8a+galaxy0 failed: Bad Gateway (502)\nThis does not happen when we try to load the tool in the original list of unpaired .fastq files. Can you provide any help to solve this problem?\nBest regards,\nMax"}, {"role": "system", "text": "Hi @Max,\nthis problem seems to be related to the storage issue. We hope it will be fixed this week. Sorry for the inconvenience.\nRegards"}], [{"role": "user", "text": "Hi,\nI am concatentating some RNA-seq datasets and have been noticing the stated gb size is off. This only seems to be occurring with forward strands, as my reverse strand concatenated file is the appropriate size. It was saying 21gb when it should have been 24gb (I double checked this on Galaxy.org) I made no errors when entering the data. I ran it a second time and while it is green it is saying 0 bytes for the size, and there does not appear to be any data.\nI am wondering if maybe the tool is experiencing a bug?"}, {"role": "system", "text": "Hi @R_R_R,\nthanks for the report, I\u2019ll check it."}, {"role": "system", "text": "Hi @R_R_R\na few comments; I recommend you to use this tool for concatenating the FASTQ files Concatenate. It is the UNIX version, and is supposed to be faster; the failure can be due to a memory issue. Regarding the different datasets files, the size provided by Galaxy is an estimation. I suggest you to count the lines of both files (the one in usegalaxy.org and in usegalaxy.eu) by using the Count lines tool.\nRegards"}], [{"role": "user", "text": "I have tried the gubbins analysis for 314 genomes but after >5 days, no results are available.\nThe initial fastq files are from ion torrent, and then I ran the Snippy, the Snippy-core, and the Snippy-clean_full_aln, all of them gave green results. But with Gubbins, error.\nI tried with a subset and the error persists.\nDoes anyone know how to proceed?"}, {"role": "system", "text": "Hi Monique, same here. Do you have any feedback?"}, {"role": "system", "text": "Hi @Monique_Tiba and @carlos.pqc,\nwe have increased the number of cores up to 6 and the tool has been updated to version 3.2.1. I hope it will solve the problems. Please, try to run Gubbins again from Monday.\nRegards"}], [{"role": "user", "text": "As you can see in the image, the Kmer distribution field is empty: \"No options available\"\nWhat am I missing?\nIs there another tool I must use first to generate a Kmer distribution?\n                    \n            \n          \n\n"}, {"role": "system", "text": "Hi @fullmooninu\nThere are two versions of the tool: KrakEN and `KrakEN2\".\nThe admins of the EU server would need to create/provide the built-in index for KraKEN.\nThis older version of the KrakEN tool suite is available at usegalaxy.org and usegalaxy.org.au \u2013 and it has a Kmer indexed at both.\nBTW \u2013 it looks as if you are searching the tool panel with a typo. Try \u201ckraken\u201d instead of \u201cbraken\u201d to find all tools in the suite. Or, with \u201ckraken2\u201d. The two versions are incompatible as far as I know, with distinct indexes.\nTutorials: Search Tutorials\nping @gallardoalba @wm75"}, {"role": "system", "text": "I installed the DBs, please check tomorrow again. Thanks!"}], [{"role": "user", "text": "Hi, I am leaning from the \u2018\u2018Exome sequencing data analysis\u2019\u2019 tutorial in order the develop the skill to analyze my genomics data against the reference genome for identifying variants on the specific genes of my samples.\nI followed the tutorial to use the \u2018\u2018SnpSift Annotate\u2019\u2019 to give variant ID to the vcf file of the \u2018\u2018father\u2019\u2019.\nHowever, the dbSNP vcf file (dbSNP_138.hg19.vcf) offered in the tutorial does not contain ID information.\nSo I could run SnpSift Annotate, but the output vcf still does not have ID.\nI also downloaded the dbSNP vcf file from the NCBI database. I could not run the SnpSift Annotate of the \u2018\u2018father vcf\u2019\u2019 against the NCBI dbSNP vcf, I guess it is because the chromosomes in the NCBI dbSNP vcf file are not named as \u2018\u2018chr#\u2019\u2019 but only \u2018\u2019#\u2019\u2019.\nDo I understand the error correctly? Any suggestions how I can solve this?\nMany thanks.\nSusan"}, {"role": "system", "text": "Hi Susan,\nso you\u2019re trying a tutorial that\u2019s currently broken in several ways, and I\u2019m feeling somewhat guilty because I was planning to fix it last week but didn\u2019t get round to it.\nHowever, by diagnosing correctly the first problem, and even trying to work around it, you\u2019re demonstrating that you have a sound understanding of the topic.\nIndeed, the dbsnp file used in the tutorial is not what it\u2019s supposed to be. I would have thought that the different chromosome names between your vcf and the NCBI file could be handled by Snpsift, but it\u2019s possible that they pose a problem. One other possibility I could think of is that the genome versions differ. Did you download the dbsnp hg19 or the hg38 version?\nAnyway, you don\u2019t really need to run this step at all when using GEMINI next because that tool knows about the dbsnp ids anyway. However, if you\u2019re working on usegalaxy.org, you will soon find out that GEMINI is not currently available there. You can find it on usegalaxy.eu, but at a newer and heavily reworked version than what\u2019s described in the tutorial.\nSo overall this experience will not be as comfortable as it should be, and I\u2019m sorry for that.\nIf this is not super urgent, you may be better off waiting 1 or 2 weeks longer for my fixes.\nBest,\nWolfgang"}], [{"role": "user", "text": "Hello to everybody!!\nI am running the trinotate workflow and I noticed that the \u201cSignalP\u201d has a new versions available.\nSignalP is in v3 instead of v5.\nHope you can update this tool.\nThanks a lot in advance!"}, {"role": "system", "text": "Hi Agustin,\nI think you could ask the v3 wrapper developer for upgrading it\n  \n      \n\n      GitHub\n  \n\n  \n    \n\npeterjc - Overview\n\n  peterjc has 168 repositories available. Follow their code on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      github.com\n  \n\n  \n    pico_galaxy/tools/protein_analysis at master \u00b7 peterjc/pico_galaxy\n\n  master/tools/protein_analysis\n\n  Galaxy tools and wrappers for sequence analysis. Contribute to peterjc/pico_galaxy development by creating an account on GitHub.\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nor try yourself making a pull request \nCiao"}], [{"role": "user", "text": "I have been using LDA on several data sets. For some it seems to be successful and for other very similar data sets (all that has changed is my grouping variable of interest) the analysis does not work beyond the Effect Size calculator. I have pasted the error message below. Please let me know what the next best steps are.\nError Message:\n/galaxy_venv/local/lib/python2.7/site-packages/rpy2/rinterface/init.py:185: RRuntimeWarning: Error in (function (file = \u201c\u201d, n = NULL, text = NULL, prompt = \u201c?\u201d, keep.source = getOption(\u201ckeep.source\u201d),  :\n:1:47: unexpected numeric constant\n1: effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\"1\n^\nwarnings.warn(x, RRuntimeWarning)\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/run_lefse.py\u201d, line 89, in \nif params[\u2018rank_tec\u2019] == \u2018lda\u2019: lda_res,lda_res_th = test_lda_r(cls,feats,class_sl,params[\u2018n_boots\u2019],params[\u2018f_boots\u2019],params[\u2018lda_abs_th\u2019],0.0000000001,params[\u2018nlogs\u2019])\nFile \u201c/export/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/lefse.py\u201d, line 199, in test_lda_r\nrobjects.r(\u2018effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\u2019+p[0]+\u2019\"]) - mean(LD[sub_d[,\u201cclass\u201d]==\"\u2019+p[1]+\u2019\"]))\u2019)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/init.py\u201d, line 358, in call\np = _rparse(text=StrSexpVector((string,)))\nrpy2.rinterface.RRuntimeError: Error in (function (file = \u201c\u201d, n = NULL, text = NULL, prompt = \u201c?\u201d, keep.source = getOption(\u201ckeep.source\u201d),  :\n:1:47: unexpected numeric constant\n1: effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\"1\n^\nThank you for any and all help!"}, {"role": "system", "text": "have you solve that problem? I\u2019ve met this problem, too."}, {"role": "system", "text": "Could someone of you provide test data or a link to some public history for reproducing the issue.\nFor my reference, linked issue here: https://github.com/galaxyproject/tools-iuc/issues/2816"}], [{"role": "user", "text": "Hi All,\nI just stood up a new Galaxy server (21.05). I have installed FASTQC 0.73 and allowlisted it but the HTML is still being blocked I think as the output is still blank. When I uncheck it from the allowlist I do see the sanitized html output and the jobs are completing fine.\nI have also tied older versions of fastqc.\nNot seeing anything in logs\u2026\nAnyone have any ideas?\nThanks,\nMorgan"}, {"role": "user", "text": "The trick here was I did not have:\n          XSendFile on\n          XSendFilePath /\n\nset in the     <Location \"/\"> section of httpd conf."}, {"role": "user", "text": "Or more likely when using nginx make sure you follow the wiki:\nhttps://docs.galaxyproject.org/en/master/admin/nginx.html#sending-files-with-nginx"}], [{"role": "user", "text": "I tried 3 different ftp clients (FileZilla, CyberDuck, and CoreFTP) and all of them are unable to open a connection on usegalaxy.org using my username and password.  I am able to sign in with my username and password on the website, but the server rejects it through one of the ftp clients.  I get these errors in FileZilla:\n\n\n\n\nStatus:\nResolving address of usegalaxy.org\n\n\n\n\nStatus:\nConnecting to 129.114.60.60:21\u2026\n\n\nStatus:\nConnection attempt failed with ECONNREFUSED - Connection refused by server, trying next address.\n\n\nStatus:\nConnecting to 129.114.60.56:21\u2026\n\n\nStatus:\nConnection attempt failed with ECONNREFUSED - Connection refused by server.\n\n\nError:\nCould not connect to server\n\n\n\nI seems as if my account is not allowed to access the ftp server on galaxy, since I don\u2019t have the \u201cchoose ftp file\u201d button when I click \u201cupload data\u201d.   Any suggestions on how to get this to work so I can upload my 64 GB files?"}, {"role": "user", "text": "I\u2019ve now tried it on the command line and it say the Galaxy Main server is refusing the connection.  It allows me to log in with my username and password.  It refuses the connection at the moment of transfer. How can I get the Galaxy server to allow me to transfer files to it?"}, {"role": "system", "text": "usegalaxy.org does not support FTP uploads anymore, please see details here: The usegalaxy.org FTP service will be decommissioned on August 12, 2022"}], [{"role": "user", "text": "Hello,\nI had the same problem while using the program.\nHave you solved the problem?\nWhere should I inquire about this matter?\nCould you please tell me how to fix it?\nThank you"}, {"role": "user", "text": "Hi,\nAs you told me, the file will be uploaded, but I can\u2019t receive the microbiome result.\n(The one I uploaded is an Excel file.)\nI don\u2019t know if the file I uploaded is in the wrong format.\nI don\u2019t know where it went wrong.\nCould you please help me?\nThank you so much.\nHave a great day \nRegards\n2022\ub144 3\uc6d4 30\uc77c (\uc218) \uc624\uc804 2:21, Jennifer Hillman-Jackson via Galaxy Community Help <notifications@galaxy.discoursemail.com>\ub2d8\uc774 \uc791\uc131:"}, {"role": "user", "text": "hi,\nI uploaded the file,\nCould you tell me how to solve the problem of not opening the file in format data for LEfSe?\nThe file I uploaded is an Excel file.\nI have attached the file for my problem.\nThank you,\nRegards\n2022\ub144 3\uc6d4 30\uc77c (\uc218) \uc624\uc804 2:21, Jennifer Hillman-Jackson via Galaxy Community Help <notifications@galaxy.discoursemail.com>\ub2d8\uc774 \uc791\uc131:\n\nproblem.JPG1924\u00d7848 357 KB\n"}, {"role": "system", "text": "Hi @Kang\n\n\n\n Kang:\n\nThe file I uploaded is an Excel file.\n\n\nThe input data is expected to be in tabular format.\nIf the data is in an Excel file right now, you can output a tabular (plain text) dataset from Excel on your computer, then upload just the tabular dataset to Galaxy."}], [{"role": "user", "text": "UPDATE: I originally posted before Christmas but my posts now has updates:\n\nreflecting the use of the original Bioblend API vs the OO BioBlend API).\nwith details about manual upload to Galaxy GUI\n\nI am trying to use BioBlend to upload a file (locally stored on my computer). I have tried doing so in 2 ways:\n(1) Using the original Bioblend API\n(2) Using the OO BioBlend API\nIn both cases, I obtain the same error (at least it seems). Here is my code:\nUsing original BioBlend API (bioblend.galaxy)\nimport os\nfrom bioblend.galaxy import GalaxyInstance\nfrom bioblend.galaxy.histories import HistoryClient\nfrom bioblend.galaxy.tools import ToolClient\n\nos.chdir(\"/home/username/Documents/projects/galaxy\")\n\n# Set up GalaxyInstance ####\ngi = GalaxyInstance(\"https://usegalaxy.org\", \"my-instance-number\")\n\n# Make a new history ####\nhistoryClient = HistoryClient(gi)\nhi = historyClient.create_history(\"tmp2-100119\")\n\n# Upload datasets ####\ntoolClient = ToolClient(gi)\ncounts = toolClient.upload_file(\"example.txt\", hi[\"id\"])\ncounts2 = toolClient.upload_file(\"counted.txt\",hi[\"id\"]) # not working\n\nUsing OO BioBlend API (bioblend.galaxy.objects)\nimport os\nfrom bioblend.galaxy.objects import GalaxyInstance\n\n os.chdir(\"/home/username/Documents/projects/galaxy\")\n\n# Set up GalaxyInstance ####\ngi = GalaxyInstance(\"https://usegalaxy.org\", \"my-instance-number\")\n\n# Make a new history ####\nhi = gi.histories.create(\"tmp-100119\")\n\n# Upload datasets ####\ncounts = hi.upload_file(\"example.txt\")\ncounts2 = hi.upload_file(\"counted.txt\", file_type=\"tabular\") # not working\n\nIn both code version, I have the same problem: I can upload a simple file called space-delimited example.txt which looks like this below. On the Galaxy GUI, I can see this file and it says \u201cformat: txt\u201d and \u201cuploaded txt file\u201d.\n1 2 3\nA B C\nD E F\n\nHowever, I cannot upload my counted.txt file. This file is a tab delimited file, which has 51,827 lines. It represents a matrix of counts I want to perform DE analysis on. It looks like so (first few lines):\nGeneID\tS1\tS2\tS3\tS4\tS5\tS6\tS7\tS8\tS9\ngene1    0\t 1\t 0\t 2\t 0\t 1\t 0\t13      34\ngene2\t 4\t 0\t 0\t 3\t 0\t 5\t 0\t 1      13\ngene2\t13\t44\t94\t30      45\t34\t46\t40\t44\ngene3\t 2\t 0\t 0\t 0\t 0\t 0\t 0\t 5       6\n\nThe error I get in my terminal is as follows (the upload_file commands takes a min or so before the error appears). There are slightly different for both codes but they both in a ConnectionError: (\u2018Connection aborted.\u2019, RemoteDisconnected(\u2018Remote end closed connection without response\u2019,)) error.\nTraceback (most recent call last):\n\n  File \"<ipython-input-14-345175af4c4e>\", line 1, in <module>\ncounts2 = hi.upload_file(\"counted.txt\", file_type=\"tabular\") # not working\n\nFile \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/objects/wrappers.py\", line 1000, in upload_file\nout_dict = self.gi.gi.tools.upload_file(path, self.id, **kwargs)\n\nFile \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/tools/__init__.py\", line 162, in upload_file\nreturn self._tool_post(payload, files_attached=True)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/tools/__init__.py\", line 232, in _tool_post\nreturn self._post(payload, files_attached=files_attached)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/client.py\", line 152, in _post\nfiles_attached=files_attached)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxyclient.py\", line 137, in make_post_request\ntimeout=self.timeout)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/api.py\", line 116, in post\nreturn request('post', url, data=data, json=json, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/api.py\", line 60, in request\nreturn session.request(method=method, url=url, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/sessions.py\", line 533, in request\nresp = self.send(prep, **send_kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/sessions.py\", line 646, in send\nr = adapter.send(request, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/adapters.py\", line 498, in send\nraise ConnectionError(err, request=request)\n\nConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n\nI have managed to upload this counted.txt file to the Galaxy GUI successfully. It says \u201cformat: tabular\u201d and \u201cuploaded tabular file\u201d. I have even manage to run some analyses on it. This file is 2.1 MB in size.\nI am very lost as to what is causing this error. Any help would be deeply appreciated."}, {"role": "system", "text": "How large is the file?\nDoes it contain unusual content (HTML, XML)?\nWhat happens when you try to load it with the Upload tool through the GUI. Does it load Ok? Do you have to use FTP first because of the size? Does \u201cautodetect\u201d for format detect what you would expect?"}, {"role": "user", "text": "Hi @jennaj, apologies for the delay in my response. I have updated my post and given more details. Hopefully that should answer your questions. I suspect something is wrong in the sense that BioBlend can\u2019t recognize my file type but even when I tell it what type it is (tabular), the error remains. Many thanks."}, {"role": "system", "text": "This has been later reported on https://github.com/galaxyproject/bioblend/issues/271 and solved in the BioBlend master branch."}], [{"role": "user", "text": "Hello,\nI have been use galaxy for the last couple of weeks in order to analyze RNA-seq. Today I logged in to my account and found out my History has been deleted and the datasets are are accessible only through User>datasets. I need those files but cannot figure out how to get them back all at once.\nThanks for the help."}, {"role": "system", "text": "Hi @HUJI_stu\nIt appears that you have been using two accounts at UseGalaxy.org. One does not contain any data at all (datasets or histories) and the other contains both. Each account was activated but just the account with data was logged into since account creation/activation (in Jan 2021).\nIt is very important to have/create/use just one account at each public Galaxy server.\nI am going to send you a direct message with more details so you can 1) find your most recent work and 2) we can clear up the multiple account issues together before those become administratively problematic. If something else is going on, we can also resolve that in the private direct message.\nThe two accounts may have been created during a class.\n\nAre you certain that you were logged into your own Galaxy account while working? One account was logged into and activated on 2021-01-15, but was not logged into when any data was created. If you are not logged into your Galaxy account while working, then it is the same as using no registered account. Data is not retained once you close your browser window, and it is lost.\n\nImportant: It isn\u2019t possible to have datasets available (User > Datasets) when there are no active histories to contain them (User > Histories) when logged in .\nIf you have not closed your browser window yet, it may be still possible to log into Galaxy and have the work saved into your own account. Try that first before anything else is done. Then write back. You can write back in the direct message to keep your email/credentials private. This topic is public \u2013 please do not post back your email address or any other private information here. Use the direct message instead. And even in that direct message, do not post back your password. Administrators will never ask for or need it.\nThanks!"}], [{"role": "user", "text": "I am following along with the de-novo transcript analysis tutorial here.  I have been using collections to organize the data as I go (1 collection for Megakaryocyte data and 1 for G1E data).\nThe collections were not a problem until I needed to run featureCounts() on the alignment files from all 4 samples (the 2 MegaK replicates with the 2 G1E replicates).  It does not appear that featureCounts() will allow you to select multiple collections for analysis (ie. it only allows me to choose either the MegaK collection, or the G1E collection, but not both).  I also cannot seem to figure out how to \u2018unpack\u2019 the collections into individual files so that I can run all four files together.\nAny help on either 1) running two collections together or 2) \u2018unpacking\u2019 the collections so that I can submit the individual files together,  would be greatly appreciated."}, {"role": "user", "text": "I seem to have answered my own question.  I will leave this here incase anyone else has the same question in the future.\nTo run multiple collections together I used the Merge Collections function to merge my two collections into one, then selected the merged collection as the input to featureCounts.\nTo split collections into individual files (or subgroups of files) I used the Filter List (from a text file) command.  I found that I needed to have the exact file names in order for the filtering to work, partial names were not matched.  To separate a collection out into individual files I called Filter List multiple times using a single file name each time.  I found the easiest way to make the text files was by clicking the upload button and then selecting the Paste/Fetch Data button in the popup display and then pasting in the file names I wanted to extract."}], [{"role": "user", "text": "Trackster won\u2019t let me add multiple datasets, it keeps getting stuck when I click \u2018add new\u2019 in \u2018loading histories\u2019. Help!"}, {"role": "system", "text": "Welcome, @Wendi_B\nYes, Trackster is a bit problematic right now.  Specifically, add more datasets to an existing visualization does not work (at least for all cases I have tested). The same is true whether the visualization is based on a built-in database or a custom genome build\u2019s database. We are working to fully characterize the problem and get it fixed quickly. A development ticket exists (too cryptic to post, and we are not quite sure if it will fully resolve the issue) \u2013 so I\u2019ll post the end-user oriented issue ticket back once completed, likely early Monday. More testing is needed first. But we wanted to let you know now that we\u2019re aware of the problem, plan to fix it with priority, and appreciate that you took the time to create a public post about the issue. It will help both you and others.\nUntil Trackster issues are resolved, alternative genome browsers are listed in an expanded dataset when the database (genome) is assigned. Common options linked are below. Depending on any particular dataset\u2019s metadata (datatype, database), other visualization options may display as linked resources.\n\n\nUCSC Main \u2013 Only some genomes are available. If a link does not show up, it is not at UCSC.\n\nIGV \u2013 Are a few versions, with links presented as appropriate. The publically hosted version has many genomes available. A local version is easy to download/install, has a GUI interface, and can have even more pre-built indexes added in. IGV also has the option to create your own custom genome index in a local IGV, matching a custom genome build created in Galaxy, to view multiple datasets together with the genome fasta backbone. If you just want to browse a single dataset from a novel genome, a scaffold index will be created on the fly.\n\nMore details for using IGV are in this prior Q&A. You can also click on the tags I added to your post to find more related Q&A in the content of different use cases.\n  \n    \n    \n    Opening Unicycler assemblies with IGV local usegalaxy.org support\n  \n  \n    The assembly is done, but moving the data into any 3rd party external application can require some reformatting or the creation of additional files that the other application is expecting \u2013 or they can require none \u2013 it depends on the external application. \nIn this case (IGV), a fasta index is required for new genomes. This would be true even if you decided to load the fasta directly into IGV as a new genome (instead of loading it directly from Galaxy). The \u201cGalaxy-to-Local IGV\u201d data web-based l\u2026\n  \n\n\nThanks for reporting the issue!"}], [{"role": "user", "text": "Hello, I have a matrix and I want the header values of its columns to move to the end of the matrix the size of a column because the header of each column is placed on top of the previous column and the last column is left without a header.\nPlease tell me what tool to use?"}, {"role": "system", "text": "Hi @maryam-gh99\nThis tutorial covers many common data manipulations: Data Manipulation Olympics\nThe manipulation will probably go something like this:\n\nIsolate the header line from the matrix\nAdd a new column to the start of that header line\nPut the header back on the matrix\n\nNote: If are you viewing the dataset in Galaxy with the  and noticed this as a potential problem, consider using the Cut tool first, to isolate the problematic column and double check that the header is actually missing. Sometimes the matrix header or the data values are so long (or short!) that they skew the view but the data content itself is Ok."}], [{"role": "user", "text": "I\u2019m trying to run featureCounts for zebrafish, however it returns the error:\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file.\nThe specified gene identifier attribute is \u2018gene_id\u2019\nAn example of attributes included in your GTF annotation is\u2019 ID = id1; Parent = rna0; Dbxref = GeneID: 192301, Genbank: NM_173235.3, ZFIN: ZDB-GENE-020419-25; gbkey = mRNA; gene = rpl24; product = ribosomal protein L24; transcript_id = NM_173235.3 \u2019\nSomething similar occurs when running htseq-count.\nDoes anyone know how to solve?"}, {"role": "system", "text": "Welcome @petenbender\n\n\n\n petenbender:\n\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file.\nThe specified gene identifier attribute is \u2018gene_id\u2019\nAn example of attributes included in your GTF annotation is\u2019 ID = id1; Parent = rna0; Dbxref = GeneID: 192301, Genbank: NM_173235.3, ZFIN: ZDB-GENE-020419-25; gbkey = mRNA; gene = rpl24; product = ribosomal protein L24; transcript_id = NM_173235.3 \u2019\n\n\nYour annotation appears to be in GFF3 format, not GTF.  There is no gene_id attribute in the 9th column of GFF3 data.\nBoth Featurecounts and HTseq-count expect GTF input, as explains on the tool forms and as reported in errors.\nPlease review the FAQs below if you are not sure about the datatype difference. The tags added to your post also point to similar help from prior Q&A.\n\nExtended Help for Differential Expression Analysis Tools\nCommon datatypes explained\n\nThanks!\nThanks!"}], [{"role": "user", "text": "I\u2019m having a problem with the \u2018Funannotate predict annotation\u2019 tool:\n\n[May 25 11:25 AM]: OS: Rocky Linux 8.6, 36 cores, ~ 103 GB RAM. Python: 3.8.15\n[May 25 11:25 AM]: Running funannotate v1.8.15\n[May 25 11:25 AM]: ERROR: sordariomycetes_odb10 busco database is not found, install with funannotate setup -b sordariomycetes_odb10\nMaybe the busco databases got clobbered during and update?"}, {"role": "user", "text": "Just a quick update - I tried different versions of the tool that I found on galaxy.eu and I got the same error each time"}, {"role": "system", "text": "Hi @Michael_Thon\nPlease try using the prior version of the Funannotate database (not a prior tool version).\nThe tool form\u2019s menu lists the 2022 database first right now because of issues discovered (last week) with the 2023 database. There isn\u2019t an issue ticket to link back, or I can\u2019t find it, but you can ask for updates later on if it still seems to be a problem after a week or so.\nThanks for reporting the problem!"}], [{"role": "user", "text": "Hello,\nI am trying to install Galaxy so that it sends jobs to slurm. I have used the tool ParallelCluster to create my cluster, which has slurm installed in /opt/slurm. I have also downloaded slurm-drmaa, as the docs suggest. I installed it with the following commands: ./configure  && make, sudo make install.\nI downloaded Galaxy by using git clone on my main directory.\nI then changed my Galaxy config files to interface with slurm. My current galaxy/config/job_conf.yml file is set up like this:\n<job_conf>\n<plugins workers=\"10\">\n    <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n        <param id=\"drmaa_library_path\">/usr/local/lib/libdrmaa.so</param>\n    </plugin>\n</plugins>\n<handlers default=\"handlers\">\n    <handler id=\"main\" tags=\"handlers\">\n        <plugin id=\"slurm\"/>\n    </handler>\n</handlers>\n<destinations default=\"slurm_part\">\n    <destination id=\"slurm_part\" runner=\"slurm\">\n</destination>\n    <destination id=\"slurm_part_12\" runner=\"slurm\">\n        <param id=\"nativeSpecification\">--ntasks=12</param>\n    </destination>\n</destinations>\n<tools>\n    <tool id=\"flexbar\" destination=\"slurm_part_12\"/>\n    <tool id=\"tophat2\" destination=\"slurm_part_12\"/>\n    <tool id=\"bowtie2\" destination=\"slurm_part_12\"/>\n    <tool id=\"bwa_wrapper\" destination=\"slurm_part_12\"/>\n</tools>\n</job_conf>\n\nNow, when I run  sh run.sh, I get this error:\ngalaxy.jobs INFO 2019-07-22 20:34:02,874 [p:29606,w:0,m:0] [MainThread] Handler 'main' will load specified runner plugins: slurm\ngalaxy.jobs.runners DEBUG 2019-07-22 20:34:02,874 [p:29606,w:0,m:0] [MainThread] Loading SlurmRunner with params: {'drmaa_library_path': '/usr/local/lib/libdrmaa.so'}\ngalaxy.jobs.runners.state_handler_factory DEBUG 2019-07-22 20:34:02,875 [p:29606,w:0,m:0] [MainThread] Loaded 'failure' state handler from module galaxy.jobs.runners.state_han               dlers.resubmit\ngalaxy.jobs.runners.drmaa INFO 2019-07-22 20:34:02,875 [p:29606,w:0,m:0] [MainThread] Overriding DRMAA_LIBRARY_PATH due to runner plugin parameter: /usr/local/lib/libdrmaa.so\n    \nTraceback (most recent call last):\n      File \"lib/galaxy/webapps/galaxy/buildapp.py\", line 49, in app_factory\n        app = galaxy.app.UniverseApplication(global_conf=global_conf, **kwargs)\n      File \"lib/galaxy/app.py\", line 204, in __init__\n        self.job_manager = manager.JobManager(self)\n      File \"lib/galaxy/jobs/manager.py\", line 26, in __init__\n        self.job_handler = handler.JobHandler(app)\n      File \"lib/galaxy/jobs/handler.py\", line 51, in __init__\n        self.dispatcher = DefaultJobDispatcher(app)\n      File \"lib/galaxy/jobs/handler.py\", line 945, in __init__\n        self.job_runners = self.app.job_config.get_job_runner_plugins(self.app.config.server_name)\n      File \"lib/galaxy/jobs/__init__.py\", line 599, in get_job_runner_plugins\n        rval[id] = runner_class(self.app, runner['workers'], **runner.get('kwds', {}))\n      File \"lib/galaxy/jobs/runners/drmaa.py\", line 68, in __init__\n        (exc.__class__.__name__, str(exc)))\n    ImportError: The Python drmaa package is required to use this feature, please install it or correct the following error:\n    ImportError: No module named drmaa\n\nI followed the instructions on the DRMAA Python github page, and installed the package using pip install drmaa, but I don\u2019t know how to configure it. Can anyone help?"}, {"role": "system", "text": "Did you ever find a solution for this?"}, {"role": "system", "text": "I was able to solve this by activating the .venv and just pip installing drmaa in case anyone else runs across this."}], [{"role": "user", "text": "Hello,\nI\u2019m a beginner user here and went through some patchy starts with aligning my RNA-seq reads through RNA STAR. I know you\u2019re not supposed to delete/re-run jobs a bunch of times, but I didn\u2019t realize that until I had done so a few times\u2026 I\u2019ve finally been letting the jobs queue, but rather than the yellow color indicating a running job, my first three datasets seem to have a pink/orange color and I\u2019m not sure what that means. Additionally, they\u2019ve been that way for almost 24 hours, so not sure if that\u2019s normal either.\nHave I just messed things up for myself by deleting and re-running jobs? Any help would be appreciated. Thanks!\nKanishk"}, {"role": "system", "text": "If a job turned red it means that an error occurred. If you click the text of the red \u201cblock\u201d the block will expand and some additional buttons appear. If you click the bug icon you may see the error."}, {"role": "user", "text": "Thanks! I think was being overly cautious/slightly color blind\u2026 that color was most likely yellow because those jobs have since finished. I think I just need to be patient!"}], [{"role": "user", "text": "I did chip seq analysis in Galaxy but when  was trying to load Narrow peaks in my IGB desktop it does not work\n\npeaks349\u00d7611 24.3 KB\n\nCan anyone help me?"}, {"role": "system", "text": "Hi @Anas_Jamshed\nDo you mean there is no link to IGB and other browsers? If yes, check There is no \"Display at UCSC browser main\" option on my dataset\nKind regards,\nIgor"}, {"role": "user", "text": "My problem is this:\n\nigb issue1599\u00d7803 68.4 KB\n\nThere is no bar showing on igb when I upload my narrow peaks file. I tried a lot of times but no response"}, {"role": "system", "text": "Hi @Anas_Jamshed\nI am not familiar with IGB. I tried it today but download of 3 Gb of reference data was too slow at home network.\nI assume IGB has similar functionality to other browsers. Can you check status of the uploaded track? On some browsers, like JBrowse, custom track data is not visible by default, users have \u2018to tick a box\u2019 to see the data. Maybe click on name of the custom (uploaded) track at the top left corner or check Available data - configure section.\nHave you checked a position where the data is available?\nIt seems your track has been connected to the browser, so it might be IGB use/configuration question.\nMaybe try it on other browsers, such as UCSC Main.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi. I need help with SAM files. I have a SAM file but i dont have idea how to view the result to finds snp, coverage, etc. is there any program to view and interpret easier? thanks"}, {"role": "system", "text": "Hi @Dardo_Dallachiesa,\nwhat is the purpose of your analysis?  SAM files store information about sequence alignment against a reference, but it doesn\u2019t include explicit information about SNPs or coverage.\nRegards"}, {"role": "user", "text": "Hi Gallardo. Thanks for the answer. I have phaseolus vulgaris genome with the genes and the phaseolus lunatus only with the raw sequence and I downloaded the reads. I want to see if some genes of vulgaris are in lunatus (like nfr1, nfr5) and be able to make the annotation. The propouse is discover variation, snps. Sorry if i dont be clear I m new in bioinformatics.\nRegards"}, {"role": "system", "text": "Hi @Dardo_Dallachiesa,\nin that case, I suggest you have a look at these two trainings:\n\nGenome Annotation with Maker\nVariant calling in diploid systems\n\nhowever, I found that the genome of P. lunatus have been annotated already: annotation. Perhaps it can be useful for you.\nLet me know if you have any additional questions as you begin the analysis.\nRegards"}], [{"role": "user", "text": "Hello!\nI am very new to galaxy and it is my first problem  I have gz archive with .fastq, but when it unpacking  it still .fastq (not .fastq.gz) And when I upload my file it appears to the right and it is red. Galaxy writes \u201cfile not found\u201d or next error:\n\nInternal Server Error\n\nGalaxy was unable to successfully complete your request\nAn error occurred.\nThis may be an intermittent problem due to load or other unpredictable factors, reloading the page may address the problem.\nI suppose I need only format .fastq.gz but how create it?\nAny help would be appreciated\nThank you"}, {"role": "system", "text": "Hello @Sofi\nThe extension of the file is one of these, correct?\n.fastq\n.fastq.gz\n.fastqsanger\n.fastqsanger.gz\nGalaxy should be able to autodetect the datatype for these read files when using the Upload tool.\n\n\n\n Sofi:\n\nGalaxy was unable to successfully complete your request\n\n\nThis should be temporary for anyone.\nI am running a test at UseGalaxy.org to double check that there is not a problem server side. More feedback soon. The tests for using the Upload tool for local file browsing, pasted URL, and fetching reads from NCBI were all successful.\nSome of these test jobs did queue first, indicating that the server is very busy.\n\nIf you are loading data from your computer (local file browsing), make sure to stay logged into Galaxy until the process fully completes or the job will error similar to your reported issue.\nIf you are loading data with public URLs, or via tools that directly fetch reads from data providers (NCBI or others), this will not require you to stay logged into Galaxy. Just make sure the job is fully started and output datasets appear in the History (queued, executing, or completed).\n\nShared test history Galaxy | Accessible History | test upload 2023-01-11"}, {"role": "user", "text": "Thank you for answer. So Galaxy can open all formats which you\u2019ve written, right?"}, {"role": "system", "text": "Yes, all are understood by Galaxy.\nException: there are rare circumstances when the specific compression program used to generate the gz format is not understood by Galaxy. The solution is to try to load uncompressed reads instead. You can then re-compress the reads in Galaxy or use them uncompressed.\nThese two tutorials cover general information about how fastq read data is handled and labeled in Galaxy. That training site can also be searched or navigated to find analysis domain protocols with more examples of read data plus workflows.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: NGS data logistics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I built a table with Merge, Paste and Cut, but now the bold header line is just 0, 1, 2 as the column names. How can I change the column names (or earlier, and how to keep them names)?\nThank you in advance."}, {"role": "system", "text": "Hi @viki\nThe bold header line: do you mean text in white font on blue background? This is \u2018metadata\u2019, hence names depend on datatype. If your file tabular, you\u2019ll see 1, 2, 3. If you deal with other tabular formats, for example, BED, you\u2019ll see other values. If you change the datatype, the header might change.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello,\nI am running VGP workflow on Galaxy but everytime i perform VGP HIFI phased assembly or VGP hybrid scaffolding, i end up with problem in busco \"Fatal error: Exit code 1 () \" or it get stuck with no or corrupted results. Everything else seems workig fine in the workflow\nAny advice how to solve it ?\nRegards"}, {"role": "system", "text": "Update\nI\u2019m guessing that you are running the workflow associated with this tutorial, or just following along manually. VGP assembly pipeline - short version \u2192 https://training.galaxyproject.org/training-material/topics/assembly/tutorials/vgp_workflow_training/workflows/main_workflow.ga\nPlease try running through it at UseGalaxy.eu or UseGalaxy.org.eu for now. We should have it functional at UseGalaxy.org within the next week or two. BUSCO in particular has problems we are currently troubleshooting.\nI\u2019m going to mark this as the \u201csolution\u201d so others will find it, but won\u2019t close it out until fully resolved.\nThanks for reporting the problem! \n\n\nHi @HT87\nAre you working at UseGalaxy.org? Did you try opening and saving the workflow in the editor before executing it? This will trap any warnings and correct metadata that is server specific.\nI think I found your bug report. But if that was not yours, please also send in a bug report from one of the error datasets. Leave all datasets/jobs in that history undeleted please. Ideally, that history would only include your raw input datasets, any pre-processing steps, and then the jobs the workflow invoked (any state).\nThe VGP workflow is still under final development, but we can still troubleshoot now.\n\nPlease post back the original URL for the workflow (what was pasted into Galaxy for the import).\nAlso please post back a link to where that workflow was sourced for context.\nThen to close the loop, generate a share link to the workflow you used (the version loaded into your account) and also post that back here.\n\nLet\u2019s start there, thanks!"}, {"role": "user", "text": "Hello @jennj\nThank you for prompt response. I tried to run it on usegalaxy.org after saving the workflow in editor before executing and it gave the same result with busco getting stuck.\nI will try running it on usegalaxy.eu based on the recent instructions and update you if the issue is resolved.\nRegards,"}, {"role": "user", "text": "Hello @jennj\nVGP workflow was successful when i moved to usegalaxy.eu, busco went fine.\nThank you"}], [{"role": "user", "text": "Hi all,\nI have a problem with the heatmap2 tool. My problems is in the final data page showing the results. It seems that the labelling is to big, thus resulting in overlapping gene names (see link below).\nhttps://usegalaxy.eu/datasets/11ac94870d0bb33a4397b2e602d9e2ab/display?to_ext=pdf\nis there a possibility to sove this problem?\nThanks in advance.\nCS"}, {"role": "system", "text": "Hi @CSm\nWe cannot see your graph from that link but the display options are known to be limited for text scaling with this version of the tool. About 30-32 total labels are the maximum before the text begins to overlap. Sometimes longer names don\u2019t display well either using those default settings.\nThis is a sample matrix dataset that displays well for reference/testing: https://raw.githubusercontent.com/galaxyproject/tools-iuc/master/tools/ggplot2/test-data/mtcars.txt\nChoices:\n\nChoose a different wrapped tool \u2013 search the tool panel with the keyword \u201cheatmap\u201d to review and test others out. You could compare your data with the example file above and see how both graph. None of these will shrink the font size but some do display in a nicer font. The versions under \u201cVisualize\u201d will have the same default resolution limits as the tool you are using now \u2013 but you could review those two and decide for yourself which is the best (if any).\nCreate your own custom graph. The EU server hosts both Rstudio and Jupyter interactive environments, where you can move data into and out of an IE from a Galaxy history with custom commands: gx_get() and gx_put(). The GTN training site has example usage, search with the IE name. Searching with the keyword \u201cheatmap\u201d also finds examples (but less than 30 labels \u2013 or no labels, sometimes just a legend): https://training.galaxyproject.org/\n\nMaybe reconsider if more than ~30 labels in a tiny font are informative or not? This wasn\u2019t my idea but it is a reasonable comment I read, originally didn\u2019t like (!!), thought about more, and then decided wasn\u2019t as off-putting as it seemed at first. Much depends on how you plan to use the graphic of course. Source (plus includes some very clear/helpful R tips): r - How to scale the size of heat map and row names font size? - Bioinformatics Stack Exchange\n\n\nHope that helps! Others are welcome to add a better reply "}], [{"role": "user", "text": "The error, parameter \u2018sp_name\u2019: an invalid option (\u2018selectkbest__k\u2019) was selected (valid options: memory,Note:,Parameter,steps), occurred in the workflow \u201cAge prediction from RNASeq (from training material)\u201d which was imported from the published workflow repository in Galaxy Europe. The other two hyperparameters, \u201celasticnet__normalize\u201d and \u201celasticnet__alpha\u201d in this step also met similar errors.\n\nerror578\u00d7596 58.9 KB\n\n\npublished workflow1544\u00d7168 23.1 KB\n\nTo solve this problem, I tried to create a new workflow according to the tutorial in https://training.galaxyproject.org/training-material/topics/statistics/tutorials/age-prediction-with-ml/tutorial.html#figure-4 by the latest version of tools. However, it still does not work. This error is weird and I hope for some help very much. Thanks a lot!  "}, {"role": "system", "text": "Hi @t-harden,\nthank you for the report; could you have a look @anuprulez? I can confirm that there\u2019s a problem in the current training.\nRegards!"}, {"role": "system", "text": "Hi @t-harden,\nCan you try to use the latest tool version (the latest version on Galaxy Europe of search cv, hyperparameter optimization tools is: Galaxy Version 1.0.8.4) in the workflow you are using? Everything else remains the same as explained in the age prediction tutorial. There was an issue with the previous version and we fixed it a couple of weeks ago. Let us know if you face further issues.\nBest,\nAnup"}], [{"role": "user", "text": "Hello,\nI\u2019m currently trying to run a sort on some bed files. The instructions for the sort are:\nsort -k1,1 -k2,2n -k3,3n\n\nhowever, I\u2019m having difficulty translating this to the sort tool in Galaxy. Is the K1, k2, k3 referring to the columns? and if so, what do 1, 2n and 3n refer to? Any help would be appreciated."}, {"role": "system", "text": "Hi @stealsh\nYou have this right:\n\nsort\n\nUse the Sort tool in Galaxy\n\n-k1,1\n\nMeans to start sorting at the first column and stop sorting this way at the first column (otherwise the sort would apply from whatever the \u201cstart\u201d was until the end of the line). Limiting to the first column is important because the \u201csorting type\u201d to apply differs by column. In this case, the sorting is \u201calphabetical\u201d. But see the tool form \u2013 alphabetical for any letters in the first column (a-b) will be what humans expect but any numbers after may not be. Example: \u201c11\u201d comes before \u201c2\u201d because the first character of \u201c11\u201d (1) is smaller than the first character of \u201c2\u201d (2).\n\n-k2,2n\n\nSort the second column with a \u201cnumerical\u201d sort (\u201cn\u201d) \u2013 that means smallest to largest number. For this case, \u201c2\u201d would be smaller than \u201c11\u201d and listed first.\n\n-k3,3n\n\nSort the third column, same rules as the second column (numerical aka \u201cn\u201d).\n\nIn practical terms, these methods will \u201ccoordinate sort\u201d the first three columns of a bed file. Most bioinformatics tools are designed to interpret chromosome positions this way for bed datasets. The rest of the line isn\u2019t considered, just passed through the tool associated with whatever was originally included per line after the \u201cchrom-start-stop\u201d data.\nTry different sort conditions on your data a few different ways, you\u2019ll notice the difference. The tool form also has a few examples that should help to clear up why the command-line was written that way."}], [{"role": "user", "text": "Hello,\nI am trying to filter a VCF dataset of depth (DP), however I do not want to filter on region.  If I leave this section blank the tool does not run when submitted.  What do I need to put in the region filter to ignore this parameter please?\nThanks"}, {"role": "system", "text": "Hi @KeeleyB\n\n\n\n KeeleyB:\n\nI am trying to filter a VCF dataset of depth (DP), however I do not want to filter on region.\n\n\nRegion shouldn\u2019t be required.\n\n\n\n KeeleyB:\n\nIf I leave this section blank the tool does not run when submitted.\n\n\nDo you mean that a gray dataset (queued) is created in the history or that the tool form itself doesn\u2019t submit at all (nothing is added to the history)? The first might be expected at times, the second may indicate that the input VCF has some technical problem.\n\nIs the assigned datatype vcf for the input dataset?\nDid you assign the datatype directly or did Galaxy autodetect the datatype?\nWhat happens if you go to pencil > Edit attributes > Datatype tab and redetect the datatype? Is it still vcf?\n\nIf the input is in vcf format, where are you working (public server URL, or describe if other). Are you using the most current version of the tool? The top of the form should report this:\n VCFfilter: filter VCF data in a variety of attributes (Galaxy Version 1.0.0_rc3+galaxy3)\nLet\u2019s start there, thanks! If you want to post back the header of your vcf dataset, that will help."}, {"role": "user", "text": "Thanks Jennifer,\nIt doesn\u2019t even add to the history so maybe it\u2019s a blip. I\u2019ll tell my student to keep trying.\nThank you\nKeeley\nDISCLAIMER: This email is intended solely for the addressee. It may contain private and confidential information. If you are not the intended addressee, please take no action based on it nor show a copy to anyone. In this case, please reply to this email to highlight the error. Opinions and information in this email that do not relate to the official business of Nottingham Trent University shall be understood as neither given nor endorsed by the University. Nottingham Trent University has taken steps to ensure that this email and any attachments are virus-free, but we do advise that the recipient should check that the email and its attachments are actually virus free. This is in keeping with good computing practice."}, {"role": "system", "text": "Hi @KeeleyB\nGiven the extra information \u2013 the job is not even submitting, correct? We made a change earlier today to fix a problem that is contributing to the odd behavior. It isn\u2019t on the server yet but expect it in the next few days.\nMeanwhile, this is the workaround:\n\nLoad up the tool form\nSubmit \u2013 this will abort and redirect to the top of the form\nClick into the blue box for \u201cregions\u201d, type anything, then remove it\nSubmit \u2013 this one will actually launch the job\n\nThis gets around the problem. Several tools are impacted, and only at usegalaxy.org. The problematic \u201cfree text\u201d will always be highlighted in a light blue box, and may be nested in advanced settings. If you run into this again with another tool, and can\u2019t find where the \u201cblue box\u201d is, post back with the full tool name and version and we can help.\nScreenshot:\n\nvcffilter-bug1292\u00d7226 22.5 KB\n\nThanks for reporting the bug !\nCross-posted in Matrix chat here: You're invited to talk on Matrix\nand Gitter chat here: galaxyproject/Lobby - Gitter"}], [{"role": "user", "text": "Hi everybody,\nI\u2019m asking if it is possible to add private conda channel to use in Galaxy. I use conda on the same server and can add a private channel with conda config --add channels file//<path_to_my_channel>.\nResulting in this :\n$ conda info\n     active environment : None\n       user config file : /home/XXX/.condarc\n populated config files : /home/XXX/.condarc\n          conda version : 4.5.12\n    conda-build version : 3.17.6\n         python version : 3.6.6.final.0\n       base environment : /home/XXX/miniconda3  (writable)\n           channel URLs : file://tmp/conCAPSID/linux-64\n                          file://tmp/conCAPSID/noarch\n                          https://conda.anaconda.org/conda-forge/linux-64\n                          https://conda.anaconda.org/conda-forge/noarch\n                          https://repo.anaconda.com/pkgs/main/linux-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/free/linux-64\n                          https://repo.anaconda.com/pkgs/free/noarch\n                          https://repo.anaconda.com/pkgs/r/linux-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n                          https://repo.anaconda.com/pkgs/pro/linux-64\n                          https://repo.anaconda.com/pkgs/pro/noarch\n          package cache : /home/XXX/miniconda3/pkgs\n                          /home/XXX/.conda/pkgs\n       envs directories : /home/XXX/miniconda3/envs\n                          /home/XXX/.conda/envs\n               platform : linux-64\n             user-agent : conda/4.5.12 requests/2.18.4 CPython/3.6.6 Linux/4.4.0-116-generic ubuntu/16.04 glibc/2.23\n                UID:GID : 658240:200198\n             netrc file : None\n           offline mode : False\n\nIs it possible to do the same under galaxy ?\nThanks in advance "}, {"role": "user", "text": "Because when I run the job in galaxy interface, I get the following error :\nPackageNotFoundError: Packages missing in current channels:\n  - pdb2fasta\n\nWe have searched for the packages in the following channels:\n            \n  - https://conda.anaconda.org/conCAPSID/linux-64\n  - https://conda.anaconda.org/conCAPSID/noarch\n  - https://conda.anaconda.org/iuc/linux-64\n  - https://conda.anaconda.org/iuc/noarch\n  - https://conda.anaconda.org/conda-forge/linux-64\n  - https://conda.anaconda.org/conda-forge/noarch\n  - https://conda.anaconda.org/bioconda/linux-64\n  - https://conda.anaconda.org/bioconda/noarch\n  - https://repo.continuum.io/pkgs/main/linux-64\n  - https://repo.continuum.io/pkgs/main/noarch\n  - https://repo.continuum.io/pkgs/free/linux-64\n  - https://repo.continuum.io/pkgs/free/noarch\n  - https://repo.continuum.io/pkgs/r/linux-64\n  - https://repo.continuum.io/pkgs/r/noarch\n  - https://repo.continuum.io/pkgs/pro/linux-64\n  - https://repo.continuum.io/pkgs/pro/noarch\n\nGalaxy search in https://conda.anaconda.org/conCAPSID/noarch instead of file//<path_to_my_channel>"}, {"role": "system", "text": "You can add custom conda channels for Galaxy to use using the conda_ensure_channels setting"}], [{"role": "user", "text": "Hello,\nI am trying to run Blastn for some data. I have tried several times in different days with different input files and some different file formats. However, I keep getting the following error message: \u201cRemote job server indicated a problem running or monitoring this job.\u201d Could you please help me figure out how to run Blastn successfully?\nThank you !"}, {"role": "system", "text": "Hi @Natalia\nI reproduced Blastn error on ORG server. Ping @jennaj\nKind regards,\nIgor"}, {"role": "user", "text": "Thank you, still not working "}, {"role": "user", "text": "Blastn works again ! Thank you ! "}], [{"role": "user", "text": "I have a bedgraph file that, when uploaded to https://usegalaxy.org, shows a \u201cdisplay at UCSC [main]\u201d link. That link is not present on my local galaxy instance.\nWhere, and how do I set that up?\nThank you"}, {"role": "system", "text": "I think your Galaxy needs to be reachable from outside for this to work. Have a look at http://dev.list.galaxyproject.org/display-at-UCSC-does-not-work-td4670008.html"}, {"role": "user", "text": "Thank you for your suggestion, but I think the problem is a little different.\nIn your thread, the person had the \u201cDisplay at UCSC\u201d link but could not connect, whereas in my case I don\u2019t even have the \u201cDisplay at UCSC\u201d link."}, {"role": "system", "text": "Make sure you uncommented this line in your config: https://github.com/galaxyproject/galaxy/blob/dev/config/galaxy.yml.sample#L643"}], [{"role": "user", "text": "Hi,\nI am working with collections of 2000-3000 files and trying to run MAFFT and/or dist.seqs on them but the submission keeps failing. I have done that with similar smaller collections (~200 files) and it worked well, but with the larger collections the submission takes forever to process and eventually ends with a \u201cjob submission fail\u201d error. Is there a limit to the number of files allowed for job submission ? If not, is there a way to make this work without having to divide the collections into smaller ones and run them one by one ?\nThanks!\nJulian"}, {"role": "system", "text": "Hi Julian,\nis this a red window that pops up with this error message? Or do you see the error elsewhere?\nThere should not be a limit in job submission, but maybe your browser just times out?\nAre you logged-in as a user?\nCiao,\nBjoern"}, {"role": "user", "text": "Hi Bjoern,\nIt\u2019s a red window that pops up with the message (it really just says \u201cJob submission fail\u201d and below just repeats the options i chose for the job), and yes I\u2019m logged-in as a user.\nHow could I know if the problem is that my browser times out ?\nThank you for your help\nJulian"}, {"role": "system", "text": "Can you please try to catch me on https://gitter.im/bgruening then we can debug this together.\nThanks,\nBjoern"}], [{"role": "user", "text": "Hi,\nDoes DESeq2\u2019s \u2018Output normalized counts table\u2019 mean typical normalization like TPM/RPKM/FPKM? Or is some other form of normalization computed internally in DESeq2?"}, {"role": "system", "text": "Hi Spring,\nBased on the Rscript, the function being used is counts(), which is part of the DESeq2 package. The function includes a parameter, normalized, which \u201cindicat[es] whether or not to divide the counts by the size factors or normalization factors before returning (normalization factors always preempt size factors)\u201d\nYou can find the description here: counts function - RDocumentation"}], [{"role": "user", "text": "I want to display the HTML output of my MultiQC report in the workflow invocation report. How can I do that? Using the templating in the Markdown\nhistory_dataset_display(output=multiqc_label)\n\nwon\u2019t work, it just outputs the raw html/css."}, {"role": "system", "text": "Hi @viki\nUsing the workflow report (aka Page) editor,\n\nchoose from the left panel Insert Object \u2192 History \u2192 Dataset\n\nThen pick the dataset the pop-up menu. This populates the Markdown with the correct target identifier.\nFor MultiQC, this would be the \u201cWebpage\u201d output (HTML datatype)\n\nThis is a simple test example at UseGalaxy.org that you could import and review. Galaxy | Accessible Page | Demo GHelp 9115\nIf you are working at your own Galaxy server where you are admin, the tool that created the HTML output might need to be added to Admin \u2192 Tool Management \u2192 Manage Allowlist to render it anywhere in the application, including in Pages. More details are in our admin docs.\nHope that helps!"}], [{"role": "user", "text": "https://galaxyproject.org/authnz/cloud/demo/\nIn step 10 python get_object.py is run.  Where is this script located?  I tried to find it in galaxy github repositories as well, but it is not available.  Has the name been changed?\npython get_object.py [API KEY]\nUltimately my goal is for Galaxy to be able to import sequences residing within my S3 bucket(s).\n\nI have installed a local instance of Galaxy within my AWS account.  It is functioning well, with our bioinformatics group running various tools.\nWithin one of my AWS S3 buckets I am storing my sequences and would like Galaxy to import these sequences (> 1000 RNA-Seq files).\nAlthough just a demo, I found the following two documents which appeared to fit my Galaxy-S3 bucket requirements:  https://galaxyproject.org/authnz/cloud/aws/ and https://galaxyproject.org/authnz/cloud/demo/\n\nI was able to define an AWS role and also generate API keys described, but was not able to run the python script get_object.py as provided per step 10.  The script was not locatable, but this is understandable since the document is simply a demo.\nOf note, I did find an object store config file which seems to have the capability to accept a S3 secret key and access key, but using a role I think is a better solution.\n"}, {"role": "system", "text": "The demo is running off a pre-baked branch of Galaxy, where a number of configurations are already made. The get_object.py script is also available from that branch, and it is used to POST a request to the cloud storage API to get a demo dataset stored in a S3 bucket into the demo history.\nTo use this functionality on your own Galaxy instance (not the pre-baked demo branch), you may need to follow the cloud storage documentation; in general:\n\n\nSetup Galaxy to let users login with their Google account;\n\n\nLogin with your Google account, and get your authnz_id by sending a GET request to the /authnz/ controller;\n\n\nSetup an AWS IAM Role and define it Galaxy;\n\n\nSend a POST request to the /api/cloud/storage/get API with a following payload (docs):\n{\n    \"history_id\": \"...\",\n    \"authz_id\": \"...\",\n    \"bucket\": \"...\",\n    \"objects\": [\n        \"your S3 object 1\",\n        \"your S3 object 2\",\n        ...\n    ]\n}\n\n\n\nPlease note that this flow might be broken on your branch (due to some recent changes), the issue is known and we\u2019re working on it.\n\nI did find an object store config file which seems to have the capability to accept a S3 secret key and access key, but using a role I think is a better solution.\n\nThere are a number of important distinctions between the objectstore-based approach and the cloud storage  API:\n\nObjectStore is an admin-level setting and the buckets you define will be used to store data belonging to all the users of that instance of Galaxy (albeit users will not see your configuration and buckets), while the API-based approach is user-specific, where the configuration of one user is not visible to the other user, and each user can have their own buckets.\nYou can configure ObjectStore to store all the data added to Galaxy (e.g., upload or generated) on S3. However, you cannot configure it to upload data stored on your S3 bucket onto one of your histories. In other words, ObjectStore cannot read your bucket containing RNA-seq data and upload them onto a history. For this we developed the cloud storage API.\n\nThe ObjectStore documentation is available from: https://galaxyproject.org/admin/objectstore/"}], [{"role": "user", "text": "I am trying to use edgeR to perform a differential expression analysis. This is a paired analysis (samples are in pairs - their healthy vs their diseased tissues).\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.20.7.2\nGalaxy Tool Version: 3.20.7.2\nTool Version: R version 3.4.1 (2017-06-30) \u2013 \u201cSingle Candle\u201d, edgeR version 3.20.7, limma version 3.34.9, scales version 0.5.0, rjson version 0.2.15, getopt version 1.20.0\nI have 2 input files. The first is input-counts.txt and looks like so:\nGeneID  M1  M4  M7 M10 M13  N1  N4  N7 N10 N13\ngene1    0   0   0   0   4   0   0   1   0   0\ngene2  589 602 646 403 390 204 357 511 266 387\ngene3    5   5   7   4   8   0   2  13   2   5\ngene5    1   1   0   0   0   0   0   0   0   0\ngene6    0   0   0   0   0   0   0   0   0   0\ngene7    0   0   0   0   0   0   0   0   0   0\netc\n\nMy factor file looks as such:\n   Sample Patient Tissue\n1      M1       1      M\n2      M4       4      M\n3      M7       7      M\n4     M10      10      M\n5     M13      13      M\n6      N1       1      N\n7      N4       4      N\n8      N7       7      N\n9     N10      10      N\n10    N13      13      N\n\nThe aim of my analysis is  to detect genes differentially\nexpressed between affected (M) and normal skin (N), adjusting for any differences between the patients.\nIn the edgeR window in Galaxy, in \u201cContrast\u201d of interest\", I have written: M-N.\nHowever, edgeR doesn\u2019t run and I get the following error:\nFatal error: Exit code 1 () Error in makeContrasts(contrasts = contrastData, levels = design) : The levels must by syntactically valid names in R, see help(make.names). Non-valid names:\n\nI am confused as to what to do. I made this file in R. I have tried changing the levels() in R and I have tried putting \u201cTissue\u201d in the second column but I still get the error each time.\nAny help would be deeply appreciated."}, {"role": "user", "text": "Seems I have found the problem. Despite Galaxy saying \u201cNOTE: Please only use letters, numbers or underscores\u201d when inputing factors manually, it seems you can\u2019t use numbers for Groups when you use a factor file instead of manual input.\n\nScreenshot from 2019-02-01 14-47-24.png901\u00d7415 25.3 KB\n\nIndeed, when I changed my file to the following, it worked. I added an X in front of the Patient numbers.\n\n\n\n\nSample\nTissue\nPatient\n\n\n\n\nM1\nM\nX1\n\n\nM4\nM\nX4\n\n\nM7\nM\nX7\n\n\nM10\nM\nX10\n\n\nM13\nM\nX13\n\n\nN1\nN\nX1\n\n\nN4\nN\nX4\n\n\nN7\nN\nX7\n\n\nN10\nN\nX10\n\n\nN13\nN\nX13\n\n\n\n"}], [{"role": "user", "text": "Hi There,\nCan anybody help me with renaming the output dataset in Nanoplot - I have managed to successfully change the output name for all the other tools in my workflow but i cant seem to get it to work with Nanoplot.\nThanks in advance,\nJesse\n"}, {"role": "system", "text": "Cross-posted: Rename workflow output based on input name - #4 by jennaj"}, {"role": "system", "text": "Hi @jessemartin\nI can\u2019t get the variables to parse correctly for the Nanoplot tool either.\nThere is an open issue that seems related: Workflow : Renaming datasets with input variable work with some tools but not others: \u00b7 Issue #13362 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nping @mvdbeek would you be able to help more?"}, {"role": "system", "text": "That was fixed in Record input datasets and collections at full parameter path by mvdbeek \u00b7 Pull Request #15978 \u00b7 galaxyproject/galaxy \u00b7 GitHub and is available on Galaxy release 23.1, which as of today is only live on usegalaxy.org"}], [{"role": "user", "text": "Hi Galaxy Community, I\u2019m needing your help.\nWhen running snippy (toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy/4.6.0+galaxy0) using a list of dataset pairs, ie paired reads grouped in a paired collection, the tool fails in some of the samples with the following message in the last line: --tmpdir \u2018/var/lib/condor/execute/dir_3665157\u2019 is not a directory.\nThe weird thing is that when I run it for the second time using the same samples in the same paired collection, some samples fail but they are different to the ones that failed in the previous run. That is no say apparently it has nothing to do with the samples as they fail randomly.\nAdditionally, I tried running the paired fastq files separately one by one, and snippy finishes successfully.\nDo you have any clue about what could be going on?\nThanks!!\nSol"}, {"role": "system", "text": "Hi @solhaim,\nsome tools fail sporadically when submitted on a collection. I know it is not really helpful answer. Try the following: re-submit a failed job and at the job setup activate an option that replaces the failed job in collection. I believe it is Attempt to re-use jobs with identical parameters? but I don\u2019t have a failed job on a collection to test it. It works well on jobs with a single output.\nKind regards,\nIgor"}, {"role": "system", "text": "Hey Sole!\nI have been trying to run snippy on galaxy for a while now (2 months maybe?) and I have the same problem as you. As far as I understood, there was an update on snippy and since then things are weird (A bad cluster node according to the support team). This is the case for both galaxy org and eu.\nGoing for a different server (Jetstream) didn\u2019t help either. In the end I had to run everything locally.\nSorry, not an answer, just previous experience.\nHave a nice day!"}, {"role": "system", "text": "Hi @solhaim,\nfound it. It is called \u2018Replace elements in collection?\u2019. Set it to Yes during re-run, and if the job completes, the output replaces the failed output in collection.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am trying to validate Star-fusion for detecting gene fusions in RNASeq data and seem to have some problems in implementing the package through the usegalaxy.org server.  The results with very clean Tophat2 fusion validated reads were nothing like they were supposed to be.  I posted a query on a Star fusion bulletin board and got a response from  the folks at Broad institute who developed the package.  They looked at what I did and got the correct answers.  Something is different in the program implementation and I would like to know how to do it correctly\nHere is what I did:\nDownloaded the CTAT file GRCh37_v19_CTAT_lib_Feb092018.source_data.tar.gz  at Index of /Trinity/CTAT_RESOURCE_LIB and undbundled the individual files.\nUploaded to the Galaxy server http://usegalaxy.org\nref_annot.cdna.allvsall.outfmt6.toGenes.sorted\nref_annot.gtf\nref_genome.fa\nUploaded from NCBI by fastq-dump\nSRR6796340    Anaplastic Large T-Cell Lymphoma     Expect NPM1-ALK chr5->chr2\nSRR6796341    Anaplastic Large T-Cell Lymphoma-    technical replicate\nSRR6796358    Diffuse Histiocytic Lymphoma              Expect NPM1-ALK chr5->chr2\nSRR6796374    Large Cell Immunoblastic Lymphoma Expect NPM1-ALK chr5->chr2\nSRR6796384    Bladder Transitional Cell Carcinoma   Expect FGFR3-TACC3 chr4->chr4 high counts\nSRR6796352    Chronic Myeloid Leukemia                 Expect BCR-ABL1 chr9->chr22\nI ran RNA Star first then Star-fusion or Star-fusion in control of the RNA Star aligner program.  The run parameters are below.\n\n\n\n\nRNA STAR\n\n\n\n\n\n\n\n\n\nDataset Information\n\n\n\n\n\n\n\nNumber:\n87\n\n\nName:\nRNA STAR on data 51, data 53, and data 73: chimeric junctions\n\n\nCreated:\nSat Dec 29 19:01:27 2018 (UTC)\n\n\nFilesize:\n4.9 GB\n\n\nDbkey:\n?\n\n\nFormat:\ninterval\n\n\nJob Information\n\n\n\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.6.0b-1\n\n\nGalaxy Tool Version:\n2.6.0b-1\n\n\nTool Version:\n\n\n\nTool Standard Output:\nstdout\n\n\nTool Standard Error:\nstderr\n\n\nTool Exit Code:\n0\n\n\nHistory Content API ID:\nbbd44e69cb8906b5b3d043534aebc71b\n\n\nJob API ID:\nbbd44e69cb8906b5f73bf7040b268a06\n\n\nHistory API ID:\nde36650321118931\n\n\nUUID:\n8fe466e2-9dee-48ca-91b4-b395c6757088\n\n\nTool Parameters\n\n\n\n\n\n\n\nInput Parameter\nValue\n\n\nSingle-end or paired-end reads\npaired\n\n\nRNA-Seq FASTQ/FASTA file, forward reads\n73: SRR6796352 (fastq-dump)\n\n\nRNA-Seq FASTQ/FASTA file, reverse reads\n73: SRR6796352 (fastq-dump)\n\n\nCustom or built-in reference genome\nhistory\n\n\nSelect a reference genome\n53: ref_genome.fa\n\n\nGene model (gff3,gtf) file for splice junctions\n51: ref_annot.gtf\n\n\nLength of the genomic sequence around annotated junctions\n100\n\n\nCount number of reads per gene\nTRUE\n\n\nWould you like to set output parameters (formatting and filtering)?\nno\n\n\nOther parameters (seed, alignment, limits and chimeric alignment)\nstar_fusion\n\n\nJob Resource Parameters\nno\n\n\nInheritance Chain\n\n\n\n\n\n\n\nSTAR-Fusion\n\n\n\n\n\n\n\nDataset Information\n\n\n\n\n\n\n\nNumber:\n91\n\n\nName:\nSTAR-Fusion on data 48, data 51, and others: fusion_candidates.final\n\n\nCreated:\nSat Dec 29 21:47:32 2018 (UTC)\n\n\nFilesize:\n17.9 KB\n\n\nDbkey:\n?\n\n\nFormat:\ntabular\n\n\nJob Information\n\n\n\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/iuc/star_fusion/star_fusion/0.5.4-3\n\n\nGalaxy Tool Version:\n0.5.4-3\n\n\nTool Version:\nsoftware version: STAR-Fusion_v0.5.4\n\n\nTool Standard Output:\nstdout\n\n\nTool Standard Error:\nstderr\n\n\nTool Exit Code:\n0\n\n\nHistory Content API ID:\nbbd44e69cb8906b569580af0b3b0f6c0\n\n\nJob API ID:\nbbd44e69cb8906b539bb05ac571e27ea\n\n\nHistory API ID:\nde36650321118931\n\n\nUUID:\n8426b56c-6591-4871-94eb-1e7f258e39ad\n\n\nTool Parameters\n\n\n\n\n\n\n\nInput Parameter\nValue\n\n\nUse output from earlier STAR run or let STAR Fusion control running STAR\nuse_chimeric\n\n\nChimeric junction file from STAR (with STAR-Fusion settings)\n87: RNA STAR on data 51, data 53, and data 73: chimeric junctions\n\n\nSource for sequence to search\nhistory\n\n\nSelect the reference genome (FASTA file)\n53: ref_genome.fa\n\n\nGene model (gff3,gtf) file for splice junctions and fusion gene detection\n51: ref_annot.gtf\n\n\nResult of BLAST\u00b1blastn of the reference fasta sequence with itself\n48: ref_annot.cdna.allvsall.outfmt6.toGenes.sorted\n\n\nSettings to use\ndefault\n\n\nJob Resource Parameters\nno\n\n\n\nEither way the results were the same.  None of the fusions listed were the correct ones and at most one fusion partner on the correct chromosome distant from the expected break point.  What mystifies is that the people at the Broad Institute got the expected results what one would expect.  When running RNA Star for linking to fusion, the Star fusion instance on Galaxy on the server it generates 4  output files, including one with the chimeric fusion junctions which I input into the Star fusion run.  Is this the correct one to use?  I noticed that the Github Star fusion tutorial instructions indicate downloading a mutation resource and then using something out of this zip folder.  Sounds like the versions of Star-fusions are differently set up.  This might be close to where the error is being generated.\nIf you can figure out what I did wrong in setting this up I would greatly appreciate suggestions on how to fix it."}, {"role": "system", "text": "I got the same issue. Anyone has a solution to it. It seems that the STAR-fusion version 0.5.4 is not compatible with the STAR ver 2.6.0."}, {"role": "system", "text": "Update\nThis was the immediate problem:\n\n\n\n pa.saunders:\n\n|Input Parameter|Value|\n|Single-end or paired-end reads|paired|\n|RNA-Seq FASTQ/FASTA file, forward reads|73: SRR6796352 (fastq-dump)|\n|RNA-Seq FASTQ/FASTA file, reverse reads|73: SRR6796352 (fastq-dump)|\n\n\nThe same single dataset was entered for two different inputs. This would fail the tool now in the most current version of Galaxy. My guess is that the previous output @pa.saunders had was corrupted (or empty or truncated) and that is what lead to scientific problems with downstream tools.\n\n@weiguo.job If you are having a problem now, it is very unlikely to be exactly the same as the original problem in this question.\nTo troubleshoot your problem:\n\nMake sure that you are using the most current version of both tools.\nVerify your inputs (content + format). You can search at this forum or the FAQs at the GTN with keywords like a datatype (fasta, gtf, fastqsanger) or tool name to review common issues/solutions.\nThis FAQ covers troubleshooting in general: understanding-input-error-messages\n\nReview tutorials\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf that doesn\u2019t help to solve your problem, please create a new topic and explain more about what is going wrong in your situation now. Please include enough context so we can help. troubleshooting-errors"}], [{"role": "user", "text": "I\u2019m having an issue with the FASTQC tool. It appears to run to completion (turns green as if done), but when I expand the box to show view/download options, etc., it says there are 0 bytes of data, and there isn\u2019t anything to view. However, this is only happening for some of the FASTA sequences I\u2019ve tried. For some of them it provides data, but some it does not. Any ideas? Also, when viewing the Job Information for the ones that show 0 bytes of data, the Tool Standard Output and Tool Standard Error features say \u201cEmpty\u201d\u2026 I can open the fasta sequences on my computer and see there are sequences, so I KNOW there is data there, the tool just isn\u2019t actually running?"}, {"role": "system", "text": "Hello,\nThe UseGalaxy.org server has some technical issues that started about 30 minutes ago.\nWe are activating tracking this. The scope is not clear yet. An important part of our infrastructure is unstable at the source.\nStatus\n\nhttps://status.galaxyproject.org/\nUnresolved incident: usegalaxy.org network instability.\nUpdate: * Resolved - The network appears to have stabilized and Galaxy is accessible.*\n\nUpdates will post to this thread and the status link above\nQueued jobs should not be impacted. Failed or odd results will likely need to be rerun but that isn\u2019t possible yet.\nThanks for reporting the problem!"}], [{"role": "user", "text": "Hi, All.  When attempting to update to v22.01 using ansible, we seem to be running into a version conflict between the versions of cwltool and pyparsing requested in galaxy/server/lib/galaxy/dependencies/pinned-requirements.txt.  The exact error is:\nThe conflict is caused by:\nThe user requested pyparsing==3.0.7\ncwltool 3.1.20211107152837 depends on pyparsing!=3.0.2\ncwltool 3.1.20211107152837 depends on pyparsing<3; python_version <= \"3.6\"\nAny suggestions for resolving this would be greatly appreciated.\nMany thanks,\nDevin"}, {"role": "system", "text": "Hi Devin,\nwhat Python version are you using? You need at least Python 3.7 for Galaxy 22.01."}], [{"role": "user", "text": "Hi, I  am a beginner in run Galaxy. Starting from this morning, my galaxy stopped to run, the jobs in the history say \u201cthis job is waiting to run\u201d and this status has last for a couple of hours. Anyone can tell me what I can do? I appreciate your help."}, {"role": "system", "text": "Dear @junnan.fang,\nIf you are a beginner with galaxy then please have a look at the training material for a Galaxy introduction Galaxy Training!.\nYour question, depends on the galaxy instance you are working with. If you are working with usegalaxy.eu or .org, then try to refresh your history, or resubmit your job.\nHave a good day and best wishes,\nFlorian"}, {"role": "system", "text": "Hi @junnan.fang\nUsegalaxy.org has a banner up explaining how the server resources will be allocated over the upcoming week (and why).\nJobs are expected to queue even when there is not an event going on. If the jobs do not move from queued (gray) to execute (yellow) after 72 hours, then a rerun may help. Just be aware that this removes the original job from the queue, and places the new job back at the end of the queue. If a server is busy, and many are, a rerun will result in even longer delays \u2013 or possibly never running if you keep deleting/rerunning before the jobs have a chance to be deployed to an executing stage.\nFAQ: How Jobs Execute\nI also added a tag to your post that points to similar Q&A about the subject. In short, job queue times at public servers depend on a few factors: how busy the server is, how many concurrent jobs you have running (queued + executing), what kinds of tools are you running (some tools require more resources, and those cluster nodes are the busiest), \u2026 and special factors noted in banners (events, downtime, known server issues).\nFor right now, I\u2019d suggest leaving the job as it is, to give it some time to move through the queue. Or you can rerun as @Flow suggests, as it sounds like it hasn\u2019t been that long since you started it up and there was a small issue at usegalaxy.org earlier today \u2013 although that resulted in job failures that required a rerun (resource allocation issues) \u2013 not jobs stalling in queued status (what it seems your situation is).\nThe choice is yours on how to proceed \nps: For anyone participating in the GCC2021 training event, special queues will be set up to prioritize your tutorial work once the event starts. These particular queues will only be available to registered students and will have resources appropriate for training-sized data only (larger work will fail for resource reasons). Your event instructions will describe how to join these queues, and you can contact your instructors in Slack should you need more help. Non-event end-users will have their jobs sent the regular queues the servers support, and some may have extended queue times and related resource allocation changes \u2013 if those are in place, the server will note so in their banner."}], [{"role": "user", "text": "Hello! When using bcftools isec I came across the following error:\n( {% snippet  faqs/galaxy/histories_sharing.md %}) .\n\nCaptura de tela 2023-01-31 0954201011\u00d7537 57.1 KB\n\nI need to overlap two VCF files to find the different mutated loci, variants and detect if there are differences in zygosity between the samples. If anyone knows what I did wrong regarding the input on bcftools or has any suggestions on other tools I can use to perform this type of analysis, the help would be very much appreciated. Thank you in advance and I apologize for the basic question, I am a medical student still learning how to use Galaxy for research."}, {"role": "system", "text": "Hi @Anna\nthe message points to conflicting settings. It seems you\u2019ve changed Complement to Yes and added something into Nfiles box, resulting in the job error.\nFor de novo mutations try Complement set to Yes. However, you need to put the \u201cmutant\u201d sample first, and I am not sure how this can be done with the tool. You might consider VCF-VCFintersect.\nFor changes in zygosity, such as heterozygosity in parental sample and homozygosity in the offspring sample consider using multi-sample VCF and filter it.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello all,\nHow do I add the Admin Tab to my Galaxy account? I would like to install a tool from the Tool Box but I believe I require admin status?\nThanks in advance!"}, {"role": "system", "text": "Hello,\nThe admin tab is enabled for any email address listed in the comma-separated configuration option in galaxy.yml titled admin_users. By default, it is:\nadmin_users: None\n\nIn my local galaxy, I have set the value to\nadmin_users: myemail@example.com,anotheremail@example.com\n\nYou may need to restart your galaxy process after changing this value."}], [{"role": "user", "text": "I have set up Galaxy for the first time\non a remote virtual desktop at AWS\nIt seems to be working.\nI am accessing it via Chrome on Windows.\nI have several problems:\n\nThe desktop size it, is there a way to stretch it to the whole window?\nCopy paste from Windows to the virtual desktop doesn\u2019t work.\nI don\u2019t think file transfer from Windows to the virtual desktop is possible.\nIt seems to be laggy.\n\nI wonder if there is a better client than Chrome (for windows), which could solve these problems?\nWhat do other people use? I assume thousands of people use Galaxy at AWS, how do you guys access it? Is there a client for UNIX that works well and solves the above problems?\n\n64367515_10216887876042613_6251879186332385280_o.jpg1835\u00d71080 64.2 KB\n"}, {"role": "system", "text": "Not sure if I follow but still want to answer. You set-up galaxy on a server, think most people do it with just a commandline I assume you opened a command line in that desktop environment of your screenshot. When everthing is set up you can acces galaxy in your chrome browser by going to the ip adress of your AWS. So in your case if everything is set-up right you go to http://18.205.16.67:8080 in your browser. If you set up a production environment you can just go to http://18.205.16.67\nSo you should not open galaxy in that virtual desktop but on your own desktop. How you to that is another story, you need to make sure the server is accessible. Sometimes just by changing the ip adres in galaxy.yml to 0.0.0.0  can work. But like I said, that is another question and written in the manual as well."}, {"role": "system", "text": "It seems you have set up a genomic virtual lab image - your instance runs here: http://18.205.16.67/ and your Galaxy, which is a part of GVL image, is at http://18.205.16.67/galaxy.\nThere is no need for you to connect to the desktop of that server using VNC."}], [{"role": "user", "text": "I am trying to run RNAStar since yesterday and it won\u2019t start the job. I performed RNAStar analysis on the same data set before so this should not be the problem. There is no Error message, it just stays in cue and won\u2019t run. Does anyone have an idea on how to solve this?\nThis is only a problem on the .org server, the european server is working. It would be easier to load my data to the .org server though, so it would be great if it was running here too.\nThank you "}, {"role": "system", "text": "hi @ko1013\nThe cluster that runs RNA Star may just be busy, so please leave the job queued.\nMore feedback soon & thanks for reporting the issue.\nUpdate: Your jobs are confirmed to be queued and the server/cluster is fine. One of your jobs is now executing and the others will execute as resources become available.\nFAQ: Datasets and how jobs execute\nSimilar Q&A to yours can be found at this forum by clicking on the tag added: \u201cqueued-gray-datasets\u201d"}], [{"role": "user", "text": "Why is this so? I mean why it has been reduced or disabled? Can we have a idea of the issue?\nThank you."}, {"role": "system", "text": "I think the usegalaxy.org does not have access to appropriate compute resources temporarily. The message will be updated when that changes."}, {"role": "system", "text": "Most of the prior impacted tools are now running again with full resources now.\nThe exception is RNA STAR \u2013 the banner will update as the issues with that tool are resolved. STAR jobs run at usegalaxy.org jobs are expected to fail until resolved.\nDevelopment ticket: https://github.com/galaxyproject/usegalaxy-playbook/issues/266\nGalaxy Help topic with more details: RNA STAR Reference Genome\nClosing this topic to keep the RNA-STAR updates consolidated into the same topic/ticket.\nThanks for your patience!"}], [{"role": "user", "text": "Hello to everybody!!\nI ran the snippy tool a few times and everytime I had an error.\nIn the github of snippy I found there is a new version (4.6) and the author suggest this update for many improvements respect previous version.\nThanks a lot!"}, {"role": "system", "text": "Hi @Agustin_Baricalla,\nI\u2019ll work on it. Thanks for the request.\nRegards!"}, {"role": "system", "text": "Hi @Agustin_Baricalla,\na PR has been opened in order to update it Update Snippy v.4.6. Probably it will be available the next week.\nRegards."}], [{"role": "user", "text": "Hi . Im looking into a package in usegalaxy that can show my  RNAseq data is stranded or unstranded."}, {"role": "system", "text": "Hi @sajjadg4,\nin order to know if your RNAseq datasets are stranded or unstranded, you need to align it against the reference genome; you can use RNA STAR. Then by using the BAM/SAM Mapping Stats tool, you can infer it by comparing the number of reads which map to positive and negative strand."}], [{"role": "user", "text": "Hello All,\nI have been trying to use featurecounts to count by RNA-seq reads.  When I use a downloaded GTF from UCSC - I get the results for transcript IDs not gene IDs, so I tried to use the in-built genome as I did for the HISAT2, however despite everything being set to hg19 - no reads are being assigned and I get 0 counts for all genes.  Can anyone advise as to what I am doing wrong please?\nThanks in advance "}, {"role": "system", "text": "Hi @KeeleyB\nsimilar questions were asked on the forum. Search the forum for featureCounts and check suggestions.\nHope that helps.\nIgor"}, {"role": "user", "text": "Hi Igor,\nYes I have been reading your previously answers to post like these - but if I am using the in-built hg19 genomes for alignments how can I check that the alignments are named the same for them to be recognised\u2026  I was using in built hg19 in HISAT2 and then in-built one in Feature counts.\nThanks in advance"}, {"role": "system", "text": "hi @KeeleyB\nwhat UCSC annotation was used? Maybe try hg19.refGene.gtf.gz. It should produce gene counts with the default featureCounts settings. Usually it is very reliable with single end data.\nDo you have paired-end data (trimmed)? Have you used \u2018strandness\u2019 setting for HiSAT2 and featureCounts? What do you see in the summary file? Do you see majority of reads in Unassigned_Read_Type category? If yes, featureCounts manual gives the following description:\nreads that have an unexpected read type (eg. being a single\nend read included in a paired end dataset) and also cannot be counted with confidence\n(eg. due to stranded counting). Such reads are typically generated from a read trimming\nprogram.\nMake sure you use correct strand settings in HiSAT2 and featureCounts. To determine strandness, map one sample with unstranded settings and run Infer Experiment. Enable Count fragments instead of reads in Options for paired reads (featureCounts).\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am using the tool \u2018MEGAN: meganize a DIAMOND file\u2019 to meganize two DIAMOND (.daa) files that I have uploaded on the server. One of the files is 40 GB and the other one is 47.9 GB. The job was submitted on 9th November 2022 and it is 15th November 2022 today. It has been running for a week. Is there anyway to find out how much longer it will run for? Or can we speed up the process?\nThanks a lot.\n-Ishraq"}, {"role": "system", "text": "Hi @Ishraq\nJobs process on the public services as resources become available. There is no way to speed this up. The most effective strategies involve getting jobs in the queue as soon as possible \u2013 and the best way to achieve that is to use workflows to execute tools in a series without manual intervention.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThe job was probably originally queued for a while (gray), then it started executing (yellow). Never interrupt a queued or executing job unless you already know that there is a content problem and it needs a rerun anyway, as any rerun starts the whole process over again. #understanding-job-statuses\nSometimes the EU team has more information about specific cluster \u201cbusiness\u201d. Feel free to join the chat here usegalaxy-eu/Lobby - Gitter"}], [{"role": "user", "text": "Hello,\nI am analyzing RNA-seq data and I got stucked in the DEseq because I get the following error:\nError in DESeqDataSet(se, design = design, ignoreRank) :\nsome values in assay are negative\nCalls: get_deseq_dataset \u2026 DESeqDataSetFromHTSeqCount -> DESeqDataSetFromMatrix -> DESeqDataSet\nDoes anyone know how to solve it?\nLaura"}, {"role": "system", "text": "Hi Laura,\nI got the same error message. Did you sort your issue? could you share any information?\nThanks\nAlessandro"}, {"role": "user", "text": "Hi Alessandro,\nI forward you the answer of a Galaxy team member:\n\u201cI looked at the history and found one minor issue: the option to specify if count files have headers (\u201cFiles have header?\u201d) is not set even though they have headers. Whenever there are headers in all counts file, please set this field to \u201cYes\u201d. Otherwise, just remove the header row in all the counts file and leave this field as \u201cNo\u201d.\u201d\nSo you have to click \u201cYes\u201d on files have header?when you do the Deseq"}], [{"role": "user", "text": "Hi there, can I add any plugin to galaxy user interface? Thanks in advance."}, {"role": "system", "text": "It is unclear what you mean by a plugin. Two basic options of extending interface are webhooks and visualizations \u2013 for the latter check the Viz section of the training material"}], [{"role": "user", "text": "Hi,\nI am using Circos and trying to make tile 2dplot. My ideogram is mm10. Everytime I execute the job I can just visualise ideogram with no 2D plot."}, {"role": "system", "text": "Hi @akankshabafna\nThere is likely a format/naming problem with the data provided for the \u201c2D Data Tracks\u201d input.\nThe tutorial here provides many examples with sample data, and should help to find the problem: Visualisation with Circos\nThe Circos tool form also has some tips:\n\n\u201cNo data or plot crashes? Check that your chromosome naming scheme matches between your Karyotype file and your datasets.\u201d\nhttp://circos.ca/documentation/\n"}], [{"role": "user", "text": "Hi guys,\nI encountered an issue with my RNAseq data analysis while running Trinity and my main question is whether my data is too big or if there is a problem within the Galaxy server. I submitted the bug report 3 days ago but still no answer.\nThese are the details:\n\nI run the Trinity tool (tool ID: toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2)\non Galaxy Europe server\nFor the input RNAseq data, I used a collection of 16 paired datasets (each dataset is a pair of forward and reverse sequences previously trimmed with the Trimmomatic tool, also In Galaxy). Total of approx 42 GB of data\n\nJob API ID: 11ac94870d0bb33a197fd4c8059a6f81\nAfter submitting the starting analysis 2 jobs were created (Gene to transcript map &  Assembled transcripts)\nAll the time both jobs had a yellow flag and after 4 weeks of running the analysis I received this error with my job becoming red flagged:\n\"An error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2 \"\ntool error:\nAn error occurred with this dataset:\nCluster could not complete job\n\nTool Standard Output and Eror fields are empty.\nThese are my Tool parameters:\nAre you pooling sequence datasets? (Yes). Paired or Single-end data? (paired_collection). FASTA/FASTQ dataset collection with R1/R2 pair (257: Trimmed pairs_collection). Strand specific data  (true). Strand-specific library type (Forward-Reverse). Jaccard Clip options (False). Run in silico normalization of reads (True). Minimum Contig Length (300). Use the genome guided mode? (no). Minimum count for K-mers to be assembled (3)\nCan you please help?\nBest,\nLada"}, {"role": "system", "text": "Hi @lada_irb,\nI resubmitted your job again since the problem seems to be caused by a temporal problem in the cluster. Let me know if it runs correctly now.\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nthank you for keeping in touch and helping me out. \nyes i see it in my history at the moment and it seems like its running.Thanks!\nIn the meantime, I managed to do Trinity job and some of the downstream applications with only 10 samples (the one that had an issue and that you resubmitted is with 16 samples) so I suppose the tools are working just fine. I expect it to be running for a couple of weeks I guess.\nBtw, do you know if there is a time frame for the job to be executed on Galaxy after which it gets canceled or it will run until it\u2019s finished (assuming there is no other errors) ?\nBest,\nLada"}], [{"role": "user", "text": "Hi everyone,\nI\u2019m trying to replicate the results of the scRNAseq dataset from the following publication \u201cDissecting the multicellular ecosystem of metastatic melanoma by single-cell RNA-seq\u201d from this link https://science-sciencemag-org.ezp-prod1.hul.harvard.edu/content/352/6282/189. They have provided a series matrix at GEO \u201cGEO Accession viewer\u201d. I would like advice from the forum on the best Galaxy tools to format this file for analysis.\nI thought this file looked like a a processed dataset (like what we would get after RNA-Star Solo pre-processing), so I started out by putting this directly into the RACEID Initial Processing to make a Count Matrix (RDS). However this didn\u2019t work. When I tried to plug this directly into the RACEID clustering directly, I also got an error.\nWhat are the proper steps and tools to process a series matrix into a matrix I can put it into RACEID for analysis? Thanks for your help!"}, {"role": "system", "text": "Hello,\nPlease try with ScanPy instead. RaceID is great on smaller datasets but struggles a bit on larger ones."}], [{"role": "user", "text": "I am running Unicycler and getting the message when I try to run a job where I got 9 paired files for my pair ended reads and 3 files with Pacbio data.\n\u201cThe server could not complete the request. Please contact the Galaxy Team if this error persists. Received 9 inputs for \u2018paired_unpaired|fastq_input1\u2019 and 3 inputs for \u2018long\u2019, these should be of equal length\u201d"}, {"role": "user", "text": "The answer was available in the SARS-CoV-2 genome assembly tutorial. The Unicycler tool does not have a way to merge datasets so that needs to be done beforehand with the \u201cCollapse collection\u201d tool. Otherwise the Unicycler tool will try to run a batch job where the first dataset from the forward, reverse and long dataset produce assembly 1, the 2nd dataset in each produce assembly 2 and so on. I should have realized this when looking at the text below the collection selection box in the Unicycler tool interface which clearly states that \u201cThis is a batch mode input field\u201d but which I for some reason did not think of when looking at the problem. I posted this question as I am sure other people will make the same mistake from time to time.\nTo merge reads from several samples into a combined final assembly, we need to pass the data to Unicycler tool in partially merged form. The forward and reverse reads of paired-end data should be kept separate, and so should short and long reads. However, the tool has no option to combine data from individual samples, so we need to merge the forward, reverse, and the long reads data, respectively, across samples. Conveniently for us, the outputs of the earlier Samtools fastx tool runs have already returned the data structured into three corresponding collections for us."}], [{"role": "user", "text": "Hi everyone,\nI selected \u201cInteractive Jupyter Notebook\u201d from Tools on the left side bar to include it to my Galaxy interactive tools,but it isn\u2019t added to my history.The job is constantly running and taking so long.\nI did it many times but each time is the same.This screenshot of my interactive environment shows this problem.\nerror-galaxy1687\u00d7563 47 KB\nWhat is the problem ?\nHow to solve it?"}, {"role": "system", "text": "@Rihana you need to click on the \u201cJupyter Interactive Tool\u201d link "}], [{"role": "user", "text": "Hi All! So, I need some help getting off the ground \nI sent in a plate of 96 samples for RAD-seq: Half were my samples, half were a colleague\u2019s. However, the company returned a single file of all sequences produced in fastqsanger format, including both mine and my colleague\u2019s samples intermixed together.\nDoes anyone know how to filter this so I that only analyze my own samples?\nI tried \u201cFilter Fasta\u201d already, but this failed."}, {"role": "system", "text": "\n\n\n melissajbetters:\n\nthe company returned a single file of all sequences produced in fastqsanger format\n\n\nAll the sequences are in a single interleaved fastq file? Did they provide a sample sheet? You would need that to separate your own samples, too."}, {"role": "user", "text": "Hi jennaj,\nYes, the sequence reads all came back as an interleaved fastq file. They provided a sample sheet so I know which samples are mine and which are not, so my problem is purely a logistical one in figuring out how I select for my own samples!\nHowever, having talked to a few people, it seems my only option is the demultiplex all of the sequences first and then select which sequences I want to use in downstream analyses. If you have another idea, though, I\u2019d love to hear it!"}], [{"role": "user", "text": "Hello,\nI\u2019m adding apache as a proxy server in galaxy 20.05. But I have a problem with the wsgi socket permissions. It\u2019s created as:\n\n0 srwxr-x\u2014 1 galaxy galaxy 0 Aug 10 14:15 /opt/galaxy/var/uwsgi.sock\n\nI made apache member of the group galaxy but it still missed the write permission for group.\nI don\u2019t find a way in the galay.yml to tune the ownership/permissions, so, what is the proper way to configure the owership of the socket?\nBest,"}, {"role": "system", "text": "Try chmod-socket: 664 in the uwsgi section of galaxy.yml to make it group writable.\nYou might also need to explicitly set the Apache Group directive to the galaxy group and restart Apache."}], [{"role": "user", "text": "Hi, I have a very confusing problem with some of the galaxy outputs. I run the same tool with different samples, and for two of the ten samples, galaxy displays red boxes, with the message \u201cUnable to finish the job\u201d as you can see in the image below.\nSurprisingly, the output files are well produced. There are no errors in the tool_stderr file either.\nAnd when I run the tool from the command line, the tool runs fine\u2026\nHow can I figure out what is wrong ? Have you ever had this problem?\nThank\u2019s for your help\n\nerror_galaxy1499\u00d7636 86.2 KB\n"}, {"role": "system", "text": "Hello @vindarbo\nIf you were working at UseGalaxy.org, there was a server-side issue that impacted some jobs over the last 24 hours. Please try a rerun again now.\nWhat you posted in the screenshots are not the full stderr or stdout logs. These FAQs explain where to find those:\n\nGalaxy Training!\nGalaxy Training!\n\nIf the problem was related to the UseGalaxy.org server problem, many of the jobs failed with a message like this but some may have ended in other odd ways.\n\nstderr\n(33, \u201cHTTP server doesn\u2019t seem to support byte ranges. Cannot resume.\u201d)\n\nTry a rerun now as the issue has been corrected. You may also want to check upstream jobs \u2013 green datasets mean that the job didn\u2019t technically fail, but it could still be empty or truncated for some cases.\nLet\u2019s start there. If you need more help, please include a few more details. Items to include: confirm the server URL and the full stdout and stderr. A shared history link is also helpful."}, {"role": "user", "text": "Hi,\nThank you for your reply.\nI found the problem. In fact, this is because Galaxy expected a tabular output file, when in fact the file was separated by spaces. Thus, the first line with many spaces was too big and exceeded the size limit.\nit occured with only few samples"}], [{"role": "user", "text": "Hi, I\u2019m running a paired-end de Novo assembly with Trinity. Inputs are 2 fastq (7 gb each) for forward and reverse ends, with 31 million reads of same long.\nThe Trinity step in my history started running a few more than 48 hs ago. Is this too long? May I clean an rerun?\nHistory link:\nhttps://usegalaxy.eu/u/rivero.alexander/h/srr59\nThanks in advance."}, {"role": "system", "text": "Trinity uses a lot of resources and usually runs for very a long time. So, nothing unusual here."}], [{"role": "user", "text": "Hi,\nI was trying to download all my large collections to clear out spaces for my next analysis. But all download failed no matter when using Chrome or curl command, my browser said incomplete files and curl just closed transfer with nothing downloaded (and I tried using APIs as suggested in the tutorial but still same). The message shows as below:\n\nimage1402\u00d7120 9.56 KB\n\n\nimage1200\u00d7118 43.1 KB\n\nCan someone help me out? I thought this could be network problem but I tried different places and different time point, still not working."}, {"role": "system", "text": "Hello @Pinhki\nTwo items to check first, and one alternative method:\n\n\nFAQ instructions\nDownloading datasets using command line\n\n\nTry downloading one collection at a time.\n\n\nThe alternative is to export a history archive, and download that. The files will be in a folder inside the archive after uncompressing it. The process is the first part of this FAQ. If your collections are very large, consider putting each in a distinct history, and archiving/downloading that way in smaller data chunks. Transfer entire histories from one Galaxy server to another\n\n\nIf you need more help, please confirm the URL of the server where you are working. Actually UseGalaxy.org?"}, {"role": "user", "text": "Hi Jennaj,\nThank you for the information, I have successfully download datasets using history archive. However, I want to know more infos about the direct downloading using browser, which started failing every time after the galaxy getting updated, no matter what datasets and browsers I choose. I do download one collection each time, and I think it is much more convenient than using history archive, especially for large number of files so I don\u2019t have to rename each of them. And for your information, I used usegalaxy.org all the time. Would you like to help me out?"}, {"role": "system", "text": "Hi @Pinhki\nThe collection download functionality at UseGalaxy.org (and only that server) is undergoing some updates. Details: Download collection broken fails with \"file incomplete\" \u00b7 Issue #15390 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nFor now, exporting a history is the most reliable method."}], [{"role": "user", "text": "Hello!,\nI was running a transcriptome assembly with Trinity on usegalaxy.org with paired .fastq files that had been trimmed with Trimmomatic on my local computer (and further QC checked with FastQC).  The files contain 178 million paired reads.\nOvernight, the run was halted with the following Error:\nTool Error: Remote job server indicated a problem running or monitoring this job.\nAs far as I can tell, there are no other error identifiers, so I am unsure why the the job was halted/\nCan you offer any advice?  thanks!"}, {"role": "system", "text": "Hi @clif7050\nThere is likely a problem with the input format, possibly introduced when moving the data between your local and Galaxy Main (truncated upload).\nOr, the job is really too large to execute at Galaxy Main.\nI would suggest running Trimmomatic at Galaxy Main as well to eliminate the formatting issues being the problem. The output will be four datasets: two representing the F and R paired reads (the inputs to Trimmomatic) and two representing the F and R unpaired reads.\nUse Upload by FTP when moving the original fastq data, or paste in the public data URLs (if that is where you sourced it). I am assuming that your local is not publically web hosted, but if it is, there are more options I can share: Loading Data\nIf the Trimmomatic job fails again after that, then the job is too large. Options include:\n\nSubsample the reads. See the seqtk tools\nUpgrade your local Galaxy, if that is where Trimmomatic and FastQC was run (??) to have sufficient resources to run the job\nSet up cloud Galaxy server with sufficient resources\n\nMore help for options 2 & 3:\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n\nFAQ about what to do to fix the different error cases: https://galaxyproject.org/support/\n\n\nMy job ended with an error. What can I do? >> https://galaxyproject.org/support/tool-error/#type-execution-exceeds-maximum-allowed-job-run-time-walltime\n\n"}, {"role": "user", "text": "Thanks,\nI may have identified the problem,  Looking back at my FastQC reports that were run on my computer, they fq files were in an illumina 1.5 encoding format, and I did not realize that this is not compatible with usegalaxy programs (need Illumina 1.8+).\nThe original 2 paired files I uploaded via HTTP were .fq.gz (15.4 GB and 14.7 GB), which galaxy uncompressed during the first Trinity run attempts (and hide from history view).   Since all 4 files (2 compressed and 2 uncompressed) remain on my galaxy allocation, I am right now using FASTQ Groomer on the hidden uncompressed files to convert them to a PHRED +33 format and will attempt to use these newly generated files with Trinity.   Hopefully that works!\nThanks for your help!"}], [{"role": "user", "text": "Hi I am looking to use the latest of the sus scrofa genome in HISAT2 for BAM alignment but I believe someone from the galaxy team is needed to add the 11.1 version.\nhttps://useast.ensembl.org/Sus_scrofa/Info/Index"}, {"role": "system", "text": "Hi @rlynn01,\nI opened an issue in order to request it https://github.com/usegalaxy-eu/issues/issues/308.\nRegards"}, {"role": "system", "text": "Hi @rlynn01,\nthis genome is now available in usegalaxy.eu.\nRegards"}], [{"role": "user", "text": "Hello,\nOver the past couple of days I have been running a workflow for basic data analysis of samples which looks like this:\nTrimmomatic \u2192 SPAdes \u2192 MLST & ABRIcate\nThe past 6 samples that I ran through this workflow have been perfectly fine however with my most recent sample the Trimmomatic stage completes but the next step of SPAdes assembly does not activatev and sits on \u2018waiting to run\u2019. Is this a known issue or has anyone else run into this problem? I\u2019ve tried to run the trimmed reads into SPAdes serparately too but the same issue occurs.\nThanks for any help!"}, {"role": "system", "text": "Hi! I have the same issue, did you resolve the problem in the end?"}, {"role": "system", "text": "Consolidating into Spades genome assembly waiting to run for a long time. - #3 by jennaj"}], [{"role": "user", "text": "Hi,\nI have run into a problem when trying to count features from a genomic alignment.\nI have used RNA-star to align single end FASTQ data from the rabbit. The mapping goes well and delivers a good unique mapping of 74%. To then count features (using featurecounts) I use the same genome imported via UCSC table browser as ncbi-refseq-genome-all in GTF format. So far so good.\nAt first inspection everything looks great, genes I expect highly represented are indeed so. However, one key gene I expected to be highly represented was missing from the counts and was given as being zero, something that seemed very odd.\nI converted the aligned BAM from the RNA-star output to a bigwig for UCSC import and could clearly see this gene should certainly be present in terms of transcript counts. I also examined my GTF file and could also confirm that all the exon locations and gene annotations were present as expected for this gene and all matched the gene_id locations for the observed BigWig output under UCSC. I proceeded to inspect other genes that according to the featurecount table were zero. Many, were indeed zero, however, I came across many entries that were clearly not zero according to the aligned BigWig. Since I previously came across this issue (by chance) when performing a mouse RNA-seq experiment (but at the time just disregarded it a a \u2018glitch\u2019), I am now more concerned. I have inspected all the files and inputs I am using but as for myself, can find no good explanation for this output mismatch.\nAny ideas?\nBest regards,\nPhil"}, {"role": "system", "text": "\n\n\n Phil:\n\nTo then count features (using featurecounts) I use the same genome imported via UCSC table browser as ncbi-refseq-genome-all in GTF format.\n\n\nHi @Phil\nRetrieving GTFs from the UCSC Table Browser can introduce two problems. UCSC explains about why not to get GTFs using the TB on their site but sometimes that is missed.\n\nThe gene_id and transcript_id values will be the same value (scientific problem wherever it is used, and probably explains the odd results)\nThe output can be truncated due to output size limits (technical problem once in Galaxy)\n\nYou didn\u2019t state which genome you are using, but check in their Downloads area for GTF annotation. If your genome doesn\u2019t have this already available, write into UCSC\u2019s help and they can assist you to get a proper file.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Working with GFF GFT GTF2 GFF3 reference annotation\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "im working with pseudogenes and i think i need this option (outFilterMultimapNmax) but i cant find it in star or feature counts . does galaxy even has this option ?"}, {"role": "system", "text": "The option is available for STAR - just a bit hard to find (hidden under advanced advanced options  ):\n\n\nWould you like to set output parameters (formatting and filtering)?: Yes\n\n\nWould you like to set additional output parameters (formatting and filtering)?: Yes\n\nMaximum number of alignments to output a read\u2019s alignment results, plus 1\n\n\n\n\n"}], [{"role": "user", "text": "Hello,\nSo yesterday I was able to use stringtie to get gene and transcript counts from my aligned BAM files with no issues. Today I tried to run new samples and I was able to get assembled transcripts but no counts. I figured this was an issue with my new files. So I took the files that worked perfectly yesterday and ran stringtie on them again (same settings, files, etc.) and now these are also not outputting any counts. I\u2019m honestly stumped on this one and not sure if it is just an issue with the site at this point. Any advice would greatly help my sanity. Thank you."}, {"role": "system", "text": "Hi @Stephen_Willis\nI tested StringTie on usegalaxy.org and got the count files. The tool was tested with a gene annotation file in history with Use reference transcripts only set to Yes, and without annotation. In the latter scenario it produces only gene count file.\nHow did you submitted the job, by clicking on the tool in the tool menu or by re-run? If through the tool menu, have you activated the counting option? It is off by default.\nKind regards,\nIgor"}, {"role": "user", "text": "\u200bHello,\nI tried to do it by re-run and it did work now, but also changing:\n\u201cOutput gene abundance estimation file? = True\u201d\nAlso worked. Is that what you meant by activating the counting option? Because that was set to false in my other run (which worked). I mean it is working now, which is really all that matters but I\u2019m still confused.\nBest,\nStephen"}, {"role": "system", "text": "Hi Stephen,\nI am sorry for the confusing explanation. With the default settings StringTie only produces a gene annotation file. It can produce count and abundance files, as additional outputs. For example, you can enable output of abundance estimation files, as in your reply. To get counts, try gene annotation in your history, and change Output files for differential expression? to DESeq2/EdgeR/limma-voom.\nFor counting I use dedicated tools, such as featureCounts.\nKind regards,\nIgor"}], [{"role": "user", "text": "Fastqsanger.gz is not working in the trim galore or trimmomatic flexible trimming tool as a paired end collected. Ive tried to use a D interlacer to get it to work but the drop down list doesnt allow me to select my  SRA data.  Is there something i could do to fix this?"}, {"role": "system", "text": "Hello @Preet_Kaur\nItems to check:\n\n\nFirst, be sure that you are using the option on the tool forms to look for collections of input reads. faqs/galaxy/#selecting-a-dataset-collection-as-input\n\n\nIf the form is looking for a collection but doesn\u2019t find your specific collection, next check that the individual files in your collection were assigned a datatype that matches the input field on the tool form.\n\n\n\n\n\nfastq unavailable -- Tool does not recognize inputs? How to check why\n\n\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:\n\nCreate a new empty history\nLoad up the tool form\nReview the input datatypes listed in the tool form\u2019s input area\nNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\n\n\n\n\nWhy the Fastq De-interlacer tool would help is not clear. Is the existing collection a list of datasets? Or a list of dataset pairs? If the reads are already split into pairs, then you won\u2019t need that tool.\n\n\nThis tool is a great way to get reads from SRA into Galaxy that are sorted into collections: Faster Download and Extract Reads in FASTQ format from NCBI SRA\n\n\nThese tutorials can help more.\n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: NGS data logistics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using dataset collections\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Multisample Analysis\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you are still stuck after reviewing, and please create a shared history link and post that back in your reply. faqs/galaxy/#sharing-your-history. Leaving all datasets undeleted, including your tests, will help to get the best feedback from our community.\nLet\u2019s start there "}], [{"role": "user", "text": "I am trying to use TXSScan MacSyFinder to identify secretion systems in my bacterial genome. I upload the genome in the requested GenBank format. Upon selecting the sequence and running the analyses, results populate my \u201chistory.\u201d However, when I look at the data, seems like there is nothing there. It provides me with 5 outputs for a single analysis:\nMacsyView output\nsummary output\nreport output\noutput\nhmmer results\nThey all seem to have not data in them. The only one that has a download option is the hmmer results, but when I download the file there three files for each thing it searched for:\n_.search.hmm.out\n_.search.hmm.err\n_res_hmm_extract\nI dont know how to open these files or what to do. As this bacterium doesn\u2019t have certain secretion systems, there should be no file; however, I am pretty sure this is not really data, as there are these files for every secretion system protein that the program was searching for. So not sure what to do or why it doesn\u2019t give me any data. This bacterium should at least have one secretion system.\nAny help would be great. Thanks in advance."}, {"role": "system", "text": "This tool is specific to the public Galaxy server at https://galaxy.pasteur.fr/. There could be a data input problem or some server-side issue leading to unexpected results.\nThe tool hasn\u2019t been published to the wider Galaxy community through the Tool Shed, which means there is little help we can offer at this forum unless their admins/support staff find your post and reply later.\nIt is probably quicker to contact this server\u2019s support team directly. Find the contact email on the homepage of the server, second box down, center pane. It is also listed here: https://galaxyproject.org/use/galaxy-pasteur/\nHope that helps!"}], [{"role": "user", "text": "Dear all,\nI recently installed Galaxy on Ubuntu.\nNo problem with the installation per se.\nBut I realize that most of the modules are not available at least in the main package.\nIs there a way to download them and to install them aftewards?\nFor instance the section \u201cMultiple alignement\u201d is simply missing and the clustal W module is not available.\nThank you for your help.\nC\u00e9cile"}, {"role": "user", "text": "Other important modules which are missing are all the \u201cNGS \u2026\u201d modules (and more specifically the \u201cNGS: QC & manipulation\u201d one) and the FASTA manipulation module.\nThank you once again for your help.\nC\u00e9cile"}, {"role": "user", "text": "and the NCBI Blast+ module\u2026\nThank you\n"}, {"role": "system", "text": "You can install tools of your choice into Galaxy you maintain. This tutorial is a good start https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/"}], [{"role": "user", "text": "Hello! I have readspergene.out.tab as an output after running star. Is it possible to create a count matrix using that file for input into DESeq2 using galaxy, instead of running feature count as per the galaxy DESeq2 tutorial?"}, {"role": "system", "text": "Welcome, @kayleen !\nSorry I think I don\u2019t understand your question. You want to use \u201creadspergene.out.tab > DESeq2\u201d instead of \u201cBAM (mapped reads)+GTF>HTseqcount>DESeq2\u201d ?\nStar manual, about the per genes option, states that:\n\n7 Counting number of reads per gene.\nWith --quantMode GeneCounts option STAR will count number reads per gene while mapping. A read is counted if it overlaps (1nt or more) one and only one gene. Both ends of the paired-end read are checked for overlaps. The counts coincide with those produced by htseq-count with default parameters. This option requires annotations in GTF format (i.e. gene id tag for each exon) specified in --sjdbGTFfile at the genome generation step or at the mapping step provided in option. STAR outputs read counts per gene into ReadsPerGene.out.tab file with 4 columns which correspond to different strandedness options:(\u2026)\n\nSo I guess you are good to go.\nIf you\u2019re asking about the DESeq2 input structure, you can check the DESeq2 manual and see some example input data"}, {"role": "user", "text": "Hi David,\nThank you for your reply! I managed to convert my read counts per gene by extracting the second column  If that helps anyone else facing the same problem as I did!"}], [{"role": "user", "text": "I tried to perform mapping with BWA-MEM using fastq files to reference genome, which absolutely worked yesterday. But since this morning I kept getting error saying:\n[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\n[31mFATAL: e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nWould you be able to assist me on this? I also tried using the fastq files of the ones successfully mapped yesterday, but it also returned as error. I do not understand why it kept getting error.\nThank you so much!"}, {"role": "system", "text": "Same thing with featureCounts, DESEQ2, HISAT2\u2026\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800"}, {"role": "system", "text": "Hello @Vincent and @sweetroot\nThe problem has been resolved. Thanks for reporting it!\n\n  \n    \n    \n    Error : Could not lookup the current user's information: user: unknown userid 819800 -- Resolved at UseGalaxy.org 09/12/2022 usegalaxy.org support\n  \n  \n    Hello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one : \n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800 \nSome help ? \nThanks a lot\n  \n\n"}], [{"role": "user", "text": "Hi,\nNew Galaxy user, so just getting used to it.\nI was trying to upload a FASTQ file of 300Mb, uploaded it using the FTP and everything went smoothly but, the file has been charging for two hours now to the server. It stills with the clock and in grey.\nIn your experience, it takes so long for large files to be uploaded? should I re-load the charging?\nThank you very much for your help.\nBest,\nEva Sandoval."}, {"role": "system", "text": "Welcome, @evasandoval\nFew questions:\n\nDid you check that the file was completely loaded during the FTP step before you attempted to move it into the history using the Upload tool? Whatever method you used (ex: client such as Filezilla or lftp line-command) produces statistics or a message confirming a complete/successful transfer.\nAre you working at https://usegalaxy.org (just to double-check)\n\n300 MB is not that large of a file. And the Upload process to move the dataset into a history should happen quickly.\nThe server is a bit slow right now, so this could be a problem on our end. More feedback soon. But please confirm the questions if you have time, just to make sure we are working on solving the same problem.\nUpdate: We addressed the slower response times at usegalaxy.org. Your job for the data transfer FTP > Upload > History should have completed by now."}], [{"role": "user", "text": "I have been trying to download different datasets from my history, but am running into errors when trying to unzip them.\nFirst, I can download a zipped collection of my html report outputs from fastp but when I try to unzip them with Windows, I get an unspecified error. I have tried unzipping them with WinRar but it says the files are corrupted. This also happens when trying to download html reports from FastQC. Note this also happens when I try to download the html file for a single sample, as it still zips the report.\nI am using Chrome, but have also tried this on two different computers with the same result.\nAny clues on what is going on here? This problem only popped up in about the past week. I had no trouble unzipping my files before.\nThank you!"}, {"role": "system", "text": "Hi @Danielle_Ireland\nI think we got back to you by email with this exact help, but I\u2019ll just post the same for others reading.\nHTML output doesn\u2019t download from the disc icon using any browser that I know of. The browser will not understand what to do, create a gzip \u201cdirectory\u201d, then attempt to download but it never finishes. Why? The data isn\u2019t really a compressed directory \u2013 it is just a single file. HTML data is handled in a special way in Galaxy for security reasons. You cannot upload HTML data at all, and downloading requires an alternate method involving the command line.\nFAQ that applies to any composite dataset, or security protected dataset in Galaxy:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nOn a MAC, you could use the terminal program already built into the OS to access a unix compatible command line tool (wget, curl). On a PC, you could use a utility like putty https://www.putty.org/ or whatever you normally use for linux/unix access.\n\nExample for HTML data specifically:\n\ncurl -o out.html --insecure 'link'\n\nWhere the original link copied from the dataset is something like this:\n\nhttps://usegalaxy.org/api/datasets/f9cad7b01a472135987ad5307248c882/display?to_ext=html\n\nBut the external display type is stripped off the end, and the URL used for the link is this:\n\nhttps://usegalaxy.org/api/datasets/f9cad7b01a472135987ad5307248c882/display?\n\nA complete command string would be like this one. It happens to be an actual (small, test data) FastQC webpage dataset if anyone wants to test it. I\u2019ll leave it undeleted for now.\n\ncurl -o out.html --insecure 'https://usegalaxy.org/api/datasets/f9cad7b01a472135987ad5307248c882/display?'\n\nNote: If you happen to be working at a GDPR compliant server, like UseGalaxy.eu, you\u2019ll need to also set the history the datasets are in to a \u201cshared-by-link\u201d state and include your API key. Full details are in the FAQ above.\nHope that helps!"}, {"role": "user", "text": "Hi @jennaj\nThank you for responding here as I don\u2019t believe I ever received an email about this particular issue.\nMy problem is not just with HTML datasets but that any collection that I try to download (that should be a reasonable size) cannot be extracted by Windows. Note that before last week, I was able to download the entire collection at once for things like Fastp html reports but now cannot. But this is also a problem with my FastQC Raw Data collection which are txt files. I have a collection of 4 paired-end samples. I can download each txt file individually so at least I can get the data, but in the past I have definitely been able to download the entire collection at once. But for some reason the resulting zipped files will not extract anymore.\nIt seems like using the command line approach you provided may be the most robust way to get my data going forward.\nThanks,\nDanielle"}, {"role": "system", "text": "Hi @Danielle_Ireland\nOk \u2013 then that was another user who reported the same problem in a very similar way to the UseGalaxy.org mailing list.\nThis does turn out to be an actual bug from what I can tell after reviewing this more. I\u2019ve opened an issue ticket here for expanded review by our developers. Updates will post in that ticket. Bug: FastQC webpage output fails complete download from disc icon \u00b7 Issue #14365 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nI\u2019m going to mark this reply as the \u201csolution\u201d to make it easier for others to find the Q&A, but also add the tag \u201cserver open issue\u201d for accuracy as this functionality is still pending a bug remedy.\n\nThe workaround until the fix is made (tool or Galaxy itself) is the command-line method.\n\n\n\n jennaj:\n\nA complete command string would be like this one. It happens to be an actual (small, test data) FastQC webpage dataset if anyone wants to test it. I\u2019ll leave it undeleted for now.\n\ncurl -o out.html --insecure 'https://usegalaxy.org/api/datasets/f9cad7b01a472135987ad5307248c882/display?'\n\n\n"}], [{"role": "user", "text": "[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nI keep getting this error when I try to run any function.  Any advice?"}, {"role": "system", "text": "Hello,\nI meet exactly the same problem using my user account on usegalaxy.org.\nBest"}, {"role": "system", "text": "Hi @Fred and @Coltin_Albitz\nThe problem has been resolved. Thanks for reporting it!\n\n  \n    \n    \n    Error : Could not lookup the current user's information: user: unknown userid 819800 -- Resolved at UseGalaxy.org 09/12/2022 usegalaxy.org support\n  \n  \n    Hello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one : \n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800 \nSome help ? \nThanks a lot\n  \n\n"}], [{"role": "user", "text": "Another quality control question.\nI trimmed my datasets of interest with Trim Galore! After this, I ran FastQC to check if the removal of adapter and primer sequences was successful. While, based on the Adapter Content plot, the adapters were removed successfully during trimming, I still had some overrepresented sequences in some datasets:\n\nimage956\u00d7164 10.4 KB\n\nHowever, since there were no hits to indicate what these were, I copied them into a FASTA file and ran them using the NCBI BLAST + blastn tool against the locally installed BLAST databases.\nThe output of this was the following:\n\nimage946\u00d7741 23.4 KB\n\nThe reads matched whole genome sequencing contigs across multiple species (predominantly, mammalians). What I am wondering now is how to interpret this - are these overrepresented sequences mapping to some potentially conserved sequences across species and are overall false positive (in terms of not being a contaminating adapter, primer, etc.) or if it\u2019s something else?\nIf they match to innocent sequences, can I ignore the overrepresented sequences when running downstream analyses?\nThe reads I am investigating are short (~76 bp), so I would assume they have higher chances of matching something broadly than longer reads. They are also from RNAseq experiments (I read that sequences flagged as overrepresented can sometimes indicate highly expressed genes).\nWould appreciate your opinions and advice.\nThanks!"}, {"role": "system", "text": "\n\n\n Egle:\n\nIf they match to innocent sequences, can I ignore the overrepresented sequences when running downstream analyses?\n\n\nOne thing that I usually try is to review regions like that in the UCSC Genome Browser https://genome.ucsc.edu/. (I\u2019ve also just googled them, and tried at NCBI, etc). Sometimes you won\u2019t find out but can always check to see if they at least fall out during read mapping against a genome/transcriptome later on or not. Public WGS data is much \u201cnoisier\u201d and sometimes the age of the sequence submissions and sequencing protocols are clues, too.\nI tend to try to BLAT map the \u201cmystery sequence\u201d to a well annotated model organism (sometimes a few, if I don\u2019t get hits with the first), then click into the hit and view it in the genome alignment view. Consider scrolling down and turning on more tracks to suit what seems relevant to make a decision.\nI usually start with the Comparative Genomics \u2192 Conservation track plus everything in the Variation and Repeats section along with the existing defaults since all those combined provide basic annotation markers for the hit region (or regions \u2013 which can also be informative).\nThe rest is a judgement call  Happy hunting!"}], [{"role": "user", "text": "Hello,\nI have a problem to display data on UCSC Cell Browser. I 'm trying to run Atlas-Seurat-CellBrowser workflow (GitHub - ebi-gene-expression-group/container-galaxy-sc-tertiary: Galaxy container for single cell RNA-Seq tertiary analysis tools)  on my own local Galaxy (v19.05) and get html format data for UCSC Cell Browser. When I click the \u201cView Data\u201d icon of the result, UCSC Cell browser starts but the result cannot be displayed even though I click \u201cOpen\u201d button. I also uploaded the result on Human Cell Atlas Galaxy (https://humancellatlas.usegalaxy.eu) and found again it wasn\u2019t displayed.\nI tested on other versions local Galaxy and figure out t works on Galaxy version 18.05 and 19.01 but doesn\u2019t work on 19.05 and 19.09. Also I tried other clients, Mac/Windows, Safari/Firefox/Chrome only to get same behaviour.\nAnyone have the same problem? What might be wrong?\nThank you.\nYukie"}, {"role": "system", "text": "Hey Yukie,\nDid you ever find a solution? I\u2019d be very interested to know how your experience has been with using the Seurat object after the Export2CellBrowser Step, with the newer versions of the UCSC browser tools like (0.7.10+galaxy0, +18.01).\nThanks,\nThomas"}, {"role": "system", "text": "@tshum @yukieymd\nData in HTML format cannot be uploaded at public Galaxy servers intact.\nAny data in HTML format that was created at any particular Galaxy server has a specific flag that allows it to be displayed at that server when using the \u201cview data\u201d icon (\u201ceye\u201d).\nBoth are for security reasons at public Galaxy servers, including https://humancellatlas.usegalaxy.eu.\nI\u2019m not sure about all the details at https://cells.ucsc.edu/. You could contact them and ask a question, or you can try to figure out the usage first. You might just need to set the data to be in a \u201cshared\u201d state in Galaxy. Or, maybe download the data into a local file and try to load it that way. They do offer a version that can be set up locally \u2013 similar to how you set up Galaxy locally \u2013 and I would expect that permission configurations could be customized by the administrator (you).\nRelated:\n\n\nHTML data is blocked by default (Upload and display). How any particular private Galaxy server handles HTML format is configurable by the server administrators. Decisions depend on if the server is publically accessible, has moderated access/account creation methods, etc.\nData needs to be publically accessible (on a server hosted publically) and have the privacy preferences set as \u201cshared\u201d to move data from Galaxy to a different public web service. Plus, any service in the EU will be following GDPR compliance strictly.\nThat service may be another Galaxy server or it may be a third-party application.\nData in a shared state is still somewhat \u201cprivate\u201d for many use-cases \u2013 but this is not the same as encrypted/secure data transfer. Data have distinct URLs. When those data URLs are in a shared state, someone would have to correctly \u201cguess\u201d the data URLs with an open sharing access set in order to gain access.\nAny work that needs to be completely secure/protected should not be created or uploaded to public web services \u2013 Galaxy or otherwise.\nThere are many ways to use Galaxy for sensitive data with security/privacy requirements (example: personally identifiable human biological data). These deployments are usually behind a firewall and are not publicly exposed.\n\nPractical help for end-users:\n\nData Privacy in Galaxy\n\nAdministrative topics:\n\nhttps://docs.galaxyproject.org/\nsearch the site with keywords such as \u201cprivacy\u201d or review admin topics like this one Proxying Galaxy with Apache \u2014 Galaxy Project 20.09 documentation\n\n\nHope that helps!"}], [{"role": "user", "text": "i am unable to download the FastaQC quality report"}, {"role": "system", "text": "Welcome, @31-Nisarg_R_Waghmare\nI just tried to do this and it worked Ok.\n\nServer: UseGalaxy.eu\n\nResults from using tool/version: FastQC Read Quality reports (Galaxy Version 0.73+galaxy0)\nMethod: Downloading datasets\n\n\nHow to troubleshoot:\n\nTry again now, maybe the server had some temporary issue.\nIf that fails, check to make sure that the output actually contains content and is not in an error state (red)\nIf that all looks Ok, please post back a shared history link and the EU moderators here can help more. Sharing your History\n\nIf the reports are in a collection, try downloading individual files. Collection download had some issues that I think are still pending a fix (testing that now). ping @wm75 or @gallardoalba\n\n"}], [{"role": "user", "text": "Dear All,\nI would like to know, how to identify Cre-recombinase is expressed or not in my scRNAseq data? I have fastq files, Please help to guide in this. Thanks in advance."}, {"role": "system", "text": "Hi @Barani_Rajendran,\nyou can use RNA STAR for mapping the Cre-recombinase sequence against your dataset. You can examine if the Cre-recombinase mapped to the sequences in the log file generated by that tool."}], [{"role": "user", "text": "Hi everyone\nI got an error with Mothur command \u201ccluster\u201d\n\u201cThis job was terminated because it used more memory than it was allocated\u201d\nThe preceded command \u201cDist.seqs\u201d was used with cutoff \u201c0.20\u201d and the output file is ~170Gb\nI know that I can use the alternative command \u201ccluster.split\u201d which uses less memory, but for some reason I have to use cluster command.\nBTW I am using usegalaxy.org\nany advices?\nI appreciate your help in advance\nthank you"}, {"role": "system", "text": "This page can be helpfull Account quotas it looks like you are exceeding quota\u2019s."}, {"role": "system", "text": "Hi @Jalal\nWe discussed this directly via email, correct?\nSummary for others:\n\n\nThe input dataset is so large that it exceeds computing resources at UseGalaxy.org and probably would exceed the resources at any public Galaxy server.\n\n\nComputing resources are distinct from data storage (quota space) associated with accounts.\n\n\nThe 16s tutorial here explains why splitting clusters is a reasonable choice for many: 16S Microbial Analysis with mothur (extended)\n\n\nIf that method does not suit your needs or the needs of anyone running any analysis that is too large for public servers, then moving to a private Galaxy server where you can control the resources is one way forward.\n\n\n\nWebinar: Use Galaxy on the web, the cloud, and your laptop too >> Webinar: Use Galaxy on the web, the cloud, and your laptop too\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources (scroll down for the platform choices matrix)\nThe GVL version of Cloudman is one choice and AWS offers grants. AWS Programs for Research and Education\n\n\nThanks!"}], [{"role": "user", "text": "Hi,\nI have been dealing with the shotgun data and want to use the MetaPhlAna for my analysis.\nI am wondering at the option of \" Cached database with clade-specific marker genes\u201d where i have to select MetaPhlAn2 clade-specific marker genes. but my page is showing \u201cNo option is available\u201d.\nWhy it is showing as no option to me or what to do for the step? Please help\nimage1920\u00d71080 188 KB"}, {"role": "system", "text": "Hi, did you get any answer or solution to this? I\u2019m dealing with the same question. Best"}, {"role": "user", "text": "Actually not. But i have use another webpage of galaxy like galaxy australia and europe where they have in built"}], [{"role": "user", "text": "Hello - complete n00b here so hopefully its something very quick.\nI am running a local instance of Galaxy 22.05 on Ubuntu.\nI\u2019ve managed to set up the instance, become admin and install some tools from the toolshed (RNA-star, edgeR, featurecounts) without any issues (they are all running fine).\nHowever, when I try to install \u2018Dexseq\u2019, I seem to run into an issue where it is stuck on \u2018Installing tool dependencies\u2019. I don\u2019t see any obvious error message but this is the output from Terminal (just keeps repeating this with new values of 127.0.0.1:X.\nuvicorn.access INFO 2023-03-02 07:34:46,163 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:54370 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nSuvicorn.access INFO 2023-03-02 07:34:51,184 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:36840 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:34:56,201 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:36842 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:01,217 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:37582 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:06,235 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:37596 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:11,255 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:56610 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:16,273 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:56622 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:21,287 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:58406 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\n\n\nI have re-installed and restarted Galaxy several times and its a no-go.\nAny help appreciated !"}, {"role": "system", "text": "Hello @Mehul_Vora\nIf you still need help, please start here: Galaxy Training!\nFull docs are here, including dependency handling topics: Releases \u2014 Galaxy Project 23.0.1.dev0 documentation"}], [{"role": "user", "text": "Hello,\nI\u2019m trying to download the entire history from Galaxy through FTP. I first exported the history to the FTP remote site, and then tried to connect to galaxy.eu through WinSCP using the parameters shown in screenshot #1. But I had problems downloading the file because of permission issues (screenshot #2). Could you help me?\n\nScreenshot 2022-08-15 151815994\u00d7658 26.4 KB\n\n\nimage1912\u00d7685 73.9 KB\n"}, {"role": "system", "text": "Hi @Lu_Yin,\nthe current FTP configuration doesn\u2019t allow downloading files from Galaxy by using the FTP protocol.\nRegards"}], [{"role": "user", "text": "Hi everyone,\nim trying to use Slurm for our local galaxy, but somehow all jobs immediately fail when i run them. When i go back to singularity, everything works again. I can also run jobs on slurm with sbatch or so, but when starting a job in the Galaxy UI, they become yellow and then red with this error: \u201cThis job failed for reasons that could not be determined.\u201d I\u2019ll attach all logs that might be useful. I would be very grateful if someone could have a look. I feel like it is a problem because im running slurm on the same machine as the galaxy.\nIs slurm needed or recommended if i want to run jobs to run with more than one core? I thought slurm to be most efficient because i could use it with tpv so i dont have to find out how many resources each tool would need.\n------------slurm.log--------------------------\n#log while running job\n[2023-07-04T16:46:05.716] Launching batch job 24 for UID 999\n[2023-07-04T16:46:10.257] [24.batch] sending REQUEST_COMPLETE_BATCH_SCRIPT, error:0 status:256\n[2023-07-04T16:46:10.259] [24.batch] done with job\n------------slurmctld.log--------------------------\n#Log from job run\n[2023-07-04T16:46:05.504] _slurm_rpc_submit_batch_job: JobId=24 InitPrio=4294901754 usec=545\n[2023-07-04T16:46:05.713] sched: Allocate JobId=24 NodeList=galaxyworkstation #CPUs=1 Partition=debug\n[2023-07-04T16:46:10.258] _job_complete: JobId=24 WEXITSTATUS 1\n[2023-07-04T16:46:10.258] _job_complete: JobId=24 done\n#Log when starting slurm\n[2023-07-04T17:04:57.311] error: Node galaxyworkstation appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.\n[2023-07-04T17:04:57.698] Terminate signal (SIGINT or SIGTERM) received\n[2023-07-04T17:04:57.795] Saving all slurm state\n[2023-07-04T17:04:58.153] error: Configured MailProg is invalid\n[2023-07-04T17:04:58.154] slurmctld version 21.08.5 started on cluster cluster\n[2023-07-04T17:04:58.157] No memory enforcing mechanism configured.\n[2023-07-04T17:04:58.185] Recovered state of 1 nodes\n[2023-07-04T17:04:58.185] Recovered information about 0 jobs\n[2023-07-04T17:04:58.185] select/cons_res: part_data_create_array: select/cons_res: preparing for 1 partitions\n[2023-07-04T17:04:58.186] Recovered state of 0 reservations\n[2023-07-04T17:04:58.186] read_slurm_conf: backup_controller not specified\n[2023-07-04T17:04:58.186] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure\n[2023-07-04T17:04:58.186] select/cons_res: part_data_create_array: select/cons_res: preparing for 1 partitions\n[2023-07-04T17:04:58.186] Running as primary controller\n[2023-07-04T17:04:58.186] No parameter for mcs plugin, default values set\n[2023-07-04T17:04:58.186] mcs: MCSParameters = (null). ondemand set.\n[2023-07-04T17:05:03.191] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2\n------------slurm.conf--------------------------\n\nThis file is maintained by Ansible - ALL MODIFICATIONS WILL BE REVERTED\n\nDefault, define SlurmctldHost or ControlMachine to override\nControlMachine=localhost\nConfiguration options\nAuthType=auth/munge\nClusterName=cluster\nCryptoType=crypto/munge\nProctrackType=proctrack/pgid\nSelectType=select/cons_res\nSelectTypeParameters=CR_CPU_Memory\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nSlurmctldPidFile=/run/slurmctld.pid\nSlurmctldPort=6817\nSlurmdLogFile=/var/log/slurm/slurmd.log\nSlurmdParameters=config_overrides\nSlurmdPidFile=/run/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/lib/slurm/slurmctld\nSlurmUser=slurm\nStateSaveLocation=/var/lib/slurm/slurmctld\nNodes\nNodeName=galaxyworkstation CPUs=32\nPartitions\nPartitionName=debug Default=YES Nodes=galaxyworkstation State=UP\n------------scontrol show job--------------------------\n#scontrol show job\n#scontrol show job 18\nJobId=18 JobName=g67_secure_hash_message_digest_niklas_petzold_tum_de\nUserId=galaxy_ans(999) GroupId=galaxy_ans(999) MCS_label=N/A\nPriority=4294901759 Nice=0 Account=(null) QOS=(null)\nJobState=FAILED Reason=NonZeroExitCode Dependency=(null)\nRequeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0\nRunTime=00:00:04 TimeLimit=UNLIMITED TimeMin=N/A\nSubmitTime=2023-06-30T16:07:07 EligibleTime=2023-06-30T16:07:07\nAccrueTime=2023-06-30T16:07:07\nStartTime=2023-06-30T16:07:08 EndTime=2023-06-30T16:07:12 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-06-30T16:07:08 Scheduler=Main\nPartition=debug AllocNode:Sid=galaxyworkstation:22799\nReqNodeList=(null) ExcNodeList=(null)\nNodeList=galaxyworkstation\nBatchHost=galaxyworkstation\nNumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0::\nTRES=cpu=1,mem=1M,node=1,billing=1\nSocks/Node=* NtasksPerN:B:S:C=0:0:: CoreSpec=*\nMinCPUsNode=1 MinMemoryNode=1M MinTmpDiskNode=0\nFeatures=(null) DelayBoot=00:00:00\nOverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\nCommand=(null)\nWorkDir=/media/Galaxy2022/data/jobs/000/67\nStdErr=/media/Galaxy2022/data/jobs/000/67/galaxy_67.e\nStdIn=/dev/null\nStdOut=/media/Galaxy2022/data/jobs/000/67/galaxy_67.o\nPower=\n-------------------groupvars-------------------\n#Job_config\nGalaxy Job Configuration\ngalaxy_job_config:\nrunners:\nlocal_runner:\nload: galaxy.jobs.runners.local:LocalJobRunner\nworkers: 4\nslurm:\nload: galaxy.jobs.runners.slurm:SlurmJobRunner\ndrmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1\nhandling:\nassign: [\u2018db-skip-locked\u2019]\nexecution:\n#default: local_env\n#default: singularity\ndefault: slurm\nenvironments:\nlocal_env:\nrunner: local_runner\ntmp_dir: true\nslurm:\nrunner: slurm\nsingularity_enabled: true\nenv:\n- name: LC_ALL\nvalue: C\n- name: SINGULARITY_CACHEDIR\nvalue: /tmp/singularity\n- name: APPTAINER_TMPDIR\nvalue: /tmp\nsingularity:\nrunner: local_runner\nsingularity_enabled: true\nenv:\n# Ensuring a consistent collation environment is good for reproducibility.\n- name: LC_ALL\nvalue: C\n# The cache directory holds the docker containers that get converted\n- name: APPTAINER_CACHEDIR\nvalue: /tmp/singularity\n# Apptainer uses a temporary directory to build the squashfs filesystem\n- name: APPTAINER_TMPDIR\nvalue: /tmp\ntools:\n- class: local # these special tools that aren\u2019t parameterized for remote execution - expression tools, upload, etc\nenvironment: local_env\n#Slurm configuration\nSlurm\nslurm_roles: [\u2018controller\u2019, \u2018exec\u2019] # Which roles should the machine play? exec are execution hosts.\nslurm_nodes:\n\nname: galaxyworkstation # Name of our host\nCPUs: 32         # Here you would need to figure out how many cores your machine has. For this training we will use 2 but in real life, look at htop or similar.\nSockets: 2\nCoresPerSocket: 8\nThreadsPerCore: 2\nslurm_config:\nSlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2\nSelectType: select/cons_res\nSelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node\n\n------------journalctl while running job--------------------------\n#Journalctl when running job\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools INFO 2023-07-04 16:46:04,383 [pN:main.2,p:13025,tN:WSGI_0] Validated and populated state for tool request (15.737 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,398 [pN:main.2,p:13025,tN:WSGI_0] Handled output named out_file1 for tool secure_hash_message_digest (2.066 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,414 [pN:main.2,p:13025,tN:WSGI_0] Added output datasets to history (15.295 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,417 [pN:main.2,p:13025,tN:WSGI_0] Setup for job Job[unflushed,tool_id=secure_hash_message_digest] complete, ready to be enqueued (2.917 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.execute DEBUG 2023-07-04 16:46:04,417 [pN:main.2,p:13025,tN:WSGI_0] Tool secure_hash_message_digest created job None (29.101 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.web_stack.handlers INFO 2023-07-04 16:46:04,464 [pN:main.2,p:13025,tN:WSGI_0] (Job[id=71,tool_id=secure_hash_message_digest]) Handler \u2018default\u2019 assigned using \u2018db-skip-locked\u2019 assignment method\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.execute DEBUG 2023-07-04 16:46:04,481 [pN:main.2,p:13025,tN:WSGI_0] Created 1 job(s) for tool secure_hash_message_digest request (97.784 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:04,499 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cPOST /api/tools HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,539 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:45:27.511849 HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,607 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&limit=1000&q=update_time-ge&qv=2023-07-04T14:45:30.441Z&details=ac07c226041e4298,efb7f17bc8d9ab0f,d166293c8b6f10d0 HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,674 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/users/dba0b16b7fc2d9dd HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:04,741 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&order=hid&offset=0&limit=100&q=deleted&qv=false&q=visible&qv=true HTTP/1.0\u201d 200\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler DEBUG 2023-07-04 16:46:05,052 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.mapper DEBUG 2023-07-04 16:46:05,130 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: slurm\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler DEBUG 2023-07-04 16:46:05,154 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to slurm runner\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,218 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: slurm)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,227 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /media/Galaxy2022/data/jobs/000/71\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners DEBUG 2023-07-04 16:46:05,260 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] Job [71] queued (104.998 ms)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler INFO 2023-07-04 16:46:05,266 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,392 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Job wrapper for Job [71] prepared (107.937 ms)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver] found description [None]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [ExplicitSingularityContainerResolver] found description [None]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Image name for tool secure_hash_message_digest: python:3.7\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,395 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [CachedMulledSingularityContainerResolver[cache_directory=/srv/galaxy_ans/var/container_cache/singularity/mulled]] found description [ContainerDescription[identifier=/srv/galaxy_ans/var/container_cache/singularity/mulled/python:3.7\u20131,type=singularity]]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.command_factory INFO 2023-07-04 16:46:05,427 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Built script [/media/Galaxy2022/data/jobs/000/71/tool_script.sh] for tool command [python \u2018/srv/galaxy_ans/server/tools/filters/secure_hash_message_digest.py\u2019 --input \u2018/media/Galaxy2022/data/datasets/f/7/f/dataset_f7fbfb00-7fd5-49dd-985a-c246018f4e3f.dat\u2019 --output \u2018/media/Galaxy2022/data/jobs/000/71/outputs/galaxy_dataset_0f3d2303-4e23-4ec1-a255-9b77d77ecee1.dat\u2019 --algorithm \u201cmd5\u201d]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners DEBUG 2023-07-04 16:46:05,493 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) command is: mkdir -p working outputs configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: if [ -d _working ]; then\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]:     rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: else\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]:     cp -R working _working; cp -R outputs _outputs; cp -R configs _configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: fi\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: cd working; SINGULARITYENV_GALAXY_SLOTS=$GALAXY_SLOTS SINGULARITYENV_GALAXY_MEMORY_MB=$GALAXY_MEMORY_MB SINGULARITYENV_GALAXY_MEMORY_MB_PER_SLOT=$GALAXY_MEMORY_MB_PER_SLOT SINGULARITYENV_HOME=$HOME SINGULARITYENV__GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR SINGULARITYENV__GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR SINGULARITYENV_TMPDIR=$TMPDIR SINGULARITYENV_TMP=$TMP SINGULARITYENV_TEMP=$TEMP singularity -s exec --cleanenv -B /srv/galaxy_ans/server:/srv/galaxy_ans/server -B /srv/galaxy_ans/server/tools/filters:/srv/galaxy_ans/server/tools/filters -B /media/Galaxy2022/data/jobs/000/71:/media/Galaxy2022/data/jobs/000/71 -B /media/Galaxy2022/data/jobs/000/71/outputs:/media/Galaxy2022/data/jobs/000/71/outputs -B /media/Galaxy2022/data/jobs/000/71/configs:/media/Galaxy2022/data/jobs/000/71/configs -B \u201c$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR\u201d -B \u201c$TMPDIR:$TMPDIR\u201d -B \u201c$TMP:$TMP\u201d -B \u201c$TEMP:$TEMP\u201d -B \u201c$_GALAXY_JOB_HOME_DIR:$_GALAXY_JOB_HOME_DIR\u201d -B /media/Galaxy2022/data/jobs/000/71/working:/media/Galaxy2022/data/jobs/000/71/working -B /media/Galaxy2022/data/datasets:/media/Galaxy2022/data/datasets -B /srv/galaxy_ans/var/tool_data:/srv/galaxy_ans/var/tool_data -B /srv/galaxy_ans/var/tool_data:/srv/galaxy_ans/var/tool_data --home $HOME:$HOME /srv/galaxy_ans/var/container_cache/singularity/mulled/python:3.7\u20131 /bin/bash /media/Galaxy2022/data/jobs/000/71/tool_script.sh > \u2018\u2026/outputs/tool_stdout\u2019 2> \u2018\u2026/outputs/tool_stderr\u2019; return_code=$?; echo $return_code > /media/Galaxy2022/data/jobs/000/71/galaxy_71.ec; cd \u2018/media/Galaxy2022/data/jobs/000/71\u2019;\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: [ \u201c$GALAXY_VIRTUAL_ENV\u201d = \u201cNone\u201d ] && GALAXY_VIRTUAL_ENV=\u201c$_GALAXY_VIRTUAL_ENV\u201d; _galaxy_setup_environment True; python metadata/set.py; sh -c \u201cexit $return_code\u201d\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:05,501 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) submitting file /media/Galaxy2022/data/jobs/000/71/galaxy_71.sh\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa INFO 2023-07-04 16:46:05,505 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) queued as 24\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:05,801 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) state change: job is running\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,796 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:46:04.427855 HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,846 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&limit=1000&q=update_time-ge&qv=2023-07-04T14:46:04.555Z&details=ac07c226041e4298,efb7f17bc8d9ab0f,d166293c8b6f10d0 HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,908 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/users/dba0b16b7fc2d9dd HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:07,979 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&order=hid&offset=0&limit=100&q=deleted&qv=false&q=visible&qv=true HTTP/1.0\u201d 200\nJul 04 16:46:08 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:08,353 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/entry_points?running=true HTTP/1.0\u201d 200\nJul 04 16:46:10 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:10,938 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:46:05.984307 HTTP/1.0\u201d 200\nJul 04 16:46:11 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:11,110 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) state change: job finished, but failed\nJul 04 16:46:11 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.slurm WARNING 2023-07-04 16:46:11,125 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) Job failed due to unknown reasons, job state in SLURM was: FAILED"}, {"role": "system", "text": "Hi @niklas.petzold\nLet\u2019s cross-post this question to the Admin chat. They may reply here or there, and feel free to join the chat. You're invited to talk on Matrix\nThanks for posting all the details and logs \u2013 super helpful. If you could post back the version of Galaxy you are running, that would probably also help. Using the most recent stable release is usually best: Galaxy Documentation \u2014 Galaxy Project 23.0.2.dev0 documentation"}, {"role": "user", "text": "Hi Jennaj,\nthanks for you reply and linking the question in the chat  i also posted the question in the admin chat a few days ago.\nCatbro gave me a hint that it is already known that there is an issue with slurm when using Ubuntu 22.04 since it uses cgroup2 instead of cgroup1. I added a task to the playbook to use cgroup as described by Nate Coraor:\n- name: Disable cgroupv2\ncopy:\ncontent: |\nGRUB_CMDLINE_LINUX_DEFAULT=\u201c$GRUB_CMDLINE_LINUX_DEFAULT systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller=1\u201d\ndest: /etc/default/grub.d/99-cgroupv1.cfg\nmode: 0644\nnotify:\n- update-grub\n- reboot\nBut the problem still remains the same.\nI am following the newest version of the hands-on of the Galaxy Admin Training Path, so i am running Galaxy release 23.0. My playbook etc. is pretty much the same as the reference files here:\n  \n      \n\n      github.com\n  \n\n  \n    GitHub - hexylena/git-gat at step-9\n\n  step-9\n\n  GAT ... in Git! Contribute to hexylena/git-gat development by creating an account on GitHub.\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nDoes it make sense to change the Ubuntu version? Which version is used for development and is most stable/tested? The training suggests 20.04-22.04"}, {"role": "user", "text": "After adding the cgroup patch and updating all packages, as described by Nate Coraor, it suddendly worked. I put the update tasks into a new playbook, which looked like this:\n\n\nhosts: galaxyworkstation\nbecome: true\nbecome_user: root\npre_tasks:\n\n\nname: Update repos\napt:\nupdate_cache: yes\ncache_valid_time: 900\ntags:\n\nupgrades\n\n\n\nname: Upgrade packages\napt:\nupgrade: yes\nautoremove: yes\nnotify:\n\nreboot\ntags:\nupgrades\n\n\n\nname: Install other packages\npackage:\nname:\n- acl\n- bc\n- sudo\n- make\n- build-essential\n- git\n- nano\n- vim-nox\n- vim-pathogen\n- emacs-nox\n- virtualenv\n- python3-pip\n- jq\n- htop\n- zlib1g-dev\n- libbz2-dev # planemo \u2192 samtools\n- liblzma-dev # planemo \u2192 samtools\n- tree\n- byobu\n- screen\n- cockpit # web console\n- moreutils # for gat-cli\n- silversearcher-ag # For @hexylena\nstate: latest\ntags:\n- packages\n\n\nname: Ensure unnecessary stuff is purged\npackage:\nname:\n- python\n- fail2ban\n- snapd\n- emacs-lucid\n- emacs-gtk\nstate: absent\n\n\nhandlers:\n\nname: update-grub\ncommand: /usr/sbin/update-grub\nname: reboot\nreboot:\n\n\n\nI hope this might help anyone at some point  Thanks for the help!"}], [{"role": "user", "text": "Hi all, How do i get more disk space. I have exceeded my allocated disk space for my analysis."}, {"role": "system", "text": "Could you remove old/unnecessary datasets to free up space? As far as I know, the only other way is to install your own appliance."}, {"role": "system", "text": "The first step you can follow is to try to free up some space from your account, and here are a few good practices for data management in Galaxy and a tutorial explaining how to delete data from Galaxy.\nAfter that, if you still need more space, you can apply for a quota extension for a limited time period."}], [{"role": "user", "text": "Hi,\nI recently installed Galaxy release_21.09 on Ubuntu 21.04. as described in the \u201cGet Galaxy\u201d tutorial and am now following the \u201cAdding custom tools tutorial\u201d (Adding custom tools to Galaxy - Galaxy Community Hub). The first problem is that at Step 4 no tool_config.xml exists in the /config folder. The only files that exist are tool_config.xml.sample and tool_config.xml.main. After adding the section to either of these files, the tool does not show up in galaxy after >sh run.sh.\nI also tried creating a new file tool_config.xml and copying tool_config.xml.[sample/main] to tool_config.xml and also to edit the galaxy.yml #tool_config_file accordingly. The tool never shows up.\nI also ran >sh run.sh --daemon to check the log, but it does not contain any errors regarding tools.\nDoes anyone know what I am doing wrong or could it be that the tutorial I am using is not up to date?\nBest,\nLeander"}, {"role": "system", "text": "I can\u2019t fully debug it now but you could try to stop galaxy and remove the file galaxy/config/integrated_tool_panel.xml and restart it. On top of my head you need to rename tool_config.xml.sample to tool_config.xml.\nIt is also possible that you need to clear the cache of your browser."}], [{"role": "user", "text": "Hello!\nWhen I use Galaxy every time getting same error, was thinking it\u2019s something temporary, but unfortunately it\u2019s not.\nCould you please help me and recommend something?\n(admin redacted for data privacy)\n\nerror1589\u00d7935 172 KB\n"}, {"role": "system", "text": "Hi @Dana_Trunova\nIt looks like you clicked on the pencil icon  of a dataset to reach the \u201cEdit Attributes\u201d functions. The server is unable to load that data, possibly because of a metadata problem or some server side problem. You could try making a copy of the history in your own account there to see if that resets the metadata. If not, you\u2019ll need to contact an administrator.\nThe administrators of the repeatexplorer public Galaxy server do not follow this forum, and it is difficult for others to help more, especially for any actual technical server-side problems. Instead, please try contacting them directly via email.\nQuote from there website:\n\n\n\nrDNA analysis with Repeatexplorer2\n\n\n\nIf you need help with RepeatExplorer or you want to report a problem and our wiki is not able to give you answers, please contact server administrator.\n\n\n"}], [{"role": "user", "text": "hello\ni was redirected to this support page while tryin to log in into galaxy galaxian https://bioinf-galaxian.erasmusmc.nl/galaxy/\nI have an accoutn for galaxy but i not recognozed on the galazxian\nthanks\nirene"}, {"role": "system", "text": "Every Galaxy instance has independent user database. User account from usegalaxy.org will not work on https://bioinf-galaxian.erasmusmc.nl/galaxy/. You have to create instance specific account."}], [{"role": "user", "text": "Dear Galaxy Support,\nI ran the ChIP-Seq data analysis tutorial.\nI got error in Step 6: Plot the signal between samples (In attached picture).\n\nConcatenate two datasets into one dataset -Got Output\nSortBED  -Got Output\nMergeBED -Got Error\n\nI was still able to perform\n4. computeMatrix (tried with output of Concatenate)\n5. plotHeatmap\nI have attached pictures of the steps and output pictures.\nCould anybody try the tutorial and give advice on my issue?\nThanks in advance.\nJP\n\nI have detailed the issue in the attached picture.\nTutorial Source: https://galaxyproject.github.io/training-material/topics/chip-seq/tutorials/formation_of_super-structures_on_xi/tutorial.html\n\nimage.png935\u00d7665 261 KB\n\n\n\n"}, {"role": "system", "text": "the \u201c29. SortBed\u2026\u201d dataset appears to not be sorted properly; this seems to be verified by the fragment of error message I can make out:\n\u201cError: Sorted input specified, but\u2026\u201d\nthen it lists the second entry in \u201c29.\u201d \u2013 whose Start is sooner than the preceding data-point (ie, not in ascending order)\nIs it possible you accidentally sorted by the wrong value? The tutorial simply says \u201cSort the following bed,bedgraph,gff,vcf file\u201d so it expects the default sort parameter \u201cSort by: chromosome, then by start position (asc)\u201d"}], [{"role": "user", "text": "Hi!\nI tried using the ribogalaxy bowtie tool to remove ncRNA however, after the job was run I couldn\u2019t find any new files other than the mapping statistics for my data.\nThe mapping statistics said that 98% of the reads had not aligned (mean 2% were ncRNA) but then there was no new file in my history.\nWould anyone be able to help me?\nThanks!\nJanhavi"}, {"role": "system", "text": "Cross-posted here: Contact RiboGalaxy Support"}], [{"role": "user", "text": "Hello everybody,\nI have been getting this error message multiple times now since I queued a lot of jobs in the Trim Raw Reads tool. I get the same message when trying to purge my deleted data sets: \nserver error1915\u00d71060 145 KB\n\nShould I just wait and see if the problem persists or can I reset somehow?\nThank you for your help.\nBest,\nSven"}, {"role": "system", "text": "Hi @clsven\nThis usually means that the server you are working at is very busy. Given that it occurred across multiple actions, the best first-pass solution is to wait a bit then try again.\nIt looks like you are working on your own Galaxy server. A local Galaxy server uses your own computer for computational resources unless you configured it differently. Also, the version of Galaxy appears to be a few years out of date.\nUpgrading your server would be a very good idea, especially if you installed newer tools: Releases \u2014 Galaxy Project 21.01 documentation\nAdvanced configuration help is here: Galaxy Deployment & Administration \u2014 Galaxy Project 21.01 documentation\nAdmin training is here: GitHub - galaxyproject/admin-training: Galaxy Admin Training\nThanks!"}, {"role": "user", "text": "Hi @jennaj\nThank you for your reply. Yes, it is a local Galaxy server but it is not my own one. It is called \u201cCAFU - A Galaxy framework for exploring unmapped RNA-Seq data\u201d and is accessible under http://omicstudio.cloud:4001/\nSo I guess I should contact the people running that server then?\nBest,\nSven"}, {"role": "system", "text": "\n\n\n clsven:\n\nSo I guess I should contact the people running that server then?\n\n\nYes, the administrators of that server would be the people for you to contact.\nhttps://galaxyproject.org/use/cafu/\nThey can always reach out to the Galaxy Admin working group if they need help beyond what the docs cover. Gitter would be a good place to start for them: galaxyproject/admins - Gitter"}], [{"role": "user", "text": "Hello,\nI am doing a de novo assembly with Trinity. It was running for 14 days and then ended with no error (the job box is green), but both the assembly and the gene to transcripts map are empty (0 bytes).\nThe only \u201cerror\u201d I see in the output is this:\nSequence: CCAATATTAAGCCACGATTAAGGAis smaller than 25 base pairs, skipping\nrepeating for a bunch of sequences, and nothing else.\nThe input files are paired-end, RF stranded, Trimmomatic output files, not interleaved. I used them for assembly with Trinity in the same history almost a year ago, and everything worked perfectly.\nDoes anyone have an idea as to what could be causing this?\nThanks!"}, {"role": "system", "text": "Hi @magdalenagrgic\n\n\n\n magdalenagrgic:\n\nSequence: CCAATATTAAGCCACGATTAAGGAis smaller than 25 base pairs, skipping\nrepeating for a bunch of sequences, and nothing else.\n\n\nThis message means that sequences were skipped during assembly. You might want to rerun Trimmomatic and add in a length filtering criteria (require minimum read length of 25 bases) to pass through. That reduces the pre-processes that Trinity needs to run during execution.\n\n\n\n magdalenagrgic:\n\nI used them for assembly with Trinity in the same history almost a year ago, and everything worked perfectly.\n\n\nIf this was a rerun started by loading up the prior tool form (via the \u201crerun\u201d icon), check to see if you are using the most current version of the Trinity tool available on the server where you are working (UseGalaxy.eu?). \u201cVersions\u201d can be navigated from the upper right corner of any tool form. Older versions may no longer be supported.\nTry this:\n\nRerun Trimmomatic to filtered reads >= 25 bases\nRerun Trinity twice \u2013 using the original and the most current tool version\nIf both of those fail, please send in a bug report from the error(s) to the server admins\nAt the same time, you can follow up here by posting back a #sharing-your-history link. That link can be posted back publicly or ask for a moderator to start up a private chat. We\u2019ll need all inputs and outputs to be undeleted and for you to note which dataset numbers are involved.\n\nThe job could really be too large to complete as-is but filtering the read sequences by length is one item to address that would have no scientific impact on the results. Other solutions would probably involve one or more of these: downsampling the reads (Seqtk tools), including a reference annotation (\u201cgenome guided mode\u201d), not pooling samples, and/or tuning more of the assembly parameters.\nLet\u2019s start there "}, {"role": "user", "text": "Hi Jenna  ,\nthank you very much for the fast response!\nI may have expressed myself a bit awkwardly - it was not a rerun of the same job. What I wanted to say is that these exact datasets were previously used with the Trinity as well so I don\u2019t think there\u2019s something wrong with them. The only difference in this run is that I put all of my samples for one assembly instead of separating them into two different assemblies like the last time. That comes up to around of 500M reads, so it could be, as you say, that the job was too large.\nThank you for all the suggestions, I\u2019ll try them.\nCould you please also tell me how to start a private chat to share my history?\nMagdalena"}, {"role": "system", "text": "\n\n\n magdalenagrgic:\n\nThe only difference in this run is that I put all of my samples for one assembly instead of separating them into two different assemblies like the last time. That comes up to around of 500M reads, so it could be, as you say, that the job was too large.\n\n\nHi @magdalenagrgic\nThanks for explaining. Combining more reads into a single assembly can definitely impact runtime and performance.\n500M reads in the same assembly is about 10 times the size of datasets the authors used for benchmarking statistics: Trinity Memory Usage. Assembly can be impacted for scientific content reasons (depth, coverage, redundancy) \u2013 but I don\u2019t think that is the root issue right now. The issue is simply volume versus resources.\nWhat I would suggest is to consider downsampling the reads before assembly. One tool choice is Sektk. That way you can include all samples together but not run up against computational issues. There is much discussion about assembly strategies at the different bioinformatics forums, including here: https://groups.google.com/g/trinityrnaseq-users.\n\n\n\n magdalenagrgic:\n\nCould you please also tell me how to start a private chat to share my history?\n\n\nI\u2019ll start up a direct message. But \u2026 I don\u2019t think that reviewing your history right now will change the advice. It sounds like the reads are in the correct technical format, can pass through QA steps, and will assemble when in a smaller batch.\nAt this forum, a new user\u2019s trust level will increase with time and activity, and the option to start up a direct message will become available. The goal is to try to keep initial questions and general discussion/troubleshooting public. This helps us to build up the knowledge base and helps you to reach more potential helpers "}], [{"role": "user", "text": "What does this error mean?\nncol(countData) == nrow(colData) is not TRUE"}, {"role": "system", "text": "This probably indicates a usage or data input problem with a tool.\n\n\n\n Gabriella_Rudy:\n\nncol(countData) == nrow(colData) is not TRUE\n\n\nTechnically, this is reporting that the number of expected columns is not matching the number of rows in some calculation \u2013 but that is not enough to offer actionable help.\nPlease reply back with a bit more information:\n1 - What is the name and version of the tool are you using? Click on the rerun button and copy the name/version of the tool back in the reply.\n2 - Where are you running the tool? If a public server, capture the base URL. If your own Galaxy, what version was installed?\nLet\u2019s start there. We might recognize the problem/usage issue and be able to offer help, or this will guide us about what other information is needed to diagnose/offer troubleshooting help."}, {"role": "user", "text": "Thank you for your response.\n1- The name and version of the tool is DESeq2\n2- (Galaxy Version 2.11.40.2)- I am running it through usegalaxy.org (so I guess I am running it on a cloud server?)\nMore info: the first column of my data is gene name and the rest of the columns are count data.  I am thinking the issue might have something to do with the first column."}, {"role": "system", "text": "Hi - Thanks for the extra information!\nThe problem is with the format of the count input datasets. Each sample needs to be run through a supported count tool and entered individually under Factor/Factor levels. Details are on the DEseq2 tool form.\nSince your data has the counts in a matrix, that won\u2019t work with DEseq2 as currently wrapped for Galaxy.\nOptions:\nTo use DESeq2, I would suggest generating the counts within Galaxy. Or, you could run one of your samples in Galaxy, compare those results to your per-sample counts generated outside of Galaxy (individual counts, before merged into a matrix), and make changes to sync up the formatting as needed.\nConsider alternative tools. Limma/EdgeR do support count matrix input. How to use each is described on their tools forms with example data input content/format.\nLimma DE (with matrix input) is included in this provisional Galaxy Training Network (GTN) Tutorial: https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html\nNote:  At this time, that tutorial still under development. Once the tutorial finalized, it will be listed here: https://galaxyproject.github.io/training-material/topics/transcriptomics/. Feedback to the GTN about it would be welcomed to help us improve it \u2013 find the feedback form at the end of the tutorial. If you want to use all the tools in the tutorial, those might not be all at Galaxy Main https://usegalaxy.org server yet \u2013 but I\u2019m fairly certain the tools are at Galaxy EU https://usegalaxy.eu server (the server used for development of the tutorial) \u2013 and if not for some reason, that could be part of your feedback."}], [{"role": "user", "text": "Hi,\nI am using Windows10 and trying to upload some bam.files to Galaxy Australia FTP using fileZilla.\nI have been following the steps according to the tutorial video from this site: Galaxy FTP Upload - Galaxy Community Hub\nHowever, I always end up getting the following error messages:\nError:|Connection timed out after 20 seconds of inactivity|\nError:|Could not connect to server|\nAnyone know how to fix it ?\nThanks"}, {"role": "system", "text": "Hi @Frank,\nthanks for the report; I have informed the sysadmin.\nRegards"}, {"role": "system", "text": "Hi, upload by ftp is not available at Galaxy Australia for some time. Files in GB range can be uploaded using web interface. Alternatively, upload files using URLs. Australian users can import data from CloudStor."}], [{"role": "user", "text": "I am trying to run the pipeline from this Clustering 3K PBMCs with Scanpy on my own data from 10x genomics but when I try to run quality control as well as plotinf functions on the annotated data matrix I get the error that the variable names are not unique. Is there anyway to make them unique in galaxy rather than using Python?\nThanks!"}, {"role": "system", "text": "Hi @joshua.harvey.20,\nas Pavan suggested you, you can use Manipulate Annadata tool with \u201cFunction to manipulate the object\u201d \u2192 \u201cMakes the var index unique by appending \u20181\u2019, \u20182\u2019, etc\u201d.\nRegards"}], [{"role": "user", "text": "Hi All,\nI trimmed my paired datasets for adapters using Cutadapt. However, the output of the Cutadapt is not visible if I want to run HISAT2 subsequently. RNA STAR can easily use detect Cutadapt datasets for further processing. I can circumvent this problem by generating a new list of dataset pairs, but I assume there must be a faster way, because, when building a new list of dataset pairs, I also need to edit the pair name in order to keep a better overview further down the pipeline. Am I overlooking something?"}, {"role": "system", "text": "Hey Henk,\nProbably HISAT2 has a problem to realize that your data is valid as an input. Please check that the data output of Cutadapt is in a fasta/fastq format and please also check if the data is not marked as hidden in Galaxy."}, {"role": "user", "text": "Hi F,\nThanks for your quick reply. The files are of fastqsanger.gz type and I was able to get the trimmed database visible for further analyses when I unhide them. Unfortunately, I do need to edit the attributes in order to keep better track of the individual pairs for downstream applications. Perhaps I do still overlook something or is this indeed a little flaw in a, for the rest, pleasant platform to work on.\nHenk"}, {"role": "system", "text": "Update 4/22/21\nRe-tested with the prior test, plus created a new extracted workflow and reran that. Everything is working.\nI had forgotten that Cutadapt will split a paired-end collection into two collections: one contains the forward reads, and one contains the reverse reads. Maybe this is the problem you had? And this example usage will help you to sort out the problems you were having? The end result from HISAT2 is a single collection.\nThe data is a bit large in the history, but you don\u2019t need to actually import it \u2013 use the \u201cview\u201d function, all the details are available.\nThe workflow is very simple and doesn\u2019t consume any extra quota space, so importing and perhaps using it or reviewing it in the editor will help even more. You\u2019ll see that a single collection of paired-end reads was input to Cutadapt, then resulting collections of the forward and reverse reads were input to HISAT2, producing one final collection of mapped BAMs (one per pair).\nHistory: https://usegalaxy.org/u/jen-galaxyproject/h/copy-of-test-cutadapt-listpaired--hisat2\nWorkflow: Galaxy | Accessible Workflow | Workflow constructed from history 'Copy of 'test cutadapt listpaired' + hisat2'\n\nHi @Henk\n\n\n\n Henk:\n\nHowever, the output of the Cutadapt is not visible if I want to run HISAT2 subsequently. RNA STAR can easily use detect Cutadapt datasets for further processing.\n\n\nThe data is in a paired-end dataset collection with the datasets inside it assigned the datatype fastqsanger.gz, correct? The output from CutAdapt?\nHISAT2 should be able to recognize that dataset collection and process it directly.\nThe collection should be in the active history, with this option set on the HISAT2 tool form. Note: The screenshot is from loading the tool with a new, empty history that does not contain inputs of the proper datatype. It is a good way of discovering what the appropriate/recognized inputs are. When you run this, the input should be populated with one or more paired collections assigned one of those specified datatypes. Compressed will be recognized and uncompressed automatically as part of the pre-processing.\n\nhisat2-paired-collection-input1914\u00d7286 25.1 KB\n\nIf you do not get a result like that, something is going wrong, and we can follow up on that. Rebuilding the collection or unhiding collection elements should not be needed (although they both are valid workarounds for this problem!). Let\u2019s try to fix it at the source. Cutadapt had some issues with Workflows in the past \u2013 but am pretty sure those are resolved now (most current Galaxy release + most current tool version).\nFew questions:\n\n\nWhere are you using Galaxy? URL if a public server. And are you using the most current tool versions available there? We can follow up from there \u2013 an example might be good to review (directly, not publically) \u2013 if needed, we\u2019ll explain how.\n\n\nIf you own Galaxy (local, cloud), are you upgraded to the latest Galaxy version? Are the tools updated from the ToolShed? Has this worked before or it is a new problem? If using a Workflow, did this stop working after changes (not necessarily these steps)? You might even want to check to see if some sample of this works at one or any of the UseGalaxy.* servers (as a comparison).\n\n\nLet\u2019s start there. This was an issue before, so seems odd to have it come up again. I\u2019ll also review/rerun our prior test cases at UseGalaxy.org and will write back with that result.\nThanks!"}], [{"role": "user", "text": "Hi, my name is Julia. Im new to Galaxy NGS and want to edit my Fastq files: I would like to crop bases from the beginning of the read and from the end of the read. As far as Ive seen, there are only fastq tools, which allow me to crop fastq files depending on their quality scores.\nI want to crop fastq files independently from the quality scores.\nThanks for your help in advance!\nJulia"}, {"role": "system", "text": "You can use the \" Trim sequences\" tool to trim from the beginning and end of the read. So e.g. if you have 100 bp reads, and set \u201cFirst based to keep\u201d to 10 and \u201cLast base to keep\u201d to 85\" you will end up with 75 bp reads - from the 10th base to the 85th base.\nI hope this helps."}], [{"role": "user", "text": "Hi,\nI run a private Galaxy instance and a toolshed. I recently upgraded from 19_05 to 19_09 and also moved to python3. Since this upgrade, my tools from the local toolshed disappeared from the \u2018Tools\u2019 pane. All tools from my toolshed were contained with a separate target section within the Tools pane, and that section header has disappeared from the user interface too. Within the Galaxy admin interface the tools are still shown as installed. If I uninstall and the install a tool from my local toolshed (and re-enter the target section), then the tools return to the Tools pane. However, when I try to run these tools I get an error:\nInstance <ToolShedRepository at 0x7f4b5c113610> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: http://sqlalche.me/e/bhk3)\nThere are some tickets on the galaxy github repo about this error message, but they have all been resolved through commits that I believe I have pulled in or the posted work around does not resolve the issue.\nAny help getting past this error would be appreciated.\nthanks,\n-Will"}, {"role": "system", "text": "There is an important section in the 19.09 release notes about tool configuration files: https://docs.galaxyproject.org/en/master/releases/19.09_announce.html#tool-configuration-file-handling-changes\nAre your tools from local toolshed perhaps loaded from a separate configuration file? That could be one of the causes I can think of (and you\u2019d need to change Galaxy\u2019s configuration as described in the above linked document)\nAs of the is not bound to a Session error, we believe we have fixed that, but it took plenty of PRs over time, please make sure you are running the latest commit of  the release_19.09 branch."}], [{"role": "user", "text": "Hello,\nI\u2019m doing some basic de novo transcriptome analysis and have found that while I\u2019m assembling transcripts in Stringtie, my downstream jobs such as running stringtiemerge, gffcompare, and featurecounts all finished even though I still had some original Stringtie jobs going\u2026\nAnd then DESeq2 fails to run. This doesn\u2019t seem right, or even possible. Does anyone have any insight on this?"}, {"role": "system", "text": "That should not happen, can you maybe share a history with us in which this happens?\nI have never seen this before."}, {"role": "user", "text": "Unfortunately I ended up removing those datasets and starting over (I was running out of space). Since that attempt, I\u2019ve just been running jobs in batch mode one at a time to avoid this issue. I will update this thread if this issue occurs again."}, {"role": "system", "text": "Related Q&A Stringtie not running\nIt seems that the problem was some setting in the workflow used."}], [{"role": "user", "text": "Hi,\nI\u2019m trying to import a Dataset from User History\nIn the Datasets dropdown list, from History item is visible but from User History does not show up for a non-admin user.\nFor an admin of the Galaxy instance,  from User History is present.\nThere is no error in the galaxy log.\nIn galaxy.yml, the library_import_dir and user_library_import_dir have been configured (they point to the same directory).\nA subdirectory has been manually created under user_library_import_dir for the non-admin user with rwx permissions.\nI\u2019ve noticed that in $GALAXY_ROOT/client/src/components/LibraryFolder/TopToolbar/FolderTopBar.vue\nuser_library_import_dir is set to false (function data()). Is this related to my problem?\nThanks a lot,\nJin"}, {"role": "system", "text": "Hi @Jin\nWith default settings, a general user with non-admin status has access to:\n\nTheir own datasets (including any they imported from others in a fully shared state)\nAnd, any datasets in a global shared state (Data Library).\n\nOnly admins have access to other classes of data.\n\n\n\n Jin:\n\nI\u2019ve noticed that in $GALAXY_ROOT/client/src/components/LibraryFolder/TopToolbar/FolderTopBar.vue\nuser_library_import_dir is set to false (function data()). Is this related to my problem?\n\n\nFor this option, I\u2019m not sure if it will do what you want by itself. But, you could try setting it to true, restart, and see what happens. Then review the other similar options to fine-tune. Each is described here: Configuration Options \u2014 Galaxy Project 21.09.1.dev0 documentation"}, {"role": "user", "text": "@jennaj\nThanks a lot.\nI\u2019ve modified the FolderTopBar.vue file (with a field name change to check) and restarted Galaxy but nothing has changed.\nIs there any kind if \u2018compilation\u2019 to do? (sorry for the question, I\u2019m a Vue newbie)\nBest,\nJin"}, {"role": "user", "text": "Actually this is a bug in v20.09 which is described in https://github.com/galaxyproject/galaxy/issues/10982\nand fixed in later commits."}], [{"role": "user", "text": "I have searched through all the threads on this topic and I have yet to find a reasonable explanation for why I continue to encounter this error. I have already run Cufflinks on my samples, but when I attempt to run Cuffmerge (cuffmerge -p 8 -g /scratch/sbag/Tikshana/BANANA/RAW/Mac.gff3 -s /scratch/sbag/Tikshana/BANANA/RAW/Mac_genome.fa /scratch/sbag/Tikshana/BANANA/cuff/assemblies_Macbal.txt -o /scratch/sbag/Tikshana/BANANA/cuff/Macbal_merge), I get the following error:\nGFF Error: duplicate/invalid \u2018transcript\u2019 feature ID=Ma03_t01040.3\n[FAILED]\nError: could not execute cuffcompare\ni could not understant how to resolve can you please explain me clearly."}, {"role": "system", "text": "Hi @tikshana_yadav\nSome gff3 datasets are released by data providers that contained duplicated \u201cID\u201d identifiers. These cause problems with almost all tools. Using gtf annotation is a better choice.\nPlease note that the tools you are using are deprecated. Details, reasons, and alternatives are covered in this prior Q&A:\n  \n    \n    \n    Cuff Suite Not Running -- Deprecated Tool Suite \n  \n  \n    Cufflinks, Cuffcompare, and Cuffnorm stopped running. I tried resume paused jobs but still paused.\n  \n\n\nThanks!"}, {"role": "user", "text": "Thank you for your kind information. Can you please suggest me some tools to get differential expression of genes in Musa balbisiana with non replicated data which has to annotated with Musa accuminata"}, {"role": "user", "text": "Thank you all for your kind help. I simply want to suggest the future that if this sort of error will occur just check whether your input file is correct or not go through each and every location and check the path as well as your file name. Just keep everything simple do not complex anything. This is what I resolved these days. A small mistake in your command may hamper your time and experiment.\nIn short my bam file in samtools was not created properly so I run all command again and checked my path that resolved my issue/error. If you face the same just go through it make it SIMPLE"}], [{"role": "user", "text": "Helllo\nI\u2019m trying to run fastQC on my fastq.gz file. The FastQC job itself seemed successful,and there was no error report. All dependencies were done. But there was no image in the webpage report like this:\n\nScreenshot from 2020-10-15 09-43-581280\u00d71024 293 KB\n \nScreenshot from 2020-10-15 09-44-091280\u00d71024 168 KB\n \nScreenshot from 2020-10-15 09-44-141280\u00d71024 156 KB\n \nScreenshot from 2020-10-15 09-44-191280\u00d71024 156 KB\n \nScreenshot from 2020-10-15 09-44-251280\u00d71024 160 KB\n\nWhat can i do to fix it?\nthank you"}, {"role": "system", "text": "You need to whitelist the fastqc tool first. This is because of security.\nGo to the admin page, in the menu on the left you will see \u201cmanage whitelist\u201d. On this page you can white list the fastqc tool. And you will see images after that."}], [{"role": "user", "text": "Hi everyone,\nI\u2019m having some troubles understanding what I\u2019ve done wrong.\nI want to assembly a transcriptome from different samples retrieved from the NCBI. Since I had several forward and reverse reads I used the concatenate tool to obtain one dataset for all the forward reads and one for all the reverse reads. At this point I\u2019ve built a collection (with the build dataset pair function, named:  All sequences-cleaned ) and I\u2019ve performed trimmomatic and FASTQC (both ran well).\nAfter that I ran Trinity with these parameters:\n|Are you pooling sequence datasets?|No|\n|Paired or Single-end data?|unmerged_paired_collection|\n|FASTA/FASTQ dataset collection with R1/R2 pair| 142: All sequences-cleaned\nThis gave me this error:\nExecution resulted in the following messages:\nFatal error: Exit code 2 ()\n\nDetected Common Potential Problems\nThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\nCan someone help me to fix this problem?\nThanks!"}, {"role": "system", "text": "Hi @Letizia_Iuffrida\nTrinity can pull files for a single job, hence no need for concatenation.\nMaybe try assembly on a single set of data (F and R reads). Does it work? If the answer is positive, try Trinity with with pulling data set to Yes. I completed a test Trinity job on Galaxy Europe on several samples using pulling option, and don\u2019t see any issue with the tool.\nAs for the error message:\nThe tool was executed with one or more empty input datasets.\nI don\u2019t know if it is generic (aka generated for any failed job) or specific, but please preview the input files.\nKind regards,\nIgor"}, {"role": "user", "text": "Thanks @igor for the reply, I used the concatenate tool to simplify the previous preparation since I have more than ten samples and it\u2019s easier for me to have just one dataset with all the R reads and one for the F reads. Anyway, I\u2019ll try to run Trinity with a single run.\nI checked the input files and they seems ok to me.\nI was thinking, could the problem be related to the fact that I\u2019m trying to assemble about 45-50 GB and maybe it\u2019s too heavy for the server? I was reading in the forum that maybe this is the problem\u2026"}, {"role": "system", "text": "Hi @Letizia_Iuffrida\nDo you mean total size of gzipped FASTQ files is 50 GB? It might be too big, but you need a feedback from the server admin. I assume, in silico normalization of reads was enabled. Maybe consider using khmer: Normalize By Median on merged files before Trinity.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi everyone,\nI used to run the galaxy-stable container (on Ubuntu) with an export folder mapped to an external drive. Now, I have moved to a different computer, attached the external drive and tried to start the container mapping the same folder as the export folder. However, the container does not start and the postgresql database does not initialize (spawn error, \u201ccould not connect to database\u201d). Can anyone help?\nThanks!"}, {"role": "system", "text": "Welcome @cass\nThe original and new were both sourced from here, correct? Docker Hub\nThis is the directory listing: Galaxy Docker - Galaxy Community Hub\nIt links to this release log: GitHub - bgruening/docker-galaxy-stable: Docker Images tracking the stable Galaxy releases.\nSome admin changes were introduced in the latest release. Maybe you were running an earlier release on your old computer, but installed the latest on your new computer? And that is creating a conflict? Maybe try installing the version you had originally, then upgrade? (just a guess)\nThere is a Gitter (or Matrix) chat for help with the Galaxy Docker container if you need more help. I\u2019ll post for help there, and they may reply here or there. Plus, feel free to join the chat \u2013 the version(s) of the container and the Ubuntu installs would probably help with troubleshooting.\nGitter: bgruening/docker-galaxy-stable - Gitter\nMatrix: Follow the link at Gitter to to the room and join to post\nLet\u2019s start there "}, {"role": "system", "text": "(Have also replied in the Galaxy Gitter - bgruening/docker-galaxy-stable - Gitter)\nHaving a quick read of the problem, I\u2019m going to assume you\u2019re moving from one Ubuntu machine to another one. My gut feeling is that there might be some underlying differences between the machines and the way the external drive is mounting.\nCheck that you can read/write into other folders on your external drive on the new machine. If not, it sounds like an I/O error and you\u2019ll need to have a dig into the mounting side of things.\nOtherwise, see if you can read/write into the external drive, log into the container and make sure you can run some basic bash commands. If you can do that, make sure PostGreSQL fires up and you can create new databases.\n(Sorry I can\u2019t dig into the problem any more right now)"}], [{"role": "user", "text": "I\u2019m learning single cell RNA data preprocessing\nWhen I try to extract UMI-tools, I get the message \u201cPlease provide a value for this option\u201d in the Reads in FASTQ section, and it does not open any selectable options for me.\nI have attached the photo of this error to you\n\nScreenshot (30)1366\u00d7768 129 KB\n"}, {"role": "system", "text": "You have a dot character in a uploaded file name, this may be the cause. You can click the pencil (edit) icon and change the datatype to fastq. But try not to put a dot in a filename unless it is before the file extension."}], [{"role": "user", "text": "Trying to convert a 23andme tsv file to vcf using bcftools convert to vcf. Building a lesson for UG class.\nI get this error: Fatal error: Exit code 255 ()\n\u2013tsv2vcf requires the --samples option\npython: : Unknown error 59090864\nCan someone specify proper options to get the bcf conversion to work?"}, {"role": "system", "text": "This pypi package might give you better results.\nIf that fails, there\u2019s also a perl script that I have successfully used."}, {"role": "user", "text": "I knew about the pypi and perl script but\nI was hoping to get my students to execute this within Galaxy if at all possible"}, {"role": "system", "text": "To use the Galaxy wrapped BCFtools convert to vfc tool/function with tsv input, both the reference genome and sample information need to be specified at runtime. This is noted on the tool form \u2013 highlighted in the screenshot attached below.\nSample names can be entered directly on the form or supplied as a \u201clist\u201d dataset file in a tabular format. Include/exclude are both possible. For help with what a sample content represents and the proper formatting, please see: http://samtools.github.io/bcftools/bcftools.html#convert (is linked from the bottom of the tool form).\n\nIf you run into problems even with those entered, double check both of the below are true:\n\n\nThe reference genome fasta has the same exact chromosome identifiers on the \u201c>\u201d title lines as the VCF includes for mapping positions. No extra whitespace or description content + consistently wrapped at 40-80 bases. The tool NormalizeFasta can help to reformat fasta data correctly in most cases and the final formatting is the same as that of a custom genome.\n\n\nThe sample names are exactly the same between the form entry/list and the VCF content. This is a file that you create/upload to Galaxy or by using the Upload tool\u2019s \u201cpaste\u201d function (use the gear icon option to \u201cconvert spaces to tabs\u201d to ensure proper tabular formatting).\n\n\nSome help links:\n\nFormat help for Tabular/BED/Interval Datasets\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\n\nHope this works out!"}], [{"role": "user", "text": "Does anyone know if it\u2019s possible to do a local install of Galaxy on an older iMac? Can only update to OS X 10.13 and have about 150 GB of free space. Can I use an external drive for data? I have lots of space available there\u2026Thanks!!!"}, {"role": "system", "text": "It may be possible, but you\u2019re likely to encounter numerous issues along the way, and it\u2019s questionable whether the end result will be worth the effort.\nThe one real limiting factor in the end would probably be RAM. I assume your system does not have more than 4GB memory? Then you may be able to get Galaxy running, but most likely you won\u2019t have enough free memory left to do any kind of interesting analyses with it."}], [{"role": "user", "text": "Hi,\nWhen trying to use the get method in a Jupyterlab  I see this:\n(base) root@b049c3385e8d:/storage/galaxy/jobs_directory/001/1474/working/jupyter# get -i 2730\n/opt/conda/lib/python3.8/site-packages/bioblend/galaxy/histories/__init__.py:120: FutureWarning: The history_id parameter is deprecated, use the show_history() method to view details of a history for which you know the ID.\n  warnings.warn(\n/import/2730\n\nAnd no file is uploaded.\nWhat would be the places to check to solve this?"}, {"role": "system", "text": "Hi @maikenp\nHelp resources:\n\nAdmin doc: Galaxy Interactive Environments \u2014 Galaxy Project 21.05.1.dev0 documentation\n\nUsage tutorials are here: Search Tutorials\n\n\nMaybe check to make sure your command matches the format in the tutorials? If it does, then you\u2019ll need to go through the configuration steps in the admin doc to make sure nothing was missed.\nIf you need more help, would you please share more details about your local Galaxy configuration? Include what you have done/checked so far. If this worked before, has something changed? Or is this is your first time using it?"}, {"role": "user", "text": "Hi,\nyes, sorry for being rather sparse in the initial post.\nWe have successfully set up our galaxy instance and have been running notebooks in it for a while. We had issues with put() that were solved, but did not try get() until now.\nIt is typically the simple routine in Anne\u2019s tutorial we are trying now: JupyterLab in Galaxy\nOtherwise things work as they should. I.e. the notebooks run as they should, we can put files back into the history. But issuing get() gives the message shown.\nI will have a closer look at the admin docs, and compare with our setup."}, {"role": "user", "text": "Yes, so the problem was related to collection datasets. I realized I had to unhide to get the id of each of the items in the collection, as get does not work on a collection.\nRealized by looking closer at the tutorial  JupyterLab in Galaxy where it does indeed inform about unhiding hidden datasets."}], [{"role": "user", "text": "Why have fastqc and trimmomatic been deprecated and what should I use as a replacement?"}, {"role": "system", "text": "None of the two has been deprecated afaik. Where did you get this information?\nedit: I see what you mean now, the tool panel on usegalaxy.org seems mixed, we will fix it. The tools are not deprecated, just positioned wrongly in the panel."}, {"role": "user", "text": "Thank you, marten."}, {"role": "system", "text": "It has been fixed, thanks for reporting it."}], [{"role": "user", "text": "Hello,\nCan anyone of you help me config my galaxy server. I gave my server 16 virtual cores in HyperV and found some informations that i have to change some params in galaxy.yml file(galaxy/config). Whats the best way to do my galaxy faster. Now i\u2019ve got\nbuffer-size: 16384\nprocesses:3\nthreads:15\noffload-threads:2\nThanks in advance\nMaciej"}, {"role": "system", "text": "Hi @Maciej,\nwhat do you mean by \u201cfaster\u201d? Is the web interface slow? Or are the tools that you execute only using one core? Galaxy on its own is just a web server, that is handling API requests, if you don\u2019t have 100s simultaneous users, the web server should not need 16 cores. However, the tools that Galaxy will execute can use many cores and a lot of memory. This depends obviously on the tool you are running and you can give every single tool its own resources.\nWe have covered all those aspects in our training material here: Galaxy Training!"}, {"role": "user", "text": "Hi @bjoern.gruening,\nThank you for respond.\nI just want to improve performance for my lab members(it will be maximally 30 users, but maybe 10 at the same time). I have a virtual machine on Hyper V(16 cores and 32 GB\u2019s of RAM) and I just want to make calculations faster for each user. So the question is how to boost performance for calculations ? Do I need to add more cores or ram for my VM? Do I need to change some config files for it ?\nThanks in advance\nMaciej"}, {"role": "system", "text": "Yes, in that case, you need to change the job_conf.xml file to give specific resources to your tools. Like BWA should get 6 cores and 16GB of memory for example."}], [{"role": "user", "text": "Hi! I\u2019m a beginner in Galaxy. I have tried several times, in different days, to run a job with ANNOVAR on biomina, but the job was failed. This message appeared: Job cannot be completed due to a cluster error, please retry it later: Unknown error: 271.\nCan anyone help me?\nTnx!"}, {"role": "system", "text": "What server are you using?"}, {"role": "user", "text": "I\u2019m using biominavm-galaxy.biomina.be. I\u2019ve already used it for other vcf without problems"}, {"role": "system", "text": "\n\n\n A_Van:\n\nbiominavm-galaxy.biomina.be\n\n\nThis error can mean that there is an input problem or an actual cluster problem.\nEach public Galaxy server is independently administered. If contact information is not on the home page of the server (is this case it isn\u2019t), that can usually be found here on the server\u2019s page: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\nFor the server http://biominavm-galaxy.biomina.be/galaxy/, see: Biomina - Galaxy Community Hub"}], [{"role": "user", "text": "Dear Galaxy helper,\nI am trying load some data into galaxy from NCBI however, when I run the SRR numbers most of them only come in as 1 file for single end read but not for the paired end. My data is paired. Here are some examples of how my data is unpacked\n\nScreenshot 2022-01-28 at 18.57.45586\u00d7962 56.4 KB\n\nwhen I go into the single end data it looks like this\n\nI am new to galaxy but I assume my FASTQ file should come in like this\n\nScreenshot 2022-01-28 at 19.28.162266\u00d71366 446 KB\n\nOnly one of them shows it like this.\nThank you in advance"}, {"role": "system", "text": "Hi @HG123\nThis accession has two technical reads and one biological read per spot.\n\nThe NCBI tools are sorting the read data in different ways.\n\n\n\nFaster Download and Extract Reads in FASTQ is only extracting the single biological read with the default settings. That single biological read is sorted into a single-end collection.\n\n\nDownload and Extract Reads in FASTA/Q is extracting all three reads into one result dataset. It sounds like this is what you want from your examples.\n\n\n\nAlternative is to capture the URLs from the data source and paste those into the Upload tool: Run Browser : Browse : Sequence Read Archive : NCBI/NLM/NIH. This would be the same results as the Download and Extract Reads in FASTA/Q tool.\n\nOnce you have the reads uploaded, sort them directly and optionally create collections. Examples of how to sort read by identifiers are in this FAQ. Use the regular expression matching options, not the \u201cde-interlacer\u201d tool.\nRelated Q&A: Failing to load single-cell raw fastq files into Galaxy"}, {"role": "user", "text": "Hi Jenna,\nThank you for the reply.\nI am totally new to this bioinformatic and galaxy. When you say two technical read do you mean paired end read? From your answer I understand that the dataset is paired and should therefore be downloaded as FASTA/Q instead of FASTQ if I want to do some downstream analysis?\nHowever, I got total FASTQdump for one data with the FASTQ download\nPicture attached(SRR8368415) thats why I got very confused by the other data.\nBear with me for asking simple questions"}, {"role": "system", "text": "Hi @HG123\nData source at NCBI: Run Browser : Browse : Sequence Read Archive : NCBI/NLM/NIH\n\nThe sequencing protocol is described in the Metadata tab (link outs there provide more details)\nThe read types are broken down in the Reads tab\n\nGalaxy Training Network (GTN) tutorials: https://training.galaxyproject.org/\n\nTutorials can be searched with keywords or browsed in topic categories.\nExample single-cell\n\nExample 10X\n\n\nThere is a training event in March that you might want to consider joining. Single-cell analysis will be covered.\n\nAll events: Galaxy Event Horizon - Galaxy Community Hub\n\nFrom that page find the GTN Smorgasbord2 (Tapas) event: GTN Sm\u00f6rg\u00e5sbord 2: 14-18 March | Gallantries\n\n"}], [{"role": "user", "text": "Hi Galaxians. I have obtained Illumina paired-end reads of microbiome metagenomes from a human sample and would now like to remove human (contaminant) sequences from them. I have tried installing Bowtie2 on Anaconda but didn\u2019t get very far as I am new to Anaconda and Bowtie2. I found some instructions at this link for doing this but sadly, I do not know how to implement it. Is it possible to add this function to Bowtie2 in Galaxy? Or is there a tool in Galaxy that can perform a similar function?\nThank you in advance for your help."}, {"role": "system", "text": "There is Removal of human reads from SARS-CoV-2 sequencing data. It\u2019s demonstarting things with SARS-CoV-2 sequencing reads, but try to work through it and it should be rather obvious how to apply this to your data, I hope."}], [{"role": "user", "text": "Hi all,\nWhen I implement my workflow, it simply will not run, despite giving the following message:\nSuccessfully invoked workflow  RNAseq_test_singleend .\nYou can check the status of queued jobs and view the resulting data by refreshing the History pane, if this has not already happened automatically.\nNothing appears in the History or any other History (Sharing results to new History = NO). Simply nothing happens.\nThis has been an issue before however there was no user end fix:  https://biostar.usegalaxy.org/p/21100/index.html\nHere is my workflow: https://usegalaxy.org/u/t_little/w/rnaseqtestsingleend\nAnd my history: https://usegalaxy.org/u/t_little/h/rings\nHope both of these are accessible and that the issue can be fixed!\nTi."}, {"role": "system", "text": "I have the exact same problem so it seems like a widespread issue. I\u2019ve retried my workflow ~10 times, and haven\u2019t had any change. My workflows went through perfectly until about 2:30 east coast time yesterday.\n-Paige"}, {"role": "user", "text": "Yes my workflows were working perfectly early on yesterday (GMT; 24th Jan) but today they are not since at least about 11am GMT and may have stopped working yesterday evening as well."}, {"role": "system", "text": "Hello,\nThanks for the description of the issue, the timing, and sharing the example data.\nWorkflow execution was transiently disrupted, please try executing your workflows now. Testing the above workflow with the shared data worked on Main for me.\nLet us know if the problem persists.\nThanks for using Galaxy!\nMo"}], [{"role": "user", "text": "Hello,\nI have sequenced on an Illumina platform a virus grown in eggs. I want to assemble the virus genome, but the vast majority of the reads in the fastq file are not of the virus. I guess only less than 1% of the reads belong to the virus.\nWhat tool should I use to filter all of the non-target DNA? I assume I can\u2019t start the assembly before that."}, {"role": "system", "text": "Hi @omer\nPlease see this GTN tutorial for an example. That particular protocol uses Bowtie2 to remove reads that align to the non-target genome but other mapping tools are discussed.\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Unicycler assembly of SARS-CoV-2 genome with preprocessin...\n\n  DNA sequence data has become an indispensable tool for Mo...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello!  Recently, I was trying to create a gxformat2 Galaxy workflow by hand, and import the workflow file to usegalaxy.org via the import via \u201cArchived Workflow URL\u201d method .  When the file was well-formed, everything went fine.  However, if the file had a syntax error, the import would fail with the following error message:\n\nimage813\u00d7175 6.16 KB\n\nAlas, this message doesn\u2019t contain enough information to help me correct the problem(s).\nIs there a way to get more details about the errors in a gxformat2 file at import time?\nThanks in advance for the help,\nSteve"}, {"role": "system", "text": "\n\n\n Steve_Von_Worley:\n\ngxformat2\n\n\nHi @Steve_Von_Worley\nAdding in more error capture information from a failed workflow import is a good idea.  Please make an issue ticket at this repository Issues \u00b7 galaxyproject/galaxy \u00b7 GitHub and include examples if you can.\nFor now, these are the resources available:\n\nGitHub - galaxyproject/gxformat2: Galaxy Workflow Format 2\nand this includes some basic testing Contributing \u2014 gxformat2 0.17.0.dev0 documentation\n\n"}, {"role": "system", "text": "At this point you\u2019d be best off running gxwf-lint (part of gxformat2 pip install gxformat2) on your hand-written files (or planemo workflow_lint, which should run equivalent checks)."}], [{"role": "user", "text": "I have tried to run RNA STAR with Mus_musculus.GRCm39.108.gtf.gz as input for \u201cGene model (gff3,gtf) file for splice junctions\u201d and it gives error.\nInterestingly I can run RNA STAR with gencode.vM10.annotation.gtf.gz as input for \u201cGene model (gff3,gtf) file for splice junctions\u201d successfully. However, tools like Annotate ID gives an error and Gene IDs can not be converted to Gene Symbols.\nPlease can someone help me with this. Thanks."}, {"role": "system", "text": "@ptrivedi\nwhat reference genome do you use for mapping? Gencode.vM10 is for GRCm38 - see GENCODE - Mouse Release M10, while Mus_musculus.GRCm39.108.gtf.gz is for GRCm39 assembly.\nPrecomputed mm39 index is not available on usegalaxy.org or usegalaxy.org.au. Do you use Galaxy Europe?\nSTAR jobs can fail for several reasons, and without looking into the input files and job settings I can only guess. Other common issues are different chromosome names in reference genome and annotation. Genomes in Galaxy use \u201cUCSC style\u201d chromosome names (chr1, chr2 etc), while some annotations use 1, 2, etc. These are different text strings. If you use a custom mm39 genome in the history, STAR jobs might fail because of insufficient memory.\nI assume you mean AnnotateMyID. I tested it on a count table made using an old gencode annotation vM4. The annotation is for mm10/GRCm38. In the count table ENSEMBL genes names contain versions (.1, .2 etc), while example of input file in the tool description does not have versions. I stripped versions in the count table by replacing dots with tabular character, basically, split the 1st column into two. This makes the gene names compatible with AnnotateMyIDs. No such issue if you use non-default settings, such as gene_name for gene identifier, just change input type in AnnotateMyID to Symbol.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI am trying to set up Galaxy proxying (on our 19.01 instance) with Apache.\nAccording to https://docs.galaxyproject.org/en/latest/admin/apache.html, I set up configuration as is :\n\nin httpd.conf :\nProxyPass /galaxydev unix:///tmp/galaxy_dev.sock|uwsgi://\n\nRewriteEngine On\nRewriteRule ^/galaxydev/$ /galaxydev [R,L]\nRewriteRule ^/galaxydev/static/style/(.) /home/galaxydev/galaxy/static/june_2007_style/blue/$1 [L]\nRewriteRule ^/galaxydev/static/(.) /home/galaxydev/galaxy/static/$1 [L]\nRewriteRule ^/galaxydev/favicon.ico /home/galaxydev/galaxy/static/favicon.ico [L]\nRewriteRule ^/galaxydev/robots.txt /home/galaxydev/galaxy/static/robots.txt [L]\n\nin galaxy.yml :\nuwsgi :\nsocket: /tmp/galaxy_dev.sock    ###syntax  unix:///tmp/galaxy_dev.sock is not recognized ???\nmount: /galaxydev=galaxy.webapps.galaxy.buildapp:uwsgi_app()\nmanage-script-name: true\n\ngalaxy :\ncookie_path: /galaxydev\nAccessing \u201cgalaxy_host\u201d/galaxydev with a browser fails. Apache error_log returns this error :\n[Tue Apr 09 11:35:57.850743 2019] [:error] [pid 19307:tid 47544504309504] [client 195.221.174.148:63192] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 11:36:06.537558 2019] [:error] [pid 19211:tid 47544525321984] [client 195.221.174.148:63195] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:13.854039 2019] [:error] [pid 19307:tid 47544514815744] [client 195.221.174.148:63669] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:14.718221 2019] [:error] [pid 19307:tid 47544519018240] [client 195.221.174.148:63670] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:14.953882 2019] [:error] [pid 19307:tid 47544523220736] [client 195.221.174.148:63671] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:46:12.715991 2019] [:error] [pid 19307:tid 47544529524480] [client 195.221.174.148:63704] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 16:52:12.155767 2019] [:error] [pid 19307:tid 47544542131968] [client 195.221.174.167:59696] AH10097: error parsing URL //: Invalid host/port\nI am stuck with the \u201cURL //: Invalid host/port\u201d error message.\nWould you have any hint ?\nRegards."}, {"role": "user", "text": "This seems to be related to an apache mod_proxy_uwsgi bug.\nFor those stuck in same situation, I used this workaround :\n\nconfigure uwsgi proxying on a TCP socket (ie uWSGI on a TCP socket)\ncheck apache2/mod_proxy_uwsgi.c and apply following fix if needed : https://github.com/unbit/uwsgi/commit/6ce856c9fdb98833f9142f84c92b75f546b32dc2#diff-85cd090aa23e09feb1f28c92ae5aded6\n\n"}], [{"role": "user", "text": "Hi,\nI\u2019ve been using Filezilla for FTP transfers to the Main and I tried today and no matter what configuration of ports, account information, etc. I attempted, I could not connect to the server. I know that galaxy refuse SFTP requests so it wasn\u2019t surprising that this didn\u2019t work, but I also tried modifying Filezilla\u2019s network configuration and that didn\u2019t work either. I tried Cyberduck and I tried reinstalling the client and neither worked. I\u2019m on MacOS 11.2.2 and my firewalls are turned off. I just updated to 11.2.2 a couple days ago and am wondering if this might have something to do with this issue. Regardless, I tried everything I could find online to fix the problem and came up empty handed, so I figured I\u2019d bring this up here. Any help is appreciated.\nNick"}, {"role": "user", "text": "Worth noting that I also just tried to FTP in Terminal and the connection was refused there as well."}, {"role": "system", "text": "I\u2019m having the same issue. Any sugestion?"}, {"role": "system", "text": "A Galaxy team member restore the FTP connection after a system scheduled maintenance. Now is working."}], [{"role": "user", "text": "Hello Team,\nI would like to bring to your notice that the command RNA STAR is not running and the status says \u201cThis job is waiting to run\u201d for almost 18 hours by now. How can I fix this?\nThanks"}, {"role": "system", "text": "Hi @Mohammed_Thangameera,\ncurrently, the Galaxy community organizing a special training week. Probably it can explain that, due to unusual servers overload.\nRegards"}, {"role": "user", "text": "Thanks for the reply! It\u2019s been more than 4 days by now and still, the status looks the same. It is just queued. Is there anything that I can do to fix this issue?"}, {"role": "system", "text": "Hi @Mohammed_Thangameera\nFor the jobs at usegalaxy.org:\n\n\nWhen working at any public Galaxy server, you are using a shared resource. That means that some of your jobs will run, then some of other people\u2019s jobs will run, then more of yours, repeat. During the training last week, all servers had extra limits on concurrent jobs plus the usegalaxy.* servers overall were very busy. This week everything is back to normal but still busy, and processing a large number of jobs will still take time.\n\n\nFor the RNA STAR jobs that already failed, that is due to an input problem. The reference GTF used is labeled with Ensembl chromosome names. The built-in rn5 index is sourced from UCSC and uses a different chromosome naming format/label. The mismatch triggers a failure.\n\n\nTry using a GTF from UCSC instead. The Q&A below discusses where to get a human reference annotation in GTF format, but the URL to the rat annotation is similar (replace \u201chg38\u201d with \u201crn5\u201d). The same is true for every genome with an annotation track that is hosted by UCSC and indexed in Galaxy.\n\n\n\nStringtie merge error\n\n\nThe hg38 reference annotation GTF is now best sourced from UCSC. Not from the Table Browser but from the Downloads area here . There are a few Gene tracks to choose from. You can review what each of those represents then upload the selected GTF to Galaxy with the Upload tool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\n\n\nThe solution is\n\nget the correct GTF into Galaxy\nrerun all of your jobs using that GTF instead\n\nThose new jobs will execute as resources become available. The jobs already queued up will also fail, so be sure to purge those to clear them from your queue.\nHope that helps!"}], [{"role": "user", "text": "Our local galaxy production server is facing issue of downloading file larger than 1 GB.\nGalaxy version is 20.01\nwe have tried downloading through wget | curl options and tried to change nginx parameters.\nPlease assist."}, {"role": "system", "text": "Did you also used the  try x-accel-redirect option?\nAlso take a look at a solution I have posted earlier:\n  \n    \n    \n    Local galaxy nginx max file download size 1GB \n  \n  \n    If I download a file, fasta or zip of lets say 3GB the downloaded file is always max 1GB. Where do I need to look to solve this? \nI tried to add proxy_max_temp_file_size 0; to my nginx.conf but that is not working. \nIn my galaxy.log I get the following error: \nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n\nEDIT: \nI am still searching for a solution (and a cause) so here an update. My problem is not solved yet. \nAt the moment I have two galaxy servers running, both a different s\u2026\n  \n\n"}], [{"role": "user", "text": "Hello,\nI uploaded files in Galaxy with SRA accesion list. It created 2 files: Single-end (fastq-dump) and Paired-end (fastq-dump). My workflow accepted Single-end (fastq-dump) however Galaxy does not recognize the fastq-dump for paired-end as a collection. I assumed that is because of  the batch mode file. Now, i am trying to separate the forward and the reverse reads and create two separated collection. I cannot find any function that would help me manipulate the fastq-dump. (Fastq splitter was a disappointment). Any hints where they might hide?"}, {"role": "system", "text": "Hi,\nTry the tool Fastq De-Interlacer. It is covered in the NCBI SRA link below, but the others might help, too.\nFAQs: https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nReformatting fastq data loaded with NCBI SRA\n\nIt is not clear if you are having issues with collections or not. The tutorials below cover those plus ways of doing batch data uploads. https://galaxyproject.org/learn/\n\n\nDataset collections - modern studies usually include many samples. Collection are designed to simplify complex, multi-sample analyses as shown in this tutorial.\nMore from the Galaxy Training Network https://galaxyproject.github.io/training-material/topics/galaxy-data-manipulation/\n\n\nThanks!"}, {"role": "user", "text": "I have trouble with batch file mode. Is a dataset within a dataset (with paired-end fastq files and each of them contain 2 separate files (one forward and one reverse)). Apparently this is too much for Galaxy when I choose dataset collection as an input for my RNA-seq workflow. So I have to separate the forward and the reverse files in two separate collection. However, now I am trying the Fastq interlacer. It might work in Hisat2 as it has an option for paired-end Fastq files that are joined in one file.\nThanks for your answer."}, {"role": "system", "text": "The \u201ctype\u201d of the dataset collection created will matter for the data to show up on an input field, as well as the \u201cdatatype\u201d of the data inside the collection, and the \u201ccontent\u201d of that data inside each dataset (this part doesn\u2019t have a distinct metadata attribute \u2013 could be forward fastq reads, reverse fastq reads, or interlaced forward/reverse fastq reads). Collections are built up from those to create four options.\nMany options \u2026 but maybe the explaination below will help.\nHISAT2\n\nFour different content inputs variations are possible\nThree can be given as one of three different dataset types: individual files, multiple individual files, or a dataset collection that fits the overall entry type.\nOne can be only given as a dataset collection (item 3 below).\nThat translates to 10 different choices, just for the fastq input.\n\nScreenshots, then I\u2019ll explain what each can contain:\nThe four input \u201ccontent\u201d types for this tool (many will only have the first three but expect that to evolve over time as interlaced inputs are added to more tools):\n\ninput-content.png1200\u00d7422 24.3 KB\n\nThe tool form changes dynamically based on the choice made.\nThe three dataset types available for the first two and the last from the list above are in the screenshots below. The declared \u201ccollection\u201d will only accept a paired-end collection (so, you won\u2019t see the options below for it - a collection is already expected).\n\n\n\nWhere:\n\n\nSingle-end: Single end sequences in each original individual dataset. Those can be entered as individual datasets, or selected as multiple datasets, or be put in a \u201clist\u201d type of dataset collection.\n\n\nPaired-end: Paired-end sequences, forward in an original individual dataset and reverse in an original individual dataset. Those can be entered as individual datasets, or selected as multiple datasets, or be put into two distinct \u201clist\u201d type of dataset collections. One list for the forward, one list for the reverse.\n\n\nPaired-end Dataset Collection: Paired-end sequences, forward and reverse added to a dataset collection that contains one pair of F/R reads or a combined \u201clist of pairs\u201d  if you have more than one pair. The collection is created from data that has the forward reads in one or more datasets originally, each with a matching reverse read dataset).\n\n\nPaired-end data from single interleaved dataset: Paired-end sequences, forward and reverse interleaved (interlaced) in an individual dataset (this is what you had/have, before \u201cde-interlacing\u201d).  Those can be entered as individual datasets, or selected as multiple datasets, or be put in a \u201clist\u201d type of dataset collection.\n\n\nHope that helps!"}], [{"role": "user", "text": "@jennaj @gmauro\nCircos job executed error free before. On re-run gives an error\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /usr/local/tools/_conda/envs/mulled-v1-3cf5c7d734898be0f8d76d0fbf3face5f99ca4b3a9b294235f9271dd27115368/bin/\u2026/lib/Circos/Colors.pm line 348,  line 424.\nPlease suggest."}, {"role": "system", "text": "\n\n\n akankshabafna:\n\nCircos job executed error free before. On re-run gives an error\n\n\nHi @akankshabafna\nWould you please try at UseGalaxy.org instead for now? Use the most current version of the tool Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy10).\nI can replicate the problem at the UseGalaxy.eu server across several tool versions, and we are still looking into why.\nMoving this to a new topic for followup"}], [{"role": "user", "text": "I am trying to run goseq on an RNA-seq dataset from A. thaliana. I need to import a GO category file, and I have tried with two different files, but it keeps failing.\nMost recently I downloaded the GO file from TAIR functional annotation data | Zenodo but it is still giving me the following error:\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Rocky Linux 8.6 (Green Obsidian)\nMatrix products: default\nBLAS/LAPACK: /usr/local/tools/_conda/envs/mulled-v1-ac9928cd0c80e707f8c859eeebc088f04f34d841be6e3b5ac8f7a331c\nHere is the link to my history with the files I used, two different GO files and the failed jobs if anyone can help- Galaxy | Europe\nThanks!"}, {"role": "system", "text": "Hi @jbliss\ncheck examples of expected input files at the bottom of goseq job setup page. All files have two columns and no header. Maybe consider adjusting your files to the expected format using tools in Text Manipulation section (remove first lines, cut columns). Maybe convert GZipped file with GO categories into plain text tabular by clicking at Edit Attributes (pencil icon) > Convert tab in the middle window) > Select an appropriate option in pull-down menu.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am trying to run StringTie on 6 files in batch mode, but upon clicking \u201cExecute,\u201d nothing happens. Trying this one file at a time also isn\u2019t working. No pending jobs show up in my history. I have also tried using different browser to no avail.\nNot sure what could be going wrong since there are no jobs to check errors\u2026"}, {"role": "user", "text": "Update: other programs such as HISAT2 are running just fine. My issue seems to be StringTie specific."}, {"role": "user", "text": "Issue resolved. Apparently I had to designate an output transcript prefix name under Advanced Options. I\u2019ve never had to do this before and didn\u2019t think to expand the Advanced Options menu before\u2026\nOh well, hopefully if someone else has the same issue, this will help!"}], [{"role": "user", "text": "Have 24 fq.gz files (2 GB each) that I would like to upload to usegalaxy. eu. Using filezilla, I get the following error:\nStatus: Resolving address of galaxy.uni-freiburg.de\nStatus: Connecting to 132.230.68.85:21\u2026\nError:   Connection timed out after 20 seconds of inactivity\nError:   Could not connect to server\nStatus: Disconnected from server\nAny idea what goes wrong?"}, {"role": "system", "text": "Hi Henk,\nplease, follow the instructions at Galaxy Europe | FTP service instructions"}], [{"role": "user", "text": "Hello, I want to use the Paste tool to place a column next to other columns in my data\nBut the number of rows in my two files are not equal\nAnd my file is so big that I can\u2019t see how exactly the unequal number of rows in two files will harm my data.\nIt is very important to me that the columns are placed together in such a way that each row of the columns of my file is exactly in front of its own row of the column of my second file.\nplease guide me?"}, {"role": "system", "text": "hi @maryam-gh99,\nbased on the description you are after Join tool. Galaxy Europe probably has several versions of Join tool, like Join two datasets, Join two files and some other. Usually these tools allow \u2018fill up\u2019 of a missing data in non-matching rows.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello. I am trying to identified the regions that are differently enriched (H3K4me3) between 2 groups using Diffbind, but keep getting following error. I\u2019d appreciate any help!\nError in wilcox.test.default(toplot[[i]], toplot[[j]], paired = FALSE) :\nnot enough \u2018y\u2019 observations\nCalls: dba.plotBox \u2026 pv.plotBoxplot \u2192 pvalMethod \u2192 wilcox.test.default\nWarning messages:\n1: Unable to perform PCA. Make sure there aren\u2019t fewer sites than there are samples.\n2: In bxp(list(stats = c(4.28039961112653, 4.28039961112653, 5.75150066843621,  :\nsome notches went outside hinges (\u2018box\u2019): maybe set notch=FALSE"}, {"role": "system", "text": "Dear  @shill,\nIt seems to me that (a) diffbind could not find enough differentially enriched binding sites or (b) you do not have enough replicates (datasets). Maybe set the FDR threshold a bit higher, e.g., to 0.1. I could also recommend to use MACS2 instead of diffbind for ChIP-Seq data.\nHave a good day and best wishes,\nFlorian"}, {"role": "user", "text": "Thank you so much for the reply! I tried Diffbind with higher thresholds but ended up with the same error. Which MACS2 tool would you use to compare 2 groups, 4 samples in the each group?"}, {"role": "system", "text": "Yes. However, be careful about the pooling option in macs2 (see this post). MACS2 pooling is just combining the replicates into one entity and not taking the difference of the replicates into account. So it is better to perform a robust peak analysis as described in the post.\nCheers,\nFlorian"}], [{"role": "user", "text": "I have installed galaxy in local machine and downloaded Human Genome Resources from:\nhttps://www.ncbi.nlm.nih.gov/projects/genome/guide/human/index.shtml.\nHow use this data in tools Data menu without uploading it every time to local machine?"}, {"role": "system", "text": "There are several possibilities:\nVia data tables / data managers\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: Reference Genomes in Galaxy\n\nResources related to configuration and maintenance of Gal...\n\n\n  \n  \n    \n    \n  \n  \n\n\nTools need to support it.\nAnother way is to import the data into a library (one can do this also via symlink) which can then be used in multiple histories. This way works for all tools."}], [{"role": "user", "text": "I have FASTA file which begins with carat(>) and not @. Hence, FastQC gives following error:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/046/646/46646055/_job_tmp -Xmx28g -Xms256m\nFailed to process AK1_clean_fa\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: ID line didn\u2019t start with \u2018@\u2019\nat uk.ac.babraham.FastQC.Sequenc\nHow do I fix my FASTA file to begin with @? Thanks!"}, {"role": "system", "text": "Hi @abhay03\nFasta files start with a > on the title line for each sequence. These data do not have quality scores (what most of the FastQC modules profile with statistics). Datatypes - Galaxy Community Hub\nThe tool Fasta Statistics can generate statistics and do some basic QA.\nThe FAQs here might help \u2192 datatypes. That entire page can be browser searched with keywords like datatypes. You can also search this forum.\nThis quote from one of those FAQs describes one way to convert fasta to fastqsanger:\n\nIf your data is FASTA, but you want to use tools that require FASTQ input, then use the tool Combine FASTA and QUAL. This tool will create \u201cplaceholder\u201d quality scores that fit your data. On the output, click on the pencil icon galaxy-pencil to reach the Edit Attributes form. In the center panel, click on the \u201cDatatype\u201d tab, enter the datatype fastqsanger, and save.\n\nWhether any of that is needed depends on the content of the original data, what your analysis goals are, and the tool(s) that are rejecting the fasta as a valid input. FastQC results would not be very meaningful for a fasta file converted to fastq except to generate a few basic statistics that don\u2019t involve quality scores.\nAnd, this recent Q&A is how to check what datatypes a tool is expecting as valid input: fastq unavailable -- Tool does not recognize inputs? How to check why - #2 by jennaj\nIf you want to explain more about your goals and target tools, we can help more "}], [{"role": "user", "text": "Hello everyone,\nI have two sets of sequence data for the same genome (Illumina and PacBio) and I want to combine them via Unicycler (has worked for 5 genomes so far) but somehow it does not work for my current dataset. The error message is:\nFatal error: Exit code 139 ()\n/data/jwd04/main/054/067/54067810/tool_script.sh: line 44: 3800151 Segmentation fault      (core dumped) unicycler -t \u201c${GALAXY_SLOTS:-4}\u201d -o ./ --verbosity 3 -1 \u2018fq1.fastq.gz\u2019 -2 \u2018fq2.fastq.gz\u2019 -l lr.fasta --mode \u2018normal\u2019 --min_fasta_length \u2018100\u2019 --linear_seqs \u20180\u2019 --min_kmer_frac \u20180.2\u2019 --max_kmer_frac \u20180.95\u2019 --kmer_count \u201810\u2019 --depth_filter \u20180.25\u2019 --start_gene_id \u201890.0\u2019 --start_gene_cov \u201895.0\u2019 --min_component_size \u20181000\u2019 --min_dead_end_size \u20181000\u2019 --scores \u20183,-6,-5,-2\u2019 --keep 1\ncan you please help me understand what the problem is? As mentioned above it has worked this way for other genomes from the same batches of sequencing. Thank you in advance!\nBest regards\nManuel"}, {"role": "system", "text": "Hi @Manuel\nsimilar error was discussed on the forum, eg\n  \n    \n    \n    Unicycler - hybrid assembly failure usegalaxy.org support\n  \n  \n    Dear All, \nI\u2019ve recently encountered some issues with Unicycler assembly. I\u2019ve tried to perform hybrid assembly with use of \n\n\ntrimmed Illumina reads R1 (0.17 Gb) + R2 (0.16 Gb); format: fastqsanger.gz \n\n\nnanopore reads (2.3 Gb); format: fasqsanger \n\n\nUnicycler readily deals with individual assembly of either Illumina or Nanopore reads. However, it fails to generate hybrid assembly. Any suggestions? \nmaybe is it about available working memory @ Galaxy server? \nthanks in advance, \nPiotr \nPS here \u2026\n  \n\n\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hey all!\nI have run gubbins on some snippy-core output (clean_full_allignment) but I get a failed job. The standard error is:\n\n/bin/sh: line 1:   477 Illegal instruction     (core dumped) raxmlHPC-PTHREADS-AVX2 -T 6 -safe -m GTRGAMMA -f d -p 1 -s /corral4/main/jobs/046/905/46905264/working/foo.aln.phylip -n foo.iteration_1 > /dev/null 2>&1\nFailed while building the tree.\n\nI\u2019m  confused by the message since I don\u2019t know if the problem is with the data or the tool. I understand that it crashes after the first iteration to perform the tree but everything before went ok.\nI have tried running the same job in galaxy.org and .eu, in the first case I get this message, in the latter I get a job that has been running for the last 3 days.\nAny help would be appreciated "}, {"role": "system", "text": "Hi @Alan\nAllow the job to complete at UseGalaxy.eu \u2192 that specific server has some cluster nodes that can process larger and longer running analysis. If that also fails, send in a bug report to the UseGalaxy.eu team. Maybe they can adjust the resources or suggest a better way to organize your data.\nThe jobs at UseGalaxy.org are too large with the current inputs and parameters. I reviewed the bug reports and the direct message (back through August). It is best to allow Galaxy decide where to send jobs. And, you could try with built-in index for K-12 instead."}], [{"role": "user", "text": "I have been using MultiCovBed occasionally. Lately it simply doesn\u2019t work regardless of the files loaded (ERROR: Unrecognized parameter: -f , although I used the default).\nWhile several weeks ago the versions used were:\n|Galaxy Tool Version:|2.27.0.0|\n|Tool Version:|bedtools v2.27.0|\nNow it states:\n|Galaxy Tool Version:|2.27.1|\n|Tool Version:|bedtools v2.16.1|\nSeems like an updated version of Galaxy Tool (2.27.1 vs 2.27.0) uses a downgraded version of bedtools (2.27.0 vs 2.16.1)\nHow can I overcome this and run the tool without an error just as before?\nThanks!"}, {"role": "system", "text": "Hello, That seems odd. I\u2019ll do some testing on the newer tool wrapper.\nMeanwhile, try switching to using a prior version of the tool that worked for you. Do this by loading up the tool form and clicking on the \u201cVersions\u201d menu at the top. For this tool, the choices available are:\n\nswitch_tool_version.png1276\u00d7558 62.1 KB\n\nMore feedback once review/testing complete.\nThanks for reporting the problem!"}, {"role": "user", "text": "Thanks a lot for the prompt reply.\nYup, switching to the previous version (2.27.0) works perfect!\nThanks for the tip, never used it before (or: never had to use it before\u2026)."}, {"role": "system", "text": "Update: We found the missing tool dependencies and are correcting. If interested, this is the ticket: https://github.com/galaxyproject/usegalaxy-playbook/issues/186\nOnce fixed/tested the ticket will close out and the newer version of all Bedtools tools can be used if you want. Some do have bug fixes.\nThanks again for reporting the problem!"}], [{"role": "user", "text": "Hello, I can not see my history, it is like if had desapear i was log in with my admin-redacted-email-for-privacy account. And now the history look completly empty. Can somebody help me with this?"}, {"role": "system", "text": "Hi,\n@NABB it seems you have deleted your history permanently.\n\ngrafik1260\u00d7445 46.9 KB\n\nYou can see this list if you go to Users --> Histories --> Advanced Search --> Deleted.\nYou can still see this deleted history, but your data is gone and you need to redo this. However, you should be able to see all steps and parameters.\nSorry,\nBjoern"}], [{"role": "user", "text": "Hello everyone. This might be a very basic question, but I am only starting to analyze my transcriptomic data for the first time\nMy sample sequencing was run on 4 lanes, so I have 4 FASTA files per sample. How do I merge them together, to get a complete summary of each sample sequencing?\nThank you for your help"}, {"role": "system", "text": "Hi @Goncalo_Pinheiro\nA single sample run on multiple lanes can be combined with the Concatenate tool. This stacks the reads top to bottom.\nFor QA or summary information like quality metrics, the protocol varies by the analysis domain.\nTutorials: https://training.galaxyproject.org/\nStart here: NGS data logistics"}], [{"role": "user", "text": "Hello,\nI am using Transit Gumbel tool to determine gene essentiality from my transposon libraries using the tutorial titled \u201cEssential genes detection with Transposon Insertion sequencing\u201d.\nHowever, the Gene ID (\u201corf\u201d column) that was generated does not exactly match the gene ID of my organism. The gene ID generated was listed as multiples of 5 (D7S_RS00010, D7S_RS00015, D7S_RS00020, etc).\n\nAny ideas on what have gone wrong?\nI suspected that maybe I uploaded the wrong \u201ctype\u201d of file from NCBI, instead of the gene annotation file. How do you upload gene annotation file (in the gff3 format) from NCBI to your history?\n\nThank you all!\nNatalia"}, {"role": "system", "text": "\n\n\n n-tjoks:\n\nHow do you upload gene annotation file (in the gff3 format) from NCBI to your history?\n\n\nCapture the link from the genome page at NCBI and paste that URL into the Upload tool in Galaxy. Use all default settings.\nHere is an example of where to find the file for a human build:\n\nncbi-genome-gff3-reference-annotation1258\u00d7726 87.2 KB\n"}], [{"role": "user", "text": "\nScreen Shot826\u00d7309 15 KB\n\nHello!\nThe tool \u201cspot_detection_2d\u201d seems missing on usegalaxy.eu. I could not find it in the tool list. A workflow that includes this tool cannot be executed either.\nCould someone help me to fix this problem? Thanks a lot!\nGreetings."}, {"role": "system", "text": "Hi @qgao,\nwe are working on it. I\u2019ll inform you as soon as it is fixed.\nRegards"}, {"role": "system", "text": "From where do you have this workflow? The tool ID has changed to ip_spot_detection_2d some time ago. Maybe that is an old workflow or important from an older Galaxy instance? Technically its not the same tool, as the ID is different.\nHere is the tool on EU: Galaxy | Europe\nOr do I mix something up and a different tool is missing?"}], [{"role": "user", "text": "I am using a private Galaxy server hosted by our organization which can then be used by other people in the organization. I was following this tutorial - Reference-based RNA-Seq data analysis to run RNA-seq analysis. However, I faced this error when using the Join two Datasets side by side on a specified field (Galaxy Version 2.1.3) tool. It throws this error -\nTraceback (most recent call last):\nFile \u201c/data/tools/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nI tried several things, and referred to this article - Specify column to group by -- Join two datasets -- Fixed - #6 by jennaj, but it didn\u2019t help me. I tried installing an earlier version of this tool but it gave the same error. Any help or suggestions are highly appreciated.\nThank you!"}, {"role": "system", "text": "\n\n\n priyanka8590:\n\nModuleNotFoundError: No module named \u2018galaxy\u2019\n\n\nHi @priyanka8590\nThis is a configuration problem that the administrator of the service needs to fix.  In short, the place where the job was executed did not have access to the database where the server is running.\nIf one job fails with this reason, it might have been a small hiccup and a rerun might fix it. If many jobs are failing with this reason, it is a larger problem.\nThis was due to a dependency problem in most prior Q&A at this forum. Example: Specify column to group by -- Join two datasets -- Fixed\nShould your admins need help, the Galax admin chat is here: galaxyproject/admins - Gitter."}], [{"role": "user", "text": "Hello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one :\n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\nSome help ?\nThanks a lot"}, {"role": "system", "text": "Same error using RNA STAR.\n\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\n"}, {"role": "system", "text": "Hi @Vincent and @Satomi_Kohno\nIssues should be resolved again. Thanks for reporting the problem!\nRelated Q&A Job error \"unknown userid\" at UseGalaxy.org June 15-16, 2022 -- Resolved, try a rerun - #4 by jennaj"}], [{"role": "user", "text": "Hello, I encountered an error while running SPAdes. Here is the standard output/error of the tool\ncode 17: slurm_submit_batch_job error: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits)\nGalaxy tool ID as below toolshed.g2.bx.psu.edu/repos/nml/spades/spades/3.15.3+galaxy2\nI already try re-running the job once again and still got the same result. Any solutions for this?\nMany thanks!"}, {"role": "system", "text": "Hi @Amirah\nFirst, try one more rerun now. This is a server-side cluster error and those are usually transient but tend to clump together temporally.\nIf a rerun persistently fails, we will need more information to help.\n\nThe server are you working at will matter. Please post the URL\nYou could also post back a share link to the working history. If you don\u2019t want to post that publically, please state so and a moderator will start up a direct private message.\n"}], [{"role": "user", "text": "My project involves genotyping individuals at targeted sites in the human SNP databases.  The original DNA has been enriched in the targeted SNPs  following capture hybridization.  I have bam files of the aligned sequences resulting from the capture hybridization and a list of approximately 250 targeted SNP  sites.  I would like the output file(s) to list the two alleles from each site, the allele frequency and the coverage.  I am hoping for suggestions as to how to retrieve this information from the bam files using Galaxy?"}, {"role": "system", "text": "Hi @Elaine,\nBAM files contain reads aligned to reference genome. You need to call variant first, for example Free Bayes is a good caller. You may want to work on alignments before calling variants, for example, re-align indels to left. VCF format contain information about variants including depth of coverage. Once you get list of variants, IDs from dbSNP can be added to the identified variants using tools like SnpSift Annotate SNPs from dbSnp. Have a look at GTN tutorials in\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Variant Analysis\n\n  Genetic differences (variants) between healthy and diseas...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Calling variants in diploid systems\n\n  Genetic differences (variants) between healthy and diseas...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKind regards,\nIgor"}], [{"role": "user", "text": "Reference data: no option available\nContact the administrator of this Galaxy instance if you miss reference data (\u2013config-dir)\n\nScreenshot_20230517_182122_Chrome1080\u00d72400 245 KB\n"}, {"role": "system", "text": "Hi @Fabiano_Menegidio\nFor immediate use, try using the custom genome feature.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: How to use Custom Reference Genomes?\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you need to assign a database metadata to datasets, your custom genome can be promoted to a custom build.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Adding a custom database/build (dbkey)\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\nFor use later on, please post back details about the genome and we can consider adding it to the native reference indexes. We\u2019ll need enough information to locate the data, and the genome should be from a public source. Not all genomes can be added but it doesn\u2019t hurt to ask "}], [{"role": "user", "text": "Hi all,\nI wrote a custom tool for image segmentation on our compute cluster\u2019s local instance of galaxy. I want the users to be able to see an image from the galaxy gui. As it is, the segmented images are saved and users need to navigate to the directory. I\u2019m wondering if it is possible to view an image through galaxy? If so, please point me in the right direction.\nThanks!\nJosh"}, {"role": "system", "text": "Can you explain what you mean with:\n\nAs it is, the segmented images are saved and users need to navigate to the directory.\n\nDo they download the image from the galaxy interface and they need to navigate to the downloads folder for example? Or is the tool finished and the image is written to a folder somewhere on the server and they need to navigate to that folder on the server?"}, {"role": "user", "text": "Thanks for your reply. We take images of live cells with microscopes, store the images to the server, and use the galaxy interface to run analysis on the images in the server. The analyzed image is then written to another directory on the server. To see the results of the tool, they need to navigate to the folder on the server."}, {"role": "system", "text": "Yes it is possible but you probably need to change the underlying script and the galaxy wrapper itself. Guessing that you want to keep the functionality of writing the image to a designated folder on the server I can point you toward one option (there may be more). You can change the script in a way that it also writes the images to a folder in the work directory, this directory is the folder where galaxy is executing the tool. In your script it is a relative path like output/images/image.jpg. In the tool wrapper you will have to add something like:\n<collection name=\"images\" type=\"list\" label=\"images\">\n      <discover_datasets pattern=\"__name_and_ext__\" directory=\"output/images\" format=\"jpg\"/>\n</collection>\n\nFor more information you can check the following links:\nhttps://planemo.readthedocs.io/en/latest/index.html\nhttps://docs.galaxyproject.org/en/latest/dev/schema.html\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Development in Galaxy\n\n  Galaxy is an open-source project. Everyone can contribute...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nHope this helps to point you in the right direction."}], [{"role": "user", "text": "Could you help me to convert GFF3 to GTF to use it in featureCounts?  I can not figure out how to use gffread.  Thanks."}, {"role": "system", "text": "Hi @Satomi_Kohno\nIf the GTF version is available, use that instead of trying to covert it over. There isn\u2019t only one way to do the conversion, and it doesn\u2019t work well on all GFF3 data from my experience.\nTwo things to make sure to do are:\n\nInput a GFF3 that does not contain sequences at the end\nSet the option \u201cFeature File Output\u201d \u2192 \u201cGTF\u201d\n\nIf that isn\u2019t enough help, please put a copy of the GFF3 into a new history, then generate a share a link to that history and post it back. You can include your attempts in the same history as a reference."}, {"role": "user", "text": "Thanks! It works great.\nIt did not work at the beginning, I just found out about the issue. It did not work with a compressed file, which NCBI provides. Unzip it, and then everything works fine. Thanks.\nSatomi"}], [{"role": "user", "text": "Hi all,\nI use local my Galaxy and installed plink tool. When I upload test-data, plink.bed, plink.bim and plink.fam,  plink.bed doesn\u2019t show in plink tool  pbed input.  The datatype is binary and  I cannot change datatype to pbed because the option pbed does not appear in datatype pull-down list. I can see pbed in datatype list in admin menu. How can I use it?\nThanks,\nYukie"}, {"role": "system", "text": "Hi @yukieymd\nI\u2019m not sure if the datatype (or tool) is actually useable at public Galaxy servers, including UseGalaxy.eu. The other two UseGalaxy.* servers do not even host it. Older Q&A: LPED and PBED formats (PLINK). It might work on a local Galaxy\u2026\nLet\u2019s ask the EU server admins at their chat (same people that maintain the wrapper). They may reply here or there and feel free to join \u2192 You're invited to talk on Matrix"}, {"role": "system", "text": "Hi @yukieymd\npbed is a composite datatype (i.e. a datasets consists of multiple files). You need to use the Composite tab in the upload dialog."}], [{"role": "user", "text": "What is the difference between the two?\nhow can i convert my je-demultiplexed files that are fastqsanger to fastq?"}, {"role": "system", "text": "Hi @biostaring\nfastqsanger is datatype for FASTQ files with Phred33 quality score encoding used in these days practically by all sequencing technologies. fastq datatype in Galaxy is used for FASTQ data with either unknown quality encoding or for old and obsolete illumina Phred64 encoding. Very few tools in Galaxy can handle fastq datatype. If the data was generated recently, a proper datatype is fastqsanger or fastqsanger.gz. Galaxy support gz compressed data.\nHope this answers the question.\nKind regards,\nIgor"}], [{"role": "user", "text": "I have been trying to retrieve a data set from EBI SRA for a couple of days now and keep having issues. I go to the \u201cGet Data\u201d tab in galaxy and select EBI SRA. I\u2019m then taken to the site and can search for the data set. But when I click on the file I want under the FASTQ Galaxy column I get a black screen and the following error:\nThis page isn\u2019t working\nusegalaxy.org  is currently unable to handle this request.\nHTTP ERROR 500\nI\u2019ve tried refreshing the page but then I get a different error that looks more like an error from galaxy itself (see screenshot). Is the website currently experience maintenance? I am the only with this problem and maybe it\u2019s an issue with my internet? Any insight on how to fix this problem would be much appreciated!\n\nScreen Shot 2020-10-05 at 11.23.26 AM1316\u00d790 5.99 KB\n"}, {"role": "system", "text": "Hi @nichole-lewis-1,\nthanks for reporting this problem. We\u2019re working on a fix and will let you know when this works again."}, {"role": "system", "text": "Thanks again, this should be working again."}], [{"role": "user", "text": "We are trying to upload BAM(hg19) and fastq.gz files with FTP, but the upload stops after about 70G of the files have been uploaded. Our file sizes are about 95-140G in size.\nHow can we get these files uploaded to useGalaxy?\nTthanks"}, {"role": "system", "text": "Hello \u2013 The maximum dataset size for Upload is usually 50 GB but needs to be a bit smaller for BAM datasets (around 25-35 GB). Datasets produced by tools can be larger.\nThis is true when working at the public Galaxy servers like https://usegalaxy.org. You would run into quota problems (max 250 GB total per account) and likely tool problems (exceed resources) working with such large data.\nIf you are running your own Galaxy, larger data can be loaded by administrators into data libraries using other methods instead of FTP, please see: https://docs.galaxyproject.org/en/master/admin/useful_scripts.html?highlight=data%20libraries\nGalaxy choices:\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n"}], [{"role": "user", "text": "Hi @jennaj - I\u2019ve been reading through these threads as I have had the same problem - thank you for your wisdom so far! Please could I just confirm what you mean by \u201cAvoid the gff3 version\u201d?\nBecause when I click on GENCODE\u2019s human GTF files I see they are all gff3 files. I am confused which one I need to choose (I have included a screengrab below):\n\nFor context I am running RNA STAR on Galaxy for Illumina paired-end reads of human pancreatic cancer cells that I want to align to the human hg38 genome\n\n\nimage1537\u00d7494 49.8 KB\n"}, {"role": "system", "text": "Hi @mariannebest\nThe column with the \u201cDownload\u201d links include both formats. Copy the link for the GTF, paste that into the Upload tool, and leave all \u201cauto\u201d settings at default.\nThe datatype will be detected as \u201cGFF\u201d.\nTwo additional steps are need to format this data correctly:\n\nRemove the # header lines,  #working-with-gff-gft-gtf2-gff3-reference-annotation\n\nThen redetect the datatype,  #detecting-the-datatype-file-format\n\n\nThis should result in the datatype set as \u201cGFT\u201d and the tools you are using will understand how to use this annotation with any (most?) other data that use or incorporate data also associated with the hg38 genome assembly.\nHope that helps!"}], [{"role": "user", "text": "I create several test accounts on the Galaxy server thinking I could easily remove them later. But now I\u2019m not finding a way to delete these accounts from the Galaxy UI.  Tried searching this list and docs but not finding anything.\nDo I have to do this through the PostgreSQL database and remove from the galaxy_user table?"}, {"role": "system", "text": "Hi @cjkeist\nOne way to do this is to log into each account in the UI and delete them under User > Preferences > Delete account.  The accounts will still be in the database but won\u2019t be accessible in the UI for regular non-admin users.\nIf you enabled GDPR compliance, the registered email addresses/usernames will be changed (once deleted) to anonymous placeholder keys. The passwords are already encrypted (for any account state). Database logs can also be adjusted.\nMore details here: GDPR Compliance \u2014 Galaxy Project 20.09 documentation\nHope that helps!"}], [{"role": "user", "text": "Dear all,\nI have a problem with Join two datasets:\nFirst I can cut successfully one column and six columns of interest. If I want to join them again to a new table file, I only get the header and the values from the first but from the second \u201csix columns\u201d only the header and no values.\nCould you please help me.\nThanks in advance"}, {"role": "system", "text": "Welcome, @Bernhard !\nMaybe your inputs have no common fields to join the files, which is how the Join two Datasets tool works:\nThis tool joins lines of two datasets on a common field\nCan you confirm this?"}], [{"role": "user", "text": "Hello,\nI was hoping to get some advice - I am trying to create a custom genome with combine human and rat builds to use as a reference genome for alignment with bulk RNA sequencing data. I am trying to use the UCSC database but I am not sure which settings (group, track and output format mostly) that would be required for this application. Any advice or point toward a tutorial that might help would be extremely helpful!\nThanks in advance,\nKiera"}, {"role": "system", "text": "Hi @kdwyer\nHuman or rat individually will both be too large to use as a custom reference genome. Combined will also be too large.\nYou could create your own indexed reference with this data if you are running your own Galaxy server. UCSC would be a good source. Find the fasta\u2019s in their Downloads area, as this data is too large to use the Table Browser query function.\nGeneral instructions for creating reference indexes in Galaxy are here:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Galaxy Server administration\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi there, I\u2019d like to calculate TPM from my HISAT2 output files (.bam).  I\u2019ve used Stringtie and provided a reference gtf file and have successfully gotten the TPM calculations.  However, I realized that because Stringtie also predicts new transcripts, I\u2019ve gotten TPM values for the predicted transcripts as well.\nIs there another tool I could use that doesn\u2019t include the predicted transcripts, or could I scale up the TPM values of the known transcripts after removing the TPMs of the predicted transcripts?  I\u2019m aware of Salmon/Sailfish/Kallisto but would prefer to use the HISAT2 files which were aligned to a genome rather than transcriptome for consistency.\nThanks in advance!"}, {"role": "system", "text": "Hi @julsies, Stringtie > Advanced option > Output gene abundance estimation file? The default setting is No.  Hope this helps.\nIgor"}, {"role": "system", "text": "Hi @julsies Stringtie also has Use Reference transcripts only? option, and the default setting is No.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hay,\nI am trying to extact genomic DNA out of a FASTA file and a flanked VCF file.\nI have a VCF file and a FASTA file and I want to exctact the SNP\u2019s with 200 bp flanks.\nFirst I uploaded my Fasta (this one is TAB delimited) and my VCF file. I transformed my VCF file to a pgSnp file and then get flanks with \u201cGet flanks returns flanking region/s for every gene\u201d. Then I used Extract Genomic DNA for getting te sequences. But the output I get is totally empty and says \u201cUnable to fetch the sequence\u201d.\nI hope someone is able to help me.\nKind regards,\nIlse"}, {"role": "system", "text": "Hi @Ilse,\nExtract Genomic DNA expects two inputs. If the formatting or content is off for either, you will not get valid results.\n\n\nQuery: Genomic coordinates in bed or gtf format.\n\nThe tool states interval format as a valid input, which is a less strict format datatype version than bed. Even so, the data must have bed column format/ordering for at least the first 3-6 columns (six columns if including strand).\nThe tool also states gff format as a valid input, which is a less strict format datatype version than gtf. This means that the 9th column (attributes) does need to include the stricter minimum values (gene_id and transcript_id). Do not use a gff3 input or expect errors.\nUsing VCF to pgSnp is fine. The first four columns of data are in interval format, and if you use Cut to restrict to the first four columns the data will then be in bed format. But using Get flanks will also restrict the output to be in interval format with bed column ordering.\n\n\n\nTarget: A locally-cached index on the server or a Custom genome in fasta format.\n\nTo use a \u201clocally-cached\u201d genome, that genome must be assigned as the \u201cdatabase\u201d metadata to the query input.\nTo use a use Custom genome, you might need to run NormalizeFasta on your fasta to remove the description line content (data on the \u201c>\u201d title line after the first whitespace) and wrap the bases to a consistent length (80 is good). The tool will only be able to interpret data that is actually in fasta format, not tabular. Transform with Tabular-to-Fasta, if you need to, first.\n\n\n\nAlso, check your data for chromosome/identifier naming mismatches. Between the two inputs, the \u201cchromosome\u201d names must be formatted exactly the same and the overall content based on the same reference genome/transcriptome version/build. This means that the identifiers in the first column of your interval/bed dataset must exactly match what is on the \u201c>\u201d title line of the fasta dataset.\nThe FAQs below have more details:\nFAQs: https://galaxyproject.org/support/\n\nFormat help for Tabular/BED/Interval Datasets\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\n\nHope that helps, but if not, share some more details. You could copy/paste the first few lines of both inputs and/or post back screenshots in a reply. Make sure to expand the datasets to show the currently assigned datatypes or state exactly what those are for each."}], [{"role": "user", "text": "I\u2019m trying to recover a fasta file from a blast search run in galaxy but blastdbcmd doesn\u2019t recognize the database.\nI made the blast database with makeblastdb in galaxy and I have tried it with and without parse_seq_ids - it is not recognized as a database either way."}, {"role": "system", "text": "Hi @ajnaram\nThis is an older tool wrapper that was not designed to work with BLAST+ results associated with mapping against a custom blast database. Only natively indexed genomes can be parsed, and which are available vary by Galaxy server. You could set up your own local Galaxy, index the genome, and use the tool but I am guessing that is not your goal.\nParsing this data from custom/created databases without having it specifically indexed on the server certainly seems like useful functionality. There is also likely a workaround to achieve similar results without using this exact tool.\nMore feedback soon \u2013 I\u2019m going to review all options (not those already available at Galaxy Main https://usegalaxy.org), consult with our developers, then will write back with the best way forward. Thanks for your patience!\nUpdate: the tool now works on results from makeblastdb as run in Galaxy and is quite forgiving about which \u201cindex\u201d identifier is used."}], [{"role": "user", "text": "Hello,\nI just created a galaxy account on galaxy.org, but i\u2019m in Europe and i would like to shut this one done and open one on galaxy.eu, as it is said that one can\u2019t have 2 accounts.  When using the current one, i couldn\u2019t  find the zebrafish genome on the built-in genome list for example but my colleagues on the eu version have it. How can i do that or is it considered different services and therefore ok to create a new one on the .eu ?"}, {"role": "system", "text": "Carole,\nYes, usegalaxy.org and usegalaxy.eu are considered different services, so you can create one account on each if you so wish. If you want, however, you can also have your usegalaxy.org account deleted by contacting us."}, {"role": "user", "text": "Great, thank you. How can i contact you regarding the first account\u2019s suppresion?"}, {"role": "system", "text": "Hi @Carole_A\nEvery user can now delete accounts themselves under User > Preferences.\nFor other account administrative help when working at usegalaxy.org, you can also send an email to galaxy-bugs@lists.galaxyproject.org. The email should be sent from the same email address as the account is registered under.\nIf working at another public Galaxy server, the administrative contact information is usually on the server\u2019s home page and/or at their directory listing here https://galaxyproject.org/use/. If you cannot find it, ask and we can try to help here. Please post the server\u2019s base URL for context.\nGeneral usage questions should be asked at this forum. We will usually need the public Galaxy\u2019s URL or a description of the private server (local, docker, cloud) you are working at for context eg: where sourced, version of Galaxy. The full name/version of the tool (if a tool question) also helps and will aid in community assistance.\nThanks!"}], [{"role": "user", "text": "I just wondered why featureCounts is listed under RNAseq analysis, I thought it has been superseded. Maybe I\u2019m wrong. Can someone explain? Thanks"}, {"role": "system", "text": "Hi @mwoods\nTools in the left tool panel are organized into \u201csections\u201d related to analysis domains, functionality, common operation on specific datatypes, and/or by \u201ctool suites\u201d. That is just a default organization \u2013 and there are ways to customize.\nThe tool featureCounts Measure gene expression in RNA-Seq experiments from SAM or BAM files  is nested under the section \u201cRNA-seq\u201d since that is the primary analysis domain the tool is used for.\nSome tips:\n\n\nOverview in the release notes from last September related to the latest \u201ctool panel views\u201d updates: September 2021 Galaxy Release (v 21.09) \u2014 Galaxy Project 22.01.1.dev0 documentation\n\n\nTry using the tool search at the top of the panel to find tools. Here you can use keywords to limit the results, and toggle the display to include the section labels or not.\n\n\nYou can also organize the list of tool by EDAM Ontologies, or your own list of favorites. Once you load up a tool, you can click on the star in the upper right corner of the tool form to \u201cadd to your own personal favorites\u201d then restrict the tool panel view just your own favorites.\n\n\nTry hovering over the \u201cstar\u201d and \u201clist\u201d icons to see how each is labeled, then try those functions out to learn how they work and to discover if any are helpful for you.  \nadd-tool-to-favorites1616\u00d7258 44.6 KB\n\n\n\n\nIf this doesn\u2019t address your question, would you please you explain with a bit more detail? I\u2019m not sure what \u201cI thought it has been superseded\u201d refers to.\nThanks!"}], [{"role": "user", "text": "I am trying out deseq2 and edgeR for differential analysis on usegalaxy.org, however, on both occasions, I only get plots (which look very ok) but no actual result files or tables of DEGs as output (shows \u2018this list is empty\u2019 instead). Please help, what could be wrong?"}, {"role": "system", "text": "Hi - Would you please send an email to galaxy-bugs@lists.galaxyproject.org and include the following:\n\nA shared link to the history that contains the unexpected results. How to generate a history share link: Sharing and Publishing your work. Make sure to check the box to share all objects. Unshare and re-share if you have already generated the link and are not sure if objects were shared.\nNumbers of the datasets with the empty results.\nA link to this post, so we can link the two.\n\nThe galaxy-bugs mailing list is internal to the admins of the Galaxy Main https://usegalaxy.org server and your data will remain private to our team for troubleshooting purposes only. This might be a cluster/server problem, or possibly an input/setting problem."}, {"role": "user", "text": "Thank you for your mail Jennifer. Everything seems to work fine now, surprisingly the tools worked as expected after I repeated the analysis today. I think the problem has been fixed already.\nRegards,"}], [{"role": "user", "text": "Hi all,\nWhen plotting my LEfSe results, I am getting some blanks where there should be feature names.\nI have checked my .txt file, and there are no blanks that I can see that would be contributing to this. Has anyone ever encountered a similar problem?\nThanks in advance!\nScreen Shot 2020-06-23 at 1.12.48 PM638\u00d7759 68.1 KB"}, {"role": "system", "text": "Hello @eoleary\nMy guess is that you are having similar problems as discussed in this prior Q&A: Pictures of lefse on line has incomplete display"}, {"role": "user", "text": "Hi @jennaj - You were correct, thank you! Adjusting the text settings so that the class text size was smaller (7 pt) worked.\nBest,\nElizabeth"}], [{"role": "user", "text": "Hello,\nWhen I attempt to trim reads via galaxy vs. command line, I get different results with a near 2000 BP difference.\nThese are the commands I used:\ncommand line:\njava -jar /software/trimmomatic/0.39/trimmomatic-0.39.jar PE ERR048396_1.fastq ERR048396_2.fastq trial2.trimmed.paired.R1.fastq trial2.trimmed.unpaired.R1.fastq trial2.trimmed.paired.R2.fastq trial2.trimmed.unpaired.R2.fastq ILLUMINACLIP:TruSeq3-PE-2.fa:2:40:15:8:True LEADING:15 TRAILING:15 SLIDINGWINDOW:4:20 MINLEN:35\n\nThis is what I set my galaxy options to, which should be the same as what I have set my command line to do.\n\nGalaxy1| 690x4801642\u00d71144 90.9 KB\n\n\nGalaxy21640\u00d71010 147 KB\n\n\nGalaxy31670\u00d71004 150 KB\n\nI did download these sequences from galaxy and moved them to a remote server, prior to doing that, however, I made sure to fastqc both sets of data to ensure I have the same data in each.\nAny idea where/if I\u2019ve done something wrong? Any help would be greatly appreciated"}, {"role": "system", "text": "Hi @drip2hardpanu\ndo you use the same version of Trimmomatic?\nI believe, Trimmomatic applies rules in the specified order, at least in Galaxy. It might be the case on command line, but I have not tested it. You have different order of trimming rules on command line and Galaxy. Maybe swap \u2018rules\u2019 on command line and check if you get different output.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nThe log2 fold change is calculated as log2 (treat)/(control). In the factor level section, is \u20181: Factor level\u2019 a denominator and \u20182: Factor level\u2019 is a numerator?\nGalaxy\u2019s DESeq2 both states \u201cSpecify a factor level, typical values \u200b\u200bcould be \u2018tumor\u2019,\u2018normal\u2019,\u2018treated\u2019 or \u2018control\u2019\u201d, which is a bit confusing.\nIf the order is changed, the opposite result will come out, so I think everyone should pay attention to this part."}, {"role": "system", "text": "Hi,\nin Galaxy, factor level 1 is considered the treated condition (numerator), and factor level 2 is considered the control condition. Indeed it can be a bit confusing; thanks for your feedback, it will be reported in order to clarify it."}], [{"role": "user", "text": "Hi all,\nI have a question regarding the \u201cCLIP-Seq data analysis from pre-processing to motif detection\u201d training. I understand the meaning behind switching the R2 mates with the R1 mates in \u201cumi-tools extract\u201d and \u201cSTAR\u201d steps. However, I do not get why in Van Nostrand et al. 2016 they do not switch R2 with R1 as well. Even in the processing pipeline associated with the libraries I am interested in (https://www.encodeproject.org/documents/739ca190-8d43-4a68-90ce-1a0ddfffc6fd/@@download/attachment/eCLIP_analysisSOP_v2.2.pdf) they do not switch R1 and R2 as well,m although is very similar to the training pipeline. I hope I have been clear enough.\nThanks in advance, also I want to congratulate for the amazing job you are doing!"}, {"role": "system", "text": "Dear @Alessandro_Formaggio,\nIt is not wrong, how Van Nostrand et al. 2016 or the Galaxy training material, analyzed their data. Two points:\n(a) They filter out the second read and do a peak calling with a single read peak caller Clipper. This is the step\nsamtools view: Takes output from sortSam. Only outputs the second read in each pair for use with single stranded peak caller. This is the final bam file to perform analysis on.\nin their analysis protocol. In the Galaxy training material we use a paired-end peak caller, which takes both reads into consideration.\n(b) If you kept R2 as the reverse read in STAR then your peaks are reverse oriented, if you investigated them in a Genome browser. This is not incorrect, but for a better demonstration and to reduce confusion with the peak orientation it is better to switch the read R1 and R2 in STAR.\nRemark:\n\n\nThe training material is a bit outdated and a few steps can be done nowadays differently. for example, Cutadapt supports adapter trimming for both Reads R1 and R2 and a double call is not necessary anymore. However, the double call still takes a bit care of double ligation events, that was still a problem for eCLIP at that time.\n\n\nThe people around Van Nostrand et al. 2016 updated frequently their analysis protocol and thus some steps changed over time in the description of the paper.\n\n\nI hope I could help.\nKind regards,\nFlorian"}, {"role": "user", "text": "Dear @Flow,\nThank you for you answer, I have another doubt regarding CLIP-seq analyses. I am dealing with a single end library,the author report that the Rand103Tr3 sequence (AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC) is at the 3\u2019 end of the reads and a 6 nc UMI, should I assume that the UMI is at the 3\u2019 end in the reads and not at the 5\u2019 end? It would be a little bit strange but as far as I know the UMI is located right before the Rand103Tr3 sequence. I am referring to this paper in particular (A novel class of microRNA-recognition elements that function only within open reading frames | Nature Structural & Molecular Biology). I hope my question makes sense\nThank you in advance"}, {"role": "system", "text": "Dear @Alessandro_Formaggio,\nI have not read the paper. Assuming is never a good idea in data analysis because it leads to errors. For me, it is a bit unlikely the UMI is at the 3\u2019 end in single end fashion because the sequencing error is the highest at the 3\u2019 end.\nMaybe contact the authors and clarify the read library design. I would probably also try to locate the adapter sequence and thus get an idea, where the UMI might be.\nCheers,\nFlorian"}], [{"role": "user", "text": "Hi\nI\u2019m trying to install a tool from the Toolshed but I got this error:\n\nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser.\n\nThe cookies are OK (tested on Firefox and Chrome).\nI\u2019ve found this similar topic:\nhttps://biostar.usegalaxy.org/p/8548/index.html\nSo I checked the admin_users parameter in galaxy.yml file. It was empty.\n-> First of all it\u2019s weird that I could connect as admin mode (with admin@galaxy.org login) if this field is empty.\n-> Then I added my admin login and restarted Galaxy, without success.\nI tested with several tools (mirkwood from Test Tool Shed, bamtools from Main Tool Shed), always with the last revision.\nI\u2019m using Galaxy with docker, tested on bgruening/galaxy-stable:18.05 and bgruening/galaxy-stable:19.05.\nFinally, I tested with Install new tools (Beta) mode from Tools management. This time it worked. It\u2019s a bit weird that the only way to install a tool is the beta mode.\nSo I don\u2019t really know if it\u2019s worth bothering to try to solve this problem or if anyway the old, classical way to install tools is going to be replaced by the currently beta mode\u2026\nThank you for reading me."}, {"role": "system", "text": "Hello @i-guigon\nDependency resolution with conda is now the default and recommended method for tool installation. Start here: https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html && https://docs.galaxyproject.org/en/latest/admin/conda_faq.html\nThe versions of Galaxy noted are over a year old \u2013 use the latest versions of Galaxy releases for the best functionality. You\u2019ll find that the dependency resolution functions are no longer in beta and can be customized. Using default settings is generally best unless you have specific reasons to install differently. The links above cover the options.\nIf you need help with a Docker version of Galaxy, this Gitter chat is a direct way to communicate with the image maintainers: https://gitter.im/bgruening/docker-galaxy-stable\nThe Galaxy Test ToolShed https://testtoolshed.g2.bx.psu.edu/ is a testing sandbox \u2013 meaning, tools are not in a stable/release form. Obtain tools from the Galaxy Main ToolShed https://toolshed.g2.bx.psu.edu/ instead.\nIf a tool is only available in the Test ToolShed, and not in the Main ToolShed, it may still be useable, but installation may not work \u201cautomatically\u201d and require some advanced tuning. Explore the development repository for the tool (is usually linked to the tool\u2019s ToolShed page, but not required, especially at the Test ToolShed), review open known issues, and open a new issue there describing problems as needed.\nThe tool wrapper authors are the best resource for troubleshooting installation issues (if using managed dependencies and the most current version of Galaxy itself). An exception can be tool wrappers authored by the IUC or Devteam \u2013 you can ask if there are known issues (if you can\u2019t find one yet at the development repository) or for installation help (after reviewing the development docs linked above) at this Gitter chat: https://gitter.im/galaxy-iuc/iuc\nPlease be aware that some tools may no longer be actively maintained, and some may use older or custom dependency resolution methods. ToolSheds are community resources \u2013 anyone can publish a tool there. How to tell if a tool is maintained/functional or requires a specific installation method? 1) The tool wrapper\u2019s publication date, how many times downloaded, and other notes on the tool\u2019s ToolShed page and 2) commit/update dates at the linked development repository.\nThe Test and Main ToolSheds were just upgraded, so there were recently windows where tool installation was expected to not function correctly, no matter which ToolShed was the source. Example Q&A about that at this Gitter chat: https://gitter.im/galaxyproject/dev?at=5f59fa68a5788a3c29e651a5\nCurrent and historical status/notes about downtime are at the Galaxy status page. It includes all usegalaxy.* servers and core resources like ToolSheds. https://galaxyproject.statuspage.io/\nHope that helps!"}], [{"role": "user", "text": "Hi! I am having the same problem in which my account is reported as using 100% of available space. I have deleted and the permanently deleted many data sets and yet there is no more available space. I have logged out and back in and waited days and yet the account doesn\u2019t seem to be updating, is there any way my account can be updated by a member of the galaxy team?\nMany thanks!\nAlysha"}, {"role": "system", "text": "Hi @Alysha\nThis updated FAQ covers all the ways to locate and manage data. Double-check and if you are still having problems, write back.\nhttps://galaxyproject.org/support/ >>  The account usage quota seems incorrect\n\nShould you still have problems after that review, where you are working will probably matter for more help.\nYour email address at this forum is not the same as at usegalaxy.org. Is that really where you are working?\nIf yes working at usegalaxy.org, post back confirming then I\u2019ll send you a direct message where you can share your registered account email privately, and I\u2019ll try resetting the quota administratively. You should not share your password: no administrator will ever need OR ask for it. Note: there are no known problems with this function, so there is almost certainly un-purged data in your account. But we can follow up. The server did just go through some updates.\nIf not, please clarify \u2013 the public server URL or some other configuration (local, docker, cloud: explain details such as where sourced and what version of Galaxy).\nThanks!"}], [{"role": "user", "text": "hello,\ni am using bowtie 2 and i didn\u2019t find my reference genome Caenorhabditis elegans ce11.\nthank you"}, {"role": "system", "text": "Hi @FGUENDOU\nThat reference genome is indexed at UseGalaxy.eu for Bowtie2. Please try there.\nThanks!"}], [{"role": "user", "text": "I upload SRR15324080 data in plants.usegalaxy.eu. I am using Trimmomatic flexible read trimming tool for Illumina NGS data tool.\nthe data is paired end sequencing (merged in one file). So, I select paired end (as collection) in the tool but I do not see the name of my data in Select FASTQ dataset collection with R1/R2 pair. How can I set the default for this sample? Can anyone help me?"}, {"role": "system", "text": "Hi @m.esmaeilpour\nThe quickest way to import fastq data into Galaxy from NCBI with the pairs split is with the tool Faster Download and Extract Reads in FASTQ format from NCBI SRA. The output creates collections for you and can be used as input to Trimmomatic.\nOr you can go back to the original interleaved reads and split them up \u2013 then run Trimmomatic on them. That particular tool might not fail on interleaved inputs but is also isn\u2019t designed to handle them.\nI added some tags to your post to help with how to split up the interleaved reads. And here is an FAQ and a tutorial that explain directly what interleaved fastq data is and how to manipulate it. Some tools accept that format, but most do not. But if you are trying to build up a workflow or plan to do a similar analysis again, using the tool that splits the read data at retrieval is simpler (and \u201cfaster\u201d).\nFAQ:\n\n\nReformatting fastq data loaded with NCBI SRA >> NCBI SRA Fastq\n\n\nTutorial:\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: NGS data logistics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi!\nI have sequenced a bacterial genome with a MinION and I intend to assemble it with miniasm. For this, I first intend to generate the PAF file with the sequence overlaps using minimap2. In the minimap2 tool in Galaxy (I use Galaxy Europe),  I understand that I have to set the configuration of the alignment parameters to the preset option \u201cOxford Nanopore all-vs-all overlap mapping\u2026\u201d, and run the tool using the same reads (my fastq file) as the reference (to build the index) and as dataset (where it says \u201cSelect a fastq dataset\u201d). However, the tool does not allow me to provide any fastq file where it says \u201cSelect a fastq dataset\u201d. I have verified that my file is recognized as fastq, and I have uploaded the same file twice with two different names, but it does not work. It seems that the tool does not allow to upload a fastq file in the field labeled as \u201cSelect fastq dataset\u201d. Any clues why?"}, {"role": "system", "text": "Maybe you need to use the datatype fastqsanger instead of fastq.\n\n  \n\n      github.com\n  \n\n  \n    galaxyproject/tools-iuc/blob/master/tools/minimap2/minimap2.xml\n\n\n      <?xml version=\"1.0\"?>\n<tool id=\"minimap2\" name=\"Map with minimap2\" version=\"@TOOL_VERSION@+galaxy@VERSION_SUFFIX@\" profile=\"20.01\">\n    <description>A fast pairwise aligner for genomic and spliced nucleotide sequences</description>\n    <xrefs>\n        <xref type=\"bio.tools\">minimap2</xref>\n    </xrefs>\n    <macros>\n        <import>macros.xml</import>\n    </macros>\n    <expand macro=\"edam_ontology\"/>\n    <expand macro=\"requirements\"/>\n    <stdio>\n        <exit_code range=\"1:\" level=\"fatal\" />\n        <regex match=\"\\[ERROR\\]\" source=\"stderr\" level=\"fatal\" />\n    </stdio>\n    <version_command>minimap2 --version</version_command>\n    <command>\n<![CDATA[\n    #if $reference_source.reference_source_selector == 'history':\n        ln -f -s '$reference_source.ref_file' reference.fa &&\n\n\n\n\n  This file has been truncated. show original\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt looks like the tool accepts fastqsanger, fastqsanger.gz and fasta."}], [{"role": "user", "text": "\nimage874\u00d7861 243 KB\n\nHi,\nI have done some Nucmer alignment with a set of genome sequence retrieved from UCSC genome browser against my genome assembly. And the plot looks like pic above. But the contig or scalfold names are blurred just because of the limited space on X-axis. Is there any way to view or read which contig or scalfold are best maches from the plot or the alignment files?"}, {"role": "system", "text": "Hi @daikez\nThe option Plot Size option can be adjusted \u2013 the default is \u201csmall\u201d, but \u201clarge\u201d would be better given the number of sequences. There are other plot variables that you can adjust if needed (example: Plot a particular reference or query sequence?).\nYou can also hide the tool and history panel in the Galaxy application to give the plot more space, or open your browser window larger, or download the plot and open it that way.\nHope that helps!"}], [{"role": "user", "text": "I am working with RNA seq analysis, and using Fastp but the output of Fastp after trimming still not good. Any solution.\n\nFastp-before1363\u00d71019 37.7 KB\n\n\nFastp-after1341\u00d71029 32.4 KB\n"}, {"role": "system", "text": "Welcome!\nIt is not easy to say what is happening with your data without more information.\nHave you read fastp\u2019s manual? Have you tried other quality-checking tools?"}, {"role": "system", "text": "Hi @wmsalsah,\naccording the FASTQC report, this peak is caused by a region of  ambigous base-calls in the GSM1949048_Pt2_D726_2.gz dataset, probably due to sequencing errors.\n\nScreenshot from 2021-06-22 14-18-21929\u00d7688 15.5 KB\n\nIn order to remove those reads you can reduce the N base limit parameter in fastp.\nRegards"}], [{"role": "user", "text": "Hello,\nI got a local instance of galaxy on my computer, and wanted to run a differential expression analysis for some samples of human RNA-seq data. I\u2019ve managed to download the fastq reads for the samples and I\u2019ve downloaded and indexed the human genome using HISAT2 index builder. But when I tried to align the reads from just one of the six files, it wasn\u2019t able to align them in almost 60 hours of running. So I decided that I\u2019d need to do this on the cloud. However, I don\u2019t want to have to reinstall the tools, and re-download all the files and re-index the genome. I was wondering if there was any way to take whatever I have on my computer, and put it onto the cloud as is, so that I can pick up where I left off.\nThank You"}, {"role": "system", "text": "It\u2019s not possible to transfer a local setup to CloudMan/GVL and instead the tools would needs to be installed again. The GVL does come with a number of tools pre-installed, so perhaps the tools are already available, or maybe only a couple of them would need to be installed. It also comes with a bunch of reference data so hopefully that would not need to be setup.\nAn alternative you could perhaps try is to use the GalaxyCloudRunner: https://galaxycloudrunner.readthedocs.io/en/latest/\nThis is still under development (you would likely be the first user), but in principle, it allows you to connect additional cloud machines to your local Galaxy instance. The remote instance will generally be automatically setup for the necessary tools at runtime while it\u2019ll transfer the data automatically."}], [{"role": "user", "text": "I am unable to create a custom gene categories file to use in goseq. Can some please guide me as to how we create a custom gene category file to use in goseq. I am doing RNA seq analysis and the reference genome is of the microbe Desulfovibrio alaskensis G20 (NCBI Reference Sequence: NC_007519.1). Thank you for any help."}, {"role": "system", "text": "Hi, did you find a solution for this issue?"}, {"role": "system", "text": "Consolidating with Go-seq Not listed Gene categories"}], [{"role": "user", "text": "Hello!\nFirst of all, I would like to thank everyone involved in running and supporting the Galaxy servers (I use http://UseGalaxy.eu). I think the range and choices of tools are great, the platform runs very reliably and fast, and everything is well documented.\nI have been trying to use Salmon for transcript quantification in RNA-seq. There has been a major update to the tool (https://combine-lab.github.io/salmon/). The current version is 1.2.1, while usegalaxy.eu runs 0.14.1.2. I am not sure how much work it is, but would it be possible to update to the current version?\nThank you so much!\nSebastian"}, {"role": "system", "text": "Hi, thanks for the praise!\nAs of Salmon, the tool itself is hosted here which is a github repository and the best way to reach its authors would probably be to either create an issue or contact the IUC on gitter. They know everything about tools there is to know."}], [{"role": "user", "text": "i am using a local install of galaxy and looking to add a reference genome. i used the \u201ccreate dbkey and reference\u201d data manager, and imported h38 genome. the fasta file exists on the server, but I can not see it listed as a reference genome in histat2 or any other tool. it does show up as a data table-- any idea what i need to do to make it show up as a reference genome?"}, {"role": "system", "text": "Hello,\nSome tools, such as hisat2, need an index built against that genome which is referenced in the tool\u2019s tool_data_table_conf.xml.sample file. Looking at that file for hisat2, you\u2019ll notice that it uses a table named hisat2_indexes. A data manager for that data table can be viewed at and installed from https://toolshed.g2.bx.psu.edu/view/iuc/data_manager_hisat2_index_builder/\nThere is also the option of using the galaxy reference data rsync server, detailed in this document."}], [{"role": "user", "text": "Hi all, I am looking to map my pair-ended trimmed reads to my reference genome using Bowtie2. I input the R1 and R2 fastq files and the reference genome FASTA file. It gives me the below error and I am not sure how to fix this.\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 Error, fewer reads in file specified with -2 than in file specified with -1\nthan in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1Error, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of \u2018int\u2019\nterminate called recursively\nterminate called recursively\n(ERR): bowtie2-align died with signal 6 (ABRT)\nThe R1 and R2 reads are definitely same size and I also have enough space left on my galaxy for analysis. Any help much appreciated on how I can fix this please?"}, {"role": "system", "text": "You may need to filter out reads shorter then 2. Do this with a tool that is made for paired-end data, cutadapt could be an option.\nEDIT:\nMy answer may not be applicable to this problem. See: Error with bowtie2\nHere the error looks slightly different.\nIf I quickly google \" terminate called after throwing an instance of 'int'\" I get a few different possible problems. Are you running bowtie2 and galaxy locally? And what steps have you done before mapping?"}, {"role": "system", "text": "Hi @Sayema and @gbbio\nWhen I\u2019ve seen this error, it was due to not having intact pairs present in the input fastq files.\nThat isn\u2019t a requirement always but ensuring intact pairs usually solves it. There is probably a specific option that is the trigger \u2013 but I don\u2019t know for certain which \nSo, if filtering by length isn\u2019t enough (minimum seed length), try this to get rid of reads in either read dataset that no longer have a mate present in the other read dataset. In short, ensure that intact pairs are input to the tool. Some QA tools sort the outputs by paired/not state but others will just remove reads from one end.\n\n\n\nBowtie2 parameters for stringent alignment\n\n\n\n\nFastQ Interlacer followed by FastQ Deinterlacer\n\n\n\n"}], [{"role": "user", "text": "Hi everyone,\nWe recently installed the Galaxy Server to our mainframe. However, we are having difficulty in using RNA STAR as there is no reference genome available (see attached picture). How can we rectify this?\n\nScreenshot 2022-06-13 at 17.10.202052\u00d71616 302 KB\n\nMany thanks,\nChris"}, {"role": "system", "text": "Did you added references already?\nIf not maybe it is good to check out this first:\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Reference Genomes in Galaxy\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Is there any video tutorial on how we can do this? We just simply want to add the reference genome (human) with built-in index and without the built-in gene model.\nApologies, we are very novice to this.\nThanks,\nChris"}, {"role": "system", "text": "Hi @cw951\nThe slides that @gbbio shared are part of a larger tutorial suite. Admin topics are here:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Galaxy Server administration\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nReview the section \u201cData Management & Reference Data\u201d. In short, you have several choices for the method. These include:\n\nCreate your own indexes using Data Managers. These are like other tools Galaxy, but only available for local server admins. You\u2019d need to install the tools, then access them under the \u201cAdmin\u201d tab, then either run the tools in a specific order directly or use our batch processing utility.\nLink in or copy of the indexes available at public Galaxy servers from our data server. This would be configured by a local admin. Human genome builds are included.\n\nThe how-to is covered in the tutorials and some include videos. I also added a few tags to your post that cover prior Q&A about this topic (troubleshooting, etc). If you get stuck or have problems, feel free to ask more questions.\n\n\n\n cw951:\n\nWe just simply want to add the reference genome (human) with built-in index and without the built-in gene model\n\n\nEach genome build is associated with\n\nthe genome fasta and several technical indexes. Some optional, some required. Example: items like the Samtools index would be needed to interpret BAM outputs produced by mapping tools.\nand tool specific indexes (including RNA-Star for most).\n\nHaving a built-in indexes is different from using a Custom genome/build where just the fasta is add to the working history. Why? Galaxy creates those technical and tool specific indexes at runtime as single-use temporary files. Custom genomes/builds are fine for smaller genomes (or any \u201c-ome\u201d) but larger genomes would require much more time/resources to create those indexes every time the tool is run. Convenient for some use cases but not practical for others.\nFor human, definitely use a built-in indexes. And if you chose to use the pre-computed indexes we host, you won\u2019t need to scale up your local resources to create the indexes with Data Managers. You will still need to have sufficient resources to execute a tool. For `RNA-star, I think the minimum working memory would be about 60 GB \u2013 but you can check with your own data. Run the tool at a public server with a representative dataset query and against a genome build target, click into the Job Details (  icon within the result dataset), and review the resources used in the lower part of the report.\nHope that helps!"}], [{"role": "user", "text": "Hi\nI am working with bunch of datasets and one of them wont map to hg38 reference genome. I attached the fastq formats which i think there is a problem with that. I tried every thing else( changing references, or mapping options)\nit seems i should do some thing to fix the fastq formats.\nAlso, It is miRNA-seq dataset.\nThanks\n\n88.474\u00d7536 6.27 KB\n"}, {"role": "system", "text": "Hi @amir\ndo you get an error for the mapping step, or reads do not map to the genome? If the job failed, check the error message. Run FastQC on file with reads. If the reads are all unmapped, check several sequences through UCSC Blat search, to make sure the reads have a mach to the human genome. Alternatively, run Blast search against the human genome for several sequences.\nThe reads are longer than miRNA and presumably contain adapter sequences. You can trim reads before mapping. I assume all other samples have similar untrimmed sequences.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello, I am getting this error message when trying to run \"Map with BWA-MEM \" with a fastq file. Could somebody help me?. Thanks in advanced.\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method:\n{\n\u201chistory_id\u201d: \u201c9bbf4c9f6a067ff4\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2\u201d,\n\u201ctool_version\u201d: \u201c0.7.17.2\u201d,\n\u201cinputs\u201d: {\n\u201creference_source|reference_source_selector\u201d: \u201ccached\u201d,\n\u201creference_source|ref_file\u201d: \u201chg19\u201d,\n\u201cfastq_input|fastq_input_selector\u201d: \u201csingle\u201d,\n\u201cfastq_input|fastq_input1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33ac5a1f754c20927fd\u201d,\n\u201chid\u201d: 9,\n\u201cname\u201d: \u201cFilter by quality on data 1\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201crg|rg_selector\u201d: \u201cdo_not_set\u201d,\n\u201canalysis_type|analysis_type_selector\u201d: \u201cillumina\u201d,\n\u201coutput_sort\u201d: \u201ccoordinate\u201d\n}\n}"}, {"role": "system", "text": "Welcome, @jaamdr !\nI think the usegalaxy.eu was unstable some minutes ago, I had similar problems but right now my jobs are running and finishing ok."}, {"role": "system", "text": "We fixed the server a few minutes ago. Now it\u2019s working properly"}], [{"role": "user", "text": "I am trying to access the RNA category of the toolshed at https://toolshed.g2.bx.psu.edu/. When I click RNA nothing happens and it seems to be waiting for a response from https://toolshed.g2.bx.psu.edu that never comes. The page is also very slow to load so this could be related. Can anyone advise me in this regard? Thank you so much!!"}, {"role": "system", "text": "Hello @ltrinity\nThe ToolShed had some issues earlier. Please try again now.\nGalaxy server status page: https://galaxyproject.statuspage.io/\nThanks!"}, {"role": "user", "text": "Hello, thank you for your response. The toolshed still is not working. "}, {"role": "system", "text": "Hello @ltrinity\nThere was another problem that we just resolved. This exact query worked for me. Please also try now and let us know if it doesn\u2019t work.\nThanks for your patience! Jen"}], [{"role": "user", "text": "Hello. I am using cuffdiff in the galaxy. Between my cuffdiff output, there are no isoforms differential expression file and I get 17 files in my output instead of 23. how can I get isoforms differential expression file?\nplease guide me.\nthank you."}, {"role": "system", "text": "Hi @Sevda.Hosseinzadeh\nUpdated transcriptomic analysis methods are strongly recommended, and many tools for transcriptomics analysis are covered in the Galaxy tutorials here: Galaxy Training!. You can also review the RNA-seq or RNA Analysis tool groups.\nIf you get stuck with a non-deprecated tool, review this troubleshooting FAQ first, then ask questions here with the context from your troubleshooting and a shared history.\n\n\n\n Sevda.Hosseinzadeh:\n\nBetween my cuffdiff output, there are no isoforms differential expression file and I get 17 files in my output instead of 23\n\n\nCuffdiff and many other of the \u201cCuff\u201d tools are deprecated on most public Galaxy servers. Check to see if this, or any of the tools that you may have used upstream, is marked deprecated where you are working. Deprecated tools may directly fail, or they might just produce odd results.\nEven if the \u201cCuff\u201d tool is not marked as deprecated on a particular server, it may still produce odd results or have limited/modified results for technical reasons. These tools won\u2019t be updated. That said, common problems that people used to run into usually involved the reference annotation. Upstream from that, the BAM sometimes had problems. Both were usually about missing attributes or format issues.\nThis link points to a search of our prior forum with the keywords \u201cbiostars cuffdiff\u201d. That may not fix the problem you are having (missing expected outputs), but if you really want to use the old tool, checking the usage/inputs for prior known issues is where to start.\nTo do your own search, go to the  Pan-Galactic Google Search and add in the search term (names other \u201cCuff\u201d tools used upstream? or the mapping tool?) along with the keyword \u201cbiostars\u201d to limit the results. This forum doesn\u2019t include that original discussion since it started being used a few years after those tools were deprecated."}], [{"role": "user", "text": "Hello, I have tried to download gemini database through the gemini data manager, and encountered an error related\u2026\n\nimage1321\u00d7101 18.7 KB\n\n\nimage1321\u00d7268 44.8 KB\n\nAfter everything has been downloaded\u2026 it seems like python script failed\u2026\nThere were no options to change besides some optional one which are part of database information (and failed as well when checked them, with the same error as well)\nDoes anyone knows how to help me?"}, {"role": "system", "text": "Oops, that is the consequence of an unpinned pyyaml dependency in the gemini bioconda recipe and https://github.com/yaml/pyyaml/pull/561.\nI\u2019ll try to get a fix for the DM ready today or tomorrow."}, {"role": "system", "text": "If it\u2019s super-urgent, you could also patch the line of code mentioned in the traceback to provide the \u201cLoader\u201d argument."}, {"role": "system", "text": "one question though: can you provide a bit of details on how you\u2019ve been installing the DM\u2019s dependencies and on which system?\nSince gemini requires python2 and conda doesn\u2019t have a python2-build of pyyaml > 5.3.1, you should only see a Warning, but no error w.r.t. the Loader.\nCan you check which versions of gemini and pyyaml you have ended up with?\n@apolitics ok, we\u2019ve found otu what he issue with the data manager was and released a new version on the toolshed, which should fix things.\nPlease update to that latest version and try again."}], [{"role": "user", "text": "Hello,\nI am running galaxy in a container (using image: GitHub - bgruening/docker-galaxy-stable: Docker Images tracking the stable Galaxy releases.). I was wondering if there is a way to browse files inside of the running container rather than on my local file system (i.e. my laptop) while doing the file upload.\nThank you for your time and help,\nAnna\n\nimage914\u00d7570 30.8 KB\n"}, {"role": "system", "text": "I am not sure what would be the best solution but I see you dont have an answer yet. My best guess now would be to check out data libraries. Here is some info:\n\n  \n      \n\n      galaxyproject.org\n  \n\n  \n    \n\nGalaxy Data Libraries\n\n  All about Galaxy and its community.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Libraries\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Im trying to add annotations to a vcf file.\nI had paired-end reads of 3 individuals, I aligned each pair to the human hg19 genome, then I generated the VCF file using FreeBayes to call for variants of the 3 sets of data, then i wanted to annotate these variants using ANNOVAR Annotate VCF.\nthe output was:\n\u201c0 lines\nformattabulardatabasehg19\nWARNING to old ANNOVAR users: this program no longer does line-to-line conversion for multi-sample VCF files. If you want to include all variants in output, use \u2018-format vcf4old\u2019 instead.\nNOTICE: Finished reading 2497 lines from VCF file\nNOTICE: A total\u201d\nI did this previously on a VCF file with a single set of data (1 sample) and it worked fine. I looked up the help forum, and then found an advice to split the samples and then run ANNOVAR tool. I split the samples using VCFselectsamples tool and then when I ran the Annovar on these single-sample files I obtained a file with no data (0 lines & 0bytes) for each.\nI have to point out that I re-ran the annovar tool step I used before (which worked fine) and it gave me 0 lines as well!! so does this mean ANNOVAR is down? I was also looking for alternative tools, but they all look very complicated and Im still a beginner.\nSo could someone support me with this issue? how can I get the task of annotating VCF done? I mean it should be straight forward but I spent 2 whole days reading forums and trying things and still getting no success.\nThanks in advance,\nYahia"}, {"role": "user", "text": "I am using public galaxy if that info is relevant"}, {"role": "system", "text": "Hello @yahia.elshabrawy\nYes, Annovar at https://usegalaxy.org is currently non-functional. The tool will likely be deprecated: https://github.com/galaxyproject/usegalaxy-playbook/issues/284\nAlternative VCF annotation tools can be found in the tool grouping \u201cVariant Calling\u201d. SnpEff is one example.\nGTN Tutorials may help with usage and creating a workflow. Start with those under the topic \u201cVariant Analysis\u201d.\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy  server choices. If a tutorial is supported by a pre-configured Galaxy Docker  training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nHope that helps!"}], [{"role": "user", "text": "Hello,\nI am a new user of galaxy. I want to searchThermo RAW files using maxquant on galaxy. But, it runs into an error. One of my colleaguge thinks it might be because of insufficent memory. I appreciate any idea how to solve it? Thank you for your help,\nDataset Error Report\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/1.6.17.0+galaxy4.\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 134 ()\nTool generated the following standard error:\nUnhandled Exception: System.Exception: Exception during execution of external process: 2988633\nat QueueingSystem.WorkDispatcher.ProcessSingleRunExternalProcess(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.DoWork(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.Work(Object threadIndex)\nat System.Threading.Thread.ThreadMain_ParameterizedThreadStart(Object parameter)\nat System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)\n\u2014 End of stack trace from previous location where exception was thrown \u2014\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n/usr/local/bin/maxquant: line 4: 2986039 Aborted                 (core dumped) dotnet $DIR/MaxQuantCmd.exe $@\nDetected Common Potential Problems\nThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\nThe tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices."}, {"role": "system", "text": "Hi @Lati98\nThese parts of an error message are from Galaxy\u2019s automatic input checks:\n\n\n\n Lati98:\n\nDetected Common Potential Problems\nThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\nThe tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\n\n\nTo review the inputs sent to the job in a top level summary, examine the \u201cDataset Information\u201d summary page ( icon). Were any of the input datasets included more than once? Do all inputs contain content? More checks are listed here understanding-input-error-messages.\nTutorials with example data and usage help:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Proteomics\n\n  Training material for proteomics workflows in Galaxy\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you can\u2019t find the problem, please share a few more details. The content of the \u201cDataset Information\u201d  page can be screenshot or copy-pasted back. Or sometimes better, post back a share link to the history."}], [{"role": "user", "text": "In the FeatureCounts->advanced setting-> GFF feature type filter, the default is exon, but if change to intron, the values are the same, and if leave it blank, it still the same. So it looks like this function do not work. I need to get the counts for the whole gene (GRO-seq), including UTR, exon, and intron, how can I do that?"}, {"role": "system", "text": "Hi @Cindy,\nwhich annotation file are you using? I checked the wrapper, and it seems to work fine.\nRegards"}], [{"role": "user", "text": "Hello,\nI\u2019m relatively new to all of this and managed to successfully launch my GVL server on cloudman, woo!\nI\u2019m having issues uploading data now though. At first I tried uploading BAM files via local upload, which I realize now was a bit dumb. I\u2019m now trying to setup and FTP connection to my cloudman server, but having a very difficult time.\nI read the documentation on setting up an FTP server, but for someone not in the know like myself, I\u2019m finding it difficult. I\u2019ve used FileZilla in the past for a separate EC2 connection on AWS. Is it possible for me to connect to my server via FileZilla? I have my pem key stored on FileZilla and everything, but I\u2019m getting errors connecting.\nDoes anyone have any guide for dummies or a video detailing the process?"}, {"role": "user", "text": "To add a bit more detail, I found the info about the Data Browser that was included in the more recent beta version, which is what I have. However, on the main GVL page the Data Browser won\u2019t launch when clicked, and then disappears. Once it\u2019s gone I can\u2019t reload it either."}, {"role": "system", "text": "@Jackson_Brougher you got a reply on github isn\u2019t it? Just double checking. Please do not cross post your questions "}, {"role": "user", "text": "Apologies, I did. I got a very helpful response for accessing the admin panel, and replied with the issue I was having on uploading data.\nSorry for the cross post, and thank you for the help with figuring this out!"}], [{"role": "user", "text": "\nDoes anyone know why I might be getting this error?\nI\u2019m trying to use HISAT2 on forward and reverse reads of files.\nI would think that it wouldn\u2019t matter how many reads were in the forward vs. reverse files."}, {"role": "system", "text": "Dont know much about exceptions but in general you always need to have the same number of forward and reverse reads. You can double check the number of reads with fastqc. The main question is why are the number of reads not equal? Did you do any pre-steps like quality trimming? In that case make sure you use a tool that is designed for paired-end data."}], [{"role": "user", "text": "Hello,\nI\u2019m trying to download some files directly from Ensembl FTP:\nUpload Data (Download from web or upload from disk) > Choose remote files > ENSEMBL FTP server\nUnfortunately, the only message I receive is:\n\n\u201cProblem listing file source path FileSourcePath(file_source=, path=\u2018/\u2019)\u201d.\n\nCould someone please help me to solve the problem? Or maybe suggest another way to do this?\nBest,\nMagdalena"}, {"role": "system", "text": "Hi @Magdalena_Kub,\ncould you provide more details about which file are you trying to download? I checked it with a few of them, and it seems to work fine.\nRegards"}], [{"role": "user", "text": "Dear galaxy project members,\nI have been having a lot of problems running unicycler since at least a month now. I have reported every different bug that came across and I have exchanged a few emails with people from the galaxy support team, they told me that it was due to an upgrade of the hosted tool. But trying after that I keep on having problems. Unicycler just keeps failing.\nCould you please help me on this? I really need the results.\nThank you very much!"}, {"role": "system", "text": "Hi @alanelena,\ncould you try to run the analysis in usegalaxy.eu?\nRegards"}, {"role": "system", "text": "Update, please see run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210"}], [{"role": "user", "text": "Hi, I have been trying to run SnpEff in Galaxy, but have been facing a number of issues that I am not sure how to resolve.\nBackground: I am analysis different strains of Mycobacterium tuberculosis. I have created a vcf file, and would now like to see the effects of my variants, hence I am using SnpEff.\nProblems:\n\nWhen I use the SnpEff download tool to download the SnpEff database for Mycobacterium_tuberculosis_h37rv, the tool runs successfully but outputs a file with only 2 lines. (SnpEff version number and Mycobacterium tuberculosis H37Rv).\n\nI tried using the SnpEff build tool as well with the Genbank annotation file, however it results in the same 2 line output.\n\nBecause of this problem, I directly downloaded the SnpEff database from SourceForge, and uploaded the file as a Snpeffdb to galaxy. This file however, shows as \u2018unavailable\u2019 in SnpEff eff. The issue might be with the metadata, which I am not able to edit manually and autodetect does not work.\n\nAs you can see I have tried all the approaches I can think of and nothing seems to work. I even used the galaxy.eu version. Please help, I am new to NGS analysis and will appreciate any guidance.\n@jennaj @wm75"}, {"role": "system", "text": "Hmm, from your description (which is very detailed BTW - thanks for that!) there does not seem to be anything wrong here. Those two lines of output are how Galaxy presents the newly downloaded genome to you. After all, SnpEff genome files are kept in a binary format and there is not really a more helpful way to display that data.\nWhat you should try next is to go to the SnpEff eff tool, select Downloaded snpEff database in your history under Genome source, and the version you downloaded via SnpEff download should be offered as a choice.\nBuilding the genome file yourself should also work, but is not recommended if there\u2019s an exisitng genome for download. Such genomes should be offered if you select Custom snpEff database in your history as the Genome source.\nUploading an externally built SnpEff genome to a Galaxy history is not currently supported.\nLet us know if you keep facing issues and best wishes,\nWolfgang"}], [{"role": "user", "text": "I want to use Velvet for educational purpose.\nI was able to search and use that tool until yesterday, but it suddenly disappeared.\nCould you check this problem ?\nThanks !"}, {"role": "system", "text": "Hi,\nVelvet is not available at Galaxy Main https://usegalaxy.org. Instead, we recommend Unicycler for small genome assembly.\nGTN Tutorial:  http://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html\nMaybe you are working at a different public Galaxy server? Check/compare the URL. Not all public Galaxy server administrators check posts here, instead contact them directly. Contact information is usually at the home page of the server and sometimes also here: https://galaxyproject.org/use.\nNote: The Galaxy EU https://usegalaxy.eu and Galaxy AU https://usegalaxy.au.org servers are both down: EU for scheduled maintenance, should be back soon; AU, not sure of the reason, but expect they will also be back online shortly."}, {"role": "system", "text": "\n\n\n jennaj:\n\nEU for scheduled maintenance, should be back soon\n\n\nusegalaxy.eu is back up and does host a few velvet tools, but for limited usage (training-sized datasets). Details about server status earlier today are in this Q&A: is galaxy Australia down?"}], [{"role": "user", "text": "Hello!\nI want to delete a test user. But i get the following error:\n\nUser deletion must be configured on this instance in order to allow user self-deletion. Please contact an administrator for assistance.\n\nBut i can not find any facilities neither to delete user, nor to \u201callow user self-deletion\u201d in the admin panel, moreover, google also told me nothing about this!\nHow can i delete some user?\nThanks in advance."}, {"role": "system", "text": "Hi @wormball\nThe feature was introduced in the 20.09 release. Is your server upgraded to at least that or a later release? The 20.01 release is the most current.\n*September 2020 Galaxy Release (v 20.09) \u2014 Galaxy Project 21.01 documentation << where the feature and the PR for that change are in the release notes.\nThere is also a galaxy.yml option that allows admins to delete accounts (Admin > User Management > Users). Search for the user by email and manage accounts in the browser. The default is false \u2013 change to true and remove the hash to enable.\n   # Allow administrators to delete accounts.\n   #allow_user_deletion: false\n\nThere are admin command-line options as well. The utility gxadmin is helpful and functions can be modified/added. Useful for batch admin actions when the browser is way too tedious due to volume, or queries that are not wrapped in the browser for practical reasons.\n*GitHub - galaxyproject/gxadmin: Handy command line utility for Galaxy administrators\n*Searching the Galaxy\nHope that helps!"}], [{"role": "user", "text": "Hi\nI have high-quality draft genomes and I\u2019m having issues running them on GrowthPred to calculate predicted ideal growth temperature. I\u2019ve uploaded the following files (see below) and in all instances I\u2019ve received the error \u201cERROR: sequence length not multiple of 3:\u201d (see screenshot attached).\nI\u2019ve uploaded:\nfull genome fasta\nfeature only genome fasta (should only include the CDS regions)\nGBK file obtained from ANTISMASH that should only include the CDS regions\nAny advice?\nThank you!\n\nScreen Shot 2022-06-28 at 9.41.56 AM932\u00d7178 6.02 KB\n"}, {"role": "system", "text": "I dont know this tool and could also not find it on galaxy (maybe I dont look good enough, your tag says you use local). But the error is clear, did you manually checked if the length of the CDS regions is indeed not a multiple of 3? If not, your input is not correct. So the advise would be to double check the lengths and maybe run the tool on a very small dataset where you are sure the CDS input is a multiple of 3."}], [{"role": "user", "text": "Hi,\nI have a question about conda used in galaxy. In 2020, anaconda\u2019s TOS changed and organizations larger than 200 people will have to pay for the use of the anaconda defaults repository.\n\n  \n      \n      Anaconda\n  \n  \n    \n\nAnaconda | Anaconda Commercial Edition FAQ\n\nAnaconda Commercial Edition FAQ\n\n\n  \n  \n    \n    \n  \n  \n\n\nGalaxy uses conda for dependency resolver and it is very useful. I think many of tools can be installed with conda-forge , bioconda , and iuc repository, but sometimes need to use the defaults repository. Do public and local Galaxy server admins belonging to a large for-profit organization charge for the use of the anaconda defaults repository? Has this ever been discussed in Galaxy community?\nBest,\nYukie"}, {"role": "system", "text": "Hi @yukieymd\nThank you for posting the question. I\u2019ve asked the Galaxy Admin\u2019s group for advice about if or how they handle this situation.\nThe question is here at Gitter. They may reply back to this topic or at Gitter. Also, please feel free to join the chat: https://gitter.im/galaxyproject/admins?at=5fd7b3060697c1210dbfce0e\nThanks!"}, {"role": "system", "text": "Hi,\nif you follow the best-practices and use only packages from Bioconda and conda-forge this should not be a problem as most if not all packages are on both channels. In addition, Anaconda is so generous and supporting both community projects with free CDN hosting and unlimited space.\nI hope that helps,\nBjoern"}], [{"role": "user", "text": "Hello,\nI\u2019m new to Galaxy, so I hope this is the correct way to post a question. I\u2019m attempting to assemble a bacterial genome using a pair of Illumina reads and a PacBio read with ABySS and with Unicycler. I\u2019ve tried doing this in several different ways according to the information included in the tool descriptions, but both assemblers fail to run and give a tool error. What are the correct file types and suffixes for each read in each assembler? (by suffixes, I mean the ending of \u201c1\u201d and \u201c2\u201d for paired short reads, for example).\nThank you for your help, I look forward to learning more about Galaxy."}, {"role": "user", "text": "Hi, I apologize, but I\u2019m unfortunately still having difficulty resolving this issue. I would really appreciate any suggestions. Thanks!"}, {"role": "system", "text": "Unicycler is functional. ABySS has been removed.\nGTN Tutorial for assembly (including Unicycler):\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: Assembly\n\nDNA sequence data has become an indispensable tool for Mo...\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n\n\nABySS fungal de novo genome assembly: Update, ABySS removed from usegalaxy.org, alternative usegalaxy.eu\n\n\nUpdate\nThe assembler ABySS has been technically problematic at UseGalaxy.org for some time now.\nThe tool has been removed from UseGalaxy.org.\nAn alternative usegalaxy.* server for this tool is UseGalaxy.eu\n\n"}], [{"role": "user", "text": "Hello I am helping a friend running Jbrowse and it seems like Jbrowse (ie Jbrowse genome browser, toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1) is broken on usegalaxy.org. This is the result I get running it on usegalaxy.org while it runs fine on usegalaxy.eu using the same dataset.\n\nJbrowse1355\u00d7663 15.9 KB\n\nBest regards\nTomas"}, {"role": "system", "text": "usegalaxy.org only allowlists certain versions of the jbrowse tool, I guess that one was missed. I\u2019ve pinged dannon on gitter."}, {"role": "system", "text": "Thanks for the ping!  Should be good to go now, refresh and let me know if it\u2019s not better."}], [{"role": "user", "text": "The C. elegans genome version available for the RNA STAR tool is very old, WS220 (Oct 2010 release). Many changes have been incorporated since then and the latest version (as of now) is WS284 (May 2022 release: Wormbase release WS284 - WormBase : Nematode Information Resource). Can the Galaxy Team please make this genome version available for RNA STAR analysis? Thanks!"}, {"role": "system", "text": "Hi @Surojit_Sural,\nthe installation of the latest C. elegans genome has been requested to the Galaxy admins. Since it could take a few days, meanwhile I recommend you upload the genome reference to your history and use it from them.\nRegards"}], [{"role": "user", "text": "I am running Featurecounts for Desulfovibrio alaskensis RNA seq data. I am stuck with the following error.\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file. The specified gene identifier attribute is \u2018gene_id\u2019\nCould anyone help me with this?"}, {"role": "system", "text": "Hi @jawaharrajk\nCheck your reference annotation (GTF) dataset. Does it have that attribute in the  9th column? For all rows? No headers?\nIf not, you\u2019ll need to source and format a GTF that works with this tool (and many others).\nFAQ: Datatypes - Galaxy Community Hub\nYou can also click on the tags I added for prior Q&A about this with context.\nThanks!"}], [{"role": "user", "text": "Hi,\nI am following the instruction of 16S Microbial Analysis with mothur (extended) and using the example data. Everything went well until the step of using Unique.seqs. I have attached the parameter that I used and the error messages. Can anyone please let me know what did I do wrong? Thank you!\n\nCapture1604\u00d7879 118 KB\n"}, {"role": "system", "text": "Status resolved: Troublshooting Unique.seqs: Status: resolved, please rerun prior failures\n\n\nWelcome, @diegozhu\nThis tool has a small bug right now. Please see the solution on this topic for the current options.\n\n  \n    \n    \n    Troublshooting Unique.seqs: Status: resolved, please rerun prior failures usegalaxy.org support\n  \n  \n    Jennaj, \nI am running into problems with the Unique.seq tool in Galaxy.  I use Galaxy online to teach a course and we use the 16S Metagenomics (Mouse) dataset as the training module. I have been using this approach for a couple of years now, but we have run into a Fatal Error when running the first step of the Unique.seq tool.  We have tried to run Unique.seqs from multiple accounts (students and mine), but always using Galaxy online. I have tried it from our computers at work (faster internet) \u2026\n  \n\n"}], [{"role": "user", "text": "I have been trying to perform a scRNA sequence experiment with the Galaxy tutorial Generating a single cell matrix using Alevin and keep hanging up when I get to the SalmonKallistoMtxTo10x step.  The failure happens both with the data set in the tutorial and with a smaller data set directly from 10x genomics.  Details on data set and run parameters below.  Any suggestion on how to get past this blockage would be helpful.  The error output files are empty.\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Generating a single cell matrix using Alevin\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n1k PBMCs from a Healthy Donor (v3 chemistry)\nSingle Cell Gene Expression Dataset by Cell Ranger 3.0.0\nPeripheral blood mononuclear cells (PBMCs) from a healthy donor (the same cells were used to generate pbmc_1k_v2, pbmc_10k_v3). PBMCs are primary cells with relatively small amounts of RNA (~1pg RNA/cell).\n\u2022\t1,222 cells detected\n\u2022\tSequenced on Illumina NovaSeq with approximately 54,000 reads per cell\n\u2022\t28bp read1 (16bp Chromium barcode and 12bp UMI), 91bp read2 (transcript), and 8bp I7 sample barcode\n\u2022\trun with --expect-cells=1000\n\nGTF2GeneList extracts a complete annotation table or subsets thereof from an Ensembl GTF using rtracklayer (Galaxy Version 1.42.1+galaxy6)\nEnsembl GTF file\nFeature type for which to derive annotation\ntranscript\nField to place first in output table\ntranscript_id\nSuppress header line in output?\nYes\ntranscript_id,gene_id\nAppend version to transcript identifiers?\nYes\nFlag mitochondrial features?\nNo\nFilter a FASTA-format cDNA file to match annotations?\nYes\nAnnotation field to match with sequences.\ntranscript_id\nRename galaxy-pencil the annotation table to Map\nRename galaxy-pencil the uncompressed filtered FASTA file to Filtered FASTA\n\nAlevin Quantification and analysis of 3\u2019 tagged-end single-cell sequencing data (Galaxy Version 1.3.0+galaxy2)\nTool Parameters\nInput Parameter\tValue\nSelect a reference transcriptome from your history or use a built-in index?\thistory\nTranscripts fasta file\t10 Filtered FASTA uncompressed (Hidden)\nKmer length\t31\nPerfect Hash\tFalse\nSingle or paired-end reads?\tpaired\n4 pbmc_1k_v3_S1_L001_R1_001.fastq.gz\n5 pbmc_1k_v3_S1_L001_R2_001.fastq.gz\nRelative orientation of reads within a pair\tMates are oriented toward each other (I = inward)\nSpecify the strandedness of the reads\tread comes from the reverse strand (SR)\nprotocol\t10x chromium v3 Single Cell protocol\nTranscript to gene map file\t9 Map\nRetrieve all output files\tTrue\noptional\t\nWhitelist file\t\nnoDedup\tFalse\ndumpBfh\tFalse\ndumpFeatures\tTrue\ndumpUmiGraph\tFalse\ndumpMtx\tTrue\nforceCells\tNot available.\nexpectCells\tNot available.\nnumCellBootstraps\tNot available.\nminScoreFraction\tNot available.\nkeepCBFraction\t1.0\nlowRegionMinNumBarcodes\tNot available.\nmaxNumBarcodes\tNot available.\nfreqThreshold\t3\n\nSalmonKallistoMtxTo10x Transforms .mtx matrix and associated labels into a format compatible with tools expecting old-style 10X data (Galaxy Version 0.0.1+galaxy5)\nTool Parameters\nInput Parameter\tValue\n.mtx-format matrix\t11 quants_mat.mtx\nTab-delimited genes file\t13 quants_mat_cols.txt\nTab-delimited barcodes file\t14 quants_mat_rows.txt\nPrefix to prepend to cell names / barcodes\tEmpty"}, {"role": "system", "text": "Hello! Yes absolutely - can you please share your history?"}, {"role": "system", "text": "Hey @Wendy_B I\u2019ve been experiencing the same problem (details of the error below). As for the data, I used the one here:\n(Given on Generating a single cell matrix using Alevin )\nhttps://zenodo.org/record/4574153/files/Experimental_Design.tabular\nhttps://zenodo.org/record/4574153/files/Mus_musculus.GRCm38.100.gtf.gff\nhttps://zenodo.org/record/4574153/files/Mus_musculus.GRCm38.cdna.all.fa.fasta\nhttps://zenodo.org/record/4574153/files/SLX-7632.TAAGGCGA.N701.s_1.r_1.fq-400k.fastq\nhttps://zenodo.org/record/4574153/files/SLX-7632.TAAGGCGA.N701.s_1.r_2.fq-400k.fastq\nCan you suggest why there\u2019s an error here?\nDETAILS OF ERROR:\nExecution resulted in the following messages:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/shed_tools/toolshed.g2.bx.psu.edu/repos/ebi-gxa/salmon_kallisto_mtx_to_10x/e42c217a450f/salmon_kallisto_mtx_to_10x/salmonKallistoMtxTo10x.py\u201d, line 71, in \nmmwrite(\u2019%s/matrix.mtx\u2019 % mtx_out, umi_counts.transpose())\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 101, in mmwrite\nMMFile().write(target, a, comment, field, precision, symmetry)\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 451, in write\nstream, close_it = self._open(target, \u2018wb\u2019)\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 323, in _open\nstream = open(filespec, mode)\nOSError: [Errno 30] Read-only file system: \u2018.//matrix.mtx\u2019\nGalaxy job runner generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/shed_tools/toolshed.g2.bx.psu.edu/repos/ebi-gxa/salmon_kallisto_mtx_to_10x/e42c217a450f/salmon_kallisto_mtx_to_10x/salmonKallistoMtxTo10x.py\u201d, line 71, in \nmmwrite(\u2019%s/matrix.mtx\u2019 % mtx_out, umi_counts.transpose())\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 101, in mmwrite\nMMFile().write(target, a, comment, field, precision, symmetry)\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 451, in write\nstream, close_it = self._open(target, \u2018wb\u2019)\nFile \u201c/usr/local/lib/python3.8/site-packages/scipy/io/mmio.py\u201d, line 323, in _open\nstream = open(filespec, mode)\nOSError: [Errno 30] Read-only file system: \u2018.//matrix.mtx\u2019"}, {"role": "system", "text": "Hi @pa.saunders  and @rituu-vermaa\n\n\n\n rituu-vermaa:\n\nOSError: [Errno 30] Read-only file system: \u2018.//matrix.mtx\u2019\n\n\nThis tool SalmonKallistoMtxTo10x needs some tool developer then administrative changes to work at UseGalaxy.org. I suspect Alevin has this problem as well (not confirmed yet).\nPlease try running this tutorial, or any workflows based on it, at the UseGalaxy.eu server instead for now.\nIf you need to move existing data between the servers, the how-to is in this FAQ.\nPs: @rituu-vermaa , Thanks for submitting the bug reports. I replied to some of those already re: mixed up inputs. That is a bit common with this tutorial since it repeats groups of steps with slightly different criteria a few times, which is arguably confusing but unavoidable when doing the work step-by-step and not with a workflow/collection yet. You could consider adding in dataset #tags to help keep track of the different runs (example screenshot in that tutorial with tags included). Later on when using a workflow and collections, tags or not, that will be less likely to happen, and is one reason why both of those functions are popular and worth learning about (search the GTN tutorials with keywords for help with those: \u201ccollection\u201d and/or \u201cworkflow\u201d).\nHope that helps, and apologies for the confusing tool trouble on top of a complicated tutorial!"}], [{"role": "user", "text": "Hey,\nI found that I could not download the MultiQC HTML in general. it gave me errors like the following:\nSnip20201012_21432\u00d7692 22.3 KB\nSnip20201012_61382\u00d7852 147 KB\nDoes anyone know how to troubleshoot?\nKira"}, {"role": "system", "text": "Two of us are having similar problems! Following this thread as well in case anyone has a solution.\n\n  \n    \n    \n    UseGalaxy.org scheduled maintenance downtime October 13, 2020 -- Completed usegalaxy.org support\n  \n  \n    Hello, \nI am a Galaxy user and I wish to download the FASTQC report generated as output on galaxy server, but I am unable to do so. I have run \u201cFASTQC read quality report\u201d to check the quality of my fastq reads, and the program ran successfully giving a HTML file as output. But I am not able to download/view the HTML file. \nIs this a bug of galaxy or is there anything I can do to download the file? \nKindly advise \n\u2013Lakshmi\n  \n\n"}, {"role": "system", "text": "Please see: UseGalaxy.org scheduled maintenance downtime October 13, 2020 -- status and updates"}], [{"role": "user", "text": "I wrote five tools to process genetic data. All them were verified to work correctly out of galaxy and also as isolated tools inside galaxy. Also, the intermediate results, produced by a tool and used as an input of the next one, are ok. In short, everything works well when using the tools in isolation, but not when they are laid in sequence in a workflow. To illustrate, I show here the xml files of the first two tools.\nTool1: reads a paired collection of genetic data where the genes are identified by Gene Symbol and changes the identification to UniProt Id.\nCurrent xml file is as below:\n<tool id=\"gs2up\" name=\"GS2UP: Gene Symbol to UniProt\" version=\"0.1.0\">\n  <description>-</description>\n  <command>\n    <![CDATA[ perl $__tool_directory__/GS2UP.pl $gstcga $gstcga.name $uptcga ]]>\n  </command>\n  <inputs>\n    <param type=\"data\" name=\"gstcga\" format=\"tabular\" />\n  </inputs>\n  <outputs>\n    <data format=\"tabular\" name=\"uptcga\" label=\"up_$(gstcga.name)\" />\n  </outputs>\n</tool>\n\nTool 2: reads a paired collection of genetic data, where the genes are identified by UniProt Id and for each dataset generates three output datasets. That is to say, this tool uses a paired collection as input and generates a triplet list.\nThe xml file is as below:\n<tool id=\"urgel\" name=\"URGEL\" version=\"0.1.0\">\n  <description>-</description>\n  <command>\n    <![CDATA[perl $__tool_directory__/URGEL.pl $tcga.forward $tcga.reverse $triplet.thrsh $triplet.diffrn $triplet.upreg ]]>\n  </command>\n  <inputs>\n    <param name=\"tcga\" type=\"data_collection\" collection_type=\"paired\" format=\"txt\" />\n  </inputs>\n  <outputs>\n    <collection name=\"triplet\" type=\"list\" label=\"ur_$(tcga.name)\" >\n      <data name=\"thrsh\" format=\"txt\" />\n      <data name=\"diffrn\" format=\"txt\" />\n      <data name=\"upreg\" format=\"txt\" />\n    </collection>\n  </outputs>\n</tool>\n\nAs a newbie to Galaxy, I may be incurring an error. However, I can\u2019t find it alone and would appreciate any help."}, {"role": "system", "text": "Your first tool doesn\u2019t work explicitly with collections.\nAre you mapping a collection over it?\nWhat version of Galaxy are you running?"}, {"role": "user", "text": "Thank you for the answer.\nGalaxy version 19.05.\nYes, I map a (paired) collection when running the tool alone (not in a workflow).\nMy first try was with collections explicitly. I still have that \u201cold\u201d xml. However, the situation was the same: in isolation, the tool worked well and produced the expected results; when in a track, it failed. So I digged the documentation and found the following advice:\n\nIf a tool\u2019s functionality can be applied to individual files in isolation, the implicit mapping described above should be sufficient and no knowledge of collections by tools should be needed. However, tools may need to process multiple files at once - in this case explicit collection consumption is required.\n\n(from https://planemo.readthedocs.io/en/latest/writing_advanced.html#collections)\nThen I wrote this version for the tool definition. The tool itself is very simple: it reads a two-column table, where the first column is an identifier (the Gene Symbol) and the second is a numerical value, and generates a new table with the identifier translated to UniProt Id, as required by the next workflow steps. The input is a paired collection, the output is a paired collection, but (as quoted) the tool doesn\u2019t need to be aware of that.\nThe second tool in sequence, when run in isolation, recognizes the paired collection created by its predecessor and also produces its expected results. But also fails when put in a workflow."}, {"role": "user", "text": "I solved it by myself. In \u201cEdit Workflow\u201d screen, there is a button \u201cInputs\u201d to define types of input data.\nThanks a lot."}], [{"role": "user", "text": "Hello. I am running a galaxy in my institute. Lately my galaxy has a lot of hiccups. Installing tools doesn\u2019t work. These will keep cycling on the \u201cnew\u201d status. Internally I see the error below.\nWhen I restart Galaxy the issue is fixed, only for the issue to reoccur the next day.\nWhat I can grok from the error is that it has something to do with a connection that explicitly needs to call a rollback. This is not something that can be fixed by an end-user.\nDoes this issue have any known cause? I work with PostgreSQL as a backend database, so nothing too fancy. The Galaxy version is 21.09.\nThanks!\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.tool_shed.galaxy_install.repository_dependencies.repository_dependency_manager DEBUG 2021-11-26 16:03:06,145 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Building repository dependency relationships...\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.tool_util.toolbox.base DEBUG 2021-11-26 16:03:06,145 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Appending to tool panel section: ARGalaxy Pipelines\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.web.framework.decorators ERROR 2021-11-26 16:03:06,156 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Uncaught exception in exposed API method:\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: Traceback (most recent call last):\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/web/framework/decorators.py\", line 312, in decorator\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     rval = func(self, trans, *args, **kwargs)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/webapps/galaxy/api/tool_shed_repositories.py\", line 161, in install_repository_revision\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     payload)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 714, in install\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     install_options\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 808, in __initiate_and_install_repositories\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return self.install_repositories(tsr_ids, decoded_kwd, reinstalling=False, install_options=install_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 827, in install_repositories\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     tool_panel_section_keys=tool_panel_section_keys)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 1102, in order_components_for_installation\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repo_info_dicts)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 265, in get_prior_import_or_install_required_dict\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repository, repository_dependencies = get_repository_and_repository_dependencies_from_repo_info_dict(app, repo_info_dict)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 299, in get_repository_and_repository_dependencies_from_repo_info_dict\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repository = get_repository_for_dependency_relationship(app, tool_shed, repository_name, repository_owner, changeset_revision)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 380, in get_repository_for_dependency_relationship\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     app.tool_shed_repository_cache.rebuild()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tools/cache.py\", line 304, in rebuild\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     defer(ToolShedRepository.metadata), joinedload('tool_dependencies')\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 2709, in all\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return self._iter().all()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 2847, in _iter\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     execution_options={\"_sa_orm_load_options\": self.load_options},\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/session.py\", line 1689, in execute\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     result = conn._execute_20(statement, params or {}, execution_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1582, in _execute_20\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return meth(self, args_10style, kwargs_10style, execution_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\", line 324, in _execute_on_connection\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     self, multiparams, params, execution_options\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1461, in _execute_clauseelement\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     cache_hit=cache_hit,\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1668, in _execute_context\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     conn = self._revalidate_connection()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 560, in _revalidate_connection\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     self._invalid_transaction()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 540, in _invalid_transaction\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     code=\"8s2b\",\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: sqlalchemy.exc.PendingRollbackError: Can't reconnect until invalid transaction is rolled back. (Background on this error at: http://sqlalche.me/e/14/8s2b)\n"}, {"role": "system", "text": "Hi @rhpvorderman,\nI don\u2019t have an answer yet, but I\u2019ll look into it. Can you give me an example of a tool that causes this? (or does this happen with any tool?)"}, {"role": "user", "text": "Hi @ic4f ,\nThank you for your response. I found the error. The database connection was unstable due to a configuration issue. The postgresql log had entries complaining about broken or aborted connections. The cause was found and now the issue has been resolved.\nThanks again for looking into this!\nBest regards,\nRuben"}], [{"role": "user", "text": "The annotation I am trying to use is the Zm-B73-REFERENCE-NAM-5.0_Zm00001eb.1.gff3.gz file linked here: Index of /Zm-B73-REFERENCE-NAM-5.0. I suspect this gff3 file is the source of the problems and needs to be altered somehow to work with featurecounts\nI have paired-end RNA-seq reads from maize that I am trying to convert to counts using featurecounts. I basically followed the steps outlined in this tutorial, but with a few differences (e.g. my RNA library is forward-stranded and has longer reads): Reference-based RNA-Seq data analysis\nThe creation of a collection of 2 paired ends, quality control, and initial mapping with RNA-star seemed to work, but then when I tried to do featurecounts and got an error: \u201cfailed to find the gene identifier attribute in the 9th column of the provided GTF file. The specified gene identifier attribute is \u2018gene_id\u2019. An example of attributes included in your GTF annotation is \u2018Parent=Zm00001eb000010_T001;Name=Zm00001eb000010_T001.exon.1;ensembl_end_phase=0;ensembl_phase=0;exon_id=Zm00001eb000010_T001.exon.1;rank=1\u2019.\u201d\nThen I looked into my file and saw that it was using \u201cgene_id\u201d as an identifier, so I tried GFF gene identifier: gene_id, which is used in the 9th column of my file; however, I still got \u201cfailed to find the gene identifier attribute in the 9th column of the provided GTF file\u201d error.\nThen I tried leaving the \u201cGFF feature type filter\u201d and \u201cGFF gene identifier\u201d fields empty, but I got an error saying \u201cno features were loaded in format GTF. The annotation format can be specified by the \u2018-F\u2019 option, and the required feature type can be specified by the \u2018-t\u2019 option\u201d\nWhat I tried next: I learned that featurecounts requires a genome to be specified, so I created a custom genome by uploading the fasta file Zm-B73-REFERENCE-NAM-5.0.fa.gzfrom the page linked above. I ran NormalizeFasta as suggested to create a uniform line length of 80 and created a custom genome, maize_v5. I went back in my history and tagged the gff3 file, the reads, and the RNA Star outputs as being associated with maize_v5 and ran it again, but the same errors occurred.\nWhat should I do next to troubleshoot this problem?\nThanks!"}, {"role": "system", "text": "Hi @Claire_Milsted\nSome tools only accept GTF. Some also accept GFF3 and those will have options on the tool forms where you can change the attributes to use when counting up reads versus features in the annotation.\nFeaturecounts happens to accept GFF3. So you need to do some adjustments.\nScreen Shot 2023-08-17 at 12.21.19 PM1954\u00d7564 73.6 KB\nConverting to GTF is also an option. See Working with GFF GFT GTF2 GFF3 reference annotation\nThis forum also has a LOT of troubleshooting about reference annotation. Search with tool names or datatypes. I also added some tags.\nFor the reference genome, see Reference genomes"}, {"role": "user", "text": "Upon closer inspection I think the problem might be with the column names in the gff3 file. In the drosophila gtf file, which worked, the column containing feature types is called \" Feature\", but in the maize gff3 file, which didn\u2019t work, the column is called \u201cType.\u201d I don\u2019t know how to fix this.\nWhen I try to convert to GTF using gffread, I get an error saying \u201coverlapping duplicate BED feature (ID=mRNA)\u201d"}, {"role": "user", "text": "Update: It seems to have worked! I was able to get some kind of reasonable output by just setting GFF feature type filter to \u201cgene\u201d and GFF gene identifier to \u201cID.\u201d It didn\u2019t work before when I used \u201cexon\u201d and \u201cID,\u201d because the exon columns don\u2019t have an \u201cID\u201d assigned to them, only a \u201cParent.\u201d Since what I\u2019m interested in is just the gene counts, I\u2019m happy with this result. Thanks for your hint to focus very carefully on the feature type and gene identifier terms."}], [{"role": "user", "text": "Hello,\nI was running RNA STAR and got this error: |Job Start Time|2022-09-15 23:24:13|\n\nimage886\u00d7113 3.22 KB\n\n\nFATAL:   could not open image /cvmfs/singularity.galaxyproject.org/all/mulled-v2-69a6f67cb46e41b4c393f71634a9956d5e31f3e9:8047ce78ee60f6a1f4390a36cee78a99371e4f59-0: failed to retrieve path for /cvmfs/singularity.galaxyproject.org/all/mulled-v2-69a6f67cb46e41b4c393f71634a9956d5e31f3e9:8047ce78ee60f6a1f4390a36cee78a99371e4f59-0: lstat /cvmfs/singularity.galaxyproject.org: transport endpoint is not connected\n\nWhich is an error that I see has been popping up from last month. I am wondering if there is an issue with the cvmfs dependency once again.\nPlease let me know if there is a different issue that I can try on my end. Otherwise, I would like to know when I should try re-running the workflow.\nAll the best,\nChristine"}, {"role": "user", "text": "Hello,\nI ran the same workflow with the same datasets this morning: |Job Start Time|\t2022-09-16 11:38:29\nand STAR was able to successfully finish mapping! It did not run into the same cvmfs error.\nI don\u2019t know how the error resolved itself, but I wanted to let you all know.\nThanks!\n-Christine"}], [{"role": "user", "text": "Update: The server is back up! Please rerun any failed jobs.\nThe UseGalaxy.org server is offline right now, and no one can log in. We expect the unscheduled maintenance event to be resolved within a day. Updates will post back to this topic.\nOnce the server is up, please rerun any jobs that failed.\n\n\nHello everyone,\nKindly note that I am facing issues with jobs from yesterday. I have several jobs on queue from yesterday for Nanoplot pipeline. I have run this pipeline before as well but this had never happened\nKindly help me out and let me know how I can resolve this issue.\nThanks and Regards,\nSagar Upadhyay"}, {"role": "system", "text": ""}, {"role": "system", "text": "Thanks @Sagar_Upadhyay for reporting the problems. Hopefully you have been able to rerun by now. If you run into other issues, please start up a new topic with your question. Thanks for reporting the issues!"}], [{"role": "user", "text": "Hello,\nI started a local galaxy on a local server. I was able to insert the local reference genomes for mapping in Bowtie2 and BWA-MEM along with doing indexes for it. Mapping works great and without problems.\nI wanted to do variant calling for created BAM files (FreeBayes or VarScan) and I have a problem because this local reference genome that I used for mapping does not appear in these tools. If I make a variant calling on the file hg19.fasta from history, then the numbering unfortunately does not match.\nHow to make these reference genomes (from Bowtie2 or BWA) available in other tools?\nThanks in advance for your help"}, {"role": "system", "text": "So I assume you\u2019ve been using the corresponding data_managers to build the Bowtie2 and BWA-MEM indices?\nWhat freebayes and varscan need is a genome and its samtools .fai index. There is a dedicated data manager for this type of index, too, and it\u2019s called sam_fasta_index_builder. Run that on your ref genomes and they should appear in freebayes, varscan, and quite some other tools.\nBest,\nWolfgang"}], [{"role": "user", "text": "Hi,\nI installed the Bowtie2 index builder/data manager from the toolshed (Galaxy | Tool Shed) however whenever I open it, I get the \u201cRequested version unavailable\u201d error at the top, and cannot select any FASTA files as input. I see that the dependencies are resolved for the data manager in the \u201cManage Dependencies\u201d tab of the admin panel (only dep. is bowtie2). I am using index builder v2.3.5.1+galaxy1. Any ideas why this may be? I actually have the same issue with another data manager as well on my system, so basically both of my data managers have that error.\nThanks!"}, {"role": "system", "text": "Are you trying to use this within a workflow?"}, {"role": "user", "text": "Thanks for your reply @astrov . No I am not, I am just using it directly from the Admin backend. In fact this happens with all types of data managers, not just bowtie2, and seems to happen on many other public servers too o I am not alone with this issue."}, {"role": "system", "text": "@phagepower you can ignore this for the moment. It\u2019s a Galaxy bug that we need to fix. But harmless."}], [{"role": "user", "text": "Hello\nI have an expression matrix and I want to move some of its rows with the help of Galaxy tools\nWhat tool should I use?"}, {"role": "system", "text": "Hi @maryam-gh99\nDatamash, Reverse, and Transpose are commonly used tools to manipulate a tabular matrix.\nOr, try a Filter or Select tool (there are a few versions available). You can \u201cstack\u201d results back into one file with Concatenate.\nIf you are familiar with regular expressions, tools like Sed and Search in textfiles (grep) are more options.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Dear all,\nthe function annotate my IDs failed because of a server problem with following message:\n\nob submission failed\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method\n\u2026\nCan anyone help me please?\nThanks in advance"}, {"role": "system", "text": "HI @Bernhard\nFirst, try another rerun (the EU will do one rerun automatically). This could be a simple transitory server-side fail. There is another post (different tool) that seems related. See below.\nThere could also be an input problem. There is much discussion at this forum about the tool. I added a tag to your post that will find most of it.\nShortlist of checks \u2013 details for these and more in other topics:\n\nAre the inputs green datasets that actually contain content?\nDoes the ID type/format match the parameter settings?\n\nIf all seems ok, please post back a 1) screenshot of the tool form (rerun icon will bring up the original run), the 2) expanded input dataset in the history panel, and a 3) sample of data (top of the file is best \u2013 click on the eye icon).\nLet\u2019s start there, thanks!\n\nPing EU admins @gallardoalba @bjoern.gruening @wm75 (also see: The server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method)"}, {"role": "system", "text": "@jennaj , I can\u2019t reach the usegalaxy.eu, and https://status.galaxyproject.org reports outage in europe server. Would that be the problem?"}, {"role": "system", "text": "We\u2019ve been having issues with job submission on usegalaxy.eu for several hours.\nHopefully, things are back to normal now."}], [{"role": "user", "text": "Hello, I wanted to know if it is possible to transfer several items in one history to a new history in another account?\nI understand that it is possible to copy and move items from one history to another.\nBut I wanted to know if it is possible to transfer between two accounts\u061f"}, {"role": "system", "text": "Hi @maryam-gh99\nJust in case: all public Galaxy servers usually do not support multiple registrations. It is OK to have accounts on different Galaxy servers, but only one account per server.\nYou can copy several datasets into a new history and share it either using a link or directly with another user, if you know the registration email. To share the history, go into History menu (an icon at the top right corner of the history panel (Show history option) > Share or Publish.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hey guys,\nFirst time posting here. I\u2019m running an RNA-seq analysis but I am stuck at the Limma-voom step.\nHere\u2019s the error I am getting:\nError in eval(ej, envir = levelsenv) : object \u2018Control_Old\u2019 not found\nCalls: makeContrasts \u2192 eval \u2192 eval\nExecution halted\nI just genuinely can\u2019t understand how it isn\u2019t found. I double checked the spelling and everything."}, {"role": "system", "text": "Hi, did you ever work out the solution to this problem? I\u2019m having the same issue.\nThanks!"}, {"role": "system", "text": "\n\n\n Matthew_Juber:\n\nError in eval(ej, envir = levelsenv)\n\n\nHi @EmWhat\nThis error comes up when the tool cannot build a data structure properly during processing. This is an R error, so R formatting requirements need to be met.\nThe tool form has examples/detailed help for each of these:\n\nAt least two groups are input\nEach group contains at least two count inputs\nThe same number of count files were input per group\nLabels for groups are all \u201coneword\u201d or \u201cone_word\u201d. Alphanumeric characters plus (optionally) underscores.\nIf you are inputting a matrix instead of individual count files, the same labeling rules would apply.\nSame for contrast inputs \u2013 full group labels separated by a dash \u201c-\u201d.\nWhitespace or any other characters will cause interpretation problems.\n\nIf you can\u2019t find the problem, please post back either a shared history link or a screenshot of the job details page for one of the errored outputs (specifically, the portion of the log where the tool version and inputs/parameters are listed out). How to do both is explained here troubleshooting-errors\nLet\u2019s start there \nRelated prior Q&A:  edgeR error in galaxy"}], [{"role": "user", "text": "Hi all,\nI am not able to find Bismark aligner for WGBS data on usegalaxy.org. Can you please tell how I can install it or how to access it?\nThanks!!\nAbishek"}, {"role": "system", "text": "\n\n\n srinivasa_abishek:\n\nBismark\n\n\nHi @srinivasa_abishek\nPlease use the tool suite at https://usegalaxy.eu"}], [{"role": "user", "text": "Dear Colleagues\nI would like to ask you an advice for best commercial server that can be used to analyze RNA seq. data. This is due to long time queue waiting for my analysis that I am conducting.\nThanks in advance for any advice\nAdnan"}, {"role": "system", "text": "Hello @Adnan\nAWS is one of the most popular choices. It requires very little set-up configuration (an AWS account) and all of the Galaxy administration is done through a web interface. AWS also offers grants for academic research projects.\nAll Galaxy choices:\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n\u2026and a bit more detail about how AWS and other options are used for teaching \u2013 may be informative when making your own decision for research work: https://galaxyproject.org/teach/computing-platforms/\n\n\nThanks!"}], [{"role": "user", "text": "Could Galaxy/HicExplorer includes samtools for bam files merge? Or is there a tool for merging bam files in HicExplorer? Hi-C datasets always contain multiple runs. Thanks"}, {"role": "system", "text": "Hi Chen,\nare you referring to hicexplorer.usegalaxy.eu? If so you can use this tool: Galaxy | Europe\nHope that helps,\nBjoern"}], [{"role": "user", "text": "Wondering if anyone could give a bit of advice on my situation. I have rnqseq ngs data for 6 different human cell lines. Each has a normal and a stressed condition. I have featurecounts for each of those that are the summation of the replicates, so 12 summary count files. I have successfully compared each matching line stressed-normal but was wondering if there was a simple way to visualize possible significant genes when comparing each line to another (eg: instead of just Line1 stressed vs Line1 normal, I could easily plot all of the top 20 or so genes of significance by logFC and visualize overlaps that may exist between Line1 normal and any of the other lines, Line1 normal to Line3 normal. One large multi-colored plot type? Instead of running limma on each possible comparison (66 possible pairwise comparisons, which would be a crazy amount of data to then manually try to find overlaps within).\nWe are looking for some very early preliminary hints as to comparisons between these lines with regard to up and down regulated similarities. Would obviously dig into individual comparisons in a different way after but was hoping to some sort of visualization plot method without having to run each comparison individually and then seeking out common top genes that overlap. Hope that makes sense. Tried to solve this for myself prior to asking here."}, {"role": "system", "text": "Hi @philfry9\nare you after a heatmap style plot or interactive representation?\nPlease check limma-voom Report (dataset #16) in this history: Galaxy | Australia | Accessible History | rna-seq-2\nThe input was six conditions, two replicates each, but only two contrasts were used (two comparisons). Scroll down and check heatmap output (pdf) and Glimma interactive MDPlot. Click on any DE gene on the interactive plot to see expression in other conditions.\nAre you after something like this?\nThe original tutorial: 2: RNA-seq counts to genes\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear,\nI was running Cufflinks with my BAM files. I have done it with success for the first time. However, when I try to do the second file with the same setting it comes out a wronging alert:\n\u201cRemote job server indicated a problem running or monitoring this job.\u201d\nI have searched the similar error in Galaxy Help. If I understood it right, the file formats and size or the server problems may cause this error. The second time I apply a 6Gb BAM file, which is slightly larger than my first time BAM file. Is there anything I should notice but I missed or it could be the server error?\nThanks for your reading! It would be nice to have your response!\nSincerely,\nSzu Shuo"}, {"role": "system", "text": "Hi @userlee7655\nAll of the older Tuxedo tools are in a deprecated state but some still work. This includes Tophat/Tophat2, all Cuff* tools, and cummeRbund. Tophat for Illumia, in particular, should definitely be avoided. Cuffdiff does have a known issue with the SQLlite output option (the input for cummeRbund) and that will not be fixed. There are probably other issues with that tool and/or other deprecated tools.\nWe strongly recommend updating to use current/supported (eg: maintained) differential expression tools/methods. Tutorial links below, plus an FAQ that covers the various usage issues that can come up and be addressed \u2013 many will apply to the tools/methods you are using now \u2013 as well as other tools/workflows. It is a list of summarized help for the most common usage errors.\nThat said, you could try at least one more rerun. Sometimes the particular error you got comes up due to transient cluster issues. Even if you reran already, try one more time unless you discover that there was an input problem first.\nIf it fails again, then check your inputs more carefully. If those are correct, you probably will need to move to use the current tools/methods for scientifically correct results, even if a deprecated tool does not actually error. We will not be addressing fixes for any deprecated tools and can offer only limited support if you decide to use them anyway for some reason.\nThe FAQ that includes both DE troubleshooting tips plus the current DE tutorial links: https://galaxyproject.org/support >> FAQ >> Extended Help for Differential Expression Analysis Tools\nNote: If you are using HISAT2 for mapping or decide to use it going forward, a specific setting to format the output for Cufflinks is required. The same BAM result cannot be used with both Cufflinks/Cuffdiff and Stringtie. Find the setting on the tool form under Advanced Options > Spliced alignment options > Transcriptome assembly reporting. Again, it is strongly recommended to NOT use Tophat or Tophat2 \u2013 both can produce unreliable scientific results or metadata problems that can be difficult to detect, even if the mapping job did not actually error for technical reasons. For metadata problems, sometimes you\u2019ll need to expand the mapping dataset to see the warning in a green, putatively \u201csuccessful\u201d job. A link is given to \u201credetect\u201d the metadata. Sometimes it works, sometimes not. And even if it works, there could still be content issues.\nMore troubleshooting help, summarized with more tips/links for common usage issues and solutions:\n\nPost here at the Galaxy Help forum: Troubleshooting resources for errors or unexpected results\n\nSupport FAQ. Includes help for this error and is linked both in error reports (bug icon in error datasets) and in the post above. My job ended with an error. What can I do?\n\n\nHope this advice helps!"}], [{"role": "user", "text": "Hi everyone\n    How can I get more space for hic data analysis temporary?\n\n   Thank you"}, {"role": "system", "text": "Hi @Yuliang_Wang,\nhere you can find all the information about it: Account quotas.\nRegards"}], [{"role": "user", "text": "Hi,\ni am following the tutorial at Galaxy Installation with Ansible to setup a galaxy instance using ansible.\nI am at the step afer setting up an ansible-vault where galaxy should be deployed and dependencies installed etc., unfortunately the tutorial does not take into account a situation where you are located behind a http(s) proxy as far as i can tell.\nFor the errors I got with git and pip I was able to set the proxy settings globally, but now for the nodejs install with nodeenv I am at a point where i cant seem to solve it.\nIs there some way to specify proxy information in the ansible playbook or a work around to install it (though i guess the proxy will produce more problems down the line)?\nHere is the last few outputs of ansbile and the error message for nodeenv:\nTASK [galaxyproject.galaxy : Ensure galaxy_node_version is set] ******************************************************************************************************************************\nincluded: /home/galaxy/galaxy_install_tutorial/galaxy/roles/galaxyproject.galaxy/tasks/_inc_node_version.yml for x\n\nTASK [galaxyproject.galaxy : Collect Galaxy Node.js version file] ****************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Set Galaxy Node.js version fact] ********************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Check whether nodeenv is available] *****************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Create Galaxy virtualenv] ***************************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Ensure pip is the desired release] ******************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Ensure setuptools is latest working release (<58)] **************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Install nodeenv if it doesn't exist] ****************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Report preferred Node.js version] *******************************************************************************************************************************\nok: [x] => \n  galaxy_node_version: 16.13.2\n\nTASK [galaxyproject.galaxy : Check if node is installed] *************************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Collect installed node version] *********************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Remove node_modules directory when upgrading node] **************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Install or upgrade node] ****************************************************************************************************************************************\nfatal: [x]: FAILED! => changed=true \n  cmd:\n  - nodeenv\n  - -n\n  - 16.13.2\n  - -p\n  delta: '0:04:20.971769'\n  end: '2023-01-23 12:56:49.502045'\n  msg: non-zero return code\n  rc: 1\n  start: '2023-01-23 12:52:28.530276'\n  stderr: |2-\n     * Install prebuilt node (16.13.2) .\n    Traceback (most recent call last):\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1354, in do_open\n        h.request(req.get_method(), req.selector, req.data, headers,\n      File \"/usr/lib/python3.8/http/client.py\", line 1256, in request\n        self._send_request(method, url, body, headers, encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1302, in _send_request\n        self.endheaders(body, encode_chunked=encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1251, in endheaders\n        self._send_output(message_body, encode_chunked=encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1011, in _send_output\n        self.send(msg)\n      File \"/usr/lib/python3.8/http/client.py\", line 951, in send\n        self.connect()\n      File \"/usr/lib/python3.8/http/client.py\", line 1418, in connect\n        super().connect()\n      File \"/usr/lib/python3.8/http/client.py\", line 922, in connect\n        self.sock = self._create_connection(\n      File \"/usr/lib/python3.8/socket.py\", line 808, in create_connection\n        raise err\n      File \"/usr/lib/python3.8/socket.py\", line 796, in create_connection\n        sock.connect(sa)\n    OSError: [Errno 101] Network is unreachable\n  \n    During handling of the above exception, another exception occurred:\n  \n    Traceback (most recent call last):\n      File \"/srv/galaxy/venv/bin/nodeenv\", line 8, in <module>\n        sys.exit(main())\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 1079, in main\n        create_environment(env_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 956, in create_environment\n        install_node(env_dir, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 721, in install_node\n        install_node_wrapped(env_dir, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 743, in install_node_wrapped\n        download_node_src(node_url, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 570, in download_node_src\n        dl_contents = io.BytesIO(urlopen(node_url).read())\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 606, in urlopen\n        return urllib2.urlopen(req)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 222, in urlopen\n        return opener.open(url, data, timeout)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 525, in open\n        response = self._open(req, data)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 542, in _open\n        result = self._call_chain(self.handle_open, protocol, protocol +\n      File \"/usr/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n        result = func(*args)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1397, in https_open\n        return self.do_open(http.client.HTTPSConnection, req,\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1357, in do_open\n        raise URLError(err)\n    urllib.error.URLError: <urlopen error [Errno 101] Network is unreachable>\n  stderr_lines: <omitted>\n  stdout: ''\n  stdout_lines: <omitted>\n\nPLAY RECAP ***********************************************************************************************************************************************************************************\nx : ok=78   changed=8    unreachable=0    failed=1    skipped=43   rescued=0    ignored=0\n"}, {"role": "user", "text": "I managed to \u201cfix\u201d the error by running the playbook as root user after getting access from my admins.\nPreviously I was executing it as sudo allowed \u201cnormal\u201d user."}], [{"role": "user", "text": "Dear Galaxy Community\nI have recently doing LefSe analysis ,now i am facing a problem.\n\nQQ\u622a\u56fe20181203093754.png1878\u00d7871 75.9 KB\n\nI don\u2019t know what this means.\nThanks in advance for your help!"}, {"role": "system", "text": "Hi! Could you click on the little bug icon below your dataset to show the full error message, then post it here?"}, {"role": "system", "text": "Agree, the full bug message will help. Also, check under the error dataset\u2019s \u201cView Details\u201d page () and click on the stdout and stderr links. For many tools, there will be more information about how the job failed and clues to fix the problem. Lower down on this same page is a summary of the inputs/parameters for that job run \u2013 and this condensed view can also reveal usage issues that may have been missed when reviewing the tool form itself (duplicated input datasets; nested advanced parameters).\nIf the bug/stdin/stdout info not enough to solve the problem or it is later determined to be a server-side issue and not usage, contact information for the public Galaxy server http://huttenhower.sph.harvard.edu/galaxy/ is on that server\u2019s home page and also here https://galaxyproject.org/use/huttenhower-lab/."}], [{"role": "user", "text": "I tried to use the Extract MAF blocks tool via https://usegalaxy.org/ website.\nAs the interval, I chose a BED file of hg19 assembly that I have uploaded.\nAs the MAF source, I chose the \u201c100-ways multiZ new (hg19)\u201d out of the option of the \u201clocally fetched alignments\u201d.\nI executed the tool, and immediately received the following red error message:\n\u201cAn error occurred with this dataset:\nFatal Error: The MAF source specified (100_WAY_MULTIZ_v2_hg19) appears to be invalid.\u201d\nIs there anything I can do in order to solve it? Or is it a bug?\nThank you,\nSapirl"}, {"role": "system", "text": "See updated post here: Attempting to use Stitch MAF blocks tool, but lack of documentation"}], [{"role": "user", "text": "When using the FASTQ info validates single or paired fastq files  it asks if my data is single or paired in the drop down menu.\nhow do i know if my data is single or paired? im using SRR5395998"}, {"role": "system", "text": "Hi @Preet_Kaur\nplease check description of the data. For this accession it has the following:\nAll libraries were pooled and sequenced 50bp SE in two lanes\nYou are after SE here. Paired end is PE.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I did a few DESeq2 analyses and have tried to download the results file on multiple computers (Mac and Windows) and it\u2019s showing as \u201cFailed - File incomplete\u201d within a second or 2 of attempted download. Is anyone else having issues with downloading DESeq2 files? I also noticed it is now showing up as \u201cA list with 1 tabular dataset\u201d that I have to click twice to download, which it didn\u2019t before. Other generated files (DESeq2 plots and Normalized Count files) are working and downloading fine, it\u2019s only the DESeq2 results files that are immediately failing. Thanks so much in advance for any advice anyone has!"}, {"role": "system", "text": "Hi @mrossi7\nhow do you download the collection? Do you click at Download icon (an arrow pointing down) near the collection name? If yes, try download of individual datasets: click on collection name > click on dataset name > click on Download (floppy disk) icon.\nI am having problems with download of a collection, but no issue with download of individual datasets from the same collection/list. It might be permission settings, but I could not figure it out for now. The settings I have on the main server, look identical to settings on another Galaxy server, where I don\u2019t have issues with collection download.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear All,\nI wanted to perform differential expression analysis between 7 different cell lines, however, I just noticed that just the results are based on differential expression between just the first two cell lines which were input as factors. Is there a way we can perform pairwise differential expression between all cell lines and then isolate the genes which are differentially expressed between all cell lines as we can do in limma?\nI look forward to your suggestions and some help on this topic,\nBest,\nS"}, {"role": "system", "text": "Reply posted at Gitter: galaxyproject/Lobby - Gitter\nTutorials: Galaxy Training!"}], [{"role": "user", "text": "I have searched through all the threads on this topic and I have yet to find a reasonable explanation for why I continue to encounter this error. I have already run Cufflinks on my samples, but when I attempt to run Cuffmerge (cuffdiff -o /scratch/sbag/Tikshana/BANANA/cuff/Macbal_diff -b /scratch/sbag/Tikshana/BANANA/RAW/Mac.fa -p 8 -u /scratch/sbag/Tikshana/BANANA/cuff/merged_asm/merged.gtf -L leaf,root /scratch/sbag/Tikshana/BANANA/map/Top_Macbal_6127/accept\ned_hits.bam, /scratch/sbag/Tikshana/BANANA/map/Top_Macbal_6141/accepted_hits.bam), I get the following error:\nError (GFaSeqGet): end coordinate (46954453) cannot be larger than sequence length 46622217\nGffObj::getSpliced() error: improper genomic coordinate 46954453 on chrUn_random for TCONS_00084296\nPLEASE HELP ME TO CLEAR THIS\u2026"}, {"role": "system", "text": "@tikshana_yadav\n\n\n\n tikshana_yadav:\n\nimproper genomic coordinate 46954453 on chrUn_random\n\n\nThis problem looks like a genome mismatch issue. Data was mapped against one version of the genome but the annotation was based on a different one? My guess is both happened to have a \u201cchrUn_random\u201d chromosome but they are different lengths, which means different content. All of the coordinates in your data will be wrong if this is true \u2013 and the tool will not necessarily error, just produce scientifically incorrect results.\nThat said, all of these tools are deprecated when used in Galaxy, as explained in your other post here:\n  \n    \n    \n    Cuffmerge error: duplicate/invalid 'transcript' feature ID=Ma03_t01040.3 \n  \n  \n    I have searched through all the threads on this topic and I have yet to find a reasonable explanation for why I continue to encounter this error. I have already run Cufflinks on my samples, but when I attempt to run Cuffmerge (cuffmerge -p 8 -g /scratch/sbag/Tikshana/BANANA/RAW/Mac.gff3 -s /scratch/sbag/Tikshana/BANANA/RAW/Mac_genome.fa /scratch/sbag/Tikshana/BANANA/cuff/assemblies_Macbal.txt -o /scratch/sbag/Tikshana/BANANA/cuff/Macbal_merge), I get the following error: \nGFF Error: duplicate/in\u2026\n  \n\n\nWhy and the alternatives are covered in this prior Q&A:\n\n  \n    \n    \n    Cuff Suite Not Running -- Deprecated Tool Suite \n  \n  \n    Cufflinks, Cuffcompare, and Cuffnorm stopped running. I tried resume paused jobs but still paused.\n  \n\n\nIf you want to try to troubleshoot line-command usage, there was a google group \u2013 I don\u2019t think it is very active anymore but you may find prior help. Biostars, Seqanswers, and other general bioinformatics forums will also have much prior discussion. These are usually common enough that even just a basic web search with the error message quoted will find help.\nBut avoid using the tools in Galaxy, or expected problems.\nThanks!"}, {"role": "user", "text": "Solved by checking all paths and file name, one negligible mistake might abandon the whole experiment.\nMake it simple by checking all file name and location is deeply suggested"}], [{"role": "user", "text": "The tool is still running, and the files I passed are quite large, but is 5 days normal?"}, {"role": "system", "text": "Hi @mmotoc\nSome jobs and tools will queue then execute longer than others. The server will process the work when shared resources are available.\nThe UseGalaxy.eu server was likely just busy. https://status.galaxyproject.org/\nFAQs:\n\nMy jobs aren\u2019t running!\nUnderstanding job statuses\n"}], [{"role": "user", "text": "Hi everyone,\nI have the raw illumina paired-end .fastq reads from a whole genome sequencing of my own genome done by a sequencing company. All the raw reads are about 50Gb in size. Would I be able to use the https://usegalaxy.org/ free site to do the FastQC assessment, adapter/read trimming, alignment to the reference genome and some basic form of variant calling using just the free site alone? I have a biology background and have some experience using computational biology tools, but I have never tried to assemble my own genome and I hope this could be a good learning experience for me. Does Galaxy have a tutorial for doing an assembly to hg19 reference genome using my own whole genome sequencing data?\nThank you so much for the help."}, {"role": "system", "text": "Hi @antomicblitz\nThe GTN tutorials would be a good place to start. Search with keywords like tool names and datatypes or search by domain. If you are brand new to Galaxy, running through a few introduction tutorials will be worth it. The top of each tutorial has prerequisites and \u201cwhich Galaxy\u201d types of recommendations. Galaxy Training!\nThe GTN will be hosting a training event using those tutorials in May that you may also be interested in. Most of the materials are already available if you want to start early following those learning paths: Sm\u00f6rg\u00e5sbord 2023\nHope that helps!"}], [{"role": "user", "text": "I was running a collection of FASTA files on ALphafold2. Each FASTA file contains a single line of FASTA header and a single line of amino acid sequence. The run failed and the error message generated was \u201cCould not find HHsearch database /data/pdb70/pdb70\u201d.\n\nScreenshot from 2022-06-07 17:36:431920\u00d71080 427 KB\n\n\nScreenshot from 2022-06-07 17:35:281920\u00d71080 415 KB\n\nWhat should I do now so that I can run the structure prediction?"}, {"role": "system", "text": "Known tool issue. See AlphaFold2 Error - Could not find HHsearch database /data/pdb70/pdb70 - #2 by gallardoalba"}], [{"role": "user", "text": "I want to upload the genome of camellia sinensis in fasta file into galaxy from NCBI datasets, please help me as I am a beginner. Ref-seq id is- GCF_004153795.1 Here is the NCBI link- https://www.ncbi.nlm.nih.gov/datasets/genomes/?acc=GCA_013676235.1&term=&utm_source=assembly&utm_medium=referral&utm_campaign=KnownItemSensor:org I also unable to upload the genome of it using weblink into ftp fies in galaxy, I have pasted the following link there but it\u2019s cannot fetch it. https://www.ncbi.nlm.nih.gov/datasets/genomes/?acc=GCA_013676235.1&term=&utm_source=assembly&utm_medium=referral&utm_campaign=KnownItemSensor:org"}, {"role": "system", "text": "Hi @Abhisek_Dey, I think for now you need to get the genome directly from the refseq ftp site.\nIt appears the resources for GCF_004153795.1 are at Index of /genomes/refseq/plant/Camellia_sinensis/latest_assembly_versions/GCF_004153795.1_AHAU_CSS_1, and I believe the genomic fasta file is https://ftp.ncbi.nlm.nih.gov/genomes/refseq/plant/Camellia_sinensis/latest_assembly_versions/GCF_004153795.1_AHAU_CSS_1/GCF_004153795.1_AHAU_CSS_1_genomic.fna.gz"}], [{"role": "user", "text": "I was using the \u201cAnnotated my IDs - FeatureCounts\u201d tool when i realize that some of my genes had a NA tag, so i went to the NCBI page to check and it said \u201cThis record was replaced with Gene ID: XXXX\u201d. Can i just replace them? or should i check somehow if this new gene correspond to my previous annotated ones?\nThxs"}, {"role": "system", "text": "Hi @CamiOsega\nThe annotation was based on the original record. Checking to see what changed would be a very good idea. It would be your decision about whether or not any specific changes could impact the larger results in a way significant for your analysis purposes (if any, besides the name label).\nNew or different transcripts associated with that record would probably be considered a significant change since that could make the result non-reproducible, even if just in a small way.\nHope that helps"}], [{"role": "user", "text": "Hi,\nCan anyone advise how to create a new tabular file in galaxy? which tool should be used?\nThanks\nAlex"}, {"role": "system", "text": "Hi Alex\nYou can use the \u201cUpload File - from your computer\u201d tool (see also: https://galaxyproject.github.io/training-material/topics/galaxy-data-manipulation/tutorials/get-data/slides.html ) and select \u2018Paste/Fetch data\u2019. This will allow you to directly enter text, e.g.:\n1 2 3\na b c\nMake sure you tick the box for 'Convert spaces to tabs which you will see, when you click on the \u2018Settings\u2019 wheel. To be on the safe side, select \u2018tabular\u2019 in the \u2018type\u2019 drop-down menu.\nNow, click on Start, and you will have you small table with 3 columns and 2 rows in your galaxy history\nI hope this helps\nHans-Rudolf"}], [{"role": "user", "text": "I am planning to analyze my 16S RNA sequences using Galaxy mothur Toolset.\nI watched your youtube Tutorial GTN Training - Microbial Analysis - 16S Metagenomics and analyzed some test samples following the\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: 16S Microbial Analysis with mothur (extended)\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBut the problem is that my PCR amplification targeted the V3V4 region of the 16s rRNA, using the primers set 341F and 806R.\nCan I still use the silvav4fasta database for alignment? Does it cover this region?\nI tried downloading the latest version of silva 138.1 but there is an error and I cannot perform the align.seqs command.\nI tried changing in silvav4 the start and end in the screen.seqs command but it didn\u2019t work as well.\nCould anyone please advise regarding this matter?\nThank you!!"}, {"role": "system", "text": "Hi @Funnyme186,\n\n\n\n Funnyme186:\n\nCan I still use the silvav4fasta database for alignment? Does it cover this region?\n\n\nyou should generate a customized datase to our region of interest using the  pcr.seqs command on the  mothur-compatible dataset: Silva reference files.\n\n\n\n Funnyme186:\n\nI tried downloading the latest version of silva 138.1 but there is an error and I cannot perform the align.seqs command.\n\n\nWhere did you get this dataset from?\nI recommend you to have a look at the following resources, which could help you to solve some your your doubts:\n\nMothur MiSeq SOP tutorial\nGalaxy training 16S Microbial analysis tutorial\n\nRegards"}], [{"role": "user", "text": "Dear all,\nI\u2019d trying to configure PAM auth in galaxy 20.05.\nI\u2019d like users to enter their linux credentials and make Galaxy register them automatically.\nI\u2019ve created a auth_conf.xml file like:\n\n< ?xml version=\u201c1.0\u201d?>\n< auth>\n< authenticator>\n< type>PAM< /type>\n< options>\n< auto-register>True< /auto-register>\n< pam-service>system-auth-ac< /pam-service>\n< login-use-username>True< /login-use-username>\n< /options>\n< /authenticator>\n< /auth>\n\nI then go  to the login page and use my credentials and I see the error:\n\n\u201cUncaught exception in exposed API method:\u201d\n\nLog say:\n\nrun.sh: galaxy.auth.providers.pam_auth DEBUG 2020-07-30 12:51:03,291 [p:7289,w:2,m:0] [uWSGIWorker2Core2] PAM authentication successful for diazalbe\nrun.sh: galaxy.auth.util DEBUG 2020-07-30 12:51:03,300 [p:7289,w:2,m:0] [uWSGIWorker2Core2] Email: None, auto-register with username: bob\nrun.sh: galaxy.web.framework.decorators ERROR 2020-07-30 12:51:03,301 [p:7289,w:2,m:0] [uWSGIWorker2Core2] Uncaught exception in exposed API method:\nJul 30 12:51:03 inhas06030 run.sh: Traceback (most recent call last):\n\nand the exception:\n\nrun.sh: if not(VALID_EMAIL_RE.match(email)):\nrun.sh: TypeError: expected string or bytes-like object\n\nthis is a new user but Galaxy is already expecting an email for it. That means that I need to enable user registration, register my linux account and associate an email to it (I use the very same pasword) and then login works fine.\nMy question is: is there a way to make Galaxy register any new user with valid linux credentials?\nThanks"}, {"role": "system", "text": "Hi Pelacables,\nI think you need to set login-use-email to False and also specify a\nvalue for maildomain in your auth_conf.xml file. Basically, the add the following 2 lines:\n<maildomain>example.com</maildomain>\n<login-use-email>False</login-use-email>\n\nLet us know if this resolves your issue.\nThanks,\n-Kaivan"}], [{"role": "user", "text": "Hi everyone!\nI followed the \u201cCalling variants in diploid systems\u201d tutorial step by step. I try to annotate my variants, but I can\u2019t find the SnpEff tool. I checked every Snp related tool but none of them have the parameters as same as in the turorial. Can anyone help me with that?\nThanks for your support in advance!"}, {"role": "system", "text": "We discussed about that few days ago.\nIt isn\u2019t found when you search for it. look at the section \u201cVariant Calling\u201d without any search term entered in the search box, and you will find it ."}], [{"role": "user", "text": "Dear Community,\nI was wondering if someone could help me out. I am trying to use RStudio instance via Galaxy.eu; however, I can never install Seurat (some other packages are fine). Seurat has a dependency package png which I believe causes this problem as it also cannot be installed. Error message: /bin/bash: libpng-config: command not found\nMaybe someone from the Galaxy Team can address it?\nThank you!\nAya"}, {"role": "system", "text": "From Gitter:\n\nadeellqp\n@adeellqp\nDec 24 08:37\nHello\nI want to do WGCNA in Rstudio in usegalaxy.eu but could not install the dependency \u2018png\u2019. Can anyone help me?\nBj\u00f6rn Gr\u00fcning\n@bgruening\nDec 24 09:50\n@adeellqp could you try again?\nBut I doubt this will work, we would need to add libpng to the image\nCould you use Jupyter Notebook for the time being? This also has an R kernel.\nadeellqp\n@adeellqp\nDec 24 20:21\n@adeellqp could you try again?\n\nThanks\nI have tried many times it says that libpng is not available.\nI will try to use Jupyter Notebook.\n"}], [{"role": "user", "text": "I am facing the same issue now\u2026I have been using galaxy for post 6 yrs w/o any issues however, recently it keeps giving me issues\u2026and mostly around weekends. I don\u2019t understand why none of the functions work\u2026all my tasks stay in grey for hours\u2026it was never like this before"}, {"role": "system", "text": "I also have the issue of nothing running for two days now. I hope that things will start moving again soon."}, {"role": "system", "text": "Hello @Kashish_Kumar @Isha_Singh\nThere was a cluster error over the weekend that has been corrected.\nWhat to do (for everyone who had problems over the weekend):\n\nIf your job failed (red dataset) \u2013 try rerunning now.\n\n\n \u2192 All jobs that failed Upload or other tools that transfer external data into Galaxy will need a rerun.\n\n\nIf your job is queued (gray dataset) \u2013 please leave your job queued for fastest processing. You can decide to delete and rerun, but know that all new jobs start at the back of the queue.\nThere is a small backlog of jobs, and those should process sometime today.\nPlease try at least one rerun for any failures.\nIf reruns started after now also fail, you can submit bug reports for confusing errors.\n\nThanks for reporting the problem!"}], [{"role": "user", "text": "Dear all,\nAfter I executed Trinity, I obtained 2 files as assembly transcripts.fasta and log.txt files.\nI would like to know that is it possible to download \u2018Trinity.fasta.gene_trans_map\u2019 from Galaxy Trinity. It is important for my downstream analysis.\nThank you so much,\nKamoltip"}, {"role": "system", "text": "\n\n\n Kamoltip:\n\nTrinity.fasta.gene_trans_map\n\n\nHi, This report is not currently output from Trinity as wrapped for Galaxy at https://usegalaxy.org, but you can parse out the information from the fasta results.\nTry this workflow: https://usegalaxy.org/u/jen/w/create-trinityfastagenetransmap\nThere are many ways to manipulate data. The tools I included in the workflow are simplified, and just one example of how to do this.\n\n\nReferences\nTrinity wiki that describes the fasta title line formatting: Output of Trinity Assembly \u00b7 trinityrnaseq/trinityrnaseq Wiki \u00b7 GitHub\nSample of a \u2018Trinity.fasta.gene_trans_map\u2019 file: trinity_ext_sample_data/Trinity.fasta.gene_trans_map at master \u00b7 trinityrnaseq/trinity_ext_sample_data \u00b7 GitHub\n\nThanks!"}, {"role": "system", "text": "Update:\nThe latest version of the Trinity tool, by default, now includes an additional output dataset named \u201cGene to transcripts map\u201d. Any version of Trinity at this revision or higher will produce the same additional output:\n\n\nTrinity  de novo assembly of RNA-Seq data (Galaxy Version 2.9.1)\n\nThe overall update of the Trinity tool suite also includes several other wrapped components. For the full list, please see the Main ToolShed repository: https://toolshed.g2.bx.psu.edu/view/iuc/suite_trinity/4a453a8da3b5.\nPublic Galaxy servers that have chosen to host the tool suite will generally place these tools under the RNA-seq tool panel group. Or, you can use the tool search to locate tools quickly.\nThe prior version of Trinity is considered deprecated now. At Galaxy Main https://usegalaxy.org, the final stages for that are still in progress. Once done, this ticket will close out: https://github.com/galaxyproject/usegalaxy-playbook/issues/143\nIn short, all end-users should 1) use the updated tool versions if at all possible and 2) be careful not to use the output from the older tool as an input to the updated tools.\nIf the older output is really needed for some reason, be sure to carefully review downstream tool forms and make adjustments to your data as needed. Many accept a variety of custom assembly outputs, and for some tools, the prior Trinity results should be considered to be a \u201ccustom format\u201d due to changes in key data attributes (such as fasta title lines) between the two tool versions.\nThanks!"}], [{"role": "user", "text": "Hi,\nI was wondering if it is possible to safely use sensitive data on a local server (IP address: 192.168.xxx.xxx) without SSL (installed NGINX to listen to port 80 instead of 443 and left out the nginx_ssl variables) or if there are other options to issue certificates or secure the server more without needing a DNS name, since Let\u2019s Encrypt (which is used by nginx to issue certificates) can\u2019t issue certificates for bare IP addresses.\nAny input is appreciated. Thanks in advance!"}, {"role": "system", "text": "Hi @cbass\nI\u2019ve cross posted your question to our Admin chat here You're invited to talk on Matrix\nThe community may reply here or there, and feel free to join the chat "}, {"role": "system", "text": "Heya @cbass. Yeah you have some options\n\nno ssl, the easiest, will generate warnings when people enter passwords because it\u2019s insecure\nself-signed ssl certificate, will generate warnings in the browser because it\u2019s self signed\n\nIt\u2019s really up to you what\u2019s appropriate. If your local network is controlled, no one can access the network without e.g. a wifi password, then you might be ok to skip SSL as you can trust the devices on your network. It\u2019s not generally a good idea, but knowing about your specific case it might be fine and easier than dealing with everyone\u2019s browsers not trusting a certificate."}], [{"role": "user", "text": "Dear all,\nthe output data of featureCounts have a header containing the data title and the same appears in line 1 again, too. Therefore DESeq2 doesn\u2019t run. Has anyone an idea to solve that?\nThanks in advance,\nBest Bernhard"}, {"role": "system", "text": "Dear @Bernhard,\nyou can use this tool Remove beginning of a file.\nCheers,\nFlorian"}], [{"role": "user", "text": "Hi, I was wondering if the percentage in the overrepresented sequences section of the FastQC output is really a percentage, as in 5% would show as a 5? Or would 5% show as 0.05?\nThanks you!"}, {"role": "system", "text": "Hi @gperron\nThese are true percentages with as many significant digits as there is space to write them out.\nYour example\n\nFive percent == 5.0\nPoint zero five percent == 0.05\n\nReal data, graph view\n\nOne point five five percent == \u201c1.5504756818485261\u201d\n\nSame data, raw view\n>>Overrepresented sequences\tfail\n#Sequence\tCount\tPercentage\tPossible Source\nCGGTGCTCGACCCCTCCGACCCCCGCCGGCCGCTTCGAGCCTGAGCCCTT\t76412\t1.5504756818485261\tNo Hit\n\n\nThe usage/display in Galaxy is the same as the original tool, so the documentation linked from the bottom of the tool form can be a really useful reference wherever you are running it. There is per-module help plus example reports of \u201cgood\u201d and \u201cbad\u201d data and other common use cases.\nBabraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence Data \u2192 Index of /projects/fastqc/Help \u2192 Overrepresented Sequences\nThis module lists all of the sequence which make up more than 0.1% of the total. To conserve memory only sequences which appear in the first 100,000 sequences are tracked to the end of the file. It is therefore possible that a sequence which is overrepresented but doesn't appear at the start of the file for some reason could be missed by this module."}], [{"role": "user", "text": "Hi there! I have been running differential expression analysis on a lot of datasets and I have been computing z scores to identify significant genes for a few weeks now and everything was working fine when suddenly, no matter what tool I use (deseq, hisat2, table compute, filter etc), it gives me the same error: condor_submit failed. I have been using the same inputs for weeks now with no issue. The error occurred when I was calculating z scores from a normalised counts file (which I had done literally 10 seconds earlier with the same inputs, working just fine). Any idea what is happening?"}, {"role": "system", "text": "Hi @Ioanina\n\n\n\n Ioanina:\n\ncondor_submit failed\n\n\nThis is a cluster failure. You should contact the administrators of the server where you are working.\nContact information was likely included in the original account activation email. If this was a public server, also check the homepage of the server, or in the known public Galaxy server directory listing here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub.\nIf you can\u2019t find the contact, please post back the URL of the server and we might be able to help more here."}, {"role": "system", "text": "We had a temporary issue on usegalaxy.eu yesterday, which resulted in this error for all new jobs over maybe half an hour, or so.\nIf you haven\u2019t done so yet, you can now rerun each of your failed jobs and they should complete as expected again."}], [{"role": "user", "text": "<can\u2019t use my 250GB>\nHi Guys\nI am using galaxy through my university cluster maxgalaxy.abdn.ac.uk. As a registered user (as far as I know) that I have only 250GB to use. The issue is that even after deleting all histories/data still I am using 94% of the 250GB. I permanetely deleted everything but still the issue. Any way to free up my (250GB)? or whom to contact to solve this problem?\nThanks\nMarwa"}, {"role": "system", "text": "Hi Marwa,\nare you using Usegalaxy.eu server or another Galaxy server available in your University?"}, {"role": "system", "text": "This tutorial describes how to delete old data to make more room in your account."}], [{"role": "user", "text": "On this page https://galaxyproject.org/develop/resources-tools/\nThis link is broken:\nA tool interface generator is maintained at http://cli-mate.lumc.nl.\nIt throws this error:\n\nInternal Server Error\nThe server encountered an internal error or misconfiguration and was unable to complete your request.\nPlease contact the server administrator at I.F.A.C.Fokkema@LUMC.nl to inform them of the time this error occurred, and the actions you performed just before this error.\nMore information about this error may be available in the server error log.\nIt would be great to have a quick contact link on each page to notfy the site admin of broken links."}, {"role": "system", "text": "Hi @mamiller\nThank you for bringing this to our attention. The service appears to unsupported at this time.\nI\u2019ve updated the Galaxy Development resource reference to designate the external service as currently unavailable. We may remove it entirely. If you want to contact them directly for status, try the email address or post an issue at their GitHub repository.\n\nUnavailable June 2020: A tool interface generator at http://cli-mate.lumc.nl. Github last update Aug 15, 2014 https://git.lumc.nl/humgen/cli-mate.\n\nRegarding:\n\n\n\n mamiller:\n\nIt would be great to have a quick contact link on each page to notfy the site admin of broken links.\n\n\nThe galaxyproject.org website offers two direct options for this: 1) propose a change/fix/update PR or open an issue on Github or 2) contact us via the Gitter chat to alert us about problems.\n\ngalaxyproject-org-chat-github1914\u00d7668 125 KB\n\nThanks!"}], [{"role": "user", "text": "I am a plants.usegalaxy.eu/ user and my id is jvd.choudhary@gmail.com. I am working on miRNA differential expression, for that, I use the miRdeep2 mapper, miRdeep2 quantifier workflow and generated the quantifier tabular output, further I need to use the tool for differential expression like DeSeq2 or edgeR.\nTo calculate the differential expression DeSeq2 or edgeR need the replicate data. Although, I have the single set of miRNA data \u201ccontrol\u201d and \u201ctreatment\u201d without replicate. I know, replicate dataset must require for genuine calculation, but I don\u2019t have the choice.\nIn plant.galaxy, I tried to use some alternative tools also, but I couldn\u2019t get the output. From the literature support I got to know edgeR can calculate the DE from the single set of data, but plant.galaxy it couldn\u2019t support the single set of data. So could you please enable the options of edgeR to calculate the DE or suggest some workflow to calculate the differential expression without replicate data in galaxy hub?"}, {"role": "system", "text": "Hi @jvd.choudhary,\nsorry, but Galaxy restricts edgeR to at least two replicates. You can find more information about this topic in the following post: DESeq2 or EdgeR or Limma without replication - #7 by jennaj\nRegards"}], [{"role": "user", "text": "Hi\nI try to run samples based on the Reference-based RNA-Seq data analysis tutorial and meet a problem in Deseq2 part. it\u2019s three dataset collection and i check the tabular of two sample featurecounts. Don\u2019t know how to solve it.\n\nScreenshot 2023-04-28 171409616\u00d7693 43.9 KB\n\nHere is the error log:\nError in data.frame(sample = basename(filenames_in), filename = filenames_in,  :\nduplicate row.names: /data/dnb08/galaxy_db/files/8/b/0/dataset_8b056f6c-8022-44aa-a290-fface0163b12.dat, /data/dnb08/galaxy_db/files/f/8/d/dataset_f8daec59-4cf1-456c-be52-31850ccf1cd1.dat\nThanks\nHarvey"}, {"role": "system", "text": "Hi @Harvey006\nNothing looks odd in this view. To troubleshoot more, you would need to post screenshots of most of the Dataset Details\" view. That is where the logs are located. Or, generate and post a shared history link. Troubleshooting errors\nOr, you could try reviewing the other Q&A about this tool. There is quite a bit, search with keywords or click on the tag you added. This could be something simple to fix like some minor setting in the tutorial. Retracing your steps when something goes wrong is what everyone does  and the tutorial can help. But please do post your extra data if need more help. Include the link to the tutorial step as well, and even more people can help.\nExample forum search that uses part of the error message: Search results for 'duplicate row.names' - Galaxy Community Help"}, {"role": "user", "text": "@jennaj thanks for offering help. The reason was I chose two tags to determine the dataset I wanted in every factor level. I switched to one tag and the problem was solved."}], [{"role": "user", "text": "Hi all,\nI am currently performing DE analysis starting from raw counts from an RNA-seq experiment using the Limma Voom script included in UseGalaxy and I would like to introduce age and sex as co-variates in the differential expression analysis.\nIn the menu \u201cinput information from factor file?\u201d, I have clicked yes and attached a txt file in which I have introduced the groups that I am interested in comparing in the second column, sex (coded as a discrete variable using 0 and 1) in the third column and age in the fourth column.\nThe script runs perfectly but I am not sure if I can introduce age like this, as age is a  continuous and not a discrete variable.\nCould someone confirm me if I am proceeding the right way?\nIf not, could anyone tell me the best way to proceed?\nMany thanks in advance.\nbest,"}, {"role": "system", "text": "Hi @dantedmm,\nI asked about it, and this is the response:\n\nUnfortunately, as far as I can see, it currently only handles discrete factors, not continuous variables. The factor information input gets converted to factor data type.\n\nRegards"}], [{"role": "user", "text": "I want to create a discrimination model to classify honey and fortified honey, and use it in a set of new spectral data. In the stage of creating the model, of discrimination that I am doing by FDA (Factorial Discrimant Analysis), an error appears and the model is not created. Would you know how to tell me the reason for the error?\nThe parameters I use to create the model are:\n\u201cDiscrimination\u201d\n\u201cFDA\u201d Factorial Discrimant Analysis\nSelect X data: xCAL(CovSel on (PCA on SNV(xCAL.tab):scoresXyCAL.tab(c2)):Xdata(30varsel))\nSelect Y data: yCAL(yCAL.tab)\nColumn of y class chosen for the calculation: c2: V1\nNumber of blocs for cross-validation: 10\nNumber of discriminant variables(DV): 10\nDistance choice: Mahalanobis\nExport confusion tables (CAL) for each discriminant variables (DV) models: no\nStoud: Free Access Chemometric Toolbox (FACT) version 0.10\ngetting started at: http://atoms.scilab.org/toolboxes/FACT\nLoad macros\nLoad gateways\n!\u2013error 4\nUndefined variable: test\nat line      46 of function conj2dis called by :\nat line      31 of function cvclass called by :\nat line       9 of function fda called by :\nres_da=fda(x,y.d(:,ycol),10,10,0);\n                        !--error 4 \n\nUndefined variable: res_da\n                        !--error 4 \n\nUndefined variable: res_da\n                                                             !--error 248 \n\nWrong value for argument #2: Valid variable name expected.\nStderr : Wrong value for argument #2: Valid variable name expected.\nThe URL is https://vm-chemflow-francegrille.eu/"}, {"role": "system", "text": "\n\n\n MarciaCarvalho:\n\nSelect X data: xCAL(CovSel on (PCA on SNV(xCAL.tab):scoresXyCAL.tab(c2)):Xdata(30varsel))\nSelect Y data: yCAL(yCAL.tab)\n\n\nI am not familiar with this tool but if the above are inputs then you might want to first double check the syntax. The error describes  Wrong value for argument #2: Valid variable name expected. which tells me that you might have provided a bad variable name.\nYou should also look into the tools arguments and figure out which argument is #2. The administrator of the galaxy instance you are using should be able to help you with that."}], [{"role": "user", "text": "Hi to all the comunity!\nI was wondering if there is the possibility to add redundans to Galaxy Europe.\nThis pipeline has an unofficial conda package (conda install -c genomedk redundans), I test it briefly and it works properly.\nThanks in advance!"}, {"role": "system", "text": "Hi @Agustin_Baricalla,\nthe recommended procedure for requesting a new tool is to open an issue in the IUC repository; this is an example. Would you mind opening it?  Also, in case you have some time, I encourage you to try to create the wrapper by yourself (it would be extremely useful for the Galaxy community  ). There are some supporting materials that would help you with that, such as this short tutorial.\nRegards!"}], [{"role": "user", "text": "hello,\nI wonder why my jobs is still running for some topics up to a long time (5 days). I know that trimmomatic doesn\u2019t need much CPU and memory, so do you know why this is the case?\nthanks for your help !"}, {"role": "system", "text": "Hi @berger_juliette\nAre these jobs queued (gray) or executing (yellow)? understanding-job-statuses\n\nA single trimmomatic job that has been queued for five days is probably Ok.\nA single trimmomatic job that has been executing for five days would be unusual.\n\nJobs run after any upstream jobs complete (inputs for the tool are ready to use) and as public shared resources appropriate for the tool become available. Some of your jobs run, then some of other people\u2019s jobs run, then more of yours, repeat. Using workflows can significantly speed things up.\nIf that doesn\u2019t help, would you please confirm the public server URL? The UseGalaxy.eu server is up: https://status.galaxyproject.org/. You could also share some screenshots. The top of the Dataset Information page (  icon within any dataset) has some timestamps. The history with expanded datasets (inputs + outputs) can also be informative."}], [{"role": "user", "text": "Hi, when I run an imported workflow, it gives me this output: Following tools missing: toolshed.g2.bx.psu.edu/repos/iuc/phyml/phyml/3.3.20190909. A user in Europe created the workflow and I am using usegalaxy.org."}, {"role": "system", "text": "Hi @rinku2711\nIf possible, consider running the workflow at the same server it was developed. This might be where the workflow link was shared from (?)\nOr, you can use the tool at either UseGalaxy.eu or UseGalaxy.org.au.\nUseGalaxy.org does not have this tool installed (yet). I\u2019ve asked our tool development team to review. Please follow the issue ticket for updates Install phyml 3.3.20190909 \u00b7 Issue #570 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub\nThanks!"}], [{"role": "user", "text": "As the topic name states, I am wondering if it\u2019s possible to somehow pass files during the Galaxy application startup instead of uploading them? Files that I want to upload are on remote serve and it\u2019s not possible to upload them with browser upload feature (on the same host I will run Galaxy application)\u2026 Also, I would like to make a docker image from your latest stable version and it would be best if I somehow can mount all those files into the galaxy docker container."}, {"role": "system", "text": "Hi @heisenberg\nLocal file browsing is only one way to get data into Galaxy.\n\nUpload with public remote file URL\u2019s. This works at any Galaxy server.\nAdd files to Data Libraries. This works at Galaxy servers where you are an admin. Scripts & Tricks \u2014 Galaxy Project 22.05.1 documentation\n\n"}], [{"role": "user", "text": "Hi everybody,\nI am new un the field of bioinformatics. I am trying Variant calling on RNA-seq Data, I will appreciate some good ideas, and optional workflow, if available.\nMeny"}, {"role": "system", "text": "Hi @meny.alcalay\nthis topic was discussed recently. Please search the forum.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello\nI want to get goseq done for my data but i saw this error, what this about ?\nWarning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: parallel\nAttaching package: \u2018BiocGenerics\u2019\nThe following objects are masked from \u2018package:parallel\u2019:\nclusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\nclusterExport, clusterMap, parApply, parCapply, parLapply,\nparLapplyLB, parRapply, parSapply, parSapplyLB\n\nThe following objects are masked from \u2018package:dplyr\u2019:\ncombine, intersect, setdiff, union\n\nThe following objects are masked from \u2018package:stats\u2019:\nIQR, mad, sd, var, xtabs\n\nThe following objects are masked from \u2018package:base\u2019:\nanyDuplicated, append, as.data.frame, basename, cbind, colnames,\ndirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\ngrepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\norder, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\nrbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\nunion, unique, unsplit, which, which.max, which.min\n\nLoading required package: Biobase\nWelcome to Bioconductor\nVignettes contain introductory material; view with\n'browseVignettes()'. To cite Bioconductor, see\n'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\nLoading required package: IRanges\nLoading required package: S4Vectors\nAttaching package: \u2018S4Vectors\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\nfirst, rename\n\nThe following object is masked from \u2018package:base\u2019:\nexpand.grid\n\nAttaching package: \u2018IRanges\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\ncollapse, desc, slice\n\nAttaching package: \u2018AnnotationDbi\u2019\nThe following object is masked from \u2018package:dplyr\u2019:\nselect\n\nUsing manually entered categories.\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: runGoseq \u2026 goseq -> reversemapping -> [ -> [.table -> NextMethod"}, {"role": "system", "text": "\n\n\n amir:\n\nError in [.default (summary(map), , 1) : incorrect number of dimensions\n\n\nThis looks like an input format problem or disconnected input content. Try double-checking your inputs. Extra \u201cwhitespace\u201d, empty lines, are the first item to check, and I\u2019ve shared how to find those before with you directly (here & at the galaxy-bugs mailing list, with you and others).\nA search by the tag \u201cinput\u201d at Galaxy help will list out a lot of common format/content problems in posts where format as an issue, across tools, for others reading.\nFor you @amir, I would guess that \u201cdisconnected input content\u201d is the root of the problem given the error message and since you already know how to confirm basic format. But if format checks not done yet, do that fist. After, much help is on the tool form regarding input dataset format/content, so review/compare your content versus the examples. There are columns of data between inputs and tool from entry data that ALL need to match up.\nIf that doesn\u2019t work, preserve your testing of format/content in some history, that also contains the failed run, and send me a history share link with \u201cobjects shared\u201d (direct message). I\u2019ll have somewhat limited access for admin-only details at EU since I am not an admin at the EU server, but still may be able to review what may be going wrong. If not, can ping the EU server admins, or might suggest sending in a bug report via the UI (if you do that, be sure to leave all inputs undeteled and include a link to this post for context). You can also reply to this thread that you sent in the bug report, with the Public galaxy server URL included (to confirm where this is occurring).\nThanks!"}], [{"role": "user", "text": "Hi everyone,\nI (as an admin) have to backup/download multiple histories from multipe users.\nIs there a way to do this more efficient than downloading every history manually?\nI think about the BioBlend API or something similar, is this the right way or am I completely wrong?\nBest Regards\nFabian"}, {"role": "system", "text": "Would something like this work? https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#bioblend.galaxy.histories.HistoryClient.export_history"}], [{"role": "user", "text": " How to delete rows with the same number in the bed file in usegalaxy\n (If the number of the second column is equal to the number of the third column, delete that row)\n which tool could be useful?\ninput:\nchr1 10 20 \nchr1 10 10 \nchr1 15 20 \nchr1 15 30 \nchr1 30 30\n\noutput:\nchr1 10 20 \nchr1 15 20 \nchr1 15 30\n"}, {"role": "system", "text": "HI @agrieng3,\nyou can use the Filter data on any column using simple expression  tool,  with the condition c2 != c3."}, {"role": "user", "text": "Thank you, very Good "}], [{"role": "user", "text": "Dear Galaxy team,\nI use Aspegillus niger sequence to predict its genes using Augustus.Here is the screenshot.But I get failed always.For reference I have copied the screenshot of my workflow.\nimage1920\u00d71080 314 KB\n\nCan somebody help me to get this issue  resolved please.\nBest regards,\nJagath Ranasinghe"}, {"role": "system", "text": "Hi @Jagath_Ranasinghe,\ncould you share your history with me? I already provided you my email (I\u2019m Crist\u00f3bal).\nRegards"}, {"role": "user", "text": "Dear Dr.Cristobal,\nThank you very much for your help .I shared the history in previous mail.\nBest regards,\nJagath Ranasinghe."}], [{"role": "user", "text": "Hi, I like to use the ChIPseeker to annotate my ChIP-seq result.\nMy reference genome is hg19.\nAlthough I\u2019ve tried to download it from the UCSC website, none of the downloaded files can get the analysis work.\nI don\u2019t know if my GFT file is not right for the tool.\nDoes anyone know where I can download the GTF file for my analysis?"}, {"role": "system", "text": "Hey, how did you solve this problem. I don\u2019t have available built-in GFT files as well. Don\u2019t know how to do the annotation."}, {"role": "system", "text": "@lulu and @Sheng-Chieh_Hsu\nThere is much Q&A at this site about how to obtain and format a GTF properly for use with tools. I added some tags to this post to help find it.\nThis FAQ may also help. It has a focus on DE analysis, but the reference annotation advice applies to any tool that accepts a GTF input.\n\n  \n      galaxyproject.org\n  \n  \n    \n\nGalaxy Support\n\n\n\n  \n  \n    \n    \n  \n  \n\n\n\nExtended Help for Differential Expression Analysis Tools\n"}], [{"role": "user", "text": "Hello everyone,\nI was following the tutorial to perform the MAGeCK count, but I cannot find this tool in the list.\nDo anybody knows if this tool is still available?\nThank you,\nFabio"}, {"role": "system", "text": "Which tutorial? Can you share a link/url?\nI do find the tool on https://usegalaxy.eu/. Not every training is possible on every galaxy server. To check this click the \u201cworld\u201d icon on the training page.\n\nimage1128\u00d7128 5 KB\n"}], [{"role": "user", "text": "Hello, I have used 89% of my 250GB data on my account. I deleted 94GB, but the space still shows 89% full. Can you please help me with resolving my disk space?"}, {"role": "system", "text": "\n  \n    \n    \n    Storage full despite purging all deleted files!! usegalaxy.org support\n  \n  \n    Hi @meisner \nLook under User \u2192 Histories. Open the advanced search and set \u201cstatus = all\u201d. The size of each history is the amount each is using. \nIf any of the histories are only deleted, they need to be permanently deleted (purged) to free up quota space. To find all stray deleted datasets across histories, or entire deleted histories, go to User \u2192 Preferences \u2192 Storage Dashboard and use the functions. \nAfter purging, go back to the top level page for the Storage Dashboard and click on the \u201cref\u2026\n  \n\n"}], [{"role": "user", "text": "Hello:)\nI am new to Galaxy \nI want to run the Golm Metabolome Database search on my processed GC-MS data but i dont have RI\u2026 I have to choose \u201cnone\u201d GC column, but there is no \u201cnone\u201d-option\u2026\nIf anyone knows the trick, I would be super thankful!\ncheers, Philipp\n\nimage.png1299\u00d7456 17.2 KB\n"}, {"role": "system", "text": "The algorithm behind Golm db search includes alkane RI as input data. So it not possible to unset this parameter or to set to none. There is no logic todo this using Golm Db. The Golm webservice proposes this option but it is a trap \nIf you have any question on metabolomics workflows/tools, please contact the workflow4metabolomics support at support@workflow4metabolomics.org"}], [{"role": "user", "text": "Hello,\nNew to Galaxy and Cloud AWS. Followed the instructions here (https://galaxyproject.org/cloudman/getting-started/). Got to step 1-3, where I entered the credentials of my newly setup AWS User and I get the error \u201cThe credentials you have entered are invalid.\u201d\nAny help on this would be greatly appreciated."}, {"role": "system", "text": "AWS can take up to a day to process a new account so if your account is brand new, it may take a bit of time before it is usable. Beyond that, can you please double check that the IAM user has the necessary permissions, namely EC2FullAccess and S3FullAccess."}], [{"role": "user", "text": "I would like to use the Galaxy/hutlab format to write a paper about gut microbiota.\nSo, I would like to ask four questions in order to apply to the IRB.\n\u2460If an unregistered user uploads gut microbiota data, how long does the data take to be deleted? I want to know the exact period.\n\u2461 After the data is completely deleted, will it never be restored?\n\u2462Will the data stored on the site, Galaxy/hutlab be used without the consent of the uploader?\n\u2463Will my uploaded data and analysis results are seen by anyone other than me?\nI have already browsed the following sites\uff1a\n\u30fbManaging Datasets in Galaxy - Galaxy Community Hub\n\u30fbDownloading and Deleting Data in Galaxy\nI look forward to hearing from you. Thank you."}, {"role": "system", "text": "Cross-posted here Deletion of data / Secondary use of data"}], [{"role": "user", "text": "Hi everyone,\nIs there an overview of the average or expected time it takes for every tool/algorithm from the tool panel to finish a job?\nKind regards,\nLou"}, {"role": "system", "text": "Hi @lou,\nI don\u2019t think this is possible because there are many factors involved in the running time, but you can always ask us about specific jobs. If you are using the usegalaxy.eu instance, I suggest to you post your questions about waiting times in the Gitter channel.\nRegards"}], [{"role": "user", "text": "Hi all,\nI am a beginner into RNA-Seq data analysis and was wondering if someone could explain what exactly the \u2018Specify strand information\u2019 setting from the featureCounts tool is doing. For my pair-end RNA-Seq data analysis I am following the workflow as per this tutorial. BTW, I am performing this analysis for data (biological triplicates) generated under 3 experimental conditions A, B and C. So total 9 files.\nAfter estimation of the strandedness (using the infer experiment tool) of my pair-end RNA-Seq data alignments I find that my libraries are reverse stranded (i.e. on average for all files \u201c1\u00b1,1-+,2++,2\u2013\u201d: 0.9670). Does it mean that for featureCounts I need to \u2018Specify strand information\u2019 as stranded (Reverse)?\nI have tried both combinations of Stranded (Forward) and Stranded (Reverse) with the idea that I do not want to miss out on read counts that originated from overlapping but opposite strand genes or the antisense strand of non-overlapping genes i.e. antisense strand where there are no annotated features yet on the target genome thereby \u2018discovering\u2019 potential new transcribed regions. I then created 3 dataset collections as:\n\n\u2018all counts\u2019 - containing forward and reverse count files (total 18 tabular datasets) for each sample (A, B and C).\n\u2018all forward counts\u2019 - containing only forward count files (total 9 tabular datasets) for each sample  (A, B and C).\n\u2018all reverse counts\u2019 - containing only reverse count files (total 9 tabular datasets) for each sample  (A, B and C).\n\nSubsequently, I extracted identifiers and tagged all files from all 3 dataset collections accordingly. I then performed DESeq2, filtered out genes (2-fold change and adjusted p-value<0.05). I now have 3 dataset collections each containing filtered lists (A vs B, A vs C and B vs C) i.e. DEGs from considering \u2018all counts\u2019, \u2018all forward counts\u2019 and \u2018all reverse counts\u2019. My main question is, which of these would be the correct collection to refer to? Would it be only the collection of lists obtained from considering the \u2018all reverse counts\u2019 count files?\nI would like to thank you very much in advance for your time, efforts and patience in answering my questions.\nBest regards."}, {"role": "system", "text": "Hi @agschindler\n\n\n\n agschindler:\n\nI am a beginner into RNA-Seq data analysis and was wondering if someone could explain what exactly the \u2018Specify strand information\u2019 setting from the featureCounts tool is doing.\n\n\nSetting the stand filters which mapped read will be counted up per annotation feature.\n\nReads map to a genomic regions with strand assigned during mapping (BAM).\nFeatures are placed on genomic regions with strand assigned in reference annotation (GTF, GFF3, or the built in indexes).\nFeatureCounts compares the two, and counts up reads-per-feature at the gene summary level.\n\n\n\n\n agschindler:\n\nAfter estimation of the strandedness (using the infer experiment tool) of my pair-end RNA-Seq data alignments I find that my libraries are reverse stranded (i.e. on average for all files \u201c1\u00b1,1-+,2++,2\u2013\u201d: 0.9670). Does it mean that for featureCounts I need to \u2018Specify strand information\u2019 as stranded (Reverse)?\n\n\nIf your data is stranded, then you should use stranded protocols for both mapping and counting.\nWhy? The sequencing chemistry protocol was designed to produce stranded reads. Any reads mapped or counted on the opposite strand are non-specific. That can create a lot of noise in the results, and valid results could be omitted due to the over-counting.\n\n\n\n agschindler:\n\nMy main question is, which of these would be the correct collection to refer to? Would it be only the collection of lists obtained from considering the \u2018all reverse counts\u2019 count files?\n\n\nYes, those would be the results to consider.\nFeatureCounts produces graphs describing which reads could be assigned to a feature with specificity, and when not, the reason why. Maybe examine those reports to see if you can confirm they produce the \u201cbest\u201d results?\n\n\n\n agschindler:\n\nI have tried both combinations of Stranded (Forward) and Stranded (Reverse) with the idea that I do not want to miss out on read counts that originated from overlapping but opposite strand genes or the antisense strand of non-overlapping genes i.e. antisense strand where there are no annotated features yet on the target genome thereby \u2018discovering\u2019 potential new transcribed regions.\n\n\nFor this purpose, Salmon, Sailfish and Kallisto are some choices for transcriptome-based discovery analysis."}], [{"role": "user", "text": "Hi\nI\u2019m trying to get Galaxy to work with PBS. I\u2019ve installed openpbs wit pbs/torque drmaa, configured the job_conf.xml like this:\n\n<job_conf>\n\t<plugins workers=\"4\">\n                <plugin id=\"drmaa_default\" type=\"runner\" load=\"galaxy.jobs.runners.drmaa:DRMAAJobRunner\"/>\n\t\t#<plugin id=\"pbs\" type=\"runner\" load=\"galaxy.jobs.runners.pbs:PBSJobRunner\"/>\n                <param id=\"drmaa_library_path\">/usr/local/lib/libdrmaa.so.1</param>                 \t\n\t</plugins>\n        <destinations>\n                <destination id=\"pbs_default\" runner=\"drmaa_default\"/>\n        </destinations>\n</job_conf>\n\n\nand in systemctl I get this:\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.tours._impl INFO 2023-01-18 09:21:08,518 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded tour 'core.history'\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.tours._impl INFO 2023-01-18 09:21:08,528 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded tour 'core.scratchbook'\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs.manager DEBUG 2023-01-18 09:21:08,532 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Initializing job handler\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs INFO 2023-01-18 09:21:08,532 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Handler 'main.job-handlers.1' will load all configured r>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs.runners.state_handler_factory DEBUG 2023-01-18 09:21:08,534 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded 'failure' state ha>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: pulsar.managers.util.drmaa DEBUG 2023-01-18 09:21:08,540 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Initializing DRMAA session from thread M>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: Unable to communicate with localhost(127.0.0.1)\n\nWhat seems to be the problem? UI is working and running, but the jobs dont even start they just wait.\nThank you"}, {"role": "system", "text": "Is that # sign on line 4 of your config a typo?"}, {"role": "user", "text": "Sorry it was a typo yes. Problem was resolved. We\u2019ve had wrong libs installed(from Torque).\nSolution was to reinstall pbs-drmaa from sourceforge after installing openpbs-devel.\nNow everything works and jobs run on the node."}], [{"role": "user", "text": "My Trinity assembly has been waiting to run for more than 24 hours - is this normal? Should I cancel it and run it again? Please let me know if there is something going on with the server or if this is the normal time frame for running this tool.\nThanks!"}, {"role": "system", "text": "Please see this related post for details. In short, all public Galaxy servers are very busy and jobs are queuing a bit longer than usual. Avoid deleting/restarting for the fastest processing.\n\n  \n    \n    \n    Trimmomatic Job Stuck in Queue -- Extended job queue timing across public Galaxy servers usegalaxy.org support\n  \n  \n    Good evening, I had started a Trimmomatic job on a paired end dataset at about 1:10 UTC. It\u2019s been nearly 3 hours now with the jobs stuck in queue. Are you currently experiencing a lot of traffic? \nAnother question I had was how many jobs I could run concurrently? I was going to queue FastQC as well, but I\u2019m unsure if queuing too many jobs might lead to an error. I apologize if the questions were a little basic, I\u2019ve just started using Galaxy. Thanks for your time!\n  \n\n"}], [{"role": "user", "text": "Hi\nI am using cowpie in galaxy tool.\nSomeday, it is not working well, can not upload data file due to 502 bad gateway issue.\nShould i do something to solve this problem?\nThankyou.\n\n123872\u00d7533 38.8 KB\n"}, {"role": "system", "text": "This was a server side issue with the Upload tool at https://usegalaxy.be/ that is likely resolved.\nIf anyone runs into this same issue again, please open a new issue ticket and note the server URL."}], [{"role": "user", "text": "I create .vcf file and use reference by Galaxy hg38 (GRCh38.p2). I downloaded my file after SnpSift Annotate and trying use it for SnpEff annotation in cygwin64. I have error \u201cGRCh38.p2 genom not found\u201d because it\u2019s not in .vcf but hg38 is there. When I write it in command line nothing happened\u2026 \u201cdatabase not installed\u201d\nHelp please\u2026\n\nimage826\u00d7419 146 KB\n"}, {"role": "system", "text": "\n\n\n Sofi:\n\nSnpSift\n\n\nHi @Sofi\nThis forum is focused on using tools within the Galaxy Platform. Hundreds of tools, including this one, are wrapped for Galaxy and available at many of the public services.\n\n  \n      \n\n      galaxyproject.org\n  \n\n  \n    \n\nHome - Galaxy Community Hub\n\n  All about Galaxy and its community.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you are new to Galaxy, start here.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Introduction to Galaxy Analyses\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWhen using the tool directly, or for general usage instructions, the author\u2019s website is the go-to resource. Several of the general bioinformatics forums also have discussion \u2013 a web-search would find these.\nhttps://pcingola.github.io/SnpEff/"}], [{"role": "user", "text": "Hey, I encountered an issue: I was working on Galaxy and trying to run lefse but the output files were different when I changed the way microorganisms are named.\nHere is everything that I know:\nThe dataset is green, the job did not fail.\nThe lef_usual.txt is a input file whose first column contained many names of microorganisms, and these names had some special characters like \u201c()\u201d, \u201c-\u201d and \u201c'\u201d, like  \u201cClostridium_sp._DL-VIII\u201d.\nThe lef_usual_result.svg is the output file. There are 5 biomarkers for \u201cmeNorChild\u201d.\nThen, I replaced all the names of microorganisms in the first column in lef_usual.txt with another way of naming, such as s1, s2 and s3. The new input file was named lef_number.txt.\nThe  lef_number_result.svg is the new output file. There are 11 biomarkers for \u201cmeNorChild\u201d.\nThe relevant parameters of the two tasks are the same, only the naming method of the first column of the input file has changed.\nSo, why does just changing the way microorganisms are named affect the final results? What format of naming will give the most correct results?\nThe process screenshots, input files and output files have been uploaded.\n\nlef_number1906\u00d7744 77.2 KB\n\n\nlef_number_result1050\u00d72497 143 KB\n\n\nlef_usual1888\u00d7729 76.3 KB\n\n\nlef_usual_result1050\u00d72287 281 KB\n\n\nprocess1080\u00d71964 181 KB\n"}, {"role": "system", "text": "Cross-posted here: the way microorganisms are named affect the final results of lefse"}], [{"role": "user", "text": "Minimap2 should be able to map from fastq or fasta files.  Galaxy Minimap2 cannot see fastq files.  When galaxy minimap2 is run on fasta files it fails where it previously ran effectively.  This same data was processed through galaxy a few weeks ago, something is different."}, {"role": "system", "text": "Hi @nblewett\n\n\n\n nblewett:\n\nGalaxy Minimap2 cannot see fastq files.\n\n\nYou might just need to change the datatype Changing the datatype. Next time you Upload data, allow Galaxy to \u201cauto-detect\u201d the datatype.\nIf the \u201cauto-detect\u201d guess is wrong and you are working with reads, there is very likely some problem with that data (format, content, compression that Galaxy cannot uncompress,\u2026). For the compression issues, try uncompressing locally first. If that works, upload the uncompressed data to Galaxy. If that fails, some problem was introduced upstream and the data is corrupted and not usable anywhere, not just Galaxy. Maybe an upstream data transfer failed?\nOnce you have Uploaded intact data, run some QA before attempting to map.\nMore about how to learn what datatypes a tool is looking for, plus other general fastq help is in the topic below. The datatype restrictions are a very basic quality/format sanity check \u2013 not to be confused full QA. A tool can fail for odd reasons when given data in a format that it wasn\u2019t designed to work with, or produce putatively successful results that have scientific problems. The latter isn\u2019t always easy to detect.\n  \n    \n    \n    fastq unavailable -- Tool does not recognize inputs? How to check why \n  \n  \n    Hi @TTP \nMost tools in Galaxy expect reads in fastqsanger or fastqsanger.gz format and with one of those datatypes assigned. Those datatypes represent a specific type of quality score scaling: Sanger Phred +33. \nThis is produced by Illumina 1.8+ pipelines. PacBio used Phred+33 scaling to start with and now the scaling is neutral. You can double check with a tool like FastQC or FASTQ info to see what those report. \nThere are some tools that expect just fastq or fastq.gz (scaling unspecified). But\u2026\n  \n\n\nTutorials\n\nhttps://training.galaxyproject.org/training-material/search?query=minimap\nhttps://training.galaxyproject.org/training-material/search?query=quality\n\nIf you need more help, please share more details. This is the information people at this forum will usually need. Be sure to capture the server URL. Troubleshooting errors"}], [{"role": "user", "text": "Hoping for some help.\nI have a new Galaxy 21.09 install that was done using Ansible roles following the tutorial guidance here:\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Galaxy Installation with Ansible\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nbut using the updated Ansible files (since some of the older ones mentioned in the tutorial produced installation errors).\nIt installed fine and the tools seem to work fine except that I find that there are some files that I am unable to view using the \u201ceye\u201d button - although I get the correct preview in the box.  The error I get is:\n\n404 Not Found\nThe resource could not be found.\nNo route for /_x_accel_redirect/galaxyzpool/data/7/d/e/dataset_7de6b170-fac8-4d5e-88ce-9bdbe5dd97fa.dat\nThis is when trying to see the output of the Samtools Flagstat tool.\nI also find that I cannot download some (not all) files.  And I get the error that the file is not at the location.\nThis seems to have something to do with the proxying function in NGINX (I think) but I don\u2019t have enough knowledge of how this works to fix it.\nAny help would be appreciated!"}, {"role": "user", "text": "OK.  Found another syntax error in the referenced tutorial:\nIn the galaxy.j2 file, \u201clocation /_x_accel_redirect\u201d should be \u201clocation /_x_accel_redirect/\u201d\n-Michael"}], [{"role": "user", "text": "Good day!\nI would like to inquire about how and where to acquire a .fasta file for another hypervariable region. In the tutorial, the V4 fasta file was given. I was wondering where to acquire a fasta file for other hypervariable regions such as V2 for analysis. Thank you very much."}, {"role": "system", "text": "Hi @Federico_Cristobal_I\nMothur reference help README for the SILVA v138 reference files\nMothur Q&A Creating a customized reference alignment for V1-V2 - Commands in mothur - mothur\nGalaxy Tutorials\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Metagenomics\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I am trying to do a differential gene expression analysis of the bovine oviduct. When using the featureCounts tool, I need to upload a GFF/GTF file in the \u201cGene annotation file\u201d. The reference genome I selected in the Hisat2 tool was bosTau9, so in the \u201cfeatureCounts\u201d tool I choose the option \u201cGFF/GTF file in your history\u201d and upload a GFF file from the Ensembl.org web page (https://www.ensembl.org/index.html), but it seems that the gene annotation file I retrieve (Index of /pub/release-109/gff3/bos_taurus) do not corresponds to the same reference genome (bosTau9) I used for the alignment with the Hisat2 tool because a received an error message: Fatal error: Exit code 255 ().\nI hope someone can help me.\nMany thanks."}, {"role": "system", "text": "Hi @vpabloa\nThe bosTau9 genome available and indexed in Galaxy is based on the UCSC version of the assembly. It uses slightly different chromosome names than the Ensembl version. Tools expect exact matches, and can fail or produce odd results when there is some mismatch.\nTry using the GTF annotation from UCSC. There are a few choices, and each corresponds with a Gene and Gene Prediction track (see their browser for a description). The file you want is probably this one. Copy/paste the link into the Upload tool using all defaults and it will be ready to use.\nhttps://hgdownload.soe.ucsc.edu/goldenPath/bosTau9/bigZips/genes/bosTau9.ensGene.gtf.gz\nFAQ Working with GFF GFT GTF2 GFF3 reference annotation"}], [{"role": "user", "text": "Has this entry been addressed?:\nhttps://biostar.usegalaxy.org/p/13232/index.html\nI would like to merge three (replicate) bam files in a collection/list into a single bam file.\n\nWhen I try to do this with Samtools merge, I can only select the collection but not individual bam files. And if I select the collection in which there are the bam files I see the following message.\n\nCapture11288\u00d7213 8.42 KB\n\nIs there a way to see what these data 189, data 187, and data 188 are, to know what I\u2019m merging?\nThank you!"}, {"role": "system", "text": "Dear @ysrbrs,\nThe tool merges the data in your collection. So your file Samtools merge on data 189, 187, and 188 contains the data from your three input files SRR103*. You could not select the individual data sets, because your data is in a data set collection. In order to select your individual files (e.g., to merge just two bam files of your input data) you would need to extract the data sets from the collection with the tool Extract element identifiers of a list collection.\nHave a good day and best wishes,\nFlorian"}], [{"role": "user", "text": "Hi, I have RNAseq data (Illumina Total RNA) from whole blood RNA of COVID19 patients.  Based on ddPCR, I think some have detectable virus in blood. The alignment to HG38 using HISAT2 is great, but when I substitute the SARS-CoV2 genome, I get very few alignments and it does not change between controls and COVIDs, giving maybe 50 alignments out of 17M paired end reads. Any ideas on what could be wrong, or a source for a workflow?  Many thanks, TIm"}, {"role": "system", "text": "If you\u2019re just interested in potential SARS-CoV-2 sequences, I wouldn\u2019t recommend HISAT2 necessarily, at least not as the only tool.\nOptions I\u2019d explore instead:\n\n\nDo one pass of HISAT2 alignment, then filter for unmapped reads and align those using bwa-mem (or bowtie2)\n\n\nGenerate a combined human/SARS-CoV-2 FASTA, then use bwa-mem (or bowtie2) for a single round of alignment, after which you filter for alignments to the viral genome\nGalaxy | Europe | Published History | SARS-CoV-2/human combined ONT reference is an example history for generating the combined genome. You can import it and use it as your starting point, but since your data is Illumina data (not ONT), you would want to concatenate only the hg38 and the NC_045512.2 fasta datasets.\nHere\u2019s a workflow that illustrates the approach for ONT data and uses minimap2 as the mapper:\nGalaxy | Europe | Published Workflow | SARS-CoV-2: map ONT reads to transcripts\nIt shouldn\u2019t be hard to modify it for your use case.\n\n\nThe rational behind these recommendations: HISAT2 is a great tool for aligning eukaryotic spliced RNA reads, but it isn\u2019t optimized for viral RNA. A general mapper (like bwa-mem or bowtie2) is sufficient for the viral data and may do a better job in fact.\nMy first suggestion might be the best way to get optimal sensitivity for human and viral reads, but the second option should be very comparable (or even more sensitive) if you care about the viral reads only (it will do a good enough job of getting the vast majority of human reads out of the way and will discover SARS-CoV-2 alignments reliably)."}], [{"role": "user", "text": "Hello! I am trying to use this tool to compare differences in two conditions across three samples. I get the following error message when trying to import my data:\n\u201creading in files with read_tsv\n1 2 Error in tximport::tximport(files = localFiles, type = tolower(dataAnalyed$orign),  :\nall(txId == raw[[txIdCol]]) is not TRUE\nCalls: importIsoformExpression \u2192  \u2192 stopifnot\nWarning message:\nIn txId == raw[[txIdCol]] :\nlonger object length is not a multiple of shorter object length\u201d\nThe error message suggests that I have duplicate input datasets, but I do not - they\u2019re three different inputs per condition.\nI performed transcriptome assembly using the following tutorial and I don\u2019t know if this is part of the issue?\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Genome-wide alternative splicing analysis\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nMy history:\n  \n\n      usegalaxy.eu\n  \n\n  \n    \n\nGalaxy\n            | Europe\n\n  Galaxy is a community-driven web-based analysis platform for life science research.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAny help is greatly appreciated!"}, {"role": "system", "text": "Hi @Margaret_M\nNotice how the transcripts are named in the Stringtie files \u2013 they are using the default transcript names generated for novel transcripts by that tool. These instead need to match up with the reference annotation\u2019s transcript_id attributes and the >lines in the transcript fasta.\nIt looks like you missed a step that consolidates that data: Stringtie merge. Scroll up in the tutorial to find it."}], [{"role": "user", "text": "Hello. I would like to participate virtually in the conference to be held in Brisbane, Australia. Is poster presentation possible for virtual participation? I want to present a poster."}, {"role": "system", "text": "Hi @barbaros\nGood question.\nPoster submissions use the same form as the abstract submissions: Galaxy Community Hub - Galaxy Community Hub\nAnd as far as I know, all talks, demos, and posters are currently in person only. @enis would you be able to confirm?\n\nBy submitting an abstract, assuming it is accepted into the program, at least one of the authors agrees to register for GCC2023, attend the event, and present the work.\n"}, {"role": "system", "text": "@barbaros Yes! Virtual presentation for posters is allowed under the assumption that you are available on Zoom/Slack during your scheduled poster session. One thing to keep in mind that, depending on your timezone, that session may fall in the middle of the night."}], [{"role": "user", "text": "First of all, thank you for the training in the galaxy. it made me learn a lot about bioinformatics and its processing. I am a new user, I have learned from this site for a week. but there are some questions related to this tutorial.\n\nIn some tutorials shown the processing of metagenomic data from bacteria, and it uses a taxonomy in the form of \u201csilva\u201d data (silva.v4.fasta). incidentally my research is abaout diversity of \u201cdiatom\u201d and primers that I use is rbcl, the question is, what taxonomy do I use in processing data in the galaxy later? rbcl? or other?\nif my taxonomy is rbcl, where do I get rbcl.fasta data?\n\nFor information, I am waiting for the results of my sample sent by the NGS method, and I can\u2019t wait to process it.\nI appreciate your response and looking forward to it "}, {"role": "system", "text": "Hello @guardian_cp\nThis particular analysis domain space (diatom metagenomics) is a bit out of my lane, but this publication seems interesting. The authors host rbcL taxonomic reference databases for both Mothur and DADA-2.\n\n  \n      ncbi.nlm.nih.gov\n  \n  \n    \n41598_2019_Article_51500.pdf\n\n1385.45 KB\n\n  \n  \n    \n    \n  \n  \n\n\nMaybe have a look? And others reading are certainly welcome to comment more!"}], [{"role": "user", "text": "*I try to install galaxy local instance on Virtual box machine via ansible follow by galaxy project tutorial (LINK: Galaxy Installation with Ansible) And I got this problems, Did anyone meet this before? Could you please help me with this error?\nThanks in andvanced\n[ERROR]\nTASK [galaxyproject.galaxy : Get current Galaxy DB version] ************************************************\nfatal: [head-node]: FAILED! => {\u201cchanged\u201d: false, \u201ccmd\u201d: [\u201c/srv/galaxy/venv/bin/python\u201d, \u201c/srv/galaxy/server/scripts/manage_db.py\u201d, \u201c-c\u201d, \u201c/srv/galaxy/config/galaxy.yml\u201d, \u201cdb_version\u201d], \u201cdelta\u201d: \u201c0:00:01.893473\u201d, \u201cend\u201d: \u201c2022-06-27 04:28:27.278930\u201d, \u201cfailed_when_result\u201d: true, \u201cmsg\u201d: \u201cnon-zero return code\u201d, \u201crc\u201d: 1, \u201cstart\u201d: \u201c2022-06-27 04:28:25.385457\u201d, \u201cstderr\u201d: \u201cTraceback (most recent call last):\\n  File \"/srv/galaxy/server/scripts/manage_db.py\", line 28, in \\n    invoke_migrate_main()\\n  File \"/srv/galaxy/server/scripts/manage_db.py\", line 20, in invoke_migrate_main\\n    config = get_config(sys.argv, use_argparse=False, cwd=os.getcwd())\\n  File \"/srv/galaxy/server/lib/galaxy/model/orm/scripts.py\", line 133, in get_config\\n    properties = load_app_properties(config_file=config_file, config_prefix=config_override, config_section=config_section)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 79, in load_app_properties\\n    properties = read_properties_from_file(config_file, config_section)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 107, in read_properties_from_file\\n    raw_properties = _read_from_yaml_file(config_file)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 124, in _read_from_yaml_file\\n    with open(path) as f:\\nPermissionError: [Errno 13] Permission denied: \u2018/srv/galaxy/config/galaxy.yml\u2019\u201d, \u201cstderr_lines\u201d: [\u201cTraceback (most recent call last):\u201d, \"  File \"/srv/galaxy/server/scripts/manage_db.py\", line 28, in \u201c, \"    invoke_migrate_main()\u201d, \"  File \"/srv/galaxy/server/scripts/manage_db.py\", line 20, in invoke_migrate_main\", \"    config = get_config(sys.argv, use_argparse=False, cwd=os.getcwd())\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/model/orm/scripts.py\", line 133, in get_config\u201d, \"    properties = load_app_properties(config_file=config_file, config_prefix=config_override, config_section=config_section)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 79, in load_app_properties\u201d, \"    properties = read_properties_from_file(config_file, config_section)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 107, in read_properties_from_file\u201d, \"    raw_properties = _read_from_yaml_file(config_file)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 124, in _read_from_yaml_file\u201d, \"    with open(path) as f:\", \u201cPermissionError: [Errno 13] Permission denied: \u2018/srv/galaxy/config/galaxy.yml\u2019\u201d], \u201cstdout\u201d: \u201c\u201d, \u201cstdout_lines\u201d: }\n\nimage1305\u00d7679 150 KB\n\nt"}, {"role": "system", "text": "Maybe you can find some info here: galaxyproject/admins - Gitter\nAre you executing the installation as the user \u201cgalaxy\u201d?"}], [{"role": "user", "text": "Apologies for asking an obvious question.\nPlease how do I correctly download the FASTA copy of the baboon reference genome on NCBI link below;\nhttps://www.ncbi.nlm.nih.gov/genome/assembly/1082401\nI used the download assembly button on the web browser which gave the option of RefSeq (which is about 512kb) and Genbank (about 912mb)\nI also downloaded the DNA sequence which has numerous files (about 4GB) but could not get in on to Galaxy.\nPlease help.\nThank you"}, {"role": "system", "text": "Hi @Adekunbi123456,\nyou can find the ftp URL for the baboon reference genome in the FTP directory for RefSeq assembly section (NCBI web page, right column):\n\n  \n      ftp.ncbi.nlm.nih.gov\n  \n  \n    \n\nIndex of /genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0\n\n\n\n  \n  \n    \n    \n  \n  \n\n\nnext, copy the URL which correspond to the genomic sequence and paste it directly in the Galaxy uploader:\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0/GCF_008728515.1_Panubis1.0_genomic.fna.gz\nFinally, after uploading the file, select edit attributes, convert, and select convert compressed file to uncompressed."}], [{"role": "user", "text": "I have my galaxy instance on Cloud via Cloudman. However the site connection is insecure. Any files uploaded via FTP will be clearly visible. How do i make the Connection Secure?"}, {"role": "system", "text": "Unfortunately, secure FTP is not currently supported on CloudMan out of the box.\nIt should be possible to set it up manually by obtaining an SSL certificate and installing an SFTP server, but that\u2019s likely to involve quite a bit of system administration.\nThe new GVL 5 offers SSL out of the box, although no FTP support at all. However, the latest Galaxy that comes with GVL 5 supports large file uploads so, if that\u2019s a reason for using FTP, it should be addressable.\nHope this helps."}], [{"role": "user", "text": "Hello! I am trying to use the \u201cARTIC minion\u201d tool on galaxy and even though I select different version of the ARTIC primers (from the drop down menu or bed files I upload) it still uses the V1 (when I open the log file is says that it uses V1 scheme). Can anyone tell me what can be the problem? Because I get a consensus that has a lot of Ns.\nThanks in advance!"}, {"role": "system", "text": "Welcome, @tbzh13\nTo confirm proper inputs, you could use this shared history resource to compare/replace what you are using now. SARS-CoV-2 ARCTIC V4 files - #3 by wm75"}, {"role": "system", "text": "@tbzh13 that log is a bit unfortunate and misleading, but what the tool actually does seems to be the right thing.\nI just tested with a primer scheme from one of my histories and the generated command line has this part:\nmkdir -p 'scheme/name/V1' && ln -s '/data/dnb-ds02/galaxy_db/files/020/532/dataset_20532843.dat' 'scheme/name/V1/name.scheme.bed'\nwhere '/data/dnb-ds02/galaxy_db/files/020/532/dataset_20532843.dat' is really where my primer scheme file lives on the server.\nIn other words, the tool merely always links your custom primer scheme into a \u201cscheme/name/V1\u201d folder in the job working directory, from where it gets picked up by the ARTIC minion pipeline, but unfortunately that leads to that confusing log."}], [{"role": "user", "text": "I have a VCF file (format = vcf_bgzip) and I would like to obtain multiple separate VCF files by chromosome. I would appreciate any help into how to achieve this using Galaxy. Here is a sample file:\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=1,length=249250621>\n##contig=<ID=10,length=135534747>\n## several other hashes and text\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tSampleName\n1\t69869\trs548049170\tA\t.\t.\t.\t.\tGT\t0/0\n1\t565508\trs9283150\tT\t.\t.\t.\t.\tGT\t./.\n1\t727841\trs116587930\tC\t.\t.\t.\t.\tGT\t0/0\n1\t752721\trs3131972\tT\tG\t.\t.\t.\tGT\t1/1\n"}, {"role": "system", "text": "Dear @anchor,\nYou can do this with Text reformatting with awk\nusing the following AWK Program: $0 ~ /^[1#]/ {print $0}\nYou can run this multiple times and just change the [1#] for chromosome 1 to [2#] for chromosome 2 and so on.\nCheers,\nFlorian"}], [{"role": "user", "text": "I hope you can help me.  I am attempting a\n\ndownload720\u00d71280 104 KB\n\nrna-seq analysis. This is a BAM file (the first ~100 lines are like above and the remaining lines are in this context with QNAME FLAG RNAME POS MAPQ CIGAR MRNM MPOS SEQ QUAL OPT).  I have the entire workflow done except for this part. I want to go from the raw data I was given to  fastqsanger.gz so I can use trimmomatic on it, etc.\nHow do I go about that? Or another question, what am I doing wrong?"}, {"role": "system", "text": "Hi @Coltin_Albitz\nThese tutorials cover many common analysis paths when working with RNA-seq data. Start with a few introduction tutorials if you are not familiar with Galaxy, then review tutorials that match the type of analysis you want to do. Both will help you to understand what to do when using your own data.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Introduction to Galaxy Analyses\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n\n Coltin_Albitz:\n\nI am attempting a rna-seq analysis. This is a BAM file. I have the entire workflow done except for this part.\n\n\nBAM datasets are intermediate output. These represent alignments of read sequences against a reference sequence. Mapping tools generate this kind of data.\n\n\n\n Coltin_Albitz:\n\nI want to go from the raw data I was given to fastqsanger.gz so I can use trimmomatic on it, etc.\n\n\nThe raw RNA-seq read sequencing data should already be in \u201cfastqsanger\u201d format, or \u201cfastqsanger.gz\u201d if compressed. When uploading this kind of data (fastq) to Galaxy, leave the datatype detection as \u201cauto-detect\u201d. If the data really is in this format, Galaxy will guess it correctly and QA tools like Trimmomatic will accept it as valid input.\n\nIf you need more help, please be specific when you ask a new question so we can help you. What are the analysis goals? What steps have you done so far? If you ran into job errors: what did you check already, what did the error logs report, and can you share a link to your work for context?\nFAQs:\n\nhow to ask a question others can help with\ntroubleshooting-errors\nsharing-your-history\n"}], [{"role": "user", "text": "\n\n\n\nOption 1 - Dark\nOption 2 - Pastels\nOption 3 - Original\n\n\n\n\n0\nvoters\n\n\n\n\nOption 1 - Dark\n\n49466266-84b16080-f7f7-11e8-80b9-72cb8f539ef1.png1295\u00d7935 103 KB\n\nOption 2 - Pastels\n\n49466211-63507480-f7f7-11e8-8e9f-b42eea995789.png1296\u00d7928 102 KB\n\nOption 3 - Original\n\ngrafik.png1295\u00d7931 81.2 KB\n\nRelated to a github pull request"}, {"role": "system", "text": "Thanks for the feedback everyone!  We\u2019ll take this feedback into consideration when finalising the feature implementation."}], [{"role": "user", "text": "I ran DESeq2 on usegalaxy.org and generated the figure on the left. I then took the VST file exported on Galaxy and ran the following code on R to generate a similar heatmap:\nsampleDists <- dist(t(assay(vsd)))\nlibrary(\"RColorBrewer\")\nsampleDistMatrix <- as.matrix(sampleDists)\nrownames(sampleDistMatrix) <- paste(vsd$condition, vsd$type, sep=\"-\")\ncolnames(sampleDistMatrix) <- NULL\ncolors <- colorRampPalette( rev(brewer.pal(9, \"Blues\")) )(255)\npheatmap(sampleDistMatrix,\n         clustering_distance_rows=sampleDists,\n         clustering_distance_cols=sampleDists,\n         col=colors)\n\nAs you can see from the figures, the distance on the clustering looks the same for both, but the groupings are drawn differently. Which is correct? Any idea what could have happened?\n\nheatmaps1920\u00d71080 109 KB\n"}, {"role": "system", "text": "Hi @mitchellgc\nThe Bioconductor forum has much discussion about the different normalization methods. Start with these DESeq2 normalization vs VST vs rlog  && Clarification re VST transformed data in DESeq2 && choosing the normalization method (rlog, variance stabilizing transformation)."}, {"role": "system", "text": "Deseq 2 nomalization method is based on deseq2 median ratio. You are using different normalization method in R"}], [{"role": "user", "text": "Hi all,\nI\u2019m running Galaxy on a local on-premiss server and developing using Rscript.My tool works fine Up to the point of Rscript execution, but finally returns \u201cUnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode byte 0x8b in position 1: invalid start byte\u201d. What are the possible causes?\n\nGalaxy ver.: v20.01\nPython ver.: 3.6.8\nOS: CentOS 8\noutput file format: txt\n\ngalaxy.model.metadata DEBUG 2023-05-25 15:47:41,938 [p:1938833,w:0,m:1] [DRMAARunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 502\ngalaxy.jobs.runners ERROR 2023-05-25 15:47:41,944 [p:1938833,w:0,m:1] [DRMAARunner.work_thread-3] (303/546) Job wrapper finish method failed\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/__init__.py\", line 1522, in _finish_dataset\n    dataset.set_peek(line_count=line_count)\nTypeError: set_peek() got an unexpected keyword argument 'line_count'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/runners/__init__.py\", line 540, in _finish_or_resubmit_job\n    job_wrapper.finish(tool_stdout, tool_stderr, exit_code, check_output_detected_state=check_output_detected_state, job_stdout=job_stdout, job_stderr=job_stderr)\n  File \"lib/galaxy/jobs/__init__.py\", line 1657, in finish\n    output_name, dataset, job, context, final_job_state, remote_metadata_directory\n  File \"lib/galaxy/jobs/__init__.py\", line 1525, in _finish_dataset\n    dataset.set_peek()\n  File \"lib/galaxy/model/__init__.py\", line 2621, in set_peek\n    return self.datatype.set_peek(self)\n  File \"lib/galaxy/datatypes/data.py\", line 903, in set_peek\n    est_lines = self.estimate_file_lines(dataset)\n  File \"lib/galaxy/datatypes/data.py\", line 854, in estimate_file_lines\n    dataset_read = dataset_fh.read(sample_size)\n  File \"/home/galaxy/galaxy/.venv/lib64/python3.6/codecs.py\", line 321, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n\nThanks,\nYukie"}, {"role": "system", "text": "\n\n\n yukieymd:\n\nutf-8' codec can't decode byte 0x8b in position\n\n\nHi @yukieymd\nIs the format of the data a defined datatype? If not, maybe try defining the datatype? Or, add more datatypes that are already defined but might not be in the older release you are working in.\nExample search with \u201cset_peek\u201d.  Search \u2014 Galaxy Project 20.01 documentation\nYou are working at an older release of Galaxy. Python support for version 2 & 3 was still mixed. So when you search the docs, navigate by release.\nOther \u201creasons\u201d I can think of:\n\n\n!s the dataset (file) name something that wouldn\u2019t play well with Rscript? It is better to reference datasets than to use the original file name. We only have a handful of tools that require the original file name, and those are difficult for people to use. Why? Because it is a value users can modify, and dependent on what other tools decide as the default naming (spaces, dots, other odd characters). So, if that situation can be avoided it is generally a \u201cbetter\u201d strategy.\n\n\nI suppose that could also be something based on the file content. Unexpected compression could be involved: binary vs plain text. But this loops back to defined datatypes. If your tool is not screening for appropriate input assigned datatypes, and knows how to check/manipulate which it is working with (extra uncompress loop, etc) consider adding that in. And remember, some binary data is not appropriate for a \u201cdataset peek view\u201d. Example: bam.\n\n\nA web search shows some solutions in various contexts. And, it came up a few times at our older forum. Some examples:\n\n\n\nUnicodeDecodeError: 'utf8' codec can't decode\nPython Error? unexpected keyword argument [fixed]\nOr, do a fresh search here to limit to \u201cgalaxyproject\u201d hits\n\nhttps://galaxyproject.org/search/?q=set_peek#gsc.tab=0&gsc.q=set_peek&gsc.page=1\nhttps://galaxyproject.org/search/?q=set_peek%20planemo#gsc.tab=0&gsc.q=set_peek%20planemo&gsc.page=1\n\n\n\n\n\nPlanemo for tool development: Welcome to Planemo\u2019s documentation! \u2014 Planemo 0.75.7 documentation\n\n\nYou can also ask for help at the tool development chat here. Updating your development environment might be recommended. You're invited to talk on Matrix\n\n\n\nOthers can comment, these are just my guesses ."}, {"role": "user", "text": "Hi @jennaj,\nThank you very much for providing various helpful information. Regarding the datatype defined as \u201ctxt\u201d in the output section of the tool XML file, to be honest, I\u2019m not entirely sure about it since I\u2019m not the author of the Rscript. I think it would be a good idea to reach out to the author of the Rscript and inquire about it.\nBest,\nYukie"}], [{"role": "user", "text": "I am trying to follow a procedure in the following link - detailed instructions are in supplementary. I believe I have everything working until the step where I have to run BLAST + makeblastdb on a very large file (I believe ~ 100GB, although I haven\u2019t been able to successfully download what the procedure uses - it\u2019s supposed to be all bacterial nucleotide sequences). Instead, I have downloaded the BLAST pre-formatted nucleotide database (https://www.ncbi.nlm.nih.gov/public/?blast/db/FASTA/nt.gz) using FTP to download it. I have a local galaxy set up to handle the large file sizes and am running it from an external hard drive. I thought these databases were supposed to be pre-formatted, but:\n\n\nWhen I try to run NCBI BLAST+ blastn on my assembled sequence against the database (after I uncompressed it via Galaxy) I get an error:\nFatal error: Exit code 137 ()\n/Volumes/MBLab/galaxy/database/jobs_directory/000/26/tool_script.sh: line 25: 87409 Killed: 9               blastn -query \u2018/Volumes/MBLab/galaxy/database/objects/7/6/c/dataset_76cf539a-0e17-416d-b488-604ddd55b8ea.dat\u2019 -subject \u2018/Volumes/MBLab/galaxy/database/objects/1/8/9/dataset_189ca2b3-5d9d-4629-bb24-2811979970ef.dat\u2019 -task \u2018megablast\u2019 -evalue \u20180.001\u2019 -out \u2018/Volumes/MBLab/galaxy/database/objects/b/5/a/dataset_b5a49b6e-b95a-40b7-9f72-cf421e820fe5.dat\u2019 -outfmt \u20186 std sallseqid score nident positive gaps ppos qframe sframe qseq sseq qlen slen salltitles\u2019 -num_threads \u201c${GALAXY_SLOTS:-8}\u201d\n\n\nWhen I try to run NCBI BLAST+ makeblastdb on the uncompressed nt file (uncompressed via Galaxy -> Edit dataset attributes -> Convert -> Convert compressed file to uncompressed) I get an error:\nFatal error: Exit code 1 ()\nBLAST Database creation error: Error: Duplicate seq_ids are found:\nGNL|BL_ORD_ID:553373\n\n\nI am currently downloading a Refseq bacterial database (fromhere) because Refseq is nonredundant. I am going to try NCBI BLAST+ makeblastdb on those once I have them up to see if that helps, although the file is massive (300+gb). Any advice is very welcome - I struggle with this and have no experience here. I will update how the RefSeq approach goes."}, {"role": "system", "text": "Hello @aroebuck\nNCBI BLAST+ tools had some corrections today at UseGalaxy.org. Full testing is still in progress but so far have turned out well. Even if your combination of tools/inputs is not marked as \u201cpass\u201d yet in the tracking ticket\u2019s test matrix, consider a rerun anyway.\n\nIssue tracking ticket: BLAST failures at usegalaxy.org \u00b7 Issue #318 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub\n\n\nFor any reruns, be sure to use the most current version of all tools in the BLAST+ tool suite (2.10.1+galaxy0), including NCBI BLAST+ makeblastdb Make BLAST database (Galaxy Version 2.10.1+galaxy0).\nFor very large data, any public Galaxy server can be a problematic choice. 300GB of raw data would exceed your quota space (storage). And even if that was increased, memory for data storage is unrelated to the memory used to execute tools \u2013 and the example you state would certainly fail for resources.\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\nChoices matrix: Galaxy Choices - Galaxy Community Hub\n\nThe GVL version of Cloudman is one choice and AWS offers grants. AWS Programs for Research and Education\n\n\nI added some tags to your post that link to prior Q&A about the GVL and other cloud options.\nYou also may want to consider joining our upcoming webinar:\n\n  \n    \n    \n    Dec 9 Webinar: Use Galaxy on the web, the cloud, and your laptop too \n  \n  \n    This Galaxy webinar will introduce the many options for using Galaxy right now, anywhere in the world. We\u2019ll cover \n\nPublicly accessible Galaxy servers on the web (there are over 100 of them)\nResearch and commercial cloud platforms, from Australia (Nectar) to the US (Jetstream, AnVIL), and everywhere in between (CloudLaunch)\nOn your laptop, using pre-packaged containers and virtual machines.\n\nThe webinar will be at 11am Eastern US time (see your time). Enis Afgan, Anton Nekrutenko, and Mike Scha\u2026\n  \n\n\nThanks!"}, {"role": "user", "text": "Thank you,\nI ended up re-trying after reading something elsewhere. I tried with the parsing parameter set to \u201cYes\u201d (the instructions say to leave it set to \u201cNo\u201d . That seemed to do the trick and the database was generated! Hopefully this was not a problematic thing to do."}], [{"role": "user", "text": "Hello, I\u2019ve been facing a certain problem for some time now regarding SPADES and metaSPADES. There are always error messages, of the most varied possible, at the time of requesting the work or minutes after the execution of the same. Most of these are memory errors or incorrect data entry (it sometimes mentions duplicate input files) - I always follow the manual and so far I\u2019ve only managed to run my data twice. These are paired end data (individual datasets). My data is metagenomic libraries; each file is between 400 and 500GB in size. Is this some SPADE/metaSPADE instability?\nThanks in advance for your help!"}, {"role": "system", "text": "Hi @hugovalerio1,\ncould you try to run your analysis in usegalaxy.eu? Regards"}, {"role": "user", "text": "Thank you very much, solved! I don\u2019t know what the problem with galaxy.org and galaxy.au, but with galaxy.eu everything went well, everything is working fine. Thanks for the tip!"}], [{"role": "user", "text": "Hello,\nI am using GALAXY USA from December 2021 using 2 email IDs. But now I am trying to log in to my account using both of my email IDs (admin-redacted-for-privacy). It says it is an invalid ID. It would be great if someone could help me with my login.\nThank you\nJawahar"}, {"role": "system", "text": "https://usegalaxy.org/static/terms.html"}], [{"role": "user", "text": "Hi,\nCan I share data between two Galaxy servers? Like I am using Galaxy Australia and I want to share my data with Galaxy Europe.  Is this possible?"}, {"role": "system", "text": "It is possible but you have to \u2018fetch\u2019 the data to the Galaxy you want to work with. One way is to right click on the \u2018download\u2019 icon on the dataset and copy the link. Then paste the link in the upload modal of the other Galaxy and let it fetch the dataset for you (from the original Galaxy)."}], [{"role": "user", "text": "Hello,\nI downloaded GTF file from NCBI and uploaded it to my Galaxy. I ran HISAT2 along with my sample but I incurred an error while using it with StringTie.\nError : no valid ID found for GFF record\nI even removed the header comments and ran Stringtie against my HISAT result but still got an error.\nDoes anyone seem to know what the problem is?\nGTF File after removing the header lines\n\n\n\n\nCP083173.1\nGenbank\ngene\n1\n1155\n.\n+\n.\ngene_id K9E40_00005; transcript_id ; gbkey Gene; gene_biotype protein_coding; locus_tag K9E40_00005;\n\n\n\n\nCP083173.1\nProtein Homology\nCDS\n1\n1152\n.\n+\n0\ngene_id K9E40_00005; transcript_id unassigned_transcript_1; gbkey CDS; inference COORDINATES: similar to AA sequence:RefSeq:WP_004115883.1; locus_tag K9E40_00005; product sugar-binding protein; protein_id UQA84770.1; transl_table 11; exon_number 1;\n\n\n\nCorresponding FNA file\n\nCP083173.1 Gardnerella vaginalis strain JNFY11 chromosome, complete genome\nATGAACATAGGTAAAAAAGCAATCGCTTTATTTGTAGGTATTGCTGTAGTTGCTGGTTTATCAGCCTGTTCAGGTTCAAG\nAGGTGGTGCATCCAAAAACGTAAGTCAAGGAATTGAAAAAGGTGCCACCATCGGTGTCTCTATGCCAACAAAGTCAGAGG\u2026\n"}, {"role": "system", "text": "Hi @Faaiza_Ibrahim_B.Sc\nmaybe consider providing a brief description of your goal(s). Are you after gene annotation or read counting/gene expression?\nYou use HiSAT2 and StringTie on bacterial data. HiSAT2 is a gapped aligner that can align reads across introns. StringTie is often used for prediction of genes in eukaryotic genomes. On other hand, many bacterial genes present in operons. Read splitting can be disabled in HiSAT2.  Not sure if SringTie is the best option for annotation of bacterial genes, but I am not familiar with the topic. For read counting maybe consider alternative tools such as featureCounts or htseq-count. You may need to tweak the setting, depending on the annotation file and your goals. Read counting tools count reads on annotation present in 3d column (type) and aggregate results using one of attributes from the last column. By default many read counting tools count reads against exons, but exon annotations may not present in some bacterial datasets (no \u2018exon\u2019 in 3d column). In this case you need to choose something else, for example, CDS or gene. You may get different results with different settings. The same for the attributes.\nAs I don\u2019t know what you are trying to archive and cannot check the data, it is hard to answer your question.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI was wondering if there is an option to display name tags in the workflow editor. This would make editing the workflow easier. As they are displayed in this tutorial Name tags for following complex histories\nImage from the tutorial below -\n\nnametag-wf837\u00d7735 84.6 KB\n\nI could not find any option to enable this.\nThanks and Regards"}, {"role": "system", "text": "Hi @microfuge\nThe tags you see in the tutorial were added into the graphic to show where the inputs specific to the tutorial flowed through the workflow. This was manually added in as far as I know.\n\nIf the input datasets are tagged TB8_R1 and TB8_R2 , these tags will propagate to the outputs of any tools run on these datasets.\n\nOnce a workflow is run, input tags and any tool output tags add to the workflow itself will show up in the output history, and in the workflow invocation (\u201cDetails\u201d tab).\nWe are working on creating another view that will look very similar (but not exact) to the workflow editing view. This will be associated with the Workflow Invocation Report \u2013 sort of an expanded history view that will include runtime details in a graphic format.\n\n\n\n microfuge:\n\nI was wondering if there is an option to display name tags in the workflow editor. This would make editing the workflow easier.\n\n\nAgree \u2013 but I\u2019m not sure where the workflow editor would source the input tags? Those are not known until the workflow has some inputs selected at runtime. If you are looking for a preview, right now that can be done by starting up a workflow run with inputs (that have tags) and reviewing the output history generated. You can quit out of those jobs if something doesn\u2019t look right.\nThese are the open and closed issues related to \u201ctags\u201d anywhere the Galaxy application. Issues \u00b7 galaxyproject/galaxy \u00b7 GitHub\nYou could request a feature enhancement. A sort of \u201cpreview\u201d in the editor, that includes example selected inputs including tags (user specified), seems useful to me, too, and I don\u2019t think it has been asked for yet. Should you decide to make a request, please link that back here \nUpdate: Decide to ask the IWC what they think. Please feel free to join the chat, too. You're invited to talk on Matrix"}, {"role": "user", "text": "Hi Jennifer (@jennaj ),\nThanks so much for the detailed explanation and the Matrix post. I will create a github issue depending on responses to the matrix post.\nThe workflow editor gives options to add and remove tags (on the left where one sets the parameters for the tool and the output names). These tags if available in the workflow diagram would be great. I agree that the tags set for input datasets would be available only at the runtime.\nTags are quite a nice feature of Galaxy and our local instance users find them very convenient.\nThanks Again and Regards"}], [{"role": "user", "text": "Hello,\nI would like to get DEG for my RNAseq. I have completed these steps: cufflinks, and cuffdiff. But when I am trying to run CummeRbund in the galaxy by using SQLite files output of cuffdiff as the input for CummeRbund, I get this error every time: The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices. I would appreciate it if you could guide me on how to fix it. How should I use CummeRbund in the galaxy?\nAnd are there other alternative tools to get DEG from FPKM data in the galaxy?\nThank you very much\nM"}, {"role": "system", "text": "Hello @MMM\nThe error means that the database wasn\u2019t created correctly. The tool suite you are using is known to have issues. Using updated protocols is recommended.\nPlease see the Galaxy tutorials here. If you run into problems, this forum has much Q&A around troubleshooting: search by tool name or datatype.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi,\nI have paired-end stranded libraries.Can someone please let me know what should I choose for these 3 options to run HISAT2 and htseq thereafter, given that my inferexperiment result is1\u00b1,1-+,2++,2\u2013\nQ1. For HISAT2, it aks to specify strand information (Unstranded/FR/RF) and then\nQ2. also asks to choose*\u2018Select the upstream/downstream mate orientations for a valid paired-end alignment against the forward reference strand*\u2019 with options (\u2013fr/\u2013rf/\u2013ff).\nQ3. for htseq it asks to choose setting for \u2018stranded\u2019 with options (yes/no/reverse).\nI used FR and --fr for HISAT2 and obtained 43% overall mapping. Then I used those BAM files to perfomr htseq and selected YES for the \u2018stranded\u2019 option. After doing MultiQC, I see 0% reads were assigned.\nMany thanks,G"}, {"role": "system", "text": "Hi @GGOPA\nread strandness is determined by protocol used for preparation of library. Ideally, your sequencing service provider should give it to you as part of description for sequencing data. HiSAT2 and htseq-count settings are determined by the data. If you don\u2019t have information on strandness of your reads, either ask your sequencing provider or determine it as described in Strandness section of this tutorial: 1: RNA-Seq reads to counts\nGood description of strandness is available in another tutorial: De novo transcriptome reconstruction with RNA-Seq\nKind regards,\nIgor"}], [{"role": "user", "text": "Good day,\nI\u2019ve searched whole google and only thing i\u2019ve saw is BamTools that can operate under windows, but it requires some serious setups that i\u2019m totally confused how to make it work.\nAny other programs are *nix/macos, and i dont see alternatives.\nDoes anything else exist ?\n(To be more specific, i\u2019m talking about windows 7 x86 (because that is what installed on our lab computers))"}, {"role": "system", "text": "HI @RoskoDaneworth,\nwhy are you concerned about Windows compatibility? Most of the bioinformatics software is not able to run natively under Windows. If you want to learn commandline bioinformatics tools, I recommend to use Linux and Conda as a scientific package manager.\nMoreover, I think you are in the wrong forum. Galaxy does not require any installation for users. For example you could use BAM to FASTQ here https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtofastq/2.27.1. Simply with your webbrowser and no installation.\nHope that helps,\nBjoern"}], [{"role": "user", "text": "Hi everyone,\nI\u2019m having some problems with my Pulsar installation and was wondering if anyone can give me some advice on what I might be doing wrong.\nIn my current setup I was able to use some tools from the tool shed using Pulsar, however internal Galaxy tools do not work. For example the internal upload tool does not work on my Pulsar server using the web variant. The job returns the following error:\n\nTraceback (most recent call last): File \u201c/data/galaxy/server/files/staging/36/tool_files/data_fetch.py\u201d, line 13, in  from galaxy.datatypes import sniff ModuleNotFoundError: No module named \u2018galaxy.datatypes\u2019\nAn error occurred with this dataset\nTraceback (most recent call last):\nFile \u201c/data/galaxy/server/files/staging/36/tool_files/data_fetch.py\u201d, line 13, in \nfrom galaxy.datatypes import sniff\nModuleNotFoundError: No module named \u2018galaxy.datatypes\u2019\n\nWhen I configure my local_env.sh in my Pulsar configuration to contain export TEST_GALAXY_LIBS= 1 I also get the below error, which make me suspect that this problem is because somehow Pulsar cannot use its local Galaxy installation correctly because I\u2019ve misconfigured something.\n\nBlockquote\nSourcing file ./local_env.sh\nTraceback (most recent call last):\nFile \u201c\u201d, line 1, in \nImportError: cannot import name \u2018eggs\u2019 from \u2018galaxy\u2019 (/data/galaxy/venv/lib/python3.8/site-packages/galaxy/init.py)\nFailed to setup Galaxy environment properly, is GALAXY_HOME (/data/galaxy) a valid Galaxy instance.\n\nMy Galaxy installation is in /data/galaxy. I\u2019ve tried changing the python path in the local_env.sh to several things like:\n\nexport PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib/galaxy/jobs/rules\n\nor\n\nexport PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib/\n\nBut neither seems to be working.\nThis is my local_env.sh configuration:\n \n ## Place local configuration variables used by Pulsar and run.sh in here. For example\n \n ## If using the drmaa queue manager, you will need to set the DRMAA_LIBRARY_PATH variable,\n ## you may also need to update LD_LIBRARY_PATH for underlying library as well.\n #export DRMAA_LIBRARY_PATH=/path/to/libdrmaa.so\n \n \n ## If you wish to use a variety of Galaxy tools that depend on galaxy.eggs being defined,\n export GALAXY_HOME=/data/galaxy\n export PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib\n export TEST_GALAXY_LIBS=1\n\nAnd this is my job_conf.xml config on the side of the Galaxy server:\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"local_plugin\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n        <plugin id=\"pulsar\" type=\"runner\" load=\"galaxy.jobs.runners.pulsar:PulsarLegacyJobRunner\"/>\n    </plugins>\n    <destinations default=\"remote_cluster\">\n        <destination id=\"local_destination\" runner=\"local_plugin\"/>\n        <destination id=\"remote_cluster\" runner=\"pulsar\">\n            <param id=\"url\">http://pulsar-server-02:8913/</param>\n            <param id=\"dependency_resolution\">remote</param>\n        </destination>\n    </destinations>\n    <tools>\n    </tools>\n</job_conf>\n\nA workound to this problem I found is to configure the DATA_FETCH tool to run using the local_destination destination. However, I\u2019d like to know if there is a way to have all tools including internal ones run on the Pulsar side or whether there is a better way entirely to solve this problem.\nI also found this github issue that seems to be related to my question, but the fix from this issue did not work for me unfortunately: Galaxy eggs? \u00b7 Issue #134 \u00b7 galaxyproject/pulsar \u00b7 GitHub\nAny advice or pointers would be greatly appreciated!"}, {"role": "system", "text": "Welcome, @Turgon\nThanks for posting all of these details. I\u2019ve asked our Admin experts for advice at their chat. They may reply here or there, and feel free to join the chat! You're invited to talk on Matrix"}, {"role": "user", "text": "This issue can be marked as solved because I found out that I shouldn\u2019t be using Pulsar in the way I described above and found a better way to run a Galaxy cluster in the meantime."}], [{"role": "user", "text": "Hello everyone,\nI annotated my sequencing data with SnpEff ( SnpEff eff: annotate variants (Galaxy Version 4.3+T.galaxy1)) using the database SnpEff download: SnpEff4.3 GRCh37.75 ( SnpEff download:  download a pre-built database (Galaxy Version 4.3+T.galaxy2)).\nWhen checking some variants, I found that information regarding regulatory region variants  are not supplied (as they are using the VEP for Ensemble version 75). Does the SnpEff database generally not supply this information?\nThanks!\nAll the best,\nRose\nExample: 9_139400219_G/A\nSnpEff annotation\nA|missense_variant|MODERATE|NOTCH1|ENSG00000148400|transcript|ENST00000277541|protein_coding|25/34|c.4129C>T|p.Pro1377Ser|4205/9371|4129/7668|1377/2555||,A|upstream_gene_variant|MODIFIER|NOTCH1|ENSG00000148400|transcript|ENST00000494783|processed_transcript||n.-922C>T|||||922|\noutput VEP see picture attached\n\nBildschirmfoto 2020-05-04 um 22.40.341047\u00d7273 45 KB\n\n\n\n\n\nUploaded Variation\nLocation\nAllele\nGene\nFeature\nFeature type\nConsequence\nPosition in cDNA\nPosition in CDS\nPosition in protein\nAmino acid change\nCodon change\nCo-located Variation\nExtra\n\n\n\n\n9_139400219_G/A\n9:139400219\nA\nENSG00000148400\nENST00000494783\nTranscript\nupstream_gene_variant\n-\n-\n-\n-\n-\nrs61751542\nSTRAND=-1;SYMBOL=NOTCH1;DISTANCE=922;SYMBOL_SOURCE=HGNC;GMAF=A:0.0087236\n\n\n9_139400219_G/A\n9:139400219\nA\n-\nENSR00001317600\nRegulatoryFeature\nregulatory_region_variant\n-\n-\n-\n-\n-\nrs61751542\nGMAF=A:0.0087236\n\n\n9_139400219_G/A\n9:139400219\nA\nENSG00000148400\nENST00000277541\nTranscript\nmissense_variant\n4205\n4129\n1377\nP/S\nCcc/Tcc\nrs61751542\nENSP=ENSP00000277541;STRAND=-1;SYMBOL=NOTCH1;HGVSc=ENST00000277541.6:c.4129C>T;HGVSp=ENSP00000277541.6:p.Pro1377Ser;SYMBOL_SOURCE=HGNC;GMAF=A:0.0087236\n\n\n\n"}, {"role": "user", "text": "I would be glad for any help! Thanks a lot! Rose"}, {"role": "system", "text": "Hi @roselucia,\nthis is a good question.\nSnpEff has a regulatory_region_variant effect, but regulatory effects belong to the additional annotations described in chapter 6 of the SnpEff manual, which you have to enable explicitly.\nIn the Galaxy wrapper, this is hidden in the section Regulation options just below the SnpEff4.3 Genome Data select box if you want to use it.\nBest,\nWolfgang"}], [{"role": "user", "text": "Dear Lati98,\nHow did you resolve this issue? Would you please share your solution?\nI have the same problem and my input data format (thermo.raw) and integrity looks good, as it should be, no input file or file name duplicates etc.\nThank you in advance for your help.\nBest regrds.\n(Qc)"}, {"role": "system", "text": "Hi @qcsciphi\nThere wasn\u2019t any followup and your problem is likely different.\n\nThis is the full list of topics about the tool: Search results for 'maxquant' - Galaxy Community Help\n\nAnd these are the tutorials: Search Tutorials\n\n\nThat specific message about duplicate inputs is just an automated guess. If the tool cannot read the inputs, that message can be triggered, too or one of these extra alternative messages Galaxy Training! (varies by server if reported or not).\nShort advice:\n\nMake sure that the reads are in an uncompressed state and assigned the datatype: fasta\n\nThen review the regular expressions that are parsing the read name and description content (what is on the > title lines). The default will not work for every file, and if not a match for your specific reads could certainly result in a \u201cduplicate read name\u201d.\nThere are two different versions of this tool. One requires just a fasta and the other a thermo.raw + fasta input + mqpar.xml.\n\n\nThe tool must be able to match identifiers across all user supplied inputs plus any values parsed out of them at runtime.\n\n\nmqpar.xml file with your search parameters. RAW file names must match the names displayed in galaxy. Their paths from the local machine are ignored. E.g. a file named \u2018test01.raw\u2019 in galaxy can either be named \u2018test01.raw\u2019 or \u2018D:\\path\\to\\test01.raw\u2019 in the mqpar.xml. * required\n\nFor more help, please copy the inputs into a new history, then attempt to run the tool. If it still fails and you cannot figure out how to fix the regular expressions, please do one of these:\n\ngenerate and post back a shared link to that small testing history (probably required if the input is more than a single fasta)\nor, post back to this topic:\na. the first few sequences in full \u2013 at least three reads\nb. the regular expressions in use \u2013 will be at least two: identifier + description\nc. the tool full name including version (copied from the very top of the tool form)\nd. a copy + paste of the \u201cDataset Details\u201d view, all parts from the \u201cTool Parameters\u201d portion and down should be enough to start with. Find this by clicking on the  icon within one of the output datasets faqs/galaxy/#troubleshooting-errors\n\n\nLet\u2019s start there, thank you "}, {"role": "user", "text": "Hi JennaJ,\nThank you for your kind suggestions.\nWe are talking about the GTN training material:  \u201cMaxQuant and MSstats for the analysis of TMT data\u201d;\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: MaxQuant and MSstats for the analysis of TMT data\n\n  Training material for proteomics workflows in Galaxy\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWhen you click on the \u201cgraduation hat\u201d icon on Galaxy.eu and search for \u201cMaxQuant and MSstats for the analysis of TMT data\u201d training material now there\u2019s a message appearing, which says;\n\" Under Development!\nThis tutorial is not in its final state. The content may change a lot in the next months. Because of this status, it is also not listed in the topic pages.\"\nProbably the specialists are checking the steps that you\u2019re suggesting.\nThank you once again for your kind assistance.\nRegards,\nQc"}], [{"role": "user", "text": "Hello Everyone,\nI am very new to Galaxy. I managed to create a countdata ,factordata, annotate form the in house bulk-RNA dataset. now I am trying to do Specify Contrast(s) of interest (based on the 2: RNA-seq counts to genes) but I keep getting errors. is anyone know how to fix this issue?\n\nDataset Error Report\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.50.0+galaxy0 .\n\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 7944 did not have 5 elements Calls: read.table \u2192 scan Execution halted\n\nTroubleshooting\nThere are a number of helpful resources to self diagnose and correct problems.\nStart here: My job ended with an error. What can I do?"}, {"role": "system", "text": "Hi @DRSEI,\ncould you share your history with me?\nRegards"}], [{"role": "user", "text": "Hi, I\u2019m trying to carry out a DiffBind operation with peak files (in bed format) generated elsewhere. When selecting my input files and parameters, the default setting for Score Column for a narrowPeak file is column number 8. In my narrowPeak files, column 8 corresponds to the -log10(pvalue). Is this what the score refers to, or should I be changing the column number to correspond to another column, like abs_summit or fold_enrichment? I should note that none of the columns in the narrowPeak file are actually called \u2018score\u2019. Thanks in advance!"}, {"role": "system", "text": "Hi @alexbisias\n\n\n\n alexbisias:\n\nIn my narrowPeak files, column 8 corresponds to the -log10(pvalue). Is this what the score refers to\n\n\nUsually, yes, the p-value is what is used for \u201cscore\u201d when calculating the FDR.\n\n\n\n alexbisias:\n\nI should note that none of the columns in the narrowPeak file are actually called \u2018score\u2019.\n\n\nThe file format for narrowPeak datasets is here: Genome Browser FAQ\nYou\u2019ll notice the first few columns are similar to BED format, which has a score metric in the 5th column, but what that score represents differs between the two plus narrowPeak has two more summary attributes.\nResources\n\n\nDiffbind\n\nUsage from the tool form\u2019s help section in Galaxy:\nhttps://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf\n\nBioconductor author Q&A forum: https://support.bioconductor.org/post/search/?query=diffbind\n\n\n\n\nMACS2\n\nSee the README and ISSUES to start with https://github.com/macs3-project/MACS\n\nGoogle Group author Q&A: https://groups.google.com/g/macs-announcement/\n\n\n\n\nTutorials\n\nhttps://training.galaxyproject.org/training-material/search?query=ChIP-seq\nhttps://training.galaxyproject.org/training-material/search?query=macs2\nAnd, the author resources have more examples\n\n\n\n\n\nThe GTN and Bioconductor groups will be hosting a training later this month. Please consider joining for direct expertise help from the original tool authors and scientists from both communities  Sm\u00f6rg\u00e5sbord 3: A week of Free, Online, Galaxy Training supported by the Global Galaxy Training Community"}], [{"role": "user", "text": "I\u2019d like to compare the result of aligning a small RNA-seq dataset using HISAT2 vs Bowtie. (I realize Bowtie is rather old, but I\u2019d like to compare to old  data analyzed with Bowtie.) I aligned the same FASTQ file using the 2 programs, and then ran samtools stats. The \u201craw total sequences\u201d line in the stats output for the HISAT2 bam file matches up with the number of sequencing reads in the original FASTQ file (based on FASTQC), but the total sequences line from the Bowtie bam file is inflated by more than a million sequences. Can anybody see what I might be missing here? Am I misinterpreting the meaning of \u201craw total sequences?\u201d I expected to see different numbers of aligned reads, but not total input reads."}, {"role": "system", "text": "Welcome, @eyoungman\nI took a look at one of your datasets mapped with both tools versus different tools that count up \u201creads\u201d (and sometimes \u201calignment lines\u201d instead!).\nFastq Groomer\n\nTotal seqs in the input fastq dataset reported: 2182610\n\nFastQC\n\nTotal sequences in the input fastq dataset reported: 2182610\nTotal sequences in HISAT2 result BAM (sorted): 2509206\nTotal sequences in Bowtie result SAM (unsorted): 4345310\nTotal sequences in Bowtie result SAM (unsorted) converted with Sam-to-BAM to a BAM (sorted): 4345310\n\nSamtools stats\n\nTotal \u201craw total sequences\u201d in HISAT2 result BAM (sorted): 2182610\nTotal \u201craw total sequences\u201d in Bowtie result SAM (unsorted): 4345310\nTotal \u201craw total sequences\u201d in Bowtie result SAM (unsorted) converted with Sam-to-BAM to a BAM (sorted): 4345310\n\nHISAT2\n\nTotal input reads reported the built-in mapping stats: 2182610\n\nBowtie\n\nNo built-in stats for this older tool\n\nLine/Word/Character count\nSome plain data manipulation to prepare first: Convert BAM-to-SAM or Sam-to-Fastq or Fastq-to-tabular, remove any headers with Select, and count the alignment lines. Then Cut out the reads names (first column) and finally get the Unique occurrences of each record. This is certainly a more complicated way to do the counts but is the \u201ctruth\u201d of the content and can work on any data, with the right prep manipulations, if you are ever unsure about what is going on with a particular tool. Manipulating data this way doesn\u2019t rely on special tools to interpret metadata/flags in non-obvious ways \u2013 plus you can compare raw text data counts to those from other tools to get a better idea about what those may be actually counting up.\n\nTotal \u201cunique reads\u201d in original Fastq (dataset 322 hidden, contained in collection dataset 323 as first dataset): 2182610\nTotal \u201calignment lines\u201d in HISAT2 result BAM: 2509206\nTotal \u201cunique reads\u201d in HISAT2 result BAM: 2182610\nTotal \u201calignment lines\u201d in Bowtie result SAM: 4345310\nTotal \u201cunique reads\u201d in Bowtie result SAM: 2182610\n\nIn summary, each reported alignment line or fastq entry (Fastq, BAM, SAM) is being counted up by some tools (including, in some cases, duplicated reported reads) and others are counting up true unique reads/sequences.\nThis is unfortunate but expected, and part of the reason the updated mapping tools (including Bowtie2) produce native tool mapping stats plus output sorted/better annotated BAMs by default.\nHope that helps, both to help make sense your data and to definitively confirm that all original reads were mapped by both tools. There are probably a hundred different ways to do this with Text/data manipulation tools (and line-command, outside of Galaxy with Unix utilities/scripts), but the tools I used should be a useful start, do not involve complicated regular expressions/programming, and are all included in Galaxy with simple usage.\nThanks!"}], [{"role": "user", "text": "Hi @bjoern.gruening, today I was trying to use Methyldackel but I couldn\u2019t find it in the search tools, even shows \u2018results no found\u2019. Could that be because you are updating the tool? I tried to search for it last week but the same results. Why it does not show up in my search tool? Thank you!"}, {"role": "system", "text": "That is wired. Are you on usegalaxy.eu?\n\nYou can try this link for the lastest version Galaxy | Europe"}], [{"role": "user", "text": "how to use my own google cloud"}, {"role": "system", "text": "Hi @dadada212\nWould you please explain more about your question?\nIf it about running your own Galaxy server, choices about platforms is available here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub"}], [{"role": "user", "text": "Hello,\nI am working on Exome sequencing data.\nNeed to create PED file not sure how?\nSimilar question has been asked before but the link in the answer is not active.\nCan someone please explain how to create PED file?\nThanks"}, {"role": "system", "text": "Hi @Shuchi\nWould you please provide a few more details for context:\n\nServer URL where you are working\nTool you need the input for (full name/version \u2013 find at the very top of tool forms)\nLink to the prior Q&A you are referencing\n\nAre following a tutorial already?  Exome sequencing data analysis for diagnosing a genetic disease\n\n"}, {"role": "system", "text": "Hi @Shuchi,\nthis file needs to be created manually as follow:\n\nCopy the content by clicking on Copy:\n\n\nimage845\u00d7135 11.1 KB\n\n\nClick on Upload data \u2192 Paste/Fetch data, and paste the content:\n\nimage932\u00d7564 54 KB\n\n\nClick in Start\n\n\nIn that way, you can create the PED file.\nRegards"}], [{"role": "user", "text": "I go to \u201cget data\u201d and then \u201c Download and Extract Reads in FASTA/Q** format from NCBI SRA\u201d. I then select input type SRA accession, and use accession number SRR643156 and then execute. I get the error \u201cThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below. {}\u201d. Can someone help me import my data? It\u2019s for an online course through coursera, this is the data they say I have to work with, and I can\u2019t contact anyone through the course for help. Thanks."}, {"role": "system", "text": "\n\n\n Alisha_Maddy:\n\nSRR643156\n\n\nHi @Alisha_Maddy\nThanks for reporting the problem. NCBI can get busy, with this type of error resulting. I\u2019m running a test with that exact accession right now at UseGalaxy.org across the different NCBI fastq get data tools, and the queries are still running and likely would have failed by now if were going to.\nPlease try again if you have not already. I\u2019ll write back with an update.\n\nUpdate:\nThe test downloads of that same accession worked fine. Please try again if you haven\u2019t already."}], [{"role": "user", "text": "Hello , im doing internship in bioinformatic and i need more space and data storage . I\u2019m using interproscan and i have a set of 20 species/genomes at the moment . How can i get it  ?"}, {"role": "system", "text": "Hey @lambard_alexandre,\nyou can apply for an increased storage quota here: http://usegalaxy.eu/quota-increase\nCheers,\nSimon"}], [{"role": "user", "text": "Dear all,\nI am comparing microbial community in response to a treatment using lefse cladogram.\nI have 4 different experimental set up and I am planning to display the 4 lefse analysis together in order to compare result between experiment (see figure attached, \"CURRENT GRAPH). So far I have run 4 lefse analyses separately but I am realising that it makes the reading of the graph difficult, cause I have 4 different legend/letters  (see figure attached, \"CURRENT GRAPH). Is it possible to run 4 lefse analysis separately (from 4 distincts experiments) from the same table input, and therefore get one unique taxa legend for all 4 cladograms. I would like to get the result bellow (see figure attached, \u201cGRAPH EXPECTED\u201d).\nThanks very much for your help.\nScreenshot 2020-11-13 at 10.14.41440\u00d7553 58.3 KB"}, {"role": "system", "text": "In this case you are assuming that a given taxa color (up/down difference abundance) is the same in all the 4 cladograms. I wonder if that would be rare to happen.\nBut\nYou could simply \u201cmerge\u201d all the legend text (and the colored rectangles) in a graphics editor like inkscape, gimp, photoshop, etc. This is what I used to do. But you need to confirm what I said above.\nGood luck"}], [{"role": "user", "text": "Hi, I am trying to download RNA-seq data from the \u201cEBI\u201d website using the \u201cUpload Data\u201d. Unfortunately, only a small amount of data is loaded, for instance, 12 Mb from a 2 GB file. I have repeated this job for many several times, but, each time I encountered this problem."}, {"role": "system", "text": "Hi @karimi65\nWe\u2019ve had many reports of data download failures from EBI SRA over the prior week or so (and before that, during various time windows when they were reorganizing data). We aren\u2019t clear on what issues ENA may be having right now \u2013 perhaps just high volume \u2013 although we\u2019ve also seen some technical problems with the links one can capture from their website.\n Try getting the data from the NCBI SRA instead. Entering URLs in the Upload tool is one method plus there are several dedicated tools. Search the tool panel with the keyword \u201cncbi\u201d to review the choices.\nFor fastq data, both of these tools are great choices. The first downloads individual dataset(s); the second sorts the dataset(s) into collections. Full usage options are on each tool\u2019s form. Both are equally \u201cfast\u201d \u2013 the Galaxy tool name is simply reflective of the underlying name NCBI gave these functions.\n\nDownload and Extract Reads in FASTA/Q format from NCBI SRA\nFaster Download and Extract Reads in FASTQ format from NCBI SRA\n\nHope that works out!"}], [{"role": "user", "text": "Hi, we had a user email us who has a link to a file in Galaxy which the UCSC Genome Browser cannot open because the usegalaxy.org web browser doesn\u2019t support byte range requests:\ntrack name=\u201c6hr Veh 1\u201d type=bigWig bigDataUrl=https://usegalaxy.org/api/datasets/f9cad7b01a47213550329ab30779cd1d/display?to_ext=bigwig\nleads to:\nCouldn\u2019t open https://usegalaxy.org/api/datasets/f9cad7b01a47213550329ab30779cd1d/display?to_ext=bigwig\nI\u2019m pretty sure that it used to be possible to load Galaxy-hosted files into the UCSC Genome Browser. Is there something that has changed on your end?\nbest\nMax"}, {"role": "system", "text": "Resolved 11/14/2022\nHi @Maximilian_Haeussler \u2013 we have added in a change to restore the prior functionality. This is available at UseGalaxy.org so far, and the other usegalaxy.* servers will be updated soon.\nDetails HEAD and byte-range support \u00b7 Issue #14982 \u00b7 galaxyproject/galaxy \u00b7 GitHub\n\n\nUpdate 11/11/2022\nWe identified a change that appears to be on our side triggering the import issue. It was likely introduced in the last release. Details are in the public chat here You're invited to talk on Matrix\nHappy to be wrong about this method \u201cnot ever\u201d working  We\u2019ll post updates back here, and feel free to follow or join the chat. You can also message me directly to coordinate end-user communications around the anticipated fix when the time comes.\nThanks Max!\n\nHi @Maximilian_Haeussler\nI saw that thread at the UCSC forum, too. Unless something changed very recently, hosting files from Galaxy to the UCSC browser should definitely still be possible.\nAs a guess, I think the problem with the particular file the user was asking about is that it didn\u2019t have a \u201cdatabase\u201d key assigned. It didn\u2019t have one when I upload it here yesterday (seems Ok to post since is already public data, and I\u2019ll unshare/purge in a day or two): Galaxy | Accessible History | test bigwig. A matching database -> dbkey is required for the UCSC link to even show up in Galaxy, so how that error was produced wasn\u2019t clear.\nOh! I reread the post at UCSC. This part has clues:\n\nThe Url links to my workspace in Galaxy, where my bigWig files are saved. When I click Submit, the genome browser gives me the following warning:\n\nMaybe the user isn\u2019t linking directly from inside Galaxy, and instead trying to upload the file to the UCSC browser by URL? I forgot how that works on your end \u2013 do users target a specific dbkey/assembly for upload? That never worked from what I remember. People had to start in Galaxy and use the link inside a dataset that points to UCSC.\nI will still take another look. More feedback, probably tomorrow."}], [{"role": "user", "text": "I want use ce10ToCe11.over.chain file with the MiModD rebase tool (Galaxy name: Rebase Sites ) to convert my VCF variant coordinates to the SnpEff genome file coordinates just before i do the annotation.But i find Can\u2019t import the chain files to Galaxy rebase site tool"}, {"role": "system", "text": "Hi,\nyou need to declare the format of the chain file as tabular.\nIf you haven\u2019t done so during upload, you can do it by clicking the pencil icon on the dataset in your history now, then select the datatype tab."}], [{"role": "user", "text": "Hi! Would it be better to use FeatureCounts or StingTie for generating count tables in a RNA-Seq analysis?\nGalaxy tutorials recommend FeatureCounts, but I\u2019ve also read that StringTie is very efficient and widely used.\nI\u2019m working with Arabidopsis thaliana so I\u2019m not really interested in novel transcripts. I\u2019d just like to know what is the difference between these tools and which one should I use, since I get better variances in the PCA plot using tables from FeatureCounts than using tables from StringTie from the exact same data."}, {"role": "system", "text": "Hi @MarioG\nIf you don\u2019t care about novel transcripts and have a reference genome, go straight to Featurecounts and include a known annotation GTF.\nStringtie would include novel transcripts/genes plus known annotation \u2013 but only if the known annotation was included. You can also adjust the settings in Stringtie to only output results based known annotation. If you don\u2019t include known annotation, only counts based on the regions your read data represent (transcripts/genes) will generated \u2013 and that may be where the biggest differences are coming from.\nSo, there are a few different ways to do this and the results would not be expected to be exact between any two DE methods/pipelines based on distinct algorithms. Using differently created transcript/gene boundaries for the \u201cannotation\u201d to determine expression levels is also a factor.\nExtended Help for Differential Expression Analysis Tools\nThanks!"}], [{"role": "user", "text": "I\u2019m trying to install a custom blast db into a locally hosted instance of Galaxy.  I have edited the blastdb.loc file and it is properly detected by Galaxy, as seen in the admin datatable:\nimage1080\u00d7107 6.71 KB\nThe file path points to a folder with the following files:\nmfas.nhr  mfas.nin  mfas.nog  mfas.nsd  mfas.nsi  mfas.nsq.\nIf I go to that folder and run blast at the command line using the \u201cmfas\u201d keyword, the database is fine.\nHowever, when I run the NCBI BLAST + blastn Galaxy tool, I get this error:\n\nBLAST Database error: No alias or index file found for nucleotide database [/apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/] in search path [/apps/galaxy/galaxy_staging1/database/jobs_directory/000/9/working::]\n\nWhat am I doing wrong?  I am currently using the full filepath but I don\u2019t think it\u2019s pointing correctly.  I\u2019ve read the tutorials but it\u2019s unclear if I\u2019m supposed to provide the full filepath or a relative filepath.\nAny help would be greatly appreciated!  I\u2019m new to Galaxy admin but will need to install quite a few custom DBs for my group so I really need this working."}, {"role": "system", "text": "I am not sure but should you not add the actual database to your path? Now it is pointing to a folder and not an alias or index.\nSo instead of /apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/ I think it should be /apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/mfas\nor\n/apps/galaxy/galaxy_staging1/tool-data/app_db/mfas"}], [{"role": "user", "text": "Hello there!\nI am using galaxy on my classes, and would like to delete all users at the course end, but, there is no \u201cdelete user\u201d option on admin area.\nI use Galaxy as a Docker instance.\nThanks for any ideia on how to do it \nCheers"}, {"role": "system", "text": "Hi,\nwhen you are on this page: https:///admin/users then click on the arrow next to the user and inside the drop down you will find the option \u201cDelete\u201d.\nHope that helps,\nBjoern"}], [{"role": "user", "text": "I have paired-end reads data of NGS in 2 fastq files that are: father_R1.fq.gz and father_R2.fq.gz\nHow can I find how many reads are included in these two fastq files and their read length respectively through Galaxy?\nIs there any specific tool for it?"}, {"role": "system", "text": "Did you tried FastQC already? This would be a good start."}], [{"role": "user", "text": "Hi\uff0cI installed the workflow but encoutered the errors as follows:The server could not complete the request. Please contact the Galaxy Team if this error persists.\n{\n\u201cnew_history_name\u201d: null,\n\u201chistory_id\u201d: \u201c1cd8e2f6b131e891\u201d,\n\u201cresource_params\u201d: {},\n\u201creplacement_params\u201d: {\n\u201cplot_title\u201d: \"APEC \"\n},\n\u201cparameters\u201d: {\n\u201c0\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201chid\u201d: 108,\n\u201cid\u201d: \u201cf2db41e1fa331b3e\u201d,\n\u201cname\u201d: \u201cAPECpairedfastqs\u201d,\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c1\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c7ca8f1b7f24e5a2d\u201d,\n\u201csrc\u201d: \u201chda\u201d,\n\u201chid\u201d: 109,\n\u201cname\u201d: \u201cpp_plasmid_database.fasta\u201d,\n\u201ckeep\u201d: true,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c2\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201chid\u201d: 110,\n\u201cid\u201d: \u201cf30a35c999095ed7\u201d,\n\u201ckeep\u201d: false,\n\u201cname\u201d: \u201cplasmidfinder_plusAMR.fasta\u201d,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c3\u201d: {\n\u201csingle_or_paired|type\u201d: \u201ccollection\u201d\n},\n\u201c4\u201d: {\n\u201cnum_lines\u201d: \u201c1\u201d\n},\n\u201c5\u201d: {\n\u201ccolumn\u201d: \u201c9\u201d,\n\u201cstyle\u201d: \u201cnum\u201d,\n\u201corder\u201d: \u201cDESC\u201d,\n\u201cheader_lines\u201d: \u201c0\u201d\n},\n\u201c6\u201d: {\n\u201ccond\u201d: \u201cc9>80\u201d,\n\u201cheader_lines\u201d: \u201c0\u201d\n},\n\u201c7\u201d: {\n\u201ccolumnList\u201d: \u201cc1\u201d,\n\u201cdelimiter\u201d: \u201cSp\u201d\n},\n\u201c8\u201d: {\n\u201cexact\u201d: \u201cfalse\u201d,\n\u201cinverse\u201d: \u201cfalse\u201d,\n\u201cfile_or_type|select\u201d: \u201clist\u201d\n},\n\u201c9\u201d: {\n\u201csingle_or_paired|type\u201d: \u201ccollection\u201d,\n\u201cmlst_or_genedb|job_type\u201d: \u201ccustom_only\u201d,\n\u201cmlst_or_genedb|gene_max_mismatch\u201d: \u201c250\u201d,\n\u201coptions|select\u201d: \u201cadvanced\u201d,\n\u201coptions|min_coverage\u201d: \u201c0\u201d,\n\u201coptions|max_divergence\u201d: \u201c0\u201d,\n\u201coptions|min_depth\u201d: \u201c5\u201d,\n\u201coptions|min_edge_depth\u201d: \u201c2\u201d,\n\u201coptions|prob_err\u201d: \u201c0.01\u201d,\n\u201coptions|stop_after\u201d: \u201c1000000\u201d,\n\u201coptions|mapq\u201d: \u201c1\u201d,\n\u201coptions|baseq\u201d: \u201c20\u201d,\n\u201coptions|minins\u201d: \u201c0\u201d,\n\u201coptions|maxins\u201d: \u201c1000\u201d\n},\n\u201c12\u201d: {\n\u201cgroupcol\u201d: \u201c3\u201d,\n\u201cignorecase\u201d: \u201cfalse\u201d,\n\u201cignorelines\u201d: null,\n\u201coperations_0|optype\u201d: \u201cunique\u201d,\n\u201coperations_0|opcol\u201d: \u201c3\u201d,\n\u201coperations_0|opround\u201d: \u201cno\u201d,\n\u201coperations_0|opdefault\u201d: \u201c\u201d\n},\n\u201c13\u201d: {\n\u201ccolumnList\u201d: \u201cc1\u201d,\n\u201cdelimiter\u201d: \u201cT\u201d\n},\n\u201c14\u201d: {\n\u201cexact\u201d: \u201cfalse\u201d,\n\u201cinverse\u201d: \u201cfalse\u201d,\n\u201cfile_or_type|select\u201d: \u201clist\u201d\n},\n\u201c15\u201d: {\n\u201cchecks_0|pattern\u201d: \u201c^>\\d+(.*?).*\u201d,\n\u201cchecks_0|replacement\u201d: \u201c>\\1\u201d\n},\n\u201c16\u201d: {\n\u201cdbtype\u201d: \u201cnucl\u201d,\n\u201ctitle\u201d: \u201cMatched Plasmids\u201d,\n\u201cparse_seqids\u201d: \u201cfalse\u201d,\n\u201chash_index\u201d: \u201ctrue\u201d,\n\u201cmask_data_file\u201d: null,\n\u201ctax|taxselect\u201d: \u201c\u201d\n},\n\u201c17\u201d: {\n\u201cdb_opts|db_opts_selector\u201d: \u201chistdb\u201d,\n\u201cdb_opts|database\u201d: \u201c\u201d,\n\u201cdb_opts|subject\u201d: \u201c\u201d,\n\u201cblast_type\u201d: \u201cmegablast\u201d,\n\u201cevalue_cutoff\u201d: \u201c0.001\u201d,\n\u201coutput|out_format\u201d: \u201cext\u201d,\n\u201cadv_opts|adv_opts_selector\u201d: \u201cbasic\u201d\n},\n\u201c18\u201d: {\n\u201csureness\u201d: \u201c0.75\u201d,\n\u201clength\u201d: 0,\n\u201ccoverage\u201d: 0,\n\u201ctitle\u201d: \"APEC \",\n\u201canonymize\u201d: \u201cfalse\u201d,\n\u201ccombineincs\u201d: \u201cfalse\u201d\n}\n},\n\u201cparameters_normalized\u201d: true,\n\u201cbatch\u201d: true\n}\nanybody to fix this?"}, {"role": "system", "text": "Hi @aslangabriel\nFirst, try a rerun of what you were doing. Several of the public servers are undergoing updates. If you haven\u2019t tried what you were doing again, please try that now.\nIf the problem continues, please provide more details:\n\nWhere are you working? URL if a public server or describe if other.\nDid you get any error datasets? Do those errors describe what is going wrong (how to check error logs)?\nAre the inputs a match to what the workflow is expecting (how to check inputs)?\nWhere did you source the workflow? If it was imported (installed?), did you open the workflow after importing and check for warnings? Try this if you haven\u2019t already.\nIf you can, please post share links: to the history with the inputs, to the imported workflow, and to the output history (if not the same as the input). That will provide context the community can help with.\n\nLet\u2019s start there "}], [{"role": "user", "text": "Hello,\nI am trying to perfrom a de novo assembly from nanopore reads. All the reads are stored in a fasta file. I tried to run the flye assembler on the dataset, but 3 hours later, It returns with an error.\ntoolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/2.9+galaxy0 .\nDoes anyone know what this error means, and point me in the direction to fix it.\nI can provide more information if needed.\nThanks"}, {"role": "system", "text": "Hi @Bolivian_Ram,\ncould you share your Galaxy history with me? I\u2019ll have a look. You can share it with me by using my email address .\nRegards"}], [{"role": "user", "text": "Hello Galaxy Community,\nI am struggling to set up the GalaxyCloudMan through an AWS to my Galaxy through usegalaxy.org.au.\nFirst according to the instructions I have made AWS account > create group (selecting ec2 and s3 full access) > users > add users to group > and download the credentials access key and secrete key. These keys I have used to launch GalaxyCloudMan through CloudLaunch.\nIn CloudMan Console I access the Galaxy and I registered using my mail ID and password that I made on CloudLaunch. Galaxy on the cloud interface appear but not fully active.\nActually, I am Coursera student where according to the instructions the Galaxy CloudMan is launched from CloudMan console step to the cloud selecting new cloud cluster in the account available on usegalaxy.org.\nHere is my QUESTION as well as PROBLEM that I do not have Cloud tab or option in my galaxy account available on https://usegalaxy.org.au/. Therefore, I could not launch or access the Galaxy CloudMan to my Galaxy account (https://usegalaxy.org.au/).\nWhat step am I missing to follow? What is required for cloud option to appear in Galaxy.\nKindly resolve this problem\nNeeti Nirwal"}, {"role": "system", "text": "Hi @NirwalNeeti\nThe way cloud Galaxy is started up has changed a bit since that tutorial was published. Launch one from here instead: https://launch.usegalaxy.org/\nUse the GVL version for the best performance and reliability (it is based on the Galaxy release 20.01)\nThe other Cloudman option is considered deprecated. So shut down and remove the old one, and start over with the GVL version. Your account at the new cloud server will be distinct from your accounts on any public Galaxy server.\nMeaning, you need to add yourself as an administrator and create an account through the Galaxy website created inside the GVL appliance. Once both are done, you\u2019ll be the administrator of the server with full control over configuration/administration (adding cluster nodes, installing new tools, and the like).\nRelated Q&A: Galaxy Cloudman or GVL?\nThanks!"}], [{"role": "user", "text": "Hi\none of my friends has a workflow which is in another galaxy server.\nShe send me a work flow in pdf file and some file with .ga.\nI want to know is there anyway she can share that workflow with me in galaxy ? because i cant move it in my workflow section.\nThanks"}, {"role": "system", "text": "Hi,\nnot sure what\u2019s inside the pdf, but you should be able to import the .ga workflow definition file into any Galaxy server.\nWhile logged in, select the Workflow tab from the top menu, then select Upload or import workflow (i.e., the little icon in the top right corner of your workflow list view). On the next page, you will want to provide your .ga file as an Archived workflow file.\nNote that this imports the workflow definition, but in order to run the workflow, you also have to have all tools used in it installed on the  Galaxy instance you are working on."}], [{"role": "user", "text": "Hello!\nI am trying to run Galaxy on the Genetics and Genomics Analysis Platform (GenAP) hosted by Compute Canada and am running into an error with various tools.\nI have fastq files from Illumina RNA sequencing that I am trying to assemble into a transcriptome using Trinity or to map it to a reference genome of a closely related species using HISAT2.\nWhen I try to run either of these tools I get the following error:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: \u2018time\u2019\nI get this error if I try to run Trinity, HISAT2, or Bowtie2. This is the full error message I get when I try to run Trinity:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: \u2018time\u2019\n{\n\u201chistory_id\u201d: \u201cf597429621d6eb2b\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1\u201d,\n\u201ctool_version\u201d: \u201c2.9.1\u201d,\n\u201cinputs\u201d: {\n\u201cpool|pool_mode\u201d: \u201cYes\u201d,\n\u201cpool|inputs|paired_or_single\u201d: \u201cpaired\u201d,\n\u201cpool|inputs|left_input\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cNS.1543.001.NEBNext_dual_i7_176\u2014NEBNext_dual_i5_176.Fish1-unfed_R1.fastq.gz (as fastqsanger)\u201d,\n\u201ctags\u201d: ,\n\u201ckeep\u201d: false,\n\u201chid\u201d: 1,\n\u201cid\u201d: \u201c0a248a1f62a0cc04\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201cpool|inputs|right_input\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cNS.1543.001.NEBNext_dual_i7_176\u2014NEBNext_dual_i5_176.Fish1-unfed_R2.fastq.gz (as fastqsanger)\u201d,\n\u201ctags\u201d: ,\n\u201ckeep\u201d: false,\n\u201chid\u201d: 2,\n\u201cid\u201d: \u201c03501d7626bd192f\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201cpool|inputs|strand|is_strand_specific\u201d: \u201cfalse\u201d,\n\u201cpool|inputs|jaccard_clip\u201d: \u201cfalse\u201d,\n\u201cnorm\u201d: \u201ctrue\u201d,\n\u201cadditional_params|min_contig_length\u201d: 200,\n\u201cadditional_params|guided|is_guided\u201d: \u201cno\u201d,\n\u201cadditional_params|long_reads\u201d: null,\n\u201cadditional_params|min_kmer_cov\u201d: 1,\n\u201c__job_resource|__job_resource__select\u201d: \u201cno\u201d\n}\n}\nI seem to be able to run other tools (FastQC, Trimmomatic) and I have also run HISAT2 on these same fastq files using the usegalaxy.org instance.\nIf anyone has any advice or can diagnose this problem, I\u2019d really appreciate it!\nThank you!"}, {"role": "user", "text": "FYI, solved my own issue by playing around with some of the sample data from Galaxy tutorials.\nMy file format was \u201cfastqsanger.gz\u201d so I changed the datatype to \u201cfastqsanger\u201d and this seems to have solved the error I was getting.\nNow let\u2019s see where I\u2019ll get stuck next!\nThank you Galaxy community!"}], [{"role": "user", "text": "Hi\nI am new to Galaxy and bioinformatics and just going through the training. I can not find the \u2018select first\u2019 or the \u2018compare two datasets\u2019 tools in the tool panel  and when I go to the Tool Shed to search for them they don\u2019t appear to be there. Has the name of these tools changed?\nThanks "}, {"role": "system", "text": "Hi @Kate78\nSome of these basic manipulation tools are native to the Galaxy application, and won\u2019t be in the ToolShed. They should be found with a tool panel search but that has a few problems right now.\nAt UseGalaxy.* servers the two tools you asked about are under the GENERAL TEXT TOOLS group and nested in sub-groups:\n\nText Manipulation \u2192 Select first lines from a dataset (Galaxy Version 1.0.1)\nJoin, Subtract, and Group \u2192 Compare two Datasets to find common or distinct rows (Galaxy Version 1.0.2)\n\nOnce you find a tool and load the tool form, you can star it in the upper right corner to add it to your shorter list of favorites.\nWe are working to improve/repair the tool search. Let\u2019s leave this topic open in case you or others have more tools that are difficult to find. We can also share these with the developers.\nI can confirm the two tools you already posted cannot be found with a tool panel search at UseGalaxy.org or UseGalaxy.eu right now (instead, scroll through the sections above). Other general text tools are also impact. The tool search is working properly at UseGalaxy.org.au.\nYou can run through tutorials at any of the servers listed under the \u201cAvailable at these Galaxies\u201d menu at the top of each. Some tools are really only available at specific public sites or a special Docker container. The exceptions are the \u201cIntroduction to Galaxy\u201d tutorials \u2013 these should work at most public sites even if not listed.\n\nFor everyone: if a tool is difficult to find at an \u201cAvailable Galaxy\u201d, please post a reply and include the following:\n(quote and replace text)\n\n\nI am working at: public server URL\n\n\nThe tutorial is: the tutorial URL\n\n\nProblems finding the tool(s): tool names as given in the tutorial\n\n\nThanks!"}], [{"role": "user", "text": "I am currently trying to upload bed files in order to use the deepTools computeMatrix tool. The bed files appear to be loaded in correctly, but whenever I run the computeMatrix tool I get the below error.\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: objectstore, _call_method failed: get_filename on Dataset(id=29454816), kwargs: {}\nAfter the error occurs, the upload bed file job appears red and gives an error that my input file has chrX, but the genome file does not. I am using mm10 when I upload the bed files.\nAnother person in the lab has used the exact same files and it has worked for them, so any help would be appreciated."}, {"role": "system", "text": "Hi - If the same data/tool worked for someone else on the same server, this was likely a transient server problem. This doesn\u2019t look like a partial data upload problem.\nPlease try uploading the dataset again new, then rerun tools against it. Make sure the upload is complete and that the exact database mm10 is assigned to the dataset once in the history. There are a few mm10 variants that are named in a similar way, so this is worth confirming.\nIf you get another failure, please send in a bug report by clicking on the bug icon in one of the red datasets. Include a link to this Galaxy Help post in the comments. Be sure to leave all inputs and outputs (even if some are hidden) in an undeleted state. FAQ: Issue Reporting - Galaxy Community Hub\nI\u2019m not sure what this means, but mm10 is indexed natively to Galaxy, so this probably isn\u2019t a custom genome fasta file. If it is for some reason, you can skip using it and instead use the native index at https://usegalaxy.org with all (or, most) tools, including this one.\n\n\n\n ap2em:\n\nbut the genome file does not\n\n\nThanks!"}, {"role": "system", "text": "Update\nThe bug report sent in revealed that the BED dataset contained extra spaces. Problematic formatting can produce many types of tool errors, but the solution is the same \u2013 fix the formatting, then rerun tools.\nTabular datasets (including BED data) can be cleaned-up a few ways.\n1. Fix the dataset formatting locally on your computer, then  Upload  it to Galaxy after.  Do this if you plan to use the data outside of Galaxy as well. Or, fix the data in Galaxy with one of the methods below and download a copy of the format-corrected BED dataset.\n2. Remove the extra spaces during  Upload  to Galaxy.  How-to: In the  Upload  tool, click on the \u201cgear\u201d icon and check the box to \u201cConvert spaces to tabs\u201d. The tool merges all consecutive \u201cwhitespace\u201d (actual spaces or tabs) into a single \u201ctab\u201d \u2013 which is what is wanted for proper tabular dataset formatting, including BED data. The screenshot below shows where to find the option.\nNote:  If you ever decide to \u201cpaste\u201d (or type in) tabular data lines with the  Upload  tool, this is a good option to use to avoid formatting issues.\nWarning:  Use this option carefully. Example: if a dataset has header lines, converting \u201cwhitespace to tabs\u201d will be applied to those header lines, not just the data lines. That is not always desirable as it can introduce another formatting problem.\n\nupload_cleanup_whitespace.png1742\u00d7640 71.2 KB\n\n3. Run the tool  Text Manipulation > Convert delimiters to TAB  on an already loaded dataset.  Use the settings shown in the screenshot below. Once the job completes, the result will have the datatype \u201ctabular\u201d assigned. Click on the pencil icon (Edit Attributes) for the dataset to change it back to BED, and save.\n\ntext-manipulation_cleanup_whitespace2.png1410\u00d7742 50 KB\n\nFAQs:\nGeneral error help: Troubleshooting resources for errors or unexpected results\nSupport links: https://galaxyproject.org/support/#getting-inputs-right\n\nFormat help for Tabular/BED/Interval Datasets\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\nTool error? Try Sorting Your Inputs\n"}], [{"role": "user", "text": "Hi everyone \nI have a list of paired-end files from Rad-Seq sequencing, for which the barcode (which were at the start of the sequences) were removed after demultiplexing. I was wondering though, I now intend to use the stacks series for a denovo map, but I noticed that my sequences all start with a part of the restriction site used (eg in my forward sequences it starts with TGCAG, and in reverse with CGG). Should I trim (with trimmomatic by example) my data before proceeding with the Stacks series ?\nThank you in advance! "}, {"role": "system", "text": "Hi Joelle  ,\nIf I am not saying a mistake, you don\u2019t have to trim your data between process_radtags step and others Stacks tools.\nCheers,\nYvan"}], [{"role": "user", "text": "I have FASTQ files and I am going to map with reference genome Aspergillus niger.But I can not find Aspergillus niger  as reference genome in Galaxy.\nCan you help me to sort out at your earliest since I have to finish this for my thesis please.\nKind regards,\nJagath Ranasinghe -Sri Lanka"}, {"role": "system", "text": "Hi @Jagath_Ranasinghe,\nyou can download the A. niger genome from the NCBI website and uploading it to Galaxy. In that way you can use it for your analysis.\nRegards"}], [{"role": "user", "text": "I am trying to set up Galaxy to work on AWS.\nAccording to the instructions here https://galaxyproject.org/cloudman/getting-started/ I am supposed to go to the CloudLaunch application and choose Galaxy CloudMan, but the link is broken and in https://launch.usegalaxy.org/catalog it notes that CloudMan is deprecated and GVL is preferable.\n\nWhat is recommended?\nThanks"}, {"role": "system", "text": "The documentation is out of date on the Getting Started page (I\u2019ll update it). The option to use is the GVL."}], [{"role": "user", "text": "Hello! I am fairly new to Galaxy, and am trying to complete a differential gene expression analysis. The tools I am using are HISAT2, featureCounts, and then DESeq2. I have run all of the programs successfully until DESeq2. DESeq2 will run and complete successfully (turns green), but it gives me 0 lines for the output, so I don\u2019t actually receive any results. Why is this happening?\nAny help is appreciated! Thanks in advance!"}, {"role": "system", "text": "Hi @edenbishop\nPlease try at least one rerun first.\nIf that also fails, double checking the inputs is the next step. This forum has much Q&A about what has worked for others, and the GTN tutorials have examples. If you get stuck, please consider posting back a shared history link and we can take a closer look. Troubleshooting errors\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Hi @jennaj ,\nThanks for the reply! I did a rerun and got an error that time. My issue was that I was using different featureCount files with the same name. I have started over and DESeq2 runs successfully now and actually gives output lines, so I\u2019m all good!\nThanks for the help!"}], [{"role": "user", "text": "Hi!\nI want to upload a file, but the link \u201cUpload File\u201d is not working, no window pops up as it normally would. Has anyone else had this problem?"}, {"role": "user", "text": "Sorry, it is the \u201cUpload File from your computer\u201d link under the tab \u201cGet Data\u201d. I just realized that there is a second button at the top left under the tools search bar called \u201cUpload Data\u201d which does work normally. So now that I have figured that out I can upload data."}], [{"role": "user", "text": "Hi,\nI have a question about the table compute tool. Whatever I change in the syntax the custom expression on the whole table does not work. It outputs an empty tabular file instead of a table with the new values. See the image below.\n\n2023-03-271476\u00d7759 17.3 KB\n\nHere the steps that I followed in table compute tool:\n\n\n\u201cInput Single or Multiple Tables\u201d: Single Table\n\nparam-file \u201cTable\u201d: iris.csv with box: input data has Column names on the first row checked\n\n\u201cType of table operation\u201d: Perform a full table operation\n\n\n\u201cOperation\u201d: Custom\n\n\n\u201cCustom expression on \u2018table\u2019, along \u2018axis\u2019 (0 or 1)\u201d: table.sub(table.min(0), 0)\n\n\n\n\n\n\n\nAny help with solving this problem would be appreciated!\nDaria"}, {"role": "system", "text": "Hi @dariakoppes\nAre you trying to compute the minimum value per column? Please try this:\n\nScreen Shot 2023-03-27 at 12.07.29 PM1426\u00d7984 73.3 KB\n\nIf you want to instead calculate the minimum value per column AND subtract that minimum from all the original values in that same column, change the operation to this:\n\nScreen Shot 2023-03-27 at 12.27.00 PM1060\u00d7186 9.94 KB\n"}], [{"role": "user", "text": "Hello everyone!!\nToday I found a complete set of Trinity tools in test.galaxyproject.org (not implemented in Main, yet) but I\u2019m concerned about the registration.\nDo I need a new account or just use the same of Main Galaxy?  I don\u2019t wanna have any problem with a duplicated account.\nThanks in advance!"}, {"role": "system", "text": "Create a new account there, these are completely separate services and there is no duplication prohibition between them."}], [{"role": "user", "text": "Hi!\nI setup my local Galaxy v20.01 and I\u2019d like to change username on User Management of Admin menu . In version v19.05, clicking on the user name button takes me to a page where I can change the email address and public name, but in v20.01, it takes me to the page of the admin user\u2019s own information. Is this a known issue?\nThanks,\nYukie"}, {"role": "system", "text": "Hi @yukieymd\nChoices:\n\nHave the user update their own account preferences.\nSet up \u201cimpersonation\u201d. When you are logged in as an admin, the option will be under Admin > User Management > Users > search for the user > find it in the pull-down menu. Galaxy Configuration \u2014 Galaxy Project 21.05.1.dev0 documentation\n\nThere is probably a way to do this line command against the database (or using the gxadmin function), but I don\u2019t know the exact details, and it may different between servers anyway. You could ask at either of these Gitter channels instead (public channels \u2013 don\u2019t post anything private!): galaxyproject/admins - Gitter or maybe galaxyproject/dev - Gitter\n\n\nThe other function was deprecated as far as I know \u2013 it presents at any server I work at as admin. Likely was a security related change. You could ask for confirmation at Gitter as well.\nThanks!"}], [{"role": "user", "text": "Dear Galaxy team,\nI am attempting to reproduce a galaxy tutorial using command line versions of the tools(Exome sequencing data analysis for diagnosing a genetic disease)\nI tried filtering the bam files using samtools (command line version) but the result I got is different if I use the galaxy platform.\nhere is the command I used on my PC samtools view -F 0x012 mapped_reads_father.bam\nPlease advise. Thanks"}, {"role": "system", "text": "Hi @vappiah\nIf you are getting a different result with any tool locally versus Galaxy, try comparing the command line Galaxy used with what you are using locally.\nFind the Galaxy command string on the Dataset Information page ( icon in each dataset) in the section \u201cJob Information\u201d. This is the same page where other job logs are found.\nYou can also scroll down a bit more to the section \u201cJob Dependencies\u201d to learn the version of any dependencies that were used. For Samtools, the version probably won\u2019t make a difference, but could with other tools, including tools used upstream.\n\n\n\n vappiah:\n\nhere is the command I used on my PC samtools view -F 0x012 mapped_reads_father.bam\n\n\nYou are trying to reproduce the filter step here in the tutorial, correct?\nIf yes, the command line is this, and is the same as you would find in the Galaxy command string log:\nsamtools view -o 'output.bam' -h   -b  -F 0xc input.bam\nHope that helps!"}], [{"role": "user", "text": "Hello, I\u2019m running a diffbind operation between two conditions from an ATAC-seq experiment with two replicates in each. However, I get an error message resulting in my output files being buggy/red.\n\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\n6hVC1_peaks_narrowPeak 6hVC1_peaks_narrowPeak  VC  NA raw\n6hVC2_peaks_narrowPeak 6hVC2_peaks_narrowPeak  VC  NA raw\n6hdTAG2_peaks_narrowPeak 6hdTAG2_peaks_narrowPeak  dTAG  NA raw\n6hdTAG3_peaks_narrowPeak 6hdTAG3_peaks_narrowPeak  dTAG  NA raw\nconverting counts to integer mode\ngene-wise dispersion estimates\nmean-dispersion relationship\nfinal dispersion estimates\nWarning message:\nNo sites above threshold\nError in pv.getPlotData(DBA, attributes = attributes, contrast = contrast,  :\nUnable to plot \u2013 no sites within threshold\nCalls: dba.plotHeatmap \u2192 pv.getPlotData\n\nDoes \u201cNo sites above threshold\u201d in the error message just suggest that there were no differentially accessible sites (which I was expecting)? If so, why does the output seem to suggest there was an error in the process, rather than just returning an \u2018empty\u2019 interval file or some \u2018blank\u2019 result? I\u2019m quite new to using diffbind on Galaxy, so I\u2019m not too familiar with the various error messages that might occur. Thank you!"}, {"role": "system", "text": "\n\n\n alexbisias:\n\nDoes \u201cNo sites above threshold\u201d in the error message just suggest that there were no differentially accessible sites (which I was expecting)?\n\n\nYes, that is what the message means.\n\n\n\n alexbisias:\n\nIf so, why does the output seem to suggest there was an error in the process, rather than just returning an \u2018empty\u2019 interval file or some \u2018blank\u2019 result?\n\n\nThe tool wrapper is around a Bioconductor tool written in R. Trapping errors for these cases usually relies on the output from R, and the same tool/inputs would likely fail with the same message if run directly.\n\n\n\n alexbisias:\n\nI\u2019m quite new to using diffbind on Galaxy, so I\u2019m not too familiar with the various error messages that might occur. Thank you!\n\n\nNo problem! Some tools run in Galaxy will produce more meaningful messages than others. Much depends on how error trapping was implemented in the original 3rd party tool. \nThe GTN FAQs here can help. Search this page with keywords or error messages:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAnd this specific FAQ explains how to find all of the failure details:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello I have been using Galaxy europe for some time without problems. Is it normal that the site does not work today?"}, {"role": "system", "text": "usegalaxy.eu is unreachable today due to a network issue.  The compute center is working to fix it, but we do not have an estimate of how long it will take."}], [{"role": "user", "text": "Hello,\nI\u2019m at a standstill trying to install a local instance of the Galaxy server.  I hoping someone on this forum who has successfully installed a local instance of Galaxy might work with me to compare install notes to see where I\u2019m messing things up.\nMy issues, as already posted on this forum, are:\n\nUpload from the local computer just hangs, no error in logs.\nSort of data hangs, again no error in log files sort data seems to hang\n\nrpy2 module is not found whenever you try to run stats on a data file, even though the module is installed according to Galaxy web UI.   rpy2 install fails\n\n\nThose are my top three issues right now.\nVersion: 22.05\nNginx: Proxy\nOS: Rocky 8\nThanks\u2026"}, {"role": "system", "text": "Hi @cjkeist\nMaybe these tutorials will help?\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Galaxy Server administration\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I would like to use the SnpSift dbNSFP tool to annotate some vcf files.\nThe description for the tool highlights that pre build versions of the database are available on the SnpSift website, which I did find on the page, https://pcingola.github.io/SnpEff/ss_dbnsfp/.\nI have attempted to import the GRCh38 database to galaxy but the download connection keeps being cut off.\nHas anyone successfully imported this database to galaxy? Is it available somewhere else?"}, {"role": "system", "text": "Hi @numbergirl86\nTry getting the data from here instead: Index of /WGSAdownload/resources/dbNSFP/\nRead this file: dbNSFP4.1c.readme.txt \u2013 it explains the contents of each file in the directory so you can choose the one you need.\nCapture the link for the data you want to use and paste it into the Upload tool. The tool form help explains how to label the data (\u201cdatatype\u201d) for the Galaxy wrapped version to recognize the data as an appropriate input.\nNote: This data is for hg38 only. There are other ways to get/filter the full data at the website you referenced, but all that prep would be done on the command line first, then the proper filtered result/format loaded up to Galaxy. The exact instructions are on that website - although it doesn\u2019t seem like you\u2019ll need to do that and can instead use the pre-built data linked above, which is far simpler, smaller, and is designed to work within Galaxy.\nHope that helps!"}], [{"role": "user", "text": "Hi,\nI have a problem with the Cloud Authorization option on Galaxy.\nI followed these steps:\n\nI log in to my account at usegalaxy.org\n\nI click User -> Preferences -> Manage Cloud Authorization ->Create New Authorization Key\nI pick Azure and enter my details, but the Save Key button never gets activated.\n\nuntitled973\u00d7727 63.9 KB\nI also tried installing Galaxy locally and followed the same steps, but the button never got activated.\nI was wondering whether anyone has had a similar problem and could give me some advice?\nThanks in advance!"}, {"role": "system", "text": "Hi @rainbowmiha\nApologies for the delay. This is beta functionality, and it should not be displayed.\nWe will be making adjustments to how this particular \u201cbeta\u201d is status is communicated in deployments. Most likely the function will be de-activated until finalized.\nDevelopment ticket if interested: https://github.com/galaxyproject/galaxy/issues/10922\nThank you for reporting the problem!\ncc @mvdbeek @vahid"}, {"role": "system", "text": "Hi @rainbowmiha,\nThank you for testing this feature. As @jennaj has mentioned, this feature is in beta status, and it seems it is broken through-out the development.\nWe will fix the UI, meanwhile, you can create authnz using Galaxy\u2019s API. For this, you may run the following (replacing ... with your values) in your terminal:\ncurl --header \"Content-Type: application/json\" \\\n--request POST \\\n--data '{\"provider\": \"azure\", \"config\": {\"tenant_id\": \"...\",\"client_id\": \"...\",\"client_secret\": \"...\"}}' \\\nhttps://usegalaxy.org/api/cloud/authz?key=...\n\nThis will return the configuration created in json format. For instance:\n{\n    \"model_class\": \"CloudAuthz\",\n    \"config\": {\n        \"client_id\": \"...\",\n        \"client_secret\": \"...\",\n        \"tenant_id\": \"...\"\n    },\n    \"user_id\": \"...\",\n    \"last_activity\": \"2020-12-15 15:37:56.123303\",\n    \"create_time\": \"2020-12-15 21:37:56.125248\",\n    \"description\": \"\",\n    \"last_update\": \"2020-12-15 15:37:56.123284\",\n    \"id\": \"...\",\n    \"provider\": \"azure\",\n    \"authn_id\": null\n}\n\nThen you should be able to use id to use the cloud storage api; send data to Azure or get your data from Azure to Galaxy.\nHowever, at the moment, there is a bug using this feature on usegalaxy.org as this feature requires OIDC which is not yet enabled on usegalaxy.org.\nI created an issue for the UI and another one for azure on usegalaxy.org."}], [{"role": "user", "text": "I used STAR to analyze the paired-end reads and then STAR-fusion with the chimeric junctions file from STAR to detect fusion transcripts. The K562 SRA file from NCBI is used for the test run. The reference genome, GTF file and BLAST+ result file are all obtained from CTAT-RESOURCE-LIB. The galaxy run page is set up as shown in snapshot below. Although the chimeric junctions file from STAR shows 1.4M hits, no fusion transcript was detected by the STAR + STAR fusion. The output showed zero line.\nI noticed that the version of STAR fusion 0.54 is really out of date and is probably incompatible with the STAR version 2.78a in galaxy. I actually tried different versions of STAR, but got the same results with zero lines in the outputs.\nI believe this is the compatibility issue with STAR fusion. Is it possible for galaxy developers to update the STAR fusion version in galaxy?\n\nstarfusion1691\u00d71273 176 KB\n"}, {"role": "system", "text": "See star fusion validation failure"}], [{"role": "user", "text": "\nGalaxy.jpg1250\u00d7850 412 KB\n\nHi, Greeting! I am a new user of Galaxy.\nSince last week, I am not able to format my data to select raw for class or subclass for Lefse. With the same data, it worked perfectly two weeks ago. Anyone could get some hints to this problem? Thanks!"}, {"role": "system", "text": "Your data appears to be in csv format (comma separated) and the tool is expecting tabular format (tab separated) for the input.\nRead the tool form help to understand more about the expected input content/format.\nYou might need to reformat the data before uploading to Galaxy or you might be able to reformat within Galaxy (with a tool such as Convert delimiters to TAB). Note that is is just a general guideline \u2013 double check your content versus the help to confirm that any transformed data actually meets the input requirements stated to use the tool successfully.\nI\u2019m not sure why this worked two weeks ago and not now. But if you want to use the tool, at this public Galaxy server (http://huttenhower.sph.harvard.edu/galaxy), this is the tool version available."}], [{"role": "user", "text": "Hello, I am trying to analyze a ChIPseq experiment for the first time. I have a collection of ten fastq files, and I queued a series of jobs on this collection: (1) FastQC, (2) Trimmomatic, (3) FastQC on the Trimmomatic output, and (4) BWA on the Trimmomatic output. The first FastQC job stalled with only 8/10 files completed. The Trimmomatic and subsequent FastQC jobs completed. The BWA appears stalled on the first 2/10 files. These have been stalled since Friday. Wondering if this is a typical run time and I should keep waiting, or if this is a sign of a problem? Very new to Galaxy. Thank you!\n\nScreen Shot 2022-09-26 at 4.05.09 PM2012\u00d71144 328 KB\n\n\nScreen Shot 2022-09-26 at 4.23.25 PM594\u00d71104 58.9 KB\n"}, {"role": "system", "text": "Hi @jar026\nThe UseGalaxy.org server is very very busy. Updates are posting in this topic.\n\n  \n    \n    \n    Fastp not running: Confirmed job delays at UseGalaxy.org 09/23/2022. Solution: allow queued jobs to process usegalaxy.org support\n  \n  \n    Update 09/26/2022 \nThe UseGalaxy.org server is still very busy. \nLeave jobs queued for the fastest processing. \n\nUpdate 09/23/2022 \nAt usegalaxy.org we were able to confirm the delays and fixed a small technical issue. \nPlease leave queued jobs queued. Avoid deleting and rerunning as that will lose the job\u2019s original place in the queue. \nThanks to everyone who reported the problem! \n\nHi @imperialadmiral121 \nThe public servers might just be busy. https://status.galaxyproject.org/ \nAre you working\u2026\n  \n\n"}], [{"role": "user", "text": "Hello.\nI m running align.seqs output file from Unique.Seqs against SILVA reference file. How long this process will take. ? Its been more than 8hr. thanks."}, {"role": "system", "text": "There were job delays due to server issues at Galaxy Main https://usegalaxy.org and Galaxy EU https://usegalaxy.org\nRecent Q&A with details: Job delays at Galaxy Main https://usegalaxy.org and Galaxy EU https://usegalaxy.eu\nGalaxy Main is processing jobs again now. Wait for your job(s) to finish, instead of rerunning, to preserve your placement in the current job queue."}], [{"role": "user", "text": "Hi,\nJust starting as a Galaxy user I got an issue conserning annotation. I have a number of featureCounts output files for DE analysis. A number of significantly differentially expressed genes with a Fc of >2 were obtained. However, when I want to annotate them using \u2018Annotate DeSeq2/DexSeq output tables\u2019 my columns with the annotation info returns only NAs. I got the impression that the cause of this is that featureCount files come with the Entrez-id while this information is not present in my .gtf file that I got from the EnSembl repository (GRCh38.102.gtf.gz; removed the header lines containing an #). Any suggestions how to change the gene Entrez-id in my featureCounts output or how to get ENS numbers in my .gtf file? Thanks for your help,\nHenk"}, {"role": "system", "text": "Hi @HUJI_stu,\nyou can use the annotateMyIDs tool in order to perform such format conversion by using the featureCounts file as input. Let me know if it works.\nRegards"}], [{"role": "user", "text": "Hi,\nI am trying to install Galaxy on my MAC and am getting the following errors on install:\nBelow is the log.\nMy Operating system is MAC OS Catalina 10.15.6 (19G73)\nThanks !\n================================================================\nSuccessfully built attmap bagit beaker cachecontrol dictobj dogpile.cache future prettytable pyperclip pyrsistent pysam pysftp pyyaml rdflib-jsonld refgenconf sqlalchemy-utils tempita ubiquerg warlock wrapt yacman scandir\nInstalling collected packages: six, python-dateutil, chardet, certifi, idna, urllib3, requests, pyjwt, pycparser, cffi, cryptography, adal, vine, amqp, appdirs, ubiquerg, attmap, attrs, azure-nspkg, azure-common, azure-cosmosdb-nspkg, futures, azure-storage-common, azure-cosmosdb-table, oauthlib, requests-oauthlib, isodate, msrest, msrestazure, azure-mgmt-nspkg, azure-mgmt-compute, azure-mgmt-devtestlabs, azure-mgmt-network, azure-mgmt-resource, azure-mgmt-storage, azure-storage-nspkg, azure-storage-blob, pytz, babel, bagit, bcrypt, tzlocal, setuptools-scm, bdbag, beaker, pyyaml, requests-toolbelt, boto, bioblend, webencodings, pyparsing, packaging, bleach, boltons, docutils, jmespath, botocore, s3transfer, boto3, numpy, bx-python, cachecontrol, cachetools, cheetah3, pbr, stevedore, pyperclip, wcwidth, cmd2, prettytable, cliff, cloudauthz, netaddr, rfc3986, oslo.i18n, wrapt, debtcollector, oslo.config, msgpack, iso8601, netifaces, oslo.utils, oslo.serialization, os-service-types, keystoneauth1, python-keystoneclient, pyasn1, rsa, pyasn1-modules, google-auth, uritemplate, httplib2, google-auth-httplib2, google-api-python-client, simplejson, deprecation, munch, decorator, requestsexceptions, dogpile.cache, jsonpointer, jsonpatch, openstacksdk, os-client-config, osc-lib, oslo.context, oslo.log, python-neutronclient, oauth2client, python-swiftclient, deprecated, pyeventsystem, tenacity, python-novaclient, pynacl, paramiko, pysftp, zipp, importlib-metadata, pyrsistent, jsonschema, warlock, pyopenssl, python-glanceclient, python-cinderclient, cloudbridge, humanfriendly, coloredlogs, psutil, shellescape, future, typing-extensions, lockfile, mistune, rdflib, ruamel.yaml.clib, ruamel.yaml, rdflib-jsonld, schema-salad, lxml, networkx, prov, pathlib2, mypy-extensions, scandir, cwltool, dictobj, docopt, ecdsa, fabric3, galaxy-sequence-utils, gxformat2, h5py, isa-rwval, kombu, markupsafe, mako, markdown, mercurial, monotonic, nodeenv, nose, oyaml, parsley, paste, pastedeploy, pastescript, webob, pulsar-galaxy-lib, pycryptodome, pykwalify, pysam, python-jose, pyuwsgi, tqdm, yacman, refgenconf, repoze.lru, routes, defusedxml, python3-openid, unidecode, social-auth-core, sortedcontainers, sqlalchemy, tempita, sqlparse, sqlalchemy-migrate, sqlalchemy-utils, svgwrite, whoosh\nSuccessfully installed adal-1.2.3 amqp-2.5.2 appdirs-1.4.3 attmap-0.12.11 attrs-19.3.0 azure-common-1.1.14 azure-cosmosdb-nspkg-2.0.2 azure-cosmosdb-table-1.0.4 azure-mgmt-compute-4.0.1 azure-mgmt-devtestlabs-2.2.0 azure-mgmt-network-2.1.0 azure-mgmt-nspkg-3.0.2 azure-mgmt-resource-2.0.0 azure-mgmt-storage-2.0.0 azure-nspkg-3.0.2 azure-storage-blob-1.3.1 azure-storage-common-1.4.2 azure-storage-nspkg-3.1.0 babel-2.8.0 bagit-1.7.0 bcrypt-3.1.7 bdbag-1.5.6 beaker-1.11.0 bioblend-0.13.0 bleach-3.1.5 boltons-20.1.0 boto-2.49.0 boto3-1.9.114 botocore-1.12.253 bx-python-0.8.8 cachecontrol-0.11.7 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.14.0 chardet-3.0.4 cheetah3-3.2.4 cliff-2.18.0 cloudauthz-0.6.0 cloudbridge-2.0.0 cmd2-0.8.9 coloredlogs-14.0 cryptography-2.9.2 cwltool-1.0.20191225192155 debtcollector-1.22.0 decorator-4.4.2 defusedxml-0.7.0rc1 deprecated-1.2.9 deprecation-2.1.0 dictobj-0.4 docopt-0.6.2 docutils-0.15.2 dogpile.cache-0.9.1 ecdsa-0.15 fabric3-1.14.post1 future-0.18.2 futures-3.1.1 galaxy-sequence-utils-1.1.5 google-api-python-client-1.7.8 google-auth-1.14.1 google-auth-httplib2-0.0.3 gxformat2-0.11.3 h5py-2.10.0 httplib2-0.17.3 humanfriendly-8.2 idna-2.9 importlib-metadata-1.6.0 isa-rwval-0.10.7 iso8601-0.1.12 isodate-0.6.0 jmespath-0.9.5 jsonpatch-1.25 jsonpointer-2.0 jsonschema-3.2.0 keystoneauth1-4.0.0 kombu-4.6.8 lockfile-0.12.2 lxml-4.5.0 mako-1.1.2 markdown-3.1.1 markupsafe-1.1.1 mercurial-5.4 mistune-0.8.4 monotonic-1.5 msgpack-1.0.0 msrest-0.5.5 msrestazure-0.5.0 munch-2.5.0 mypy-extensions-0.4.3 netaddr-0.7.19 netifaces-0.10.9 networkx-1.11 nodeenv-1.3.5 nose-1.3.7 numpy-1.16.6 oauth2client-4.1.3 oauthlib-3.1.0 openstacksdk-0.17.0 os-client-config-2.1.0 os-service-types-1.7.0 osc-lib-2.0.0 oslo.config-7.0.0 oslo.context-2.23.0 oslo.i18n-3.25.1 oslo.log-3.45.2 oslo.serialization-2.29.2 oslo.utils-3.42.1 oyaml-0.9 packaging-20.3 paramiko-2.7.1 parsley-1.3 paste-3.4.0 pastedeploy-2.1.0 pastescript-3.2.0 pathlib2-2.3.5 pbr-5.4.5 prettytable-0.7.2 prov-1.5.1 psutil-5.7.0 pulsar-galaxy-lib-0.14.0.dev3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pycryptodome-3.9.7 pyeventsystem-0.1.0 pyjwt-1.7.1 pykwalify-1.7.0 pynacl-1.3.0 pyopenssl-19.1.0 pyparsing-2.4.7 pyperclip-1.8.0 pyrsistent-0.16.0 pysam-0.16.0.1 pysftp-0.2.9 python-cinderclient-4.0.0 python-dateutil-2.8.1 python-glanceclient-2.12.0 python-jose-3.1.0 python-keystoneclient-3.17.0 python-neutronclient-6.9.0 python-novaclient-11.0.0 python-swiftclient-3.6.0 python3-openid-3.2.0 pytz-2020.1 pyuwsgi-2.0.18.post0 pyyaml-5.3.1 rdflib-4.2.2 rdflib-jsonld-0.4.0 refgenconf-0.7.0 repoze.lru-0.7 requests-2.23.0 requests-oauthlib-1.3.0 requests-toolbelt-0.9.1 requestsexceptions-1.4.0 rfc3986-1.4.0 routes-2.4.1 rsa-4.0 ruamel.yaml-0.16.0 ruamel.yaml.clib-0.2.0 s3transfer-0.2.1 scandir-1.10.0 schema-salad-4.5.20191229160203 setuptools-scm-3.5.0 shellescape-3.4.1 simplejson-3.17.0 six-1.11.0 social-auth-core-3.3.0 sortedcontainers-2.1.0 sqlalchemy-1.3.16 sqlalchemy-migrate-0.13.0 sqlalchemy-utils-0.36.5 sqlparse-0.3.1 stevedore-1.32.0 svgwrite-1.3.1 tempita-0.5.2 tenacity-4.12.0 tqdm-4.46.0 typing-extensions-3.7.4.2 tzlocal-2.0.0 ubiquerg-0.5.1 unidecode-1.1.1 uritemplate-3.0.1 urllib3-1.25.9 vine-1.3.0 warlock-1.3.3 wcwidth-0.1.9 webencodings-0.5.1 webob-1.8.6 whoosh-2.7.4 wrapt-1.12.1 yacman-0.6.7 zipp-1.2.0\nThe Galaxy client has not yet been built and will be built now.\nInstalling node into /Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv with nodeenv.\n\nInstall prebuilt node (10.15.3) .\nTraceback (most recent call last):\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1317, in do_open\nencode_chunked=req.has_header(\u2018Transfer-encoding\u2019))\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1244, in request\nself._send_request(method, url, body, headers, encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1290, in _send_request\nself.endheaders(body, encode_chunked=encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1239, in endheaders\nself._send_output(message_body, encode_chunked=encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1026, in _send_output\nself.send(msg)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 966, in send\nself.connect()\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1414, in connect\nserver_hostname=server_hostname)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 423, in wrap_socket\nsession=session\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 870, in _create\nself.do_handshake()\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 1139, in do_handshake\nself._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)\n\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/bin/nodeenv\u201d, line 8, in \nsys.exit(main())\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 1113, in main\ncreate_environment(env_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 936, in create_environment\ninstall_node(env_dir, src_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 700, in install_node\ninstall_node_wrapped(env_dir, src_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 723, in install_node_wrapped\ndownload_node_src(node_url, src_dir, opt, prefix)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 552, in download_node_src\ndl_contents = io.BytesIO(urlopen(node_url).read())\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 580, in urlopen\nreturn urllib2.urlopen(req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 222, in urlopen\nreturn opener.open(url, data, timeout)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 525, in open\nresponse = self._open(req, data)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 543, in _open\n\u2018_open\u2019, req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 503, in _call_chain\nresult = func(*args)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1360, in https_open\ncontext=self._context, check_hostname=self._check_hostname)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1319, in do_open\nraise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)>"}, {"role": "system", "text": "\n\n\n george.weingart:\n\ncertificate verify failed: unable to get local issuer certificate\n\n\n\n  \n      dev2qa.com \u2013 24 Jun 19\n  \n  \n    \n\nHow To Fix Python Error Certificate Verify Failed: Unable To Get Local Issuer...\n\nWhen i run python code in mac os, i meet a certificate verify failed error like this ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). [\u2026]\n\n\n  \n  \n    \n    \n  \n  \n\n"}], [{"role": "user", "text": "Hello everyone,\nI am trying to use maaslin from yestarday but i get always the following message\n\u201cTool request failes Maaslins: Uncaught exception in exposed API method\u201d\nIs there any internal error ? I get the same message when I try to use Lefse.\nThanks for the help\nAndy"}, {"role": "system", "text": "Hello @Andy87\nThis was happening at tool submission? The server was probably just busy, or this happened during short incident window earlier this week. Please try again now.\nIf you search with that error message here, you\u2019ll find other Q&A where this was the solution, too.\nPast and ongoing Galaxy Main https://usegalaxy.org and related server\u2019s statuses and resources are tracked here: https://galaxyproject.statuspage.io/\nPlease find links to contact information for the public Huttenhower Galaxy server hosted at http://huttenhower.sph.harvard.edu/galaxy/ in the Galaxy directory here: https://galaxyproject.org/use/huttenhower-lab/\nThanks and our apologies if this was confusing!"}, {"role": "system", "text": "This tool is only installed on the huttenhower Galaxy server that runs a very old version of Galaxy. We cannot help you there."}], [{"role": "user", "text": "Dear all,\nI\u2019m trying to develop a tool but I failed to properly filter output.\nThe aim is to repeat different type of analysis depending of the parameters. So I created a repeat parameters, with a conditional and with a select parameter. The analysis is okay but I cannot filter output data and I received empty files for non analyzed part of the code\u2026\nI tried to write the filter in different manner but it don\u2019t work into a repeat parameter\u2026\nthanks !\nAn example of my code could be :\n<inputs>\n<repeat name=\"repeat\" title=\"repeat1\">\n    <conditional name=\"conditional\">\n        <param name=\"select1\" type=\"select\">\n            <option value=\"type1\" >the type 1</option>\n            <option value=\"type2\">the type 2</option>\n        </param>\n        <when value=\"type1\">\n            <param name=\"data1\" type=\"data\" label=\"data1\">\n        </when>\n        <when value=\"type2\">\n            <param name=\"data2\" type=\"data\" label=\"data2\">\n        </when>\n    </conditional>\n</repeat>.\n</inputs>\n<outputs>\n<data name=\"output1\" format=\"pdf\" label=\"output1 from data1\">\n    <filter>repeat['conditional']['select1'] == 'type1' </filter>\n<data name=\"output2\" format=\"csv\" label=\"output2 from data2\">\n    <filter>repeat['conditional']['select1'] == 'type2' </filter>\n<outputs>\n"}, {"role": "system", "text": "I think there\u2019s a conceptual problem here. With the repeat you\u2019re adding a variable number of conditionals, but it\u2019s not clear what you would do in the output filter if you have multiple (conflicting?) options.\nGenerally speaking repeats should be represented as lists, so you would have to do something like\n<data name=\"output1\" format=\"pdf\" label=\"output1 from data1\">\n    <filter>repeat[0]['conditional']['select1'] == 'type1' </filter>\n</data>\n<data name=\"output2\" format=\"csv\" label=\"output2 from data2\">\n    <filter>repeat[0]['conditional']['select1'] == 'type2' </filter>\n</data>\n\nI haven\u2019t tested this, but I think it already shows the problem with the approach.\n\nThe aim is to repeat different type of analysis depending of the parameters.\n\nI think for this purpose it would be better to create a workflow that you map over a collection of options (see Using Workflow Parameters for how to read parameters from a file).\nAnother approach that could work would be to discover the created outputs in a list (or two lists if you want to separate pdf and csv outputs)."}, {"role": "user", "text": "Hi @mvdbeek thanks a lot,\nYour solution with repeat[0] is great, but finally i used the collection type to extract the output.\nThank you !"}], [{"role": "user", "text": "Hello!\nI am trying to use Ribogalaxy and running into a lot of errors. I am aware that RiboGalaxy has just been moved over from a different domain and so bugs are to be expected, but my data analysis is time sensitive and I am not well versed in bioinformatics.\nCould I get in touch with someone developing RiboGalaxy to help me with the bugs I am encountering?\nThank you!!"}, {"role": "system", "text": "Hi @janhavikolhe\nUpdate:\nThe servers home page https://ribogalaxy.genomicsdatascience.ie/ has contact information (an email address). They ask that bugs be reported there for now. The admins running the server would be best people to review for actual bugs versus usage issues they recognize.\n\nFind their contact information in the public Galaxy directory here: RiboGalaxy - Galaxy Community Hub\nThey have a dedicated help and forum site. The news section has information about the updates https://gwips.ucc.ie/ but the forum appears to be still under maintenance. You\u2019ll need to either wait for them to get back up (along with the custom tools/protocols), or explore other public servers."}], [{"role": "user", "text": "Hi all,\nI have been using HISAT2 for RNAseq reads alignment for quite a while.\nLast few days, the tool keeps failing with different error messages, even on inputs that I have used successfully before.\nHere a couple of examples of the error messages that I get:\n\nThis job failed for reasons that could not be determined\n\n\n(ERR): hisat2-align died with signal 9 (KILL)\n[W::sam_read1_sam] Parse error at line 34126880\nsamtools sort: truncated file. Aborting\n[main_samview] fail to read the header from \u201c-\u201d\n\n\nThis job was terminated because it used more memory than it was allocated\n\nCan anyone help?\nThanks\nLuca"}, {"role": "system", "text": "Hi @LUCA_MOLOGNI\nThe clusters that run this tool have been very busy but these errors look like they are resulting from a job that is too large to execute. I understand that these reads mapped before, but if any of these other items changed, it could impact how \u201clarge\u201d a job is:\n\nparameters\nreference genome\nreference annotation\ntool version (always use the most current)\n\nThings to troubleshoot:\n\nTry another rerun to eliminate server side transient issues\nInspect the reads \u2013 do these still pass QA tools?\nInspect the target genome if using a custom genome input \u2013 is the fasta format correct?\nThe format/content of any included reference annotation should also be checked, but usually doesn\u2019t lead to an error: the reference annotation would in practical use be ignored instead. So, scientific not technical problems.\n\nI added some tags that link to prior Q&A about those items. In short, this root problem could be input issues OR the job is actually too large to process. The middle message suggests that the target reference genome (fasta) contains a lot of sequences \u2013 remember that this tool is expecting a somewhat intact assembly: up to ~ 1000 target sequences but no more.\n\n#understanding-exceeds-memory-allocation-error-messages\n#understanding-walltime-error-messages\n"}], [{"role": "user", "text": "Is there a way to calculate the time consumed/needed when running a tool?"}, {"role": "system", "text": "Hi @Manuel-DominguezCBG, for completed jobs users can check information in info panel (click on dataset name, click on  info icon). Some tools put run time into log files. I am not aware about any option that allow estimation of run time for an active job through Galaxy interface, at least not for a user, assuming standard Galaxy setup.\nKind regards,\nIgor"}], [{"role": "user", "text": "What Galaxy instance has a robust reliable assembly gap filling tools, please?"}, {"role": "system", "text": "Have you checked SSPACE?. Also, you can search any software in the Galaxy Tool Shed"}], [{"role": "user", "text": "Hello,  There is a repository named qiime2_wrappers (URL below).  I was unable to find this repository anywhere in the public galaxy version.  Is this only available in the locally installed galaxy version?  If not, how do I \u201clink\u201d to this repository.\nhttps://toolshed.g2.bx.psu.edu/repository?repository_id=a43ea4908816827c&changeset_revision=51b9b6b57732"}, {"role": "system", "text": "Hello,\nIs this only available in the locally installed galaxy version?\n\nEach instance\u2019s tool suite is installed by administrators, so not all tools in the shed will be present in all Galaxy instances \u2013 it\u2019s at the discretion of the admin. This allows the tool shed to be permissive in accepting tools, so you can run anything you\u2019d like to download/develop on your own instance, without potentially compromising the security of every other instance (should someone upload a malicious tool).\nusegalaxy.org does not have the tool collection you mentioned, though usegalaxy.eu does appear to have some similar tools that draw from this collection:\nhttps://github.com/galaxyproject/tools-iuc/tree/master/tools/qiime/\nif those tools are insufficient, your best bet is to run that particular step in your process on your own Galaxy if possible \u2013 or befriend a Galaxy admin "}], [{"role": "user", "text": "Dear all,\nI am using a LEfSe analysis for my sequencing data. I use this format pasted to .tsv file in the notepad:\n\nimage1752\u00d7263 10.2 KB\n\nNevertheless, I do not receive any results. I got that there does not have to be any significant difference among taxons, but I have tried a lot of variable values to obtain any result and to confirm that I work correctly and the input file is ok. So I have tried every combination of \u201cSet the strategy for multi-class analysis\u201d and \u201cDo you want the pairwise comparisons among subclasses to be\u2026\u201d at the step B (LDA Effect Size) and after that didn\u2019t do anything, I setup both Alpha values to 1. In my opinion, that means I should see at least some differences. But I do not.\nSo I would like to ask you if I am performing this analysis correctly and if so, what can be the troubleshoot?\nThank you a lot!\nMartin"}, {"role": "system", "text": "Hi @m.mihula\nIf you are working with this tool at a public Galaxy site, is it this one? If so, you\u2019ll need to contact them directly for usage issues. Click into this post \u2013 it explains why and how to reach them.\n\n  \n    \n    \n    Problems with Plot Cladogram -- http://huttenhower.sph.harvard.edu/galaxy/ \n  \n  \n    Hi @liuyushu \nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server. \nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers). \nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (belo\u2026\n  \n\n"}], [{"role": "user", "text": "Hi\nI have assembled contigs (more than 64000 contigs) with a long header. How can I make the header shorter using one of Galaxy\u2019s tools? please\nRegards\nAdnan"}, {"role": "system", "text": "Hi @Adnan\nThanks for posting publicly.\nFAQ Fasta format\nThere are many ways to shorten or modify fasta title lines (what is on the > lines) \u2026 maybe try a few different ways to get exactly what you want.\nTips: Be sure that what remains for the >identifier content (not including anything after the first whitespace) will be unique within the file, or expect problems. Be aware that some tools will fail when any description content (whatever is after the first whitespace, if anything) is present, so that is enough for some people.\nNormalizeFasta\nwith the option to trim the header at the first whitespace\n\nGood for preserving original >identifier content and removing excess trailing description content.\n\nText transformation with sed\n\nThe tool form has examples, or see the link outs for more.\nAnchoring to title lines with ^> would probably be involved (line starts with a > character)\n\nMore manipulations, including several Replace function variations are here. Some may require tabular format, so: fasta > tabular > do stuff > fasta. Data Manipulation Olympics"}], [{"role": "user", "text": "Dear Admins,\nI have been running a certain workflow with my galaxy eu account for the last 3 days without problems. Today I tried to run it again but I get the following message: \u201cInvocation scheduling failed - Galaxy administrator may have additional details in logs.\u201d\nI have no more additional information, sorry. The only thing I can share is that the workflow is not automatically recognising my input (paired collection) as it was doing the previous days.\nAny help or explanation would be much appreciated."}, {"role": "system", "text": "Hi @Alan\nThe best advice would be to try again now, especially since this data and workflow was working previously.\nAnd \u2026 if it seems that the data is still not being recognized, maybe create or copy that starting data into a new history, and try from there? That resets all of the history + dataset metadata."}], [{"role": "user", "text": "Hi I am new to galaxy, so if someone has asked this question in the past I apologize. I downloaded 42 files to galaxy via the SRA run selector as a collection. I now have my FASTQ files and would like to use Trimmomatic; however, I am above the alloted space quota. Trimmomatic was run on the first 15 files already and the rest are waiting to be processed.\nMy question is if I delete/purge  the first 15 files in the collection ( FASTQ) will that allow Trimmoatic to resume or will it mess things up because the data is grouped together as a collection. I just not sure and it took forever to download so any advice would be much appreciated."}, {"role": "system", "text": "Hi @LittleBlueHeron\nOne solution is to break out the read data into distinct collections during download and processing in batches.\n\n\nTrimmomatic creates near duplicate very large fastq outputs.\nThe idea would be to get through that step with a portion of the data, purge the original files, repeat with another portion, then combine the results into one collection for downstream processing.\nFrom your description it seems like 2-4 batches would be enough.\n\nAnother is to make a small short-term extra quota request (extra 100-250 GB for a few days to a week).\n\nThis is only available for academic researchers at the public sites. How-to for accounts at UseGalaxy.org. Account quotas - Galaxy Community Hub. Make the request via email as described in that FAQ \u2013 don\u2019t post your information here publicly.\n\nFor non academics, additional resources require the use of a private Galaxy server with resources scaled to fit the work. Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\nIf that is not enough help, would you please post back a share link to this history? That context will help with review and specific solutions. \nfaqs/galaxy/#sharing-your-history"}], [{"role": "user", "text": "Hi, I am new to Galaxy.\nAre the BAM files generated after mapping using RNA STAR (version 2.7.8a+galaxy1 or newer) sorted (either by coordinate or name or tag)? After clicking on the \u2018i\u2019 logo for the \u2018*.mapped.bam\u2019 dataset details, the dataset information section says the format is BAM. The \u2018Tool Standard Output\u2019 is as follows:\nApr 28 20:52:35 \u2026 started STAR run\nApr 28 20:52:35 \u2026 starting to generate Genome files\nApr 28 20:53:23 \u2026 processing annotations GTF\nApr 28 20:53:54 \u2026 starting to sort Suffix Array. This may take a long time\u2026\nApr 28 20:54:14 \u2026 sorting Suffix Array chunks and saving them to disk\u2026\nApr 28 21:13:08 \u2026 loading chunks from disk, packing SA\u2026\nApr 28 21:14:38 \u2026 finished generating suffix array\nApr 28 21:14:38 \u2026 generating Suffix Array index\nApr 28 21:19:00 \u2026 completed Suffix Array index\nApr 28 21:19:01 \u2026 inserting junctions into the genome indices\nApr 28 21:21:04 \u2026 writing Genome to disk \u2026\nApr 28 21:21:07 \u2026 writing Suffix Array to disk \u2026\nApr 28 21:21:30 \u2026 writing SAindex to disk\nApr 28 21:21:33 \u2026 finished successfully\nApr 28 21:21:33 \u2026 started STAR run\nApr 28 21:21:33 \u2026 loading genome\nApr 28 21:21:55 \u2026 started mapping\nApr 28 21:30:30 \u2026 finished mapping\nApr 28 21:30:54 \u2026 started sorting BAM\nApr 28 21:31:45 \u2026 finished successfully\nHowever, though this output is essentially the same for my \u2018*.transcriptome-mapped.bam\u2019 file, its dataset information says the format is unsorted.bam. See attached screenshots.\n\nmappedbam1378\u00d7470 50.6 KB\n\n\ntransmappedbam1356\u00d7475 50.8 KB\n\nThe 1st line of \u2018.mapped.bam\u2019 file is \u2018@HD VN:1.4 SO:coordinate\u2019. So am I correct in assuming that perhaps it might be sorted by coordinate? The 1st line of '.transcriptome-mapped.bam\u2019 is not the same as the \u2018*.mapped.bam\u2019 but rather is \u2018@SQ SN:NR_046018.2 LN:1652\u2019.\nDo I still need to perform the Samtools sort operation for either one or both these files? I intend to use them as inputs for featureCounts.\nThanks a lot for the help in advance. "}, {"role": "system", "text": "Hi @agschindler\n\n\n\n agschindler:\n\nDo I still need to perform the Samtools sort operation for either one or both these files? I intend to use them as inputs for featureCounts.\n\n\nA datatype metadata describes the sort formats of bam data.\n\nThe first dataset with the bam datatype assigned is coordinate sorted.\nThe second dataset with the unsorted.bam datatype assigned is not.\n\nThe first is what most people would be using with Featurecounts. Instead, for transcript level expression analysis, Salmon or Kallisto are probably \u201cbetter\u201d choices scientifically. But, you can of course try out different data/parameters with any and see what happens  to explore. Maybe you can figure out how to get the data you need in novel ways.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello\nI wanted to ask if there is any way to access academic or commercial servers that allow for using more space on Galaxy.  I am planning to analyze bigger data sets that will need far more than the 250GB permitted on Galaxy. I did see some suggestions on Galaxy but could not connect to AWS or Anvil. Does anyone have experience using these servers or any suggestions for other servers that allow for additional space that can be booked?\nThanks,\nSwati"}, {"role": "system", "text": "Hi @seq2022\nSeveral choices are in the matrix here\n\n  \n      \n\n      galaxyproject.org\n  \n\n  \n    \n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy...\n\n  All about Galaxy and its community.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n seq2022:\n\nI did see some suggestions on Galaxy but could not connect to Anvil\n\n\nThe contact is here AnVIL - Galaxy Community Hub"}], [{"role": "user", "text": "Hello,\nMy VelvetOptimiser is offering different results on galaxy vs. my command line.\nHere are the inputs i used on galaxy:\n\n11668\u00d7634 42.8 KB\n\n\n21652\u00d71144 78.4 KB\n\n\n31686\u00d71380 183 KB\n\nand this is from my command line\nVelvetOptimiser.pl -s 55 -e 69 -x 2 -f '-shortPaired -fastq -separate trial2.trimmed.paired.R1.fastq trial2.trimmed.paired.R2.fastq' -f '-short -fastq trial2.trimmed.unpaired.R1.fastq' -f '-short -fastq trial2.trimmed.unpaired.R2.fastq'  \n\n(trial2.trimmed.paired.r1\u2026unpaired.R2.fastq are the same post-trimming files as the ones on galaxy)\nFor velvet, I cloned the github repo using\ngit clone https://github.com/dzerbino/velvet.git\n\nand compiled it to make the MAXKMERLENGTH = 121 with the command\nmake 'MAXMAXKMERLENGTH = 121'\n\nI\u2019ve troubleshooted quite a bit, beginning with the swapping of the format and type as per the github.\nVelvetOptimiser.pl -s 55 -e 69 -x 2 -f '-fastq -shortPaired -separate trial2.trimmed.paired.R1.fastq trial2.trimmed.paired.R2.fastq' -f '-fastq -short trial2.trimmed.unpaired.R1.fastq' -f ' -fastq -short trial2.trimmed.unpaired.R2.fastq'  \n\nI also made sure to add the velvet folder and folder containing velvet optimiser to my path.\nAny ideas what I\u2019m doing wrong? I attached pictures of my error file if that helps; any feedback would be greatly appreciated.\n\n1960\u00d71796 261 KB\n\n\n21288\u00d71270 137 KB\n"}, {"role": "system", "text": "\n\n\n drip2hardpanu:\n\nVelvetOptimiser\n\n\nMy guess is that the underlying tool versions differ. Or, maybe there is some bug.\nThe version of this tool and the resource allocated are intended for very small tutorial-sized datasets when working at public Galaxy servers. Some servers even have the tool marked as deprecated. The last wrapper update was in 2021, details: Galaxy | Tool Shed. If you want to help us to fix the wrapper, the development repository is linked from there."}], [{"role": "user", "text": "Greetings.\nOur Environment:\nGalaxy:  19_09 ;\nOS:  Ubuntu 18.04.2 LTS\nDatabase:  Postgres    <<<\u2014 one of the few deviations from default install\nGalaxy is nearly a default install and not configured to run behind nginx.\nnginx does not appear to be running nor can I find an nginx.conf file anywhere on the machine.\nThe PROBLEM\nI am a newbie admin and my users are running into an issue when downloading larger files.  They can download index files which are about 96bytes.  However, the bam files (~15MB) fail.  This is Galaxy 19_09 and it is not configured to run behind nginx;  a nearly default installation.  I\u2019ve read several posts about \u201cproxy_max_temp_file_size\u201d setting in nginx but nginx is not setup or running on the server.\nI found these posts but they are all related to nginx\n\nLocal galaxy nginx max file download size 1GB\nhttp://galacticengineer.blogspot.com/2019/05/fixing-dataset-download-problems-for.html\nhttp://lists.unbit.it/pipermail/uwsgi/2018-December/008955.html\n\nI\u2019m not sure where to go from here.  Hoping someone has experience with this issue.\nOne of the errors from Galaxy.log:\n[pid: 80680|app: 0|req: 2126/2264] 10.10.255.36 () {40 vars in 832 bytes} [Wed Apr 15 12:01:28 2020] GET /api/histories/2fdbd5c5858e78fb/contents/datasets/2ea0c1fdff88d4e3 => generated 7749 bytes in 31 msecs (HTTP/1.1 200) 3 headers in 139 bytes (1 switches on core 0)\n10.10.255.36 - - [15/Apr/2020:12:01:32 -0400] \"GET /datasets/2ea0c1fdff88d4e3/display?to_ext=bam HTTP/1.1\" 200 148073827 \"http://10.10.200.140:8080/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0\"\nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n[pid: 80680|app: 0|req: 2127/2265] 10.10.255.36 () {40 vars in 813 bytes} [Wed Apr 15 12:01:32 2020] GET /datasets/2ea0c1fdff88d4e3/display?to_ext=bam => generated 6160384 bytes in 6199 msecs (HTTP/1.1 200) 5 headers in 263 bytes (97 switches on core 1)"}, {"role": "user", "text": "Hello,  Wanted to post an update.  I just heard from the users that after I bounced that Galaxy server the issue has magically gone away.\nFor now assume this issue is resolved.  Thank you."}], [{"role": "user", "text": "I executed Spades genome assembly but its not running. When I click on the job history I get this message \"This is a new dataset and not all of its data are available yet \u201cspades\u201d. Help please."}, {"role": "system", "text": "Hi! I have the same issue, did your resolve the problem in the end?"}, {"role": "system", "text": "Welcome, @pazzini02 and @tjh\nJobs will queue first, then execute.\nPlease see faqs/galaxy/#analysis"}], [{"role": "user", "text": "My original files for a RNA-Seq-analysis are in fastq.gz format and to map (with BWA-MEM) them I tried converting them to fastqsanger format using the FASTQ Groomer. When doing so I received the error \u201cThere was an error reading your input file. Your input file is likely malformed.\u201d regarding some of the files.\nHowever, I don\u2019t think there is any problem with my data files. How can I check them?\nI have already tied to redo the jobs (grooming, mapping) after also re-importing the files."}, {"role": "system", "text": "Hi @Iisa\nIf the reads were generated from a recent Illumina protocol, they are already in fastqsanger (uncompressed) or fastqsanger.gz (compressed) format.\n\n\n\n Iisa:\n\n\u201cThere was an error reading your input file. Your input file is likely malformed.\u201d\n\n\nTry running FastQC on the reads. If that tool also fails, there is either formatting or datatype assignment problem:\n\nThe uploaded read could be truncated (before upload, or during). Check the size in Galaxy versus the size from the source. Uploading again resolves this.\nCheck to make sure that the file is actually compressed or not, and adjust the datatype to match.\nIf you can\u2019t find the problem, please share your history back for more help. You can post the link back in a public reply or ask for a moderator to start up a private direct message here instead.\n\nTutorial\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nFAQs \u2013 these cover most fastq datatypes with extra help\nfaqs/galaxy/#datatypes"}], [{"role": "user", "text": "Hi there,\nThis is Roger. I logged into my galaxy\u2019s account and all my data was removed and purged. Is there any reason for that? Can anyone help me here?\nRoger"}, {"role": "system", "text": "Please see this notice: Accounts deleted from Galaxy Main to enforce posted Terms and Conditions\nTo be very clear:\n\nPick ONE account to restore. Be very clear.\nList out your duplicated accounts to prevent this from happening again. Not all duplicated account were removed at once. Expect this to continue. We have limited resources and need to ensure fair usage.\nIf your email was from public service, you\u2019ll need to confirm your unique identity. Primary work/academic email is usually enough. Variations on the email address (common) are not accepted.\n\nAll will be private to the administrators of our server when the email is sent in and is used only to confirm your unique identity going forward, prevent accidental duplicate account detection with other unique people, prevent account loss due to new duplicates being identified (existing and/or new accounts) \u2013 IF you follow our terms of usage going forward.\nBe honest, clear, and do this immediately. We want to help you. The GCC annual conference starts on July 1st, 2019 and many will begin to travel soon. Restoration request emails that arrive after early tomorrow morning (June 27th), or that are incomplete, will result in account restoration delays on our side for up to 2-3 weeks.\nIf this happened at a different public Galaxy server, you\u2019ll need to contact them. All public Galaxy servers, and many private Galaxy servers, have an account quota of one account per distinct user per server. Each handle duplicated accounts differently. Contact information is usually on the home page of the server and sometimes (or also) here: https://galaxyproject.org/use/\nThanks for understanding!"}], [{"role": "user", "text": "Two fastq files \u2014> FASTQ Groomer \u2014> Trimmomatic \u2014> 2Pared files to BWE-MEN \u2014>RmDup \u2014> GATK happlotypeCaller\nNo error found (all green boxes). However, the last command returns 0 lines and there is a message in the output saying:\njava: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory galaxy\nThe input file used to run HaplotypeCaller looks ok\nUsing database hg38\nEDIT:\nI gave ran again the same command and now I have got another error\n\nERROR MESSAGE: SAM/BAM/CRAM file input.bam is malformed. Please see gatk-docs/Collected_FAQs_about_input_files_for_sequence_read_data_(BAM_CRAM).md at master \u00b7 broadinstitute/gatk-docs \u00b7 GitHub more information. Error details: SAM file doesn\u2019t have any read groups defined in the header.  The GATK no longer supports SAM files without read groups\n\nERROR ------------------------------------------------------------------------------------------"}, {"role": "system", "text": "On which server does that happen? It seems we need to fix the conda package or the conda installation."}], [{"role": "user", "text": "\nWhen I perform Block Cluster Test with my small RNA-seq data, I only see miRNA, tRNA and some sno RNA but no piRNA or other smallRNA. Does anybody know why?\n"}, {"role": "system", "text": "Welcome, @Wenjing\nIs this the tutorial you are following? Small Non-coding RNA Clustering using BlockClust. Tutorial data is a sub-sample of full data and while attempts are made to provide representative sub-samples, that isn\u2019t always perfect.\nMaybe the publication linked at the bottom of the tool form will help more? It is available open source. BlockClust: efficient clustering and classification of non-coding RNAs from short read RNA-seq profiles | Bioinformatics | Oxford Academic"}], [{"role": "user", "text": "Hello,\nI have trouble working with roary tool.\nI use prokka to annotate my genomes in .gff format (version 3) and then I use Roary with the generated .gff files, to analyse the pangenome.\nHowever it does not work. You will find some screenshoot of the error details.\nThank you for your help,\n\nCapture_Prokka_11010\u00d7755 83.9 KB\n \nCapture_Prokka_21008\u00d7716 81.3 KB\n \nCapture_Roary_3970\u00d7664 73.8 KB\n \nCapture_Roary_4975\u00d7495 80.7 KB\n \nCapture_Roary_51009\u00d7709 72.2 KB\n"}, {"role": "system", "text": "Hi @aperrat\nThanks for providing all the details about the problem. The error was reproducible with independent test data. It is likely a tool installation bug. The tool is newer to the https://usegalaxy.org server. Confirmed tool bug \u2013 see updated posts.\nWe will be fixing this. Details in this ticket. Once resolved, the ticket will close out: https://github.com/galaxyproject/usegalaxy-playbook/issues/293\nMeanwhile, the tool does work at Galaxy EU https://usegalaxy.eu, so that is a near-term alternative.\nThanks again for reporting the problem!"}, {"role": "system", "text": "Hi @aperrat\nThis is the same problem again. And it is not a problem with the tool installation, rather it is an actual tool bug. Thank you again for reporting it!\nEvidently there is a bug with the tool when the input datasets contain spaces in the names. The correction should be simple but is still in progress. And it can be avoided without too much trouble at usegalaxy.org or usegalaxy.eu (or anywhere else) until the tool itself is fixed.\nThis is the original ticket but with more updates, including the workarounds: https://github.com/galaxyproject/usegalaxy-playbook/issues/293#issuecomment-605089574\nIn short, remove spaces from the input file names. How-to is in the ticket above.\nThanks!"}], [{"role": "user", "text": "Hi,\nI am just trying to find why the VCF file (via Freebayes) does not include GQ?\nOnly the following is output:\nGT:DP:AD:RO:QR:AO:QA:GL\nIs there a way to process the VCF to add GQ? Or have I made an error during the initial variant calling? Is there a way to amend this please so that I can access GQ scores?\nThank you for your help"}, {"role": "system", "text": "Hi @abbi\nmaybe try the following during FreeBayes job setup:\nChoose parameter selection level > Full list of options\nAlgorithmic features > Set Algorithmic features\nCalculate the marginal probability of genotypes and report as GQ in each sample field in the VCF output > change to Yes.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear GalaxyHelp,\nI have a problem with my local instance of galaxy (20.01). I suddenly noticed it was not starting any new jobs or uploading files (staying grey forever). When I looked at the galaxy log, it was constantly starting new jobs, but the database/job_directory folders galaxy created seem \u2018empty\u2019 (empty working and output folder). As this is a development instance, I tried to completely remove the tmp and job_directory. Still, if I restart galaxy, it will try to start new jobs. This instance doesn\u2019t consist of users that started any jobs, so it is very confusing. I must say that I am working on an automated updating of libraries via the bioblend api. I\u2019m curious if it is possible that this api invokes certain jobs that are stuck in a loop?\nAfter some dependency checks which it usually does, my logfile looks like this:\nStarting server in PID 11717.\nserving on http://xxx:8080\ngalaxy.model.database_heartbeat DEBUG 2020-06-05 13:52:26,856 [p:11717,w:1,m:0] [database_heartbeart_main.web.1.thread] main.web.1 is config watcher\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,216 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,232 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Dispatching to local runner                                            galaxy.jobs DEBUG 2020-06-05 13:52:27,253 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Persisting job destination (destination id: local)                             galaxy.jobs DEBUG 2020-06-05 13:52:27,253 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Working directory for job is: /home/galaxy/galaxy/database/jobs_directory/080/80530\ngalaxy.jobs.runners DEBUG 2020-06-05 13:52:27,278 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] Job [80530] queued (45.707 ms)\ngalaxy.jobs.handler INFO 2020-06-05 13:52:27,288 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Job dispatched\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,297 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,316 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Dispatching to local runner\ngalaxy.jobs DEBUG 2020-06-05 13:52:27,337 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Persisting job destination (destination id: local)\ngalaxy.jobs DEBUG 2020-06-05 13:52:27,339 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Working directory for job is: /home/galaxy/galaxy/database/jobs_directory/080/8\n0531\ngalaxy.jobs.runners DEBUG 2020-06-05 13:52:27,389 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] Job [80531] queued (72.643 ms)\ngalaxy.jobs.handler INFO 2020-06-05 13:52:27,408 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Job dispatched\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,436 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80532) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,469 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80532) Dispatching to local runner\netc. creating more and more job directories continuously.\nDid any of you every experience something similar? I am on the edge of reinstalling the development instance, but I want to avoid this happening again of course. I was wondering if someone has an idea of forcing to stop this process for now.\nThank you so much in advance!\nAnnabel Dekker"}, {"role": "user", "text": "I used gxadmin (GitHub - galaxyproject/gxadmin: Handy command line utility for Galaxy administrators) to purge the database queue of my galaxy instance!"}], [{"role": "user", "text": "Hi,\nI\u2019ve inherited a legacy galaxy (version 16.07) installed on local Ubuntu (18.04) server, with a lot of installed tools. However, when I try to do data analysis I get the error \u2018Fatal error: Exit code 1 ()\u2019. Here is the bug report for simple fastqc (Note: I got the same/similar error message for other tools I\u2019ve tried too).\nFatal error: Exit code 1 ()\n/bin/sh: 1: fastqc: not found\nTraceback (most recent call last):\nFile \u201c/home/posavi/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/fastqc/3a458e268066/fastqc/rgFastQC.py\u201d, line 166, in \nfastqc_runner.run_fastqc()\nFile \u201c/home/posavi/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/fastqc/3a458e268066/fastqc/rgFastQC.py\u201d, line 139, in run_fastqc\nsubprocess.check_call(self.command_line, shell=True)\nFile \u201c/usr/lib/python2.7/subprocess.py\u201d, line 190, in check_call\nraise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command \u2018fastqc --outdir /home/posavi/galaxy/database/jobs_directory/017/17999/dataset_35883_files --quiet --extract -f fastq tenM.fastq\u2019 returned non-zero exit status 127\nAlso, most of the installed tools have either red arrow or red triangle flag (see the image)\n\nimage2640\u00d71530 451 KB\n\nI am also not able to either update/reinstall  any of installed tools neither to install any new tools.\nPlease let me know if there is the way to fix this problem, except to upgrade ubuntu and galaxy.\nThank you.\nMarijan"}, {"role": "system", "text": "Hi @Marijan\n\n\n\n Marijan:\n\nFatal error: Exit code 1 ()\n/bin/sh: 1: fastqc: not found\nTraceback (most recent call last):\n\n\nThis is a configuration problem. Was the Galaxy installation moved to a new location?\n\n\n\n Marijan:\n\nPlease let me know if there is the way to fix this problem, except to upgrade ubuntu and galaxy.\nThank you.\n\n\nYou\u2019ll need to be running a current version of Galaxy for the newer tool versions to be functional.\nUpgrading works best when done sequentially through releases. For your case, that would be a lot of work. Is there a reason why you need the older installation instead of starting over?"}], [{"role": "user", "text": "Hello there,\nMy account was suddenly marked for deletion for the first time on Thursday 23 october 2020. I have requested galaxy by replying to their designated email to restore my single non-public account (admin redacted for privacy). also I had declared my public server duplicated accounts for closure. Still after one day, my account has not been restored, every email comes up with same machine generated reply.\nI don\u2019t know whom to contact for restoration of my account as the data is very important to me.\nKindly let me know is there any other way to ask them for restoration.\nThanks\nFurqan"}, {"role": "system", "text": "Resolved.\n\n  \n    \n    \n    Accounts deleted from Galaxy Main to enforce posted Terms and Conditions usegalaxy.org support\n  \n  \n    Update Nov 1, 2019 \nDuplicated accounts are detected as an ongoing process. If you found your account recently deleted, or have not but do have duplicates and want to avoid problems, write to us at galaxy-bugs@lists.galaxyproject.org and list the following: \n\nThe registered email for the account you wish to keep\nIf that email is from a public domain service, include your non-public primary email address. See \u201cReminders\u201d below if you do not understand the difference.\nList out your duplicated acco\u2026\n  \n\n"}], [{"role": "user", "text": "Dear Galaxy community,\nFor both eggNOG 1.0.2 or 2.0.1 implemented in usegalaxy.eu, the \u2018Version of eggNOG Database\u2019 cannot be changed (\u2018No options available\u2019 is displayed as default), but upon \u2018Execute\u2019, this drop-down menu becomes highlighted in red with a \u2018Please verify this parameter.\u2019 being displayed, and eggNOG would not start.\nWould be great if there was a way to get eggNOG started, and any help would be appreciated.\nThanks,\nMichael\neggNOG 1.0.2\nhttps://usegalaxy.eu/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Feggnog_mapper%2Feggnog_mapper%2F1.0.2\neggNOG 2.0.1\nhttps://usegalaxy.eu/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Feggnog_mapper%2Feggnog_mapper%2F2.0.1%2Bgalaxy1"}, {"role": "system", "text": "Hi,\ncan you please try this again. I installed database 2.0.\nCiao,\nBjoern"}], [{"role": "user", "text": "I am having trouble running the COVID-19: variation analysis on ARTIC PE. I tried different versions as well but get the following error \"Workflow submission failed: Uncaught exception in exposed API method\u201d. I thought it was my fastq files but then I also tried running the data from previous history that was successful but the same error came up. The same error occurs on both .eu and .org platforms."}, {"role": "system", "text": "Hi @Kathleen_Subramoney\nPlease make sure to source the most current versions of these workflows from here: GalaxyProject SARS-CoV-2 analysis effort - Galaxy Community Hub\nIf you need more help, the community is having trouble helping. Please provide a bit more context so others can try to reproduce and offer advice.\n\nWhere the workflow was sourced\nThe exact URL of that public workflow\nShare link to that workflow after imported at either usegalaxy.org or usegalaxy.eu\n\nShare link to a history where that workflow was executed and failed\nShare link to a history where that workflow was executed and was successful\nScreenshot of the expanded User -> Workflow Invocations report for each of those runs\n\nThanks!"}], [{"role": "user", "text": "Hello to the Galaxy community. Very helpful indeed. Many thanks for your effort.\nI am new to sequencing and I am following the same Tutorial as above, but I am applying the protocol to mouse. Unfortunately, I ended up with the same problem, i.e., the final DEseq2 file did not contain the gene names, but instead the term N/A. I am using the file from UCSC web table, as follows:\n\nDataset Peek:\n\n\n\n\n1.Seqname\n2.Source\n3.Feature\n4.Start\n5.End\n6.Score\n7.Strand\n8.Frame\n9.Attributes\n\n\n\n\nchr1\nmm10_ncbiRefSeq\nstop_codon\n134202951\n134202953\n0.000000\n-\n.\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nCDS\n134202954\n134203590\n0.000000\n-\n1\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nexon\n134199215\n134203590\n0.000000\n-\n.\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nCDS\n134234663\n134234733\n0.000000\n-\n0\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\n\nIf I understood correctly I have not used the correct mouse file. Could you please help me, and point me to the correct file, and what I have done wrong, if not the wrong file?\nMany thanks."}, {"role": "system", "text": "Hi @Brists,\ncould you share your Galaxy history with me? My email is .\nRegards"}, {"role": "user", "text": "Hi gallardoalba,\nThanks for your email, appreciated. Following the instructions on the help site I realised the problem was the genome reference file from UCSC table platform. I downloaded the latest mouse genome from gencode and my problems about naming of genes and unassigned ambiguity were solved. However, there are other things that I would like to ask. Therefore, I open a new topic.\nMany thanks again for your effort on the help tutorials.\nBest regards,"}], [{"role": "user", "text": "Hi, I have been trying to follow the galaxyproject tutorial about sambam-datasets, however, when I try to map bwa-mem my fastq file, it does never run. It return with the error message that it could not align. What step shall I take? What would I be doing wrong?\nHere the exactly message: file does not contain alignment data\nBy the way, here it is the tutorial that I have been following exactly. Manupulating NGS data with Galaxy."}, {"role": "system", "text": "Dear pauloalabarse,\nPlease make sure you selected the correct reference (either from Galaxy or from your local history) and also check if you fastq files are correctly formatted.\nHere is another tutorial that might help you: Mapping\nCheers,\nFlorian"}], [{"role": "user", "text": "Hello Galaxy support team,\nI have developed 3 bioinformatics workflows on the Galaxy Australia server for processing sRNAseq data from host plants for detection of viruses and viroids.\nI would like to publish these workflows so that Galaxy-main users can also access these.\nI tried to test my workflows on the main server and received the error that one tool is not available by default.\nTherefore, I am writing to request to add the following tool to the main server.\ntoolshed.g2.bx.psu.edu/repos/artbio/cap3/cap3/2.0.0\nThank you"}, {"role": "system", "text": "\n\n\n ruvini_lelwala:\n\ntoolshed.g2.bx.psu.edu/repos/artbio/cap3/cap3/2.0.0\n\n\nHi @ruvini_lelwala\nI opened the install request here: Add cap3 2.0.0 to support a new published workflow \u00b7 Issue #524 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub\nIt looks like both usegalaxy.org and usegalaxy.eu need the tool. Please clarify in the issue ticket as needed.\nThanks!"}], [{"role": "user", "text": "Hi,\nI\u2019ve mapped RNA-seq data with Bowtie2 to the GRCm38 genome and I am trying to count using htseq.\nI have imported GTF files from bot UCSC and Ensembl but I cannot get the right annotation. It works for Ensembl transcripts IDs but when I select gene_id all counts are zero."}, {"role": "user", "text": "I\u2019ve tried a GTF file from GENCODE and worked just fine.\nAs before I used \u201cFeature type = gene\u201d and \u201cID Attribute = gene_id\u201d\u2026 I am still puzzled why I don\u2019t gent gene counts with other GTF."}, {"role": "system", "text": "There was probably a mismatch for the \u201cFeature type = gene\u201d when using the UCSC GTF (not a good choice anyway, as both gene_id and transcript_id are the same value from this data source - effectively meaning all counts will be \u201cby transcript\u201d even if reportedly \u201cby gene\u201d).\nAnd there was probably a chromosome mismatch problem when using the Ensembl GTF. Ensembl chromosome names will have a format like \u201c1\u201d when most of the genome pre-loaded indexes for mapping tool use the UCSC chromosome names with a format like \u201cchr1\u201d. See Mismatched Chromosome identifiers (and how to avoid them)\nGencode andiGenomes are the best sources for reference annotation when your genome/build version is supported by either. Gencode GTFs can be loaded directly by URL. iGenomes archives need to be downloaded and unpacked locally first, then just the genes.gtf file uploaded to Galaxy."}], [{"role": "user", "text": "Hi! I\u2019m following the tutorial for DNA Methylation Data Analysis  and cannot find the Metilene tool to detect differentially methylated regions (DMRs).\nIs there a way to upload this tool from the repository?\nAlternatively, is there another tool to use for detection of DMRs or differentially methylated loci?"}, {"role": "system", "text": "Hi @smmiguez,\nit will be installed in the next days in usegalaxy.org; meanwhile you can run the tutorial in the Galaxy instance usegalaxy.eu, where you can find Metilene.\nRegards"}], [{"role": "user", "text": "Hi!\nI downloaded my 16S files in fastq format and I\u2019m interested in convert this files to fastq.gz files. How do I do that?\nI\u2019m leaning how to analyze 16S data.\nThanks  lot"}, {"role": "system", "text": "Hi @bbarrios\ntry Edit Attributes (pencil icon) > in the middle window switch to Convert tab > in pull down menu select an appropriate option (fastqsanger.gz).\nThis is assuming you use data with Phred 33 encoding and Galaxy assigned fastasanger datatype to FASTQ files after upload.\nKind regards,\nIgor"}], [{"role": "user", "text": "I have a question about limma or edgeR or any other DEG tool. how the tool calculate the treat and control fold changes? for example in the first section we put control and second treat. So is it Control-Treat(in fold changes) or Treat-Control\nit is just about how the DEG tool calculate fold change result"}, {"role": "system", "text": "Hi @amir,\nit should be Control-Treat in your example.\nping @mblue9 as well.\nSorry for the late reply!\nBjoern"}, {"role": "system", "text": "Actually, for the limma and edgeR tools at least, I think you want Treat-Control. Control-Treat would be if you want to know what genes are up/down in the control relative to the treatment. Usually people want to know what\u2019s up/down in the treatment relative to the control. Even if you input the Control sample count files first and the Treat counts files second, it\u2019s the contrast order that specifies how the groups are compared. See the help section in the limma tool form (e.g. for Mut-WT the fold changes in the results will be up/down in Mut relative to WT)."}], [{"role": "user", "text": "Hi,\nI failed to connect to the galaxy ftp server. Below is the error message I got:\n\nWeChat Screenshot_20210303193605844\u00d7137 6.46 KB\n\nI tried to turn off the firewall but it didn\u2019t help.\nI also tried to edit the network configuration wizard and I got this error message:\n\nWhat should I do? FYI, I\u2019m physically in China so I have to use vpn first to get access to galaxy, google etc\u2026 Is this the problem?\nThanks,\nYunyang"}, {"role": "system", "text": "Hi @Yunyang_Wang,\nCould you try connecting again? It seems to be due to a system scheduled maintenance.\nRegards"}], [{"role": "user", "text": "Hello,\nI have a series of non-tool-shed tools I would like to install on my Galaxy instance.\n\n  \n      \n      github.com\n  \n  \n    \n\nTAMU-CPT/galaxy-tools\n\n\n  CPT's Galaxy Tools. Contribute to TAMU-CPT/galaxy-tools development by creating an account on GitHub.\n\n  \n  \n    \n    \n  \n  \n\n\nmacros.xml includes following requirements for all of the tools\nrequirement type=\u201cpackage\u201d version=\u201c2.7\u201d - python - requirement\nrequirement type=\u201cpackage\u201d version=\u201c1.65\u201d - biopython - requirement\nrequirement type=\u201cpackage\u201d version=\u201c2.12.1\u201d - requests - requirement\nAll of these are resolved by conda in my instance, but when I run a tool, it uses default galaxy virtualenv python executable instead of mulled conda environment causing errors. How should I let the job run inside conda environment?"}, {"role": "system", "text": "When you execute such a job, do you see the Conda environment being resolved in the Galaxy logs ?\nIn the galaxy.yml file, did you set preserve_python_environment to a non-default value ?\nDoes this happen for all tools or just python tools ?"}, {"role": "user", "text": "Thanks for your response. This issue seems to be resolved, and it seems like I caused the problem. I was running run.sh inside Ubuntu Screen instance so Galaxy can keep running when I close ssh. And Screen apparently initiated another bash shell which seems to have caused this issue. I now run inside tmux and it seems to have fixed the issue."}], [{"role": "user", "text": "Hello Community,\nCould someone guide me to someone who can help me. I am getting this error when I try to login to my galaxy account. \"Uncaught exception in exposed API method: \" Does someone know why or how can I fix this?\nThank You in Advance!"}, {"role": "system", "text": "I\u2019m having the same error, even when trying to reset my password to log in.  I\u2019m guessing there\u2019s something wrong with our account server side."}, {"role": "system", "text": "Hello @abduljan71 and @matthewstuartedwards\nThat message can result if a prior account was attempted to be \u201cre-created\u201d.\nTry creating an account with an email address not previously used for an account at the server where you are working. Even when some old account doesn\u2019t exist anymore, the email address still cannot be reused at most of the public servers. Meaning: please start completely over to get a new working account.\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#account"}], [{"role": "user", "text": "Hello siam here. I am a newbie in terms of MD simulation. For my research purpose, I need to simulate 100ns. how do I incorporate that into the galaxy server? how many steps do I need to include for a 100ns simulation? Do I need to step of ps?\nthank you."}, {"role": "system", "text": "Hi @Siam_Siraji\nPlease see the tutorials here\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Computational chemistry\n\n  Modelling, simulation and analysis of biomolecular systems\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello,\nPlease I would like to ask if it is okay and allowed to create another account on Galaxy because I used almost all the available space I have in my current account and I am planning to upload more raw data and there is no enough space for that!\nIf not then what do you suggest?\nThank you in advance!"}, {"role": "system", "text": "Hello @shamjdeed\nAt most public Galaxy servers, there is a \u201cone account per user\u201d quota. In practical terms, this means you can have many Galaxy accounts but just one at any particular public Galaxy server.\nFor Galaxy Main https://usegalaxy.org:\n\nAccount terms are explained in this FAQ: https://galaxyproject.org/support/account/\n\nHow to manage data is explained in this FAQ, including how to download data no longer in active use and how to apply for more quota space short-term for larger projects: https://galaxyproject.org/support/account-quotas/. From your comments, it sounds like you need to download then purge your old work to make room for new work, more help here: https://galaxyproject.org/support/account-quotas/#reduce-quota-usage-while-still-retaining-prior-work-data-tools-methods\n\nCreating or using a duplicated account, or someone else\u2019s account, to try to get around the account terms will cause problems like this.  Accounts deleted from Galaxy Main to enforce posted Terms and Conditions Stick to using just one account and you won\u2019t have to worry about losing your account or data unexpectedly.\n\nThe FAQs above also cover how to find out the usage terms (and if quota extensions are possible) when working at other public Galaxy servers. In short, check the server\u2019s home page or directory listing (https://galaxyproject.org/use) for that server\u2019s contact and quota information, and ask them you have questions.\nYou also have the option of setting up your own Galaxy server. A local server can be used to store data (in context). Cloud servers are popular choices for large computational work. If you use one at AWS, they offer grants for storage/computational resources (Galaxy itself is always free, and the Cloudman version is designed to be simple to administrate). These details are also linked from FAQs above and at the Galaxy resources directory (https://galaxyproject.org/use). Also, search prior Q&A with the term \u201ccloud\u201d here if interested in exploring that option.\nAll public Galaxy servers are independently administered and not all follow this forum. If you can\u2019t locate the server contact or quota info for where you are working, share back the URL and we can try to help you find it.\nThanks!"}], [{"role": "user", "text": "Dear all,\nI am a very new galaxy user. I have 4 fastq files from the same organism, 2 forward reads and 2 reverse reads. I want to merge forward by forward and reverse by reverse before mapping the genome.\nI\u2019ve found several tutorials on mapping, which are great, but I don\u2019t know how to merge my sequences.\nCould you please kindly help me with this?"}, {"role": "system", "text": "Hi @HamidGaikani,\nyou can use the Concatenate datasets tool."}], [{"role": "user", "text": "I am trying to identify and study the differentially expressed non-coding RNA, I used the FeatureCount Built-in genome for the counting, but I am not getting non-coding RNA. Any help is welcome. Thanks!"}, {"role": "system", "text": "Hello @exotic1213\nThe built-in annotation sources are for coding regions of the genome.\nIf you want to look for ncRNA, Gencode is one annotation source. Use the GTF version of the annotation, format it correctly, and adjust the tool form setting to match the annotation features/attributes.\nAnnotation:\nhttps://www.gencodegenes.org/human/\nFAQs:\nhttps://galaxyproject.org/learn/datatypes/#gtf\nhttps://galaxyproject.org/support/diff-expression/\nTutorials:\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Search Tutorials\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI also added some tags to your post that point to related prior Q&A.\nHope that helps!"}], [{"role": "user", "text": "\nScreenshot 2022-05-02 at 00.44.351920\u00d71200 141 KB\n\nMy FASTQC raw data (collection of paired-end reads) does not appear in the dropdown menu of MultiQC when I select multiple datasets or dataset collection, I have tried it with version but no avail\nPlease help"}, {"role": "system", "text": "Try flatten collection as explained here MultiQC not working correctly\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello,\nPlease I am getting error using SnpEff since yesterday, I had results a day before using the same settings and data type. Please I want to know if there is something wrong somewhere thank you.\nGavers"}, {"role": "system", "text": "Hi @Gavers_Oppong\nThe UseGalaxy.eu server was addressing maintenance the last few days. https://status.galaxyproject.org/\nReruns have worked to resolve odd behavior for other people. That might involve backing up and examining/rerunning tools run upstream from this tool (and those outputs). Please also give that a try.\nIf you are still having problems after that, send in a bug report and consider asking if the problem might be still related to any lingering maintenance issue at the EU chat https://matrix.to/#/#usegalaxy.eu:matrix.org. You can post a link to this topic for reference in a chat or in a bug report.\nThe EU moderators will be back next Monday their time (and maybe sooner!) but let\u2019s start there. If you want to post a shared history link with your final rerun errors, that will speed things up if there is maybe a new data problem. Troubleshooting errors"}], [{"role": "user", "text": "Is there any way where I can upload my VCF files (myvcf.vcf) and annotate the rsid of dbSNPs?\nI understand I should use SnpSift Annotate and I need the VCF File with ID field annotated (such as dbSNP.vcf). Is it possible that I don\u2019t have to upload this file and can access remotely via FTP? I am not sure how to do this. I am looking for dbSNPs for hg19 genome.\nI am trying to do this in the online version of Galaxy.\nThank you."}, {"role": "system", "text": "Hi @anchor,\nno I don\u2019t think there is currently a way around uploading the file to a history of yours at least once.\nIf you need to do this for more than once it may be convenient to maintain a dedicated \u201cresource\u201d files history, from which you just drag the dataset over to each analysis history. This way, as explained in Galaxy Community Hub - Galaxy Community Hub, the file will be counted in your storage quota only once.\nIn the long run, this tool should be extended to work also with preinstalled annotation data on the server. I\u2019ll give this some thought."}], [{"role": "user", "text": "Hi , I am learning about Mapping ,and when using the RNA STAR,I cannot find a reference genome called Fly (Drosophila melanogaster): dm6 Full. What should I do? How to find it or how to insert it? Thank you verymuch"}, {"role": "system", "text": "Hi @faiz1,\nyou can find the indexed genomes in usegalaxy.eu. Otherwise, you can download the reference genome from NCBI; you just need to copy the following link:\n\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/215/GCF_000001215.4_Release_6_plus_ISO1_MT/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.fna.gz\n\nThen you need to click in Upload data in the Galaxy interface, then click in Paste/Fetch data, paste the link there and finally Start.\nRegards"}], [{"role": "user", "text": "Hi the jobs are waiting to run in galaxy for long time. I have started before 8 hours and the job is not running at all."}, {"role": "system", "text": "I am facing the same issue\u2026I have switched to galaxy Europe\u2026its working for now."}, {"role": "system", "text": "Hello @Isha_Singh @Sayan_Paul\nPlease see Cluster issue at UseGalaxy.org -- Resolved 06/05/2023. Rerun failed (red) or leave queued (gray) - #3 by jennaj"}], [{"role": "user", "text": "Hi\nPlease I have a question about create a single Pie chart for each sample.\nAs attached , the photo represents the Krona pie chart for two samples, now I need to make a single pie chart for each sample for comparative. Any one can help me to do that?\nI will appreciate\nbest regards\n\nScreenshot 2021-07-07 at 2.48.30 pm1440\u00d7900 198 KB\n"}, {"role": "system", "text": "Hi @kebabiness_over9000,\nI recommend you to have a look at this tutorial 16S Microbial analysis with Nanopore data. It generates different pie charts for different samples.\nRegards."}], [{"role": "user", "text": "HI,\nWe just recently moved all of our Galaxy data to a new server with python 2.7 so that we could upgrade to the latest version of galaxy. Everything seems to be working pretty well except that when I try to re-run old GATK or deeptools tasks, the job fails.\nI have checked/compared \u201cmoved data\u201d versus \u201cre-uploaded data\u201d using these two methods:\n\n\nmoved bam data: job errors: checked file locations to make sure they match, checked permissions, run jobs with new data (error)\n\nre-uploaded\" bam data: job success: copied data (from the new server to my local machine) and re-uploaded them. Jobs run successfully with this data creating the same output as the old jobs\n\nI am wondering if anyone might know why this is happening with method 1. There does not seem to be anything wrong with the bam files or the tools.\nAny ideas would be great, thanks!\nBelow are the errors output by some of the tools after BAM were only moved (not re-uploaded) using method 1\n\nDeeptools bam compare output\nFatal error: Exit code 1 ()\nThe file one.bam does not exist\n\nGATK haplotype caller\n##### ERROR MESSAGE: File /data/galaxy/tmp/tmp-gatk-PBNEqR/gatk_input_0.bam.bai \nis malformed: Premature end-of-file while reading BAM index file /data/galaxy/tmp/tmp-gatk-\nPBNEqR/gatk_input_0.bam.bai. \nIt's likely that this file is truncated or corrupt -- \nPlease try re-indexing the corresponding BAM file.\n\nIndel realigner\n##### ERROR MESSAGE: File /data/galaxy/tmp/tmp-gatk-tXbYgP/gatk_input.bam.bai is malformed: \nPremature end-of-file while reading BAM index file /data/galaxy/tmp/tmp-gatk-tXbYgP/gatk_input.bam.bai. \nIt's likely that this file is truncated or corrupt -- \nPlease try re-indexing the corresponding BAM file.\n\n\n\nNote: Moderator reformatted post for clarity\n"}, {"role": "user", "text": "Update* I have located the bam indexes on our original server and discovered that the _metadata files (containing the indexes) in the files dir were not copied over with the rest of the files. Copying these files over has fixed this issue."}], [{"role": "user", "text": "Hello everyone.\nI am trying to operate htseq-count in galaxy. I have used HISAT2 BAM files (I am using Arabidopsis WT and Double Knock Out) and TAIR10.GFF3 annotation file.\nBut every time it shows fatal error after a while.\nRequesting you all to provide your invaluable suggestions.\nThanking you,\nMahmudul"}, {"role": "system", "text": "Hi,\nTry using the gtf version of the annotation instead. This tool does not accept gff3 formatted annotation.\nThis can get a bit confusing since the tool form states gff as an input, and will accept files with that datatype assigned. This is because many data sources provide gff+gft hybrid data \u2013 and the only difference between those two is the last column (attributes). Some also release gff3 data that is given a gff datatype (including this source: https://www.arabidopsis.org). The gff3 format is structured and labeled much differently than gff or gtf.\nThese data are incredibly difficult to automatically assign a datatype to, because there can be so much variation from sources. Custom header lines can interfere, blank lines, etc. It is usually a good idea to check the format yourself once in Galaxy to confirm the actual type. Sometimes headers need to be removed in gff or gtf datasets before certain tools will consume them properly.\nThis FAQ explains each, how they are similar/different, and reformatting/transformation tips.\n\nCommon datatypes explained\n"}], [{"role": "user", "text": "Hi\nI have a differential expressed genes files with Gene ID and Log fold change values.\nI want to find the top 20 expressed genes using limma but as I do not have the feature counts with me it is giving me error.\nCan you please suggest which tool within galaxy I can use"}, {"role": "system", "text": "Based on information you provide it is very difficult to discern what you are trying to do. Please consult the following resources first:\n\nRNAseq in galaxy Tutorial\nCornell RNAseq course\n"}], [{"role": "user", "text": "Dear support team of the GALAXY,\nI would like to use Bowtie2 for mapping NGS (ChIP-seq) data for two different microorganism, Aspergillus fumigatus and Verticillium dahliae.\nI have already checked for the corresponding reference genomes of these two microbes and they are not available as options in the GALAXY platform.\nWould it be possible please to include them in the reference genome inventory?\nPlease find details of these genomes below:\n\nAspergillus fumigatus:\nLink:\nhttps://fungidb.org/common/downloads/Current_Release/AfumigatusAf293/fasta/data/\nFile:\n[FungiDB-48_AfumigatusAf293_Genome.fasta]\n\n(https://fungidb.org/common/downloads/Current_Release/AfumigatusAf293/fasta/data/FungiDB-48_AfumigatusAf293_Genome.fasta)\n2)Verticillium dahliae:\nLink:\nftp://ftp.ensemblgenomes.org/pub/fungi/release-48/fasta/verticillium_dahliaejr2/dna/\nFile:\n[Verticillium_dahliaejr2.VDAG_JR2v.4.0.dna.toplevel.fa.gz]\n(ftp://ftp.ensemblgenomes.org/pub/fungi/release-48/fasta/verticillium_dahliaejr2/dna/Verticillium_dahliaejr2.VDAG_JR2v.4.0.dna.toplevel.fa.gz)\nMany thanks in advance,\nBest regards,\nManolis"}, {"role": "system", "text": "Hi @Manolis1\nUpload the fasta version of each and use them as Custom Genomes \u201cfrom the history\u201d. You can also promote any fasta to a Custom Build to generate a \u201cdatabase\u201d that can be assigned to datasets (some tools may require that metadata assignment).\nThere is much prior Q&A about using Galaxy this way that covers the how-to FAQs plus details and other troubleshooting. I added a few tags to your post to help in finding those.\nThe usegalaxy.eu administrators may write back, if they decide to add the genomes directly. There is a hackathon going on today, so replies may be delayed. But you don\u2019t need to wait \u2013 using a custom genome/build is accessible to everyone at all usegalaxy.* servers and most other public Galaxy servers.\nThanks!"}], [{"role": "user", "text": "Dear All,\nI\u2019ve recently encountered some issues with Unicycler assembly. I\u2019ve tried to perform hybrid assembly with use of\n\n\ntrimmed Illumina reads R1 (0.17 Gb) + R2 (0.16 Gb); format: fastqsanger.gz\n\n\nnanopore reads (2.3 Gb); format: fasqsanger\n\n\nUnicycler readily deals with individual assembly of either Illumina or Nanopore reads. However, it fails to generate hybrid assembly. Any suggestions?\nmaybe is it about available working memory @ Galaxy server?\nthanks in advance,\nPiotr\nPS here is the error report\n\ntput: No value for $TERM and no -T specified \n\ntput: No value for $TERM and no -T specified \n\ntput: No value for $TERM and no -T specified\n\n/pylon5/mc48nsp/xcgalaxy/main/staging/23588931/command.sh: line 95:\n38467 Segmentation fault      \n\n(core dumped) unicycler -t\n\"${GALAXY_SLOTS:-4}\" -o ./ --verbosity 3 --pilon_path $pilon -1'fq1.fastq.gz' -2 'fq2.fastq.gz' -l lr.fastq --mode 'conservative' --min_fasta_length '100' --linear_seqs '0' --min_kmer_frac '0.2' --max_kmer_frac '0.95' --kmer_count '10' --depth_filter '0.25' --start_gene_id '90.0' --start_gene_cov '95.0' --min_polish_size '1000' --min_component_size '1000' --min_dead_end_size '1000' --scores '3,-6,-5,-2'\n\n"}, {"role": "system", "text": "Hi @piotr-piotr-majewski!!\nPlease try running the job again now. I would suggest trying at least two, to see if you can hit a non-problematic cluster node.\nThere were some cluster issues yesterday and a few earlier today (different reason). A very small number of jobs failed, but most didn\u2019t. There is likely some small bit of cluster tuning that still needs to be done on our side, but that is unlikely to happen until Monday.\nThese are the important parts of the error message. The \u201ctput\u201d lines are a technical side-effect error output that can be ignored.\n\n\n\n piotr-piotr-majewski:\n\n/pylon5/mc48nsp/xcgalaxy/main/staging/23588931/command.sh: line 95: 38467 Segmentation fault\n(core dumped) unicycler -t \u201c${GALAXY_SLOTS:-4}\u201d -o ./ --verbosity 3 --pilon_path $pilon -1\u2019fq1.fastq.gz\u2019 -2 \u2018fq2.fastq.gz\u2019 -l lr.fastq --mode \u2018conservative\u2019 --min_fasta_length \u2018100\u2019 --linear_seqs \u20180\u2019 --min_kmer_frac \u20180.2\u2019 --max_kmer_frac \u20180.95\u2019 --kmer_count \u201810\u2019 --depth_filter \u20180.25\u2019 --start_gene_id \u201890.0\u2019 --start_gene_cov \u201895.0\u2019 --min_polish_size \u20181000\u2019 --min_component_size \u20181000\u2019 --min_dead_end_size \u20181000\u2019 --scores \u20183,-6,-5,-2\u2019\n\n\nWhat to do if a few unmodified reruns fail:\n\nMake sure that you have done some QA/QC on your data. FastQC is a great tool for diagnosing what could be done to improve the data quality (example: removing adaptor and quality trimming with Trimmomatic; the presence of other sequencing artifacts or contamination \u2013 both of which can lead to assembly issues). Help for interpreting FastQC output: Babraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence Data >> Documentation > Analysis modules Index of /projects/fastqc/Help.\nTry tuning the alignment parameters to better fit the profile of your inputs. Unicycler usage documentation is here: GitHub - rrwick/Unicycler: hybrid assembly pipeline for bacterial genomes\n\nReview a Galaxy GTN tutorial that covers the tool (bacterial assembly, but could give usage cues, even if assembling some other type of organism): Unicycler Assembly\n\n\n\n\n\n piotr-piotr-majewski:\n\nSegmentation fault\n\n\n^^ can be due to many reasons, including a job that is too big to run with the given inputs/parameters. The dataset sizes seem Ok, so if this turns out to be the reason, there is more going on content-wise. QA/QC might address that so be sure to not skip doing it. I know these worked independently, but combining the two types of inputs will impact how the overall assembly is processed.\n\nAfter trying the above, and the jobs still fail after a few reruns/parameter tunings, please send in a bug report from one of the failed assemblies and we can review the full error message, input data, settings, et cetera and offer more advice. Please include a link to this post in the comments and be sure to leave the error output and all input datasets undeleted. If the history contains multiple failed jobs (it should IF you have tested different parameters/reruns), leave all of those undeleted as well). Also, please leave your FastQC results undeleted so we don\u2019t have to run the tool (that is usually where I start when troubleshooting analysis problems involving fastq data, unless something more obvious is clearly going wrong, meaning: tool usage problems or server issues).\nThanks!"}], [{"role": "user", "text": "Hi All,\nI am trying to connect to the Galaxy ftp server via command line. I followed the tutorial; however, it is not working.\nWhen I try:\n$ lftp -u user@email.edu ftps://usegalaxy.org\nI get the error, \u201clftp: usegalaxy.org: Name or service not known\u201d\nI can successfully run:\n$ lftp -u user@email.edu htpps://usegalaxy.org\nbut when I try to transfer files I get the following error, \"\nput: Fatal error: gnutls_record_send: Error in the push function.\"\nNot sure what is going on, so I am reaching out to determine how best to connect to the Galaxy ftp so i can upload my files to my Galaxy account.\nThanks!!"}, {"role": "system", "text": "Welcome, @Jason_White\nPlease try again. I just tested both methods and they work correctly with a few simple commands:\n$ put my_file\n< chat from the server about the transfer >\n$ ls\n< my_file is listed >\n\nUsage from FAQ https://galaxyproject.org/ftp-upload/\nFor explicit FTPS:\n$ lftp -u user@email.edu usegalaxy.org\n\nFor implicit FTPS:\n$ lftp -u user@email.edu ftps://usegalaxy.org\n\nIf you cannot get line-command ftp to work, using a client like Filezilla is another choice. How-to is on the same FAQ.\nThanks!"}], [{"role": "user", "text": "Hi all, i am trying to learn galaxy as i think it can be very useful. I am currently trying to use it for QIIME2.\nI have my paired end sequences which are joined ( archaea practice nano) and my metadata but I have no idea how to import them in Galaxy as a QIIME2 artefact using the qiime2 tools import . There is a tutorial but that only shows you how to copy and paste the tutorial data.\n\nimage1635\u00d7897 62.4 KB\n\nCan anyone offer any advice?"}, {"role": "system", "text": "Hi @Martyn\nThe tool author is working to improve the way the import tool functions for the QIIME2 suite when in Galaxy. Details: Lint failure for deprecated feature, need advice on the best way to transition QIIME 2 Tools\nFor now, you\u2019ll need to try to match up your data with the descriptions available, or try some guesses for the others. The tool form will change when different options are selected, which can offer more clues.\nIt looks like you have some type of paired-end read data, in a collection (aka \u201cdirectory\u201d), along with some metadata files, so that is where I would suggest starting. It may take a few tries to see which fits best. The author\u2019s help site may help more. Find it linked all the way down in that help section.\nMaybe @ebolyen has more specific help?\nXref: TSV upload problems - #4 by jsaintvanne"}], [{"role": "user", "text": "Hi everyone,\nI used SnpEff download (Galaxy Version 4.3+T.galaxy2) to download the database SnpEff4.3 GRCh37.75  for vcf annotation. However, I have difficulties to find the information of the ensemble build used? I would be happy for help!\nThanks!\nAll the best,\nRose"}, {"role": "system", "text": "Hi @roselucia,\nthis is the ENSEMBL release used to build the SnpEff database: release 75. However, I recommend you to use the GRCh38.86 database, which is the most recent one.\nRegards"}], [{"role": "user", "text": "dear when i do this operation the galaxy produce this error \u201cFATAL: container creation failed: mount\u201d what mean?"}, {"role": "system", "text": "Hi @Carlo_Ceglia,\nthis error is caused by a temporal problem in the server. Sorry for the inconvenience.\nRegards"}], [{"role": "user", "text": "I\u2019m trying to filter reads with skipped regions/spliced alignments which have N bases in the cigar string. I\u2019m using the following tool: Filter BAM datasets on a variety of attributes (Galaxy Version 2.5.1+galaxy0). I selected the option to filter on cigar string but do not know how to give a command to filter only reads with the \u2018N\u2019 base. Does anyone know how to do this?"}, {"role": "system", "text": "Answer: use the term *N*\nScreenshot:\n\nfilter-bam-cigar-example1690\u00d71440 192 KB\n"}], [{"role": "user", "text": "please what the differences between the using of cutadapt option and trimmomatic when we want to do trimming after checking  the quality score?\nI will appreciate your help"}, {"role": "system", "text": "Hi @kebabiness_over9000,\naccording to this paper, both tools perform similarly regarding data quality after processing. However, you should take into account that Trimmomatic is specifically designed for processing Illumina paired-end and single-ended data. If your datasets have been generated by using a different technology, you should use Cutadapt.\nRegards"}], [{"role": "user", "text": "Hi,\nI\u2019ve tried to use HISAT2 on some of my Cutadapt output files, however the summary files from HISAT2 are always empty for my 48 files (i.e. 0 lines).  I tried re-uploading my data and starting the analysis again, re-running HISAT2 multiple times, and even trying the files on the main galaxy site (which didn\u2019t work either).  Strangely, I got a \u2018normal\u2019 summary file just once after re-running HISAT2 on one file, but I deleted it thinking the problem had been solved to re-run the tool on all the samples again.\nThe genome file I\u2019m aligning to has been successfully used in previous analyses, and the bam file from HISAT2 is fine for downstream analyses.  Can anyone give me some insight on what I can do to fix this?\nThe stderr output is:\nSettings:\nOutput files: \"genome..ht2\"\nLine rate: 6 (line is 64 bytes)\nLines per side: 1 (side is 64 bytes)\nOffset rate: 4 (one in 16)\nFTable chars: 10\nStrings: unpacked\nLocal offset rate: 3 (one in 8)\nLocal fTable chars: 6\nLocal sequence length: 57344\nLocal sequence overlap between two consecutive indexes: 1024\nEndianness: little\nActual local endianness: little\nSanity checking: disabled\nAssertions: disabled\nRandom seed: 0\nSizeofs: void:8, int:4, long:8, size_t:8\nInput files DNA, FASTA:\ngenome.fa\nReading reference sizes\nTime reading reference sizes: 00:00:03\nCalculating joined length\nWriting header\nReserving space for joined string\nJoining reference sequences\nTime to join reference sequences: 00:00:01\nTime to read SNPs and splice sites: 00:00:00\nUsing parameters --bmax 48513222 --dcv 1024\nDoing ahead-of-time memory usage test\nPassed!  Constructing with these parameters: --bmax 48513222 --dcv 1024\nConstructing suffix-array element generator\nConverting suffix-array elements to index image\nAllocating ftab, absorbFtab\nEntering GFM loop\nExited GFM loop\nfchr[A]: 0\nfchr[C]: 72099572\nfchr[G]: 129346882\nfchr[T]: 186624101\nfchr[$]: 258737183\nExiting GFM::buildToDisk()\nReturning from initFromVector\nWrote 90670663 bytes to primary GFM file: genome.1.ht2\nWrote 64684300 bytes to secondary GFM file: genome.2.ht2\nRe-opening _in1 and _in2 as input streams\nReturning from GFM constructor\nReturning from initFromVector\nWrote 117491317 bytes to primary GFM file: genome.5.ht2\nWrote 65856150 bytes to secondary GFM file: genome.6.ht2\nRe-opening _in5 and _in5 as input streams\nReturning from HierEbwt constructor\nHeaders:\nlen: 258737183\ngbwtLen: 258737184\nnodes: 258737184\nsz: 64684296\ngbwtSz: 64684297\nlineRate: 6\noffRate: 4\noffMask: 0xfffffff0\nftabChars: 10\neftabLen: 0\neftabSz: 0\nftabLen: 1048577\nftabSz: 4194308\noffsLen: 16171074\noffsSz: 64684296\nlineSz: 64\nsideSz: 64\nsideGbwtSz: 48\nsideGbwtLen: 192\nnumSides: 1347590\nnumLines: 1347590\ngbwtTotLen: 86245760\ngbwtTotSz: 86245760\nreverse: 0\nlinearFM: Yes\nTotal time for call to driver() for forward index: 00:01:50\nError, fewer reads in file specified with -1 than in file specified with -2\nterminate called after throwing an instance of \u2018int\u2019\n(ERR): hisat2-align died with signal 6 (ABRT)\n[bam_sort_core] merging from 40 files and 1 in-memory blocks\u2026"}, {"role": "system", "text": "Merging with Bowtie2 error-bowtie2-align died with signal 6 (ABRT)"}], [{"role": "user", "text": "Hello community!\nI have tried to use EasyFig on Windows and on MacOS but neither of them works. It seems that is a blast issue, but in several forums people are facing the same problem. I used to run EasyFig on my MacOS but after the Monterey 12.5 a lot of things stopped working.\nThus I was wondering whether is possible to include EasyFig on Galaxy?"}, {"role": "system", "text": "Hi @carlos.pqc\nThe tool is wrapped for Galaxy and available at this public server.\n\n  \n      \n\n      galaxyproject.org\n  \n\n  \n    \n\nCenter for Phage Technology (CPT) - Galaxy Community Hub\n\n  All about Galaxy and its community.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt is a new wrapper, so should work at that site. If you have problems with it, contact that domain-specific server admin for the quickest feedback. Or, if you want the feedback public, you could post here (new topic!) then share the link with the admins and request they answer here to help build up some knowledge about the server and hosted tools. You could also post back the answer they provide if you think it would help others.\nHow to move data between Galaxy servers. Moving individual datasets, not entire histories, is probably best for this case. Since you are currently working at UseGalaxy.eu, remember to set the history to a \u201cshared by URL\u201d state before capturing the dataset link. Or upload the data directly from your computer if it is there."}], [{"role": "user", "text": "In stringtie merge, do we merge the replicates of the treatment samples"}, {"role": "system", "text": "In short, the answer is usually \u201cyes\u201d. The tool can do this always but whether you want to or not depends on your analysis details.\nUsage and which inputs to include (sample GTFs/known GTF) depend on whether you are going to perform the DE analysis on only known transcripts/genes, or on only discovered (sample) transcripts/genes, or on both.\nStingtie Merge can be used to do the following:\n\n\nPre-process an existing reference annotation GTF dataset, by itself, so it can be used with Stringtie to guide assembly, and with other downstream tools. Not all public GTFs are formatted in a way other tools can interpret and this tool can be used to \u201cgroom\u201d a GTF.\n\n\nCombine the per-sample GTF results of Stingtie, and optionally the reference GTF (knowns), into a unified GTF assembly that can be used with downstream differential expression tools.\n\n\nMore details that summarize the different use-cases are in this prior Galaxy Biostars Q&A post: Question: StringTie and StringTie merge - when to apply the Guide gff (reference annotation file)?.\nPlease also see the Stringtie tool manual.\nIt would probably also help to review the Galaxy Tutorials for DE analysis. Several of the RNA-seq protocols include it \u2013 and importantly, HISAT2 (settings to create Stringtie-readable BAMs).\n\n\nGalaxy Tutorials for Galaxy Main: Start here: RNA-seq: Discovering and quantifying new transcripts - an in-depth transcriptome analysis example\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy server choices. If a tutorial is supported by a pre-configured Galaxy Docker training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n"}], [{"role": "user", "text": "Hellow everyone\nI work with my colleagues on RNA-seq human data where we are trying to analyze these data on usegalaxy.eu. we have +50 raw single-end sequencing data files. We have started our analysis by running FastQC on all data files with no problem. Now we are trying to run \u201cTrimmomatic flexible read trimming tool for Illumina NGS data\u201d on the data files to cut the first 5 bases but we are getting the error in attached image for 8 of these data files. We have tried to run these 8 data files on usegalaxy.org to see if the files will run correctly but we got the same error. Hope you can help us to troubleshoot this problem.\n\nIMG_20220728_0936321920\u00d71440 267 KB\n"}, {"role": "user", "text": "Problem solved"}], [{"role": "user", "text": "I am experiencing very slow speed of galaxy.org, I am not able to do a basic mapping using BWA-MEM application, has anyone experienced similar problem?"}, {"role": "system", "text": "Hi @abdul85\nYes, the clusters are very busy at UseGalaxy.org after a small server side issue detected/fixed today.\nJobs will still run in the order submitted. The best strategy is to leave your jobs queued to preserve the original queue placement.\nI\u2019m going to close this topic out to avoid duplicated posts.\nPlease follow this ticket instead for updates: usegalaxy.org is super slow: 11/8/2022 Confirmed job delays, please leave queued jobs queued - #3 by jennaj\nThanks for reporting the problem!"}], [{"role": "user", "text": "Hi\nwould it be possible to have the Arabidopsis thaliana TAIR10 genome installed for HISAT2?\nthanks a lot\nbest"}, {"role": "system", "text": "Hello @pej\nThat genome has not been indexed for HISAT2 at https://usegalaxy.org. We do plan to do that as part of a larger project but it won\u2019t be completed for a bit more time.\nYou don\u2019t need to wait. There are two choices:\n\nUpload the genome in fasta format and use it as a Custom Reference genome with HISAT2 (and any other tool it is not indexed for) at Galaxy Main https://usegalaxy.org. How-to: Preparing and using a Custom Reference Genome or Build. The exact genome used in Galaxy is here (a good place to source from, since the genome is already indexed for some other tools, and using the same exact genome for all steps in the analysis is very important): http://datacache.galaxyproject.org/\n\nUse the Galaxy EU https://usegalaxy.eu/ server instead, they do have the genome indexed for HISAT2 already.\n\nThanks!"}], [{"role": "user", "text": "Hi,\nI am working on galaxy 101 tutorial. I have shared my history below.\nhttps://usegalaxy.org/u/ashgourd/h/galaxy101-29mar22\nMy workflow is different from what I am seeing in the tutorial especially in the first step\u2026 Appreciate your help. Thanks"}, {"role": "system", "text": "Hi @Jayanthi_Ramprakash,\nI don\u2019t see any difference; did you find the problem?\nRegards"}, {"role": "user", "text": "Figured out. Thanks"}], [{"role": "user", "text": "Hello,\nI ran Kallisto and Dsse2 to get my list of DE genes. In the results of Dseq2, I can see there are about 20000 lines - I am assuming 1 line for each gene. However, when I download the file, I only get 9000 lines. Can you please let me know why this is happening? And how can I download the entire dataset of 20000 lines?"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI\nTry the create/download \u201cHistory Archive\u201d method.\nMore details are in this prior Q&A:\n\n  \n    \n    \n    Downloading Large Files - 1Gb Limit (Galaxy Main | wget | curl | API Key) usegalaxy.org support\n  \n  \n    @KEELE Yes, the data is large. The connection dropped for me too. \nI remembered that this has come up before. See this prior Q&A on how to \u201cresume\u201d downloads with curl and wget and see if one of those works for you. \n\n\nYour other option is to create a History archive and download that. Once uncompressed, you\u2019ll find your data inside of it. Tip: Copy just the data you need into a new history to make it smaller/faster to process this way. Copies of dataset you already have in your account do not c\u2026\n  \n\n\nThanks!"}], [{"role": "user", "text": "Can anyone tell me how to find \u201cMap with Bowtie for Illumina\u201d from the tools? I searched \u201cBowtie\u201d in the tool search, I only got \u201cBowtie2\u201d and no \u201cMap with Bowtie for Illumina\u201d.  However according to what I learned, I need use Bowtie version1 since my chip-seq sequence are short than 50bp and single ended."}, {"role": "system", "text": "Hi @Sherry\nBowtie version 1 has been deprecated by the original tool authors.\nAlternatives include: Bowtie2, HISAT2, BWA and minimap2\nMaybe test out mapping with several tools and compare what works best for your specific data?\nTutorials\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Epigenetics\n\n  DNA methylation is an epigenetic mechanism used by higher...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello,\nI am new to exome sequencing analysis. I have been looking into the theory and tried using galaxy for the same. I got 2 mutations following a published pipeline from the shared data (which uses freebayes) but only one of them was confirmed by Sanger Sequencing.\nLooking back at my SNP-Sift output, I see that although the unconfirmed variant is of good quality [\u201cQUAL\u201d is good], the CIGAR string for that variant is just represented as \u201cCIGAR=1X\u201d.\nFrom a theoretical point of view, what I could understand is that , CIGAR strings is generally represented as some matches, then some mismatches/insertion/deletion and then some matches. So, I am unable to understand how it can just be \u201cCIGAR=1X\u201d. If it is possible, please tell what does it say about the quality of that variant.\nAny help would be appreciated.\n-Debanjan Roy"}, {"role": "system", "text": "Hi,\nif the variant in question is a single nucleotide change, then CIGAR=1X is just fine though also totally meaningless. CIGAR strings in the VCF INFO field (because that\u2019s what I think we are talking about here) describe how to align an alternate allele to the reference allele. If your REF and your ALT allele both have a length of 1 nt, then the only possibility to align the two is with one mismatch, which is exactly what 1X means.\nWhat you might confuse this with is the CIGAR string associated with aligned reads in a BAM file. This version describes how to align the read to the reference sequence and that\u2019s what would probably be more relevant for assessing the quality of the variant call. However, it\u2019s typically easier to look at reads aligned to the reference in a genome browser (like IGV) than to do it manually by inspecting CIGAR strings one by one."}], [{"role": "user", "text": "I am having issues proxying my production server of Galaxy. I have set up NGINX to proxy as described in the admin documentation and can access galaxy via the proxy however, it shows up as plain text in the web browser without any graphics(see attached image). I can click on links and navigate through Galaxy, but it\u2019s difficult to do anything. Any help would be greatly appreciated. Screen Shot 2020-12-29 at 11.00.23 PM1570\u00d71692 275 KB"}, {"role": "system", "text": "Do you have this part in your config?\n        # serve framework static content\n    location blue {\n        alias $galaxy_root/static/style/;\n        expires 24h;\n    }\n    location static {\n        alias $galaxy_root/static;\n        expires 24h;\n    }\n    location robots.txt {\n        alias $galaxy_root/static/;\n        expires 24h;\n    }\n    location favicon.ico {\n        alias $galaxy_root/static/;\n        expires 24h;\n    }\n\nAnd do you have set the right location for $galaxy_root"}], [{"role": "user", "text": "Hello,\nI am trying to analyze my Infinium Human Methylation BeadChip data, and I have seen this is a possibility using the \u201cInfinium Human Methylation BeadChip\u201d tool. This is the tutorial I found: Infinium Human Methylation BeadChip\nI am confused about which Galaxies this tool is available on. I can not see the tool on my current instance using usegalacy.org.\nPlease let me know if it is possible to use the \u201cInfinium Human Methylation BeadChip\u201d tool, and which Galaxy I should be using.\nThank you very much for your help!"}, {"role": "system", "text": "Hi @kiaraeldred,\nyou can find that tool in the usegalaxy.eu instance: Infinium tool.\nRegards"}], [{"role": "user", "text": "First I need to select variants that are exonic and not in the 1000 genomes project. The 100G is coded in columns 11 to 16 as a \u201c.\u201d or \u201c\u201d used when its missing and whether its exonic or not in column 6. I uploaded a wANNOVAR csv and converted it to tsv. Im the usibf the regular filter option and no matter how many variations of the below I use it wont work.\n6==\u2018exonic\u2019 and c11==\u2018.\u2019 and c12==\u2018.\u2019 and c13==\u2018.\u2019 and c14==\u2018.\u2019 and c15==\u2018.\u2019 and c16==\u2018.\u2019\nIm also supposed to do a simple non coding variant filter on a vcf file from snpeff eff but I not see a column or any data specifically indicating exons or 1000G. How can I do ths?\nNext i tried collectinsertsizemetrics and Im getting this error\n/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/048/054/48054991/_job_tmp -Xmx7g -Xms256m\n00:13:00.688 INFO NativeLibra\nNEED help URGENTLY"}, {"role": "system", "text": "Welcome, @Zachary_Ramsay\nMaybe you found these already?\n\nSince the data is in tabular format, you can use general text manipulation tools to perform filters.\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nAnd, variant analysis tutorials are here.\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Variant Analysis\n\n  Genetic differences (variants) between healthy and diseas...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you are having problems with a specific tool, please ask a new questions and include more details so the community here can help. Items like the server where you are working, the exact tool name/version, parameters, and example data lines provide context. All would be included in a shared history link or you can post that information directly.\nGuidance is here:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Troubleshooting errors\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello Everyone,\nI want to run Racon on Canu output files. It is my first time running this pipeline, hence I have a few basic questions:\n\nFor Racon\u2019s \u201cOverlap\u201d field, which file of Canu should be used?\nI see that all outputs of Canu are in fasta format. But Racon needs the input format in MHAP/PAF/SAM for \u201cOverlap\u201d field. Do I need to change the file format? If yes, which format should I use?\n\nThanks "}, {"role": "system", "text": "Hi @SShivani22,\nRacon requires to be provided with the mapping of the reads to the assembly generated by Canu. You can generate it by using minimap2.\nRegards"}], [{"role": "user", "text": "Hi,\nI followed the tutorial \u2018Visualisation with Circos\u2019 and tried both hg18_karyotype_withbands.txt (custom) and Human hg18 (preset). After I ran the tool I obtained the following error in both cases:\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /usr/local/tools/_conda/envs/mulled-v1-3cf5c7d734898be0f8d76d0fbf3face5f99ca4b3a9b294235f9271dd27115368/bin/\u2026/lib/Circos/Colors.pm line 348,  line 886.\nHere\u2019s my history: Galaxy | Europe\nPlease, can you help me with that?\nAdam\nEdit: tried 0.69.8+galaxy9, 0.69.8+galaxy10, 0.69.8+galaxy12"}, {"role": "system", "text": "Hi @lampad\nI can reproduce the error at UseGalaxy.eu. Please submit a bug from the red error dataset to let those administrators know about the problem. Ping @hxr\nYou can try to run the tutorial at UseGalaxy.org for now instead. Use the most current version of the tool for best results (older versions might have this problem still). Direct link to the tool \u2192 circos/0.69.8+galaxy10"}, {"role": "user", "text": "Hi,\nthanks @jennaj\nThe version on usegalaxy.org seems to be working fine.\n\n\n\n jennaj:\n\nPlease submit a bug from the red error dataset to let those administrators know about the problem.\n\n\nI have submitted it.\nAdam"}], [{"role": "user", "text": "Hi,\nI tried to analyse my recently performed a MD simulation using GROMACS following a tutorial (Running molecular dynamics simulations using GROMACS ).\nParameters of the MD simulation were slightly modified to have shorter simulation (because my protein is around 350 residues).\nI then tried to analyse the results following Bio3D tutorial (Analysis of molecular dynamics simulations).\nEverything seemed to work fine exept when I tried to analyse the results. In fact lauchning RMSD Analysis using Bio3D resulted in \" /corral4/main/jobs/046/826/46826281/tool_script.sh: line 9: Rscript: not found \" alias \" Fatal error: Exit code 127 ()\"\nDo you have an idea why?\nThank you so much,\nMartin\nWorkflow : Galaxy\nDataset :87414315 (bbd44e69cb8906b5a3ee8b8c60394a49)\nHistory : 6677271 (ba081d172fac65b7)"}, {"role": "system", "text": "Update:\nPlease use the tool at UseGalaxy.eu for now.\nThe tool will be available for training purposes at UseGalaxy.org soon."}, {"role": "system", "text": "Resolved, the tool is available at all of the UseGalaxy.* servers."}], [{"role": "user", "text": "Hi everybody,\nI have paired end fastq files FO_1 and FO_2 (forward and reverse, respectively). I want to use them with qiime2. I uploaded them successfully to galaxy, however when I choose qiime2 tools import I receive error messages. The options that I am using are: SampleData[PairedEndSequencesWithQuality], Paired End fastq Manifest Phed33V2. My colleague was able to import the same two files to qiime2 on his machine using command-line codes (native qiime2), but I am unable to import them to qiime2 on galaxy. Please help. Many thanks in advance."}, {"role": "system", "text": "Hi @Gerald\nWe are troubleshooting this tool in another thread. Please read through that about why this particular tool has been troublesome, and current advice. Creating QIIME2 artefacts - #13 by Martyn\nLet\u2019s combine the two."}], [{"role": "user", "text": "I am trying to login and get data using \u2018usegalaxy.org\u2019.\nHowever, I was not able to login and get message as \u2018Uncaught exception in exposed API method:\u2019"}, {"role": "system", "text": "Hi @VIKAS_GUPTA\nYou created at least four accounts yesterday at UseGalaxy.org. Those email addresses cannot be used for a new account at UseGalaxy.org because you already created accounts using those in the past at UseGalaxy.org (plus many others).\nThere is a quota of one account per person per public Galaxy server. Everyone can have one account at each server \u2013 but no more than one account at any. These are free public shared services. When someone uses multiple accounts they are competing for resources not only against themselves, but creating an unfair resource competition environment for everyone else working at the same public server.\nAnyone using multiple accounts to subvert the public quotas has their accounts administratively removed. There is not a special notification when administrative action is applied. Instead, the reminders about terms of service are directly on the account registration form and included directly in account activation emails exactly where the link to confirm the account is located. We want everyone to understand and not have problems.\nOptions\n\nIf you have one single active account at UseGalaxy.org, keep using that one.\nIf you have two or more active accounts at UseGalaxy.org, please manage your data into one account and delete the others under User > Preferences.\nCreating a single new account with a different email address is OK if you do not have any active account(s) at UseGalaxy.org.\n\nFAQs\nOur goals are to help you successfully use public Galaxy servers and avoid future multiple account problems.\n\nRegistering Accounts - Galaxy Community Hub\nAccount quotas - Galaxy Community Hub\n\nIf you need to use Galaxy differently, please review\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\nThe GVL Cloudman version is a single or multi-user choice and AWS offers grants. Amazon Web Services (AWS) - Galaxy Community Hub && AWS Programs for Research and Education\n\nThe AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. AnVIL - Galaxy Community Hub\n\n\nThank you for understanding, and for helping us to keep UseGalaxy.org a free and fair public shared resource for researchers worldwide.\nGalaxy Team"}], [{"role": "user", "text": "Hello,\nI am attempting to complete the trimmomatic step, I keep receiving the error below:\nPicked up _JAVA_OPTIONS: -XX:MaxPermSize=2G -Xmx6G -Xms1G -Djava.io.tmpdir=/data/2/galaxy_db/tmp -Duser.home=/data/2/galaxy_db/tmp\nOpenJDK 64-Bit Server VM warning: Ignoring option MaxPermSize; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning:"}, {"role": "system", "text": "Hi @ahard\nI checked Trimmomatic in Europe with SE and PE data: no issues detected.\nMaybe consider doing a relevant tutorial, eg Quality Control\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello\nI have two tools \u201cRStudio\u201d and \u201cInteractive JupyTool and notebook\u201d in my history, but when I want to call them from the Active Interactive Tools section,I face with the message you do not have active interactive tools yet\nHow can I use these two tools?"}, {"role": "system", "text": "How long ago did you start the interactive tools? If it\u2019s been a long time you may have gotten shut down (for example for server maintenance) and restarting the tools should help. I just started Rstudio and it all seemed to work ok for me at least"}], [{"role": "user", "text": "Hi - looking for any advice or solution. I am running some workflows and an unable to complete them because a step crashes giving the \u201ccannot allocate memory\u201d error. However, it randomly occurs during different steps in the workflow. None of my steps are very demanding, they are simple things like Sort or Join Two Datasets, although I am doing a blast search on a transcriptome that I uploaded. Is there a memory cache I need to purge somewhere? I have purged deleted files and am nowhere near by disk quota.\nThanks for any help, I am stuck."}, {"role": "system", "text": "Welcome, @grakster!\nThe memory used to execute jobs is different from the memory used to store account data. How you describe the problem has in the past indicated that the jobs are approaching the memory limit of what the public server can process. In short, sometimes the jobs hit a cluster node that can execute successfully and sometimes not.\nSorting data can require quite a bit of memory, depending on the datatype and size of the dataset. There are a few different \u201csort\u201d tools, so I can\u2019t offer many more details without knowing which you are using.\nThat said, a highly fragmented transcriptome can easily exceed resources (BAM or tabular results). Filtering out smaller target sequences can often help, especially if they are too small to capture meaningful hits anyway. Modifying the hit parameters can also help (more stringent criteria). In a BAM result, the header can get very large (includes each target), causing a Sort BAM type of job to fail at the indexing or sorting steps. Often tabular output is a better choice. Only \u201chits\u201d are retained, that can be later filtered to reduce redundancy, before doing further manipulations that compare the data to other data (join functions). Tools to filter might include: Select, Filter, Sed, Awk, etc. Blast can create a great deal of \u201ccommon key\u201d, \u201ccoordinate overlapping\u201d, and \u201clow specificity\u201d hits. Sometimes running with strict parameters first, then remapping whatever didn\u2019t map originally, at a lower stringency threshold, is a good strategy to avoid generating too much output (often these are uninformative sub-hits).\nJoin Two Datasets can also be memory intensive, both with processing and with handling the output. It is possible to create pathologically large outputs where each row of the first dataset matches with each row of the second. We do recommend inputting the larger of the two datasets second on the tool form to maximize how the tool uses resources. You might need to modify your query to restrict the output/be more specific or split up the larger dataset into chunks first (tools might include: Line/Word/Character count, Select first/last lines, Remove beginning of a file), then run the join in distinct jobs, and at the end combine the results (tool: Concatenate).\nIf it turns out that you need your jobs processed as-is, and they are exceeding resources on the public Galaxy Main server at https://usegalaxy.org (or any usegalaxy.* server), the option to move to your own cloud, docker or local Galaxy is available. Enough job processing memory resources would need to be allocated, and that is usually easier with a cloud solution for a few reasons (less administrative setup, custom high-memory cluster choices). Keep in mind that Galaxy itself is always free, but commercial cloud services are not (although AWS does offer grants for education/research).  You could also check to see if your affiliations include access to academic cloud resources. If interested, please see:\n\nhttps://galaxyproject.org/choices/\nhttps://galaxyproject.org/use/\n\nFAQs: https://galaxyproject.org/support/#troubleshooting\n\nMy job ended with an error. What can I do?\nStart here since it doesn\u2019t sound like data format problems are a factor: https://galaxyproject.org/support/tool-error/#type-exceeds-memory-allocation\n\n\nHope that helps!"}], [{"role": "user", "text": "Hello, my Unicycler jobs at usegalaxy.org are stalled at \u201cThis job is waiting to run\u201d for 12 hours. Looking through past issues, this may be a PCS Bridges issue. Any advice on getting these to run?\nadmin-edit: Remove end user\u2019s email address for privacy"}, {"role": "system", "text": "Welcome @agmcarthur\nIt is true that certain tools only run at PCS Bridges and that cluster\u2019s queue can get quite long, requiring a longer-than-usual wait (as compared to other tools/clusters).\nPlus, right now that cluster is down for scheduled maintenance. Details: https://portal.xsede.org/user-news/-/news/item/11554\nTools that run at PCS Bridges:\n\nRNA-STAR\nSTAR-Fusion\nTrinity\nUnicycler\n\nThe best strategy is to:\n\nQueue up jobs that run at this cluster as soon as you can. Batch processing features such as Workflows (and Dataset Collections) can aid with this.\nPlan for the tools above taking a bit longer to complete than most other tools.\nAvoid deleting/rerunning \u2013 that will only put your new job back at the end of the queue, extending the wait time, each time it is done.\n\nIn short, queue up jobs and allow them to stay queued until execution is completed. These tools are computationally expensive and do not run well on other clusters. Practically, this means the turnover for cluster availability is a bit slower, but the resources (memory + runtime allocations) are higher. A good thing, in the big picture, but requires some patience.\nThanks!"}], [{"role": "user", "text": "Hi,\nI\u2019ve been having a similar issue for a few days now. I\u2019ve been trying to upload a file but it keeps stopping and showing the red error message \u201cWarning: Internal Server Error (500)\u201d. I\u2019ve tried rerunning the upload and have made sure there are no connection issues but the upload is still failing.\nPlease advise, and thank you in advance.\nKimia"}, {"role": "system", "text": "Welcome @kimia.aghasoleimani !\nThanks for asking about this problem. It is a temporary server-side issue impacting jobs, not just Upload, and the message is not always reported within the failed dataset itself in the History view. But it is on the Job Details page under stdout or stderr reports (\u201ci\u201d icon).\nWhat to do: Try a rerun. For this and any other jobs that fail in a similar way.\nHow to know if this is your issue: Check the stderr report.\nFor errors like this one per dataset \u2013 reported in the stderr at the very end:\n\nOSError: [Errno 122] Disk quota exceeded\n\nLikely related: Uploads fail directly within that tool, before the data is loaded into the history.\nThat said, it seems you were not getting that far into the processing, and the Upload tool is reporting a problem instead.\n\nWarning: Internal Server Error (500)\n\nBoth types of errors are most likely related to the same root issue right now.\nIf there is an actual problem with the file itself (contents or transfer), that same error in the Upload tool can result.\nSo, double-check your local data first for any obvious problems (truncated file, compression problems). If Ok, then try a rerun letting Galaxy set the datatype (auto-detect, the default), and consider using FTP to load data if your connection is slower and/or the file is very large.\nDetails:\nThe errors are related to administrative tasks running in the background at UseGalaxy.org. Specifically, we have been migrating data to a larger storage system. The prior is full \u2013 resulting in odd transient errors like those above for a fraction of jobs. Rerun those tools.\nApologies for the confusion, and thanks for reporting the problem!"}], [{"role": "user", "text": "Hi,\nHas anything changed in uploading files via the browser? or is there a temporary outage?\nAfter dragging the files (small bam files from my local PC), there is a rolling circle but upload does not begin.\nThanks for your help"}, {"role": "system", "text": "There were some server issues around the time this question was asked. All should be resolved by now.\nHistorical and ongoing usegalaxy.* server status: https://status.galaxyproject.org/"}], [{"role": "user", "text": "Hi,\nI have been working on galaxy for quite some time. my account is deleted suddenly.\nI have all my work there. could you please help me restore my account.\nemail: (admin redacted for privacy)\nusername : supriyan\nThank you"}, {"role": "system", "text": "Hello @Supriyan\nCreating multiple accounts in order to circumvent our data and job quotas degrades the service for all other users. Excess accounts are administratively removed (purged) at this server to ensure fair access to the free public resource.\nIf the terms of service seem unclear, go back and review the account activation emails you received for each (the email with the link). You can also log out and look at the account registration form again, or review the terms under the Help menu. https://usegalaxy.org/static/terms.html\n\nregistation1658\u00d7930 170 KB\n\n\n\nCurrently, you still have at least one active account.\nIf you want to switch which account is active, and can agree to use the public resource fairly going forward, reply-all back to our existing email conversation (not publicly here) and include your answers for the four questions in this topic Accounts deleted from Galaxy Main to enforce posted Terms and Conditions - #18 by jennaj. At this point, that even being possible is extremely time sensitive but we can try. Please follow up by email."}], [{"role": "user", "text": "Hello,\nI tried to install some tools to my Galaxy instance from the \u201cadmin\u201d page, but found all of them stuck on \u201ccloning\u201d. And there is no error in the \u201cerror log\u201d page. Does anyone have a clue on it? I am using Galaxy 20.09 on GVL 5.0 beta 5.\nThank you!"}, {"role": "system", "text": "When installing tools on the GVL, you must use the following values under the advanced settings during tool installation. Otherwise, the installation will not work correctly as you\u2019ve seen.\n\nimage900\u00d7566 45.8 KB\n"}], [{"role": "user", "text": "Hello,\nmy username does not exist anymore. apparently.\nWhat to do?\nBest regards,\nAntonello"}, {"role": "system", "text": "Hi @Vitantonio_Pantaleo\nFor account help, please write in to the administrators of the Galaxy server where you were working from the email address used for account registration.\n\n\nIf you are working at UseGalaxy.org, the contact email is: galaxy-bugs@lists.galaxyproject.org\n\n\nIf working somewhere else, the contact information for most public Galaxy servers is usually on the homepage of the server or included in their directory listing here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub. The information is also (probably) in the original account activation email \u2013 for both public or private servers.\n\n\nFAQs training-material/faqs/galaxy/#account\n\n\nIf you cannot find the contact, please post back the URL of the server and we can try to help more."}], [{"role": "user", "text": "I got my samples sequenced from a commercial facility. They used the following adapter sequences for sequencing\n\n|Sample ID|Index i7|Index i5|\n|S0A|GTTCTTCTGT|CCAAGGTATT|\n|S0B|CCAAGGTATT|GTTCTTCTGT|\n|S0C|TGTGGAGAGG|AACGAACCGG|\n|S5A|TTCCTTGAGG|TGCTGTCTTA|\n|S5B|GTTAATTGCG|GAGGAGATAA|\n|S5C|GAGGAGATAA|GTTAATTGCG|\n|S15A|GTTCCTCATT|AATGTCGGTT|\n|S15B|AATGTCGGTT|GTTCCTCATT|\n|S15C|TTGACAGGTA|TGGTGGTTGT|\nI am confused with the nomenclature of adapters. I am trying to use the Cutadapt tool in galaxy, and I am not sure out of the given adapter sequences which are 3\u2019(End) adapters, 5\u2019(Front) adapters, or 5\u2019 or 3\u2019 (Anywhere) Adapters. I also tried using the trimmomatic tool but I am not sure if I should use the TruSeq2 (pair ended) or TruSeq3 (pair ended). Could anyone please guide me through this so that I can start my data analysis? Thank you."}, {"role": "system", "text": "Hi @abhilash719,\ni7 index adapter corresponds to the 3\u2019 adapter and i5 index adapter to 5\u2019 adapter.  Regarding the ILLUMINACLIP options,  it depends on the technology used; according to the Trimmomatic manual TrueSeq2 is used in samples generated by GAII systems, while TrueSeq3 should be used when the samples have been generated by HiSeq and MiSeq machines."}], [{"role": "user", "text": "Hello everyone, I already used LEfSe in 2018 and have now a new dataset to analyse. I noticed when I upload my .tsv file that it looks different when I view the data with the \u201cview data\u201d function (see pictures attached). In 2018 the table had a grey bar with numbers at the top and it looked like a proper table. Now my data looks like a text file. I was wondering if something is wrong with my file, although LEfSe doesn\u2019t complain. My further analysis says there are no significant discriminative features in my 133 samples. I am not sure if I can trust the results now. Any help and/or advice is appreciated! Lefse_input_20181666\u00d7959 267 KB Lefse_input_20201667\u00d7945 397 KB"}, {"role": "system", "text": "Hi @janinewaege\n\n\n\n janinewaege:\n\nI already used LEfSe in 2018 and have now a new dataset to analyse. I noticed when I upload my .tsv file that it looks different when I view the data with the \u201cview data\u201d function (see pictures attached). In 2018 the table had a grey bar with numbers at the top and it looked like a proper table. Now my data looks like a text file. I was wondering if something is wrong with my file, although LEfSe doesn\u2019t complain.\n\n\nThe first screenshot is what would be expected for a dataset with the datatype \u201ctabular\u201d or \u201ctsv\u201d assigned.\nThe second screenshot is what would be expected for a dataset with the datatype \u201ctxt\u201d assigned. It is the default view for any plain text file (\u201ctabular\u201d, \u201ctxt\u201d, plus others).\nA very large dataset of many datatypes will revert back to the view shown in the second screenshot (\u201clarge\u201d can differ by Galaxy server). That seems to be what is happening \u2013 the files are different sizes but both have the datatype \u201ctabular\u201d assigned? Meaning, dataset 47 has fewer lines than dataset 56 (?). That would explain the difference in the \u201cView data\u201d display. This really is \u201cdisplay\u201d only and doesn\u2019t change the actual contents of the file/dataset. If you look at the expanded dataset 56 \u201cpeek\u201d view\u2019 in the history panel, you can see that it was small enough to display as \u201ctabular with the column numbers\u201d since only the first 5 lines are used for that function (for practical/design reasons).\nNote: A compressed dataset (of various types) might display as plain text (second screenshot) as well. This is not your situation but seemed worth clarifying for you (other display use-cases) or for anyone else reading this Q&A \n\n\n\n janinewaege:\n\nMy further analysis says there are no significant discriminative features in my 133 samples. I am not sure if I can trust the results now. Any help and/or advice is appreciated!\n\n\nHow data displays differently (by datatype or size) is part of the larger application settings. That is unrelated to how tools function/execute or wouldn\u2019t lead to any downstream differences analysis results \u2013 the dataset\u2019s content/data itself isn\u2019t modified, and that is what tools use as inputs.\nYou are working at this public server, correct?  Galaxy\nThe server is set up a bit differently than other public servers and hosts custom versions of tools, in particular the Lefse tool suite. Their support/admin team does not follow this forum, but there are other ways to contact them about odd or non-reproducible results. I don\u2019t think that is what your issue is (unless those results are not reproducible, maybe run the analysis again and compare?). But if you have tool concerns or odd results with a rerun comparison, you could contact the Huttenhower support/admin team for more help, especially if you think there is a technical problem unrelated to the display differences. Please see this prior Q&A for how:\n\n  \n    \n    \n    Unable to choose a row to use as class on Hutlab for LDA analysis -- Likely an input format problem \n  \n  \n    Welcome @Rosekiki \nYour header line does not match those in the examples. Scroll down on the form to the help section for the details, including linked example inputs. \nThe https://huttenhower.sph.harvard.edu/galaxy/ public Galaxy server hosts customized versions of the LefSe tools. If you need more help than a tool form provides, contact the server support team directly. This particular server\u2019s support/admin team do not track this forum as far as I know. \nContact information for public Galaxy \u2026\n  \n\n\nThanks!"}], [{"role": "user", "text": "Hi, I wish to upload a file (112 Mb) to usegalaxy.eu but it failed. After I clicked the \u201cUpload Data\u201d button and uploaded the file, it reached 100% quickly. However, after hours, the file is still loading to be available in the history (job still running and appeared in orange color). May I have your help or advice on this issue? Thanks."}, {"role": "user", "text": "It\u2019s okay. The problem is solved"}, {"role": "system", "text": "Hello @Sylvia\nThe UseGalaxy.eu server was undergoing server changes over the last few days. From my experiences during that time frame, the service was either completely inaccessible (with a fall-over landing page stating the service is not available), or I could reach the server and some functions were Ok and others were slower or stalled (Upload \u2013 both by local file and by URL, loading up tool forms \u2013 but other functions could also present as odd) .\nUseGalaxy.eu now has a banner up on the server with an explanation.\n\n17.05.2022. Since this morning, we are experiencing issues on our main storage. The service is, currently, not reliable. We are working on a solution\n\nIf you notice odd behavior again sometime in the future:\n\nA server restart might cause odd behavior for a minute or two. Refreshing your browser window and starting the problematic action over again is usually enough.\nCheck the overall status for any of the UseGalaxy.* servers and related primary resources, at the publicly posted status page here: https://status.galaxyproject.org/\n\nAsk questions at this forum if both of those don\u2019t help, or apply, or if you need more details.\nWhile waiting for a reply, you can work at any of the other UseGalaxy.* servers. One account at each has benefits: different storage/computational server resources plus your account at each has distinct resources (data storage quota).\n\nNote: I\u2019m going to modify the topic title and put this as the solution to make it easier for other people to find your question and the advice in the reply, should they run into a problem at the EU server and are not sure what is going on.\nThanks for posting!"}], [{"role": "user", "text": "Hi everyone,\nMy issue is quite strange. I have in total 16 RNA seq data. After trimming, I move on to HISAT2 for alignment.\n6 out of 16 sample give me error while other turned green.\n\"An error occurred with this dataset:\nformat bam database CHM13_T2T_v2.0\"\n\" Error, fewer reads in file specified with -1 than in file specified with -2 terminate called after throwing an instance of \u2018int\u2019 (ERR): hisat2-align died with signal 6 (ABRT) (core dumped) gzip: stdout: Broken pipe [bam_sort_core] merging from 32 files\ni dont know why it should merging 32 file, I did these pair individullly.\nThank you so much"}, {"role": "system", "text": "Hi @ks7515\nthis is the key message: Error, fewer reads in file specified with -1 than in file specified with -2.\nPlease check the input files used for the failed jobs. Properly paired reads should have identical number of F and R reads present in the same order. Two common causes: F and R reads are from different samples, or reads were trimmed as single end data resulting in different number of F and R reads.\nThe \u2018merging\u2019 part of the message probably refers to processing temporary files. Many aligners produce small files during mapping, and these files were merged into a single alignment.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello!\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error \u201cFatal error: Exit code 127 ()\u201d comes up. Further, it says that the tool generated the following error: \u201cline 9: run_deepvariant: command not found\u201d.\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - \u201cGET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1\u201d 200\nI don\u2019t understand this error. Could someone please help me out?\nI am running the same job on Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\nThanks!"}, {"role": "system", "text": "Hello, I can\u2019t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a \u201cdocker tool dependency\u201d, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\nhttps://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Tool Dependencies and Containers\n\n  Galaxy is an open-source project. Everyone can contribute...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThe requirement can be seen here:\n  \n\n      github.com\n  \n\n  \n    galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\n\n\n      <macros>\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\n    <token name=\"@SUFFIX_VERSION@\">0</token>\n    <xml name=\"edam_ontology\">\n        <edam_topics>                                                                                  \n            <edam_topic>topic_0199</edam_topic>\n        </edam_topics>\n        <edam_operations>\n            <edam_operation>operation_3227</edam_operation>\n        </edam_operations>\n    </xml>\n    <xml name=\"requirements\">\n        <requirements>\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\n        </requirements>\n    </xml>\n    <xml name=\"citations\">\n        <citations>\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\n        </citations>\n\n\n\n\n  This file has been truncated. show original\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Dear Sir,\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\nIs it ok??? or did I out something wrong\nKindly help"}, {"role": "system", "text": "Hello @Sachin_Srivastava\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\n20 GB of fastq data \u2013 uncompressed \u2013 creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you\u2019ll need to do one or more of these:\n\nTry a rerun to eliminate cluster issues\nMore QA/QC on the input reads (always recommended)\nConsider downsampling the reads (tool: Seqtk)\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: https://launch.usegalaxy.org/catalog\n\n\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\nYou didn\u2019t state where you are working. But, if by chance at Galaxy Main https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\nThanks!"}], [{"role": "user", "text": "Hello everyone!\nI\u2019m new around here, learning how to use Galaxy. I\u2019m running the tutorial 16S Microbial Analysis with mothur (extended) and I keep getting an error when trying to \u201cgenerate count table\u201d using Count.seqs\nFatal error: Exit code 1 ()\nFatal error: Matched on [ERROR]\nCan anyone help me with that, please?\nHere\u2019s a link to my history:\nhttps://usegalaxy.org/u/isissilveira/h/tutorial\nThank you!"}, {"role": "system", "text": "Hi @Isis_Silveira,\ncould you try again? I tried to reproduce the error, but it finished correctly:\nhttps://usegalaxy.org/u/gallardoalba/h/imported-tutorial\nRegards"}], [{"role": "user", "text": "Hello there, I performed a BUSCO with the hope of documenting its results for a thesis. At the end of the analysis, The analysis generated only the BUSCO full table.\nHow do I get the \"summary file activated again? Or, what other alternatives can I use to make a summary table myself?\nThank you.\n\nimage994\u00d7274 9.16 KB\n"}, {"role": "system", "text": "Dear @ayolupine,\nUnder Advanced Options \u2192 Which outputs should be generated you can select a summary image about the characteristic values of BUSCO.\nIs that what you meant?\nHave a good weekend and good success with your thesis!\nFlorian"}], [{"role": "user", "text": "Dear all,\nI got a weird graphic, and the parameter settings are all default, as follows, what\u2019s the problem,?thank you\uff01\nGalaxy34-D)_Plot_Cladogram_on_data_324800\u00d73600 4 MB\n"}, {"role": "system", "text": "Hi @liuyushu\nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server.\nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers).\nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).\n\nContact: Huttenhower Lab\n\n\nHelp/tutorials: (also linked from the server\u2019s home page, with example data available):\n\nlefse \u2013 The Huttenhower Lab\nlefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHub\n\nPrior Q&A and other related resources including other versions of this tool and different tools that may be helpful when working with this type of data:\n\nLefse Cladogram prior Q&A at this forum: Search results for 'Cladogram' - Galaxy Community Help\n\nAll Galaxy resources: Searching the Galaxy\n\n\nI also added a tag for that public server to your post. Click on the tag to review all Q&A at this forum related to troubleshooting and help/content for that server."}], [{"role": "user", "text": "Hi! I\u2019m new to Galaxy and to bioinformatics too, and actually trying to install it on my CentOS 7 server. I ran the installation as suggested on the website, but unfortunately it seems to get stuck at some point (while trying to communicate with a server?)\nHere is a screenshot\u2026\n\nErrore Galaxy.png917\u00d7696 37.2 KB\n\nWhat could be the problem?\nThanks in advance for your time!"}, {"role": "system", "text": "There is no problem \nThis is Galaxy\u2019s way of telling you that it\u2019s ready and waiting for requests. Open your web browser on the same machine and point it to 127.0.0.1:8080. You should see Galaxy\u2019s web interface."}], [{"role": "user", "text": "Hi all,\nI am trying to use EdgeR for differential gene expression analysis. When I try to run EdgeR using htseq-count files for each group in the same factor as the input (two count files per group), I end up with the following error:\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: row.names<- \u2192 row.names<-.data.frame \u2192 .rowNamesDF<-\nWarning message:\nnon-unique value when setting \u2018row.names\u2019: \u2018X0\u2019\nI\u2019m unsure how to troubleshoot this and have searched through here and was unable to find answers.\nI\u2019m sorry if this has been asked and I accidentally overlooked it!\nThanks for any/ all help!\nDan"}, {"role": "system", "text": "Hi @Dan\nThis type of error message from any Bioconductor or R tool means that there is a labeling problem or design problem. The details usually have some clues. Those can be searched against this forum https://support.bioconductor.org/ to limit the results (since some parts can be produced by other tools based on R).\nThis message is stating that the tool ran into what it thinks was a \u201cduplicated\u201d label when assigning the row \u201cnames\u201d in an intermediate processing file. More likely, it is actually reporting that a label it was looking for couldn\u2019t be found (this is what the \u2018X0\u2019 is referencing).\n\n\n\n Dan:\n\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: row.names<- \u2192 row.names<-.data.frame \u2192 .rowNamesDF<-\nWarning message:\nnon-unique value when setting \u2018row.names\u2019: \u2018X0\u2019\n\n\nClick into the \u201ci\u201d icon for a dataset to see what was used to produce it. This is an easier view to review tool inputs and parameters for potential problems.\nNext, expand the input datasets in that view.\nGroup (sample) names must be unique between all included in the same analysis job, and match the headers of the count data inputs. What to fix depends a bit on how you are suppling the design matrix and count data.\n\n\nIf using Single Count Matrix mode: Do the count files supplied have headers containing sample names that exactly match the values supplied in each matrix? If not, the tool cannot match the data up, and you\u2019ll need to add those in. Your exact error resulted in prior failures for this same situation and is my best guess so far.\n\n\nIf using Separate Count Files mode: Similar checks would apply but those sample labels are supplied on the tool form \u2192 Factor and Group and (optionally) Contrast\n\n\nGive that a review, make adjustments, and see if you can resolve the missing or conflicting labels.\nIf you get stuck, please post back a #sharing-your-history link publicly as a reply, or ask for a moderator to start up a direct message chat to share it in. Please leave all inputs and outputs undeleted, and note the error dataset number with the problem.\n\n\nQuote from the tool form help about how to format these data:\nLabels (all sections)\n\nNOTE: Please only use letters, numbers or underscores (case sensitive), and the first character must be a letter\n\nThere is a lot of Q&A at this forum about \u201cformat\u201d issues used for labels \u2013 I added a few tags to your post that will find most of those. Or, better, just check yours and simplify or add in as needed. No extra spaces and using only the characters the tool knows how to read. Most tools are quite literal when interpreting keys/values.\nInputs\n\nCounts Data:\nThe counts data can either be input as separate counts files (one sample per file) or a single count matrix (one sample per column). The rows correspond to genes, and columns correspond to the counts for the samples. Values must be tab separated, with the first row containing the sample/column labels and the first column containing the row/gene labels. The sample labels must start with a letter. Gene identifiers can be of any type but must be unique and not repeated within a counts file.\n\nExample - Separate Count Files:\n\n\n\n\n\nGeneID\nWT1\n\n\n\n\n11287\n1699\n\n\n11298\n1905\n\n\n11302\n6\n\n\n11303\n2099\n\n\n11304\n356\n\n\n11305\n2528\n\n\n\n\n\nExample - Single Count Matrix:\n\n\n\n\n\nGeneID\nWT1\nWT2\nWT3\nMut1\nMut2\nMut3\n\n\n\n\n11287\n1699\n1528\n1601\n1463\n1441\n1495\n\n\n11298\n1905\n1744\n1834\n1345\n1291\n1346\n\n\n11302\n6\n8\n7\n5\n6\n5\n\n\n11303\n2099\n1974\n2100\n1574\n1519\n1654\n\n\n11304\n356\n312\n337\n361\n397\n346\n\n\n11305\n2528\n2438\n2493\n1762\n1942\n2027\n\n\n\n\n\nAnd, tutorials are available here with example data + methods + workflows.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "user", "text": "Hi jennaj,\nThanks for your advice! I was using individual HTseq-count files produced by the HTseq tool without modifying them in any way, so I\u2019m unsure why it was giving me trouble. I ended up combining the files into a single count matrix and that fixed the problem.\nThanks again!\nDan"}], [{"role": "user", "text": "I was notice that collection files that were recognized by FastQC (for example) are not recognized by Unicycler.\nThanks!\nIsrael"}, {"role": "system", "text": "You\u2019ll need to set the datatype to fastqsanger. This is usually auto-detected but fails for nanopore files which have unusual quality scores. You can set the datatype at upload or change it for existing datasets by clicking the pen icon on the dataset\n\nthen click on the datatype tab and select fastqsanger or fastqsanger.gz depending on whether your data is compressed or not:\n\nScreenshot 2019-12-19 at 18.35.53740\u00d7434 26.5 KB\n"}], [{"role": "user", "text": "Hello,\nI trying to do a QC for my files. And I keep encountering this error. This data set had previously pass the FastQC without any error before. However, when re-upload the file to do some re-analysis, now it keeps failing to pass QC. Anyone encounters the same problem and know how to solve it?\n\u201cPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/717/40717303/_job_tmp -Xmx7g -Xms256m\nFailed to process file AG10_Phi_DeltaLL_S8_L001_R1_001_fastq_gz.gz\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry.  Your file is probably truncated\nat uk.ac.babraham.FastQC.Sequence.FastQFile.readNext(FastQFile.java:179)\nat uk.ac.babraham.FastQC.Sequence.FastQFile.next(FastQFile.java:125)\nat uk.ac.babraham.FastQC.Analysis.AnalysisRunner.run(AnalysisRunner.java:76)\nat java.base/java.lang.Thread.run(Thread.java:834)\ncp: can\u2019t stat \u2018/corral4/main/jobs/040/717/40717303/working/dataset_53bb4f0c-c6c8-45b3-a706-13605487098b_files/*/fastqc_data.txt\u2019: No such file or directory\u201d"}, {"role": "system", "text": "\n\n\n Vu_Ngo:\n\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry. Your file is probably truncated\n\n\nHi @Vu_Ngo \u2013 Check the size of your dataset as the error message indicates the data is truncated. That could have occurred during the re-upload, or maybe it wasn\u2019t fully downloaded originally.\nIf the data is intact wherever you downloaded it, you probably just need to load it again. And if it is large or you are working on a slower connection, try using FTP upload as that tool will report a partial upload (plus you can resume the transfer).\nI added some tags to your post that point to prior Q&A about how to do this."}], [{"role": "user", "text": "Hi,\nPlease can I just ask how to filter SnpEff output for exonic variants only with SnpSift?\nI have tried by calling the initial SnpEff job with sequence ontology annotations.\nWhen I then try to filter as per the sequence ontology annotations it fails. When I\u2019ve viewed the SnpEff VCF output, the sequence ontology annotations don\u2019t seem to be present despite selecting this option when running the initial annotation.\nApologies if this is a basic q, very new to bioinformatics\nThank you for your help"}, {"role": "system", "text": "Hi @abbi\ndo you  see SnpEff annotation in Info column of vcf file? It starts with ANN= or EFF=.\nYou can select exonic variants by intersection of VCF file with BED file containing location of exons or by selection of appropriate annotations using SnpSift Filter. Check SnpEff Eff HTML stats for valid values, such as missense_variant etc. Decide if you want variants in UTRs and non-coding transcripts. The syntax is described in SnpSift Filter. It might look something like: ANN[].EFFECT = \u2018missense_variant\u2019) | (ANN[].EFFECT = \u2018synonymous_variant\u2019) etc. Just in case double check filtering with \u2018has\u2019 instead of \u2018=\u2019. There might be other options.\nHope that helps.\nKind regards,\nIgor"}, {"role": "system", "text": "Hi @abbi\nThe brackets have wildcard characters, but these were interpreted as format symbols. If you decide to go with SnpSift Filter, just follow example at the bottom of job setup page.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am trying to run through https://mcmicro.org/instructions/galaxy/galaxy-running.html using the TMA workflow, which is asking for a cell type map, but that\u2019s not included in the zip file for the exemplar data. Is this something that anyone has a solution for?"}, {"role": "system", "text": "Hi @christopherjin,\nplease, report the problem in Topics tagged mcmicro under the mcmicro tag.\nRegards"}], [{"role": "user", "text": "Hello Galaxy community,\nthank you for the great work. Is it possible to add the --cut_right option to the GUI for fastp?\nIt belongs in the \u201cPer read cutting by quality\u201d options. According to GitHub - OpenGene/fastp: An ultra-fast all-in-one FASTQ preprocessor (QC/adapters/trimming/filtering/splitting/merging...) the version in Galaxy supports this. It would be great to have this!\nThank you very much,\nMichel"}, {"role": "system", "text": "Welcome, @ChoudalakisM\nI added your request to a new enhancement issue for the tool developers to consider. Please follow the ticket for updates, and feel free to join the comments/discussion: fastp: add options for \"-r, --cut_right\" to the tool form \u00b7 Issue #5183 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub\nShould you be interested in helping to make the changes, please see the contributing guidelines for how to get started  tools-iuc/CONTRIBUTING.md at main \u00b7 galaxyproject/tools-iuc \u00b7 GitHub"}], [{"role": "user", "text": "Input is a NCBI Fasta file. Executed  successfully about a week ago on same input but getting this error yesterday and today. Any suggestions.\nFATAL:   could not open image /cvmfs/singularity.galaxyproject.org/all/prokka:1.14.6\u2013pl5262hdfd78af_1: failed to retrieve path for /cvmfs/singularity.galaxyproject.org/all/prokka:1.14.6\u2013pl5262hdfd78af_1: lstat /cvmfs/singularity.galaxyproject.org: no s"}, {"role": "system", "text": "Hi @TechBull2020\nFirst, try a rerun to eliminate transient cluster errors.\nIf that fails again, would you please send in a bug report? This could be a technical problem. Please include a link to this post in the comments so we can link the two. Thanks!\nHow to: Troubleshooting errors"}, {"role": "user", "text": "Thanks. Rerunning today work. Guest it was just a transient error."}], [{"role": "user", "text": "Hi,\nI was trying de novo assembly for a fish species, but I keep getting the same error message no matter what programs I used, ABySS, SPAdes, VelvetOptimiser. Here is what the error message looks like:\n2020/11/03 15:03:16.35 parrot_run[70105] child:70213 notice: warning: system call 237 (mbind) not supported for program /cvmfs/main.galaxyproject.org/deps/_conda/envs/galaxy/bin/python\nThe input data set should be fine since I can do trimming, mapping stuff. So I\u2019m completely confused. What should I do?\nThanks,\nYunyang Wang"}, {"role": "system", "text": "GTN Tutorials for assembly (including Unicycler):\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Assembly\n\n  DNA sequence data has become an indispensable tool for Mo...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n    \n    \n    ABySS fungal de novo genome assembly: Update, ABySS removed from usegalaxy.org, alternative usegalaxy.eu usegalaxy.org support\n  \n  \n    Update \nThe assembler ABySS has been technically problematic at UseGalaxy.org for some time now. \nThe tool has been removed from UseGalaxy.org. \nAn alternative usegalaxy.* server for this tool is UseGalaxy.eu\n  \n\n"}], [{"role": "user", "text": "Greetings,\nI am trying to combine paired-end data into a collection for use in the tool: Stacks: de novo map  the Stacks pipeline without a reference genome (denovo_map.pl) (Galaxy Version 1.46.0). I uploaded the files individually and tried creating a collection a few ways.\nFirst, I selected all files (24) and created a \u201clist of pairs.\u201d This seemed to work well (it created a list of 12 pairs), but I cannot get the Stacks: de novo map tool to recognize this list as a valid type of input.\nNext, I tried to build a collection using rules, but could not figure out how to pair the files, so it ended up creating a list of 24 files. The tool recognized this collection, but I didn\u2019t want to proceed without pairing the data.\nFinally, I tried selecting each pair of files, building a dataset pair for each, and then combining these into a collection\u2026but apparently you cannot build a collection from lists of data?\nSo, my questions are:\n\nIs there a Collection operation I need to use in order to proceed using the \u201clist of pairs?\u201d One that will make the tool recognize the list as a valid collection to input?\n\nOR\n\nIs there a way to pair files when building a collection from rules?\n\nMany thanks!"}, {"role": "system", "text": "Hello,\nThe currently wrapped version of Stacks (version implied is 1.0) does not accept pair-end reads, or rather, not directly as a paired dataset (individual dataset or collection datasets). Inputs are expected to represent R1 reads, either as individual datasets or in a list collection. This means that a paired-end collection won\u2019t be recognized by the tool as a proper input. Please see the manual link below for details \u2013 including how to perform the analysis on certain types of paired-end data.\nThere is an updated version of the underlying tool: Stacks 2.0. It does accept pair-end data directly. However, Stacks 2.0 has not been wrapped for Galaxy (yet) but there is development in progress for this update. I would expect that pair-end collections would be accepted as an input in this version once completed. Please see this IUC issue ticket and related linked discussions/code changes (PRs) for status: https://github.com/galaxyproject/tools-iuc/issues/2073\nThe accepted inputs for the two two tool versions are explained in the manuals here under the section \u201cWhat types of data does Stacks (or Stacks 2.0) support?\u201d:\n\nStacks http://catchenlab.life.illinois.edu/stacks/manual-v1/#data\n\nStacks 2.0 http://catchenlab.life.illinois.edu/stacks/manual/#data\n\n\nHope that helps!"}], [{"role": "user", "text": "Hi!\nI have trouble using the GATK DepthOfCoverage tool.\nThe following error appears:\nERROR ------------------------------------------------------------------------------------------\nERROR stack trace\njava.lang.ExceptionInInitializerError\nat org.broadinstitute.sting.gatk.GenomeAnalysisEngine.(GenomeAnalysisEngine.java:\nAccording to a Google search that could be because of a too old version of java or of GATK.\nIs it possible that java or GATK are not up to date? Or is it because I\u2019m doing something wrong? Does GATK works for someone?\nThank you!\nMathias"}, {"role": "system", "text": "Hello,\nMany GATK tools currently wrapped for Galaxy are several releases outdated and are considered deprecated. They should not be used.\nThere is a newer version: https://toolshed.g2.bx.psu.edu/view/lz_hust/gatktools/01ff8dd37d4d\nIf that new version (released in June 2019) is presenting with problems, please create an issue at the development repository: https://github.com/huster16/gatk-tools\nThanks!"}], [{"role": "user", "text": "Hello everyone!\nI am working on developing some Galaxy tools.\nA problem I have come across a couple of times is programs requiring input files with specific file extensions e.g. .JPG.\nThis is challenging for me as it is my understanding that the Galaxy representation of uploaded files is arbitrarily named with the .dat extension.\nIs there a way I can control the extension of uploaded files? Can I rewrite my tool XML to work around this?\nI would really appreciate any thoughts or suggestions \nAll the best and many thanks,\nOliver"}, {"role": "system", "text": "I think you can do something with a symlink. See here for an example: Wrapping a script which implicitly expects a local file - #6 by astrov\nIf it is a python script you could do something like (no working code just an idea):\n    if os.path.splitext(galaxyinputfile)[1] == \".dat\":\n        cwd = os.getcwd()\n        jpginputFile=cwd+os.path.splitext(os.path.basename(galaxyinputfile))[0]+\".jpg\"\n        shutil.copy(galaxyinputfile, jpginputFile)\n\nThere may be better ways but this could give some inspiration already"}], [{"role": "user", "text": "Dear everyone,\nI tried to analyze RNA-seq data successfully. Unfortunately, after running the DeSEQ2 tool I get the following error:\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nestimating size factors\nestimating dispersions\ngene-wise dispersion estimates: 6 workers\nmean-dispersion relationship\nfinal dispersion estimates, MLE betas: 6 workers\nError in (function (cond)  :\nerror in evaluating the argument \u2018args\u2019 in selecting a method for function \u2018do.call\u2019: BiocParallel errors\n3 remote errors, element index: 1, 2, 3\n0 unevaluated and other errors\nfirst remote error: error in evaluating the argument \u2018x\u2019 in selecting a method for function \u2018t\u2019: no right-hand side in \u2018b\u2019\nCalls: DESeq \u2192 DESeqParallel \u2192 do.call \u2192 bplapply \u2192 bplapply\nAt the same I can use edgeR and Lima without any problem.\nI just wanted to compare the three different tool\u2019s results!!\nDoes anybody know about it?\nIni"}, {"role": "system", "text": "Hello I have the same message could you solve it?"}, {"role": "system", "text": "Hi @inivasDL , hi @DCEM_NCU\nI realized it is way too late, but came across a similar error recently. In that case the issue was caused by genes with zero counts in every sample. Filtering out zero count genes helps. Alternatively, try other tools, such as edgeR.\nKind regards,\nIgor"}], [{"role": "user", "text": "Exception in thread \u201cmain\u201d htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nat htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:245)\nat htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:165)\nat htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:182)\nat pt2.se2pe.Pop2FileWriter.write(Pop2FileWriter.java:74)\nat pt2.se2pe.Se2PeFramework.run(Se2PeFramework.java:95)\nat pt2.se2pe.Se2PeParser.parseCommandLine(Se2PeParser.java:76)\nat pt2.Main.main(Main.java:62)"}, {"role": "user", "text": "when i used popoolationTE2 to merge bam file, i get this error.But i do not know the reason"}, {"role": "system", "text": "Hi @jiahui_xie\nThis happened at the public Galaxy server UseGalaxy.org?\nIf so, this was probably a transient cluster problem (the disc was full when this job ran). Please try a rerun.\nShould you get that error again, please send in a bug report from the red error datasets (all). It would be helpful to include the URL of this Q&A topic in the comments."}], [{"role": "user", "text": "How to use featureCounts as gene_name model"}, {"role": "system", "text": "Hi @Yuliang_Wang\nyou can change gene identifier value in Advanced options during featureCounts job setup.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi, I\u2019m analysing the small RNA-seq data. I\u2019m trying to split my collection into two, corresponding to wild type and mutant. I used to use \u2018Filter list from contents of a file\u2019 option but it is missing now. What option I can use instead?"}, {"role": "system", "text": "Hi @Asyria\nmaybe try Filter collection.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello! I\u2019m trying to assemble a genome with Spades. As output files, I need to get only contigs, which I indicated when I uploaded read 1 and read 2. Spades took already more than 24 hours to load. Can you please tell me how long it can take to load Spades? Does it make sense to wait or is it better to try to reset everything and load it again?"}, {"role": "system", "text": "Spades can take a lot of time and simply deleting the job and starting it again won\u2019t do you any good.\nWhat can help though is if you first downsample your input reads so spades simply has less data to process."}], [{"role": "user", "text": "Hello, there all\nWhile I am new to galaxy, I have a basic query\nAfter uploading my datasets, I joined the paired end data by using fastq-join, with each read about 4.8 gb in size\nIt produced three different files as\n1 fastq-join on data 27( unmatched 2) ( size 4.6 gb)\n2. Fastq-join on data 27 ( unmatched 1 ) size 4.6gb\n3. Fastq-join on data 27 ( joined) size 210.4 mb\nNow my question is which file I have to choose for further downstream analysis.\nThanks in advances\nAny possible help will.be highly appriciated"}, {"role": "system", "text": "Why in particular are you using fastq-join? This joins reads only when they are overlapping, and most fastq reads will not overlap. From the size of your outputs it certainly looks like you have non-overlapping reads. Depending on what you are trying to achieve, you will either use the two read datasets (which here are largely preserved in the \u201cunmatched 1\u201d and \u201cunmatched 2\u201d datasets or you will use the joined one (but this seems very small in your example).\nIf you explain a bit more about the analysis you are trying to do it will help people advise better."}], [{"role": "user", "text": "I am new to galaxy and have little knowledge on coding. I was running differentially expressed genes using three replicates of RNAseq data from controls and samples. My workflow was from FASTQC >> Trimmomatic >> HISAT2 >> Stringtie >> Stringtie merge >> Stringtie (using Stringtie merge as a reference genome) >> DESeq2. However, the results from Stringtie giving out gene count file which had no information in it. The assembled transcript file showed mapped transcripts. So when I performed DESeq2, the error showed up. Can anyone help me out with this issue? Thanks in advance!"}, {"role": "system", "text": "Hi @PK224,\ncould you share your Galaxy history with me? I\u2019ll have a look at the results. My email is\n.\nRegards"}], [{"role": "user", "text": "Hello everyone, I\u2019m a new member  I\u2019m wrinting you because I have some issue in Galaxy when I use the tool \u201cUnique.seqs\u201d. The following error message appairs : \"Fatal error 1 () with the message \u201cTERM environment variable not set\u201d. When I look to the previous step (Screen.seqs), the contigs.report and the align.report contain 0 line. Does someone already had this issue and know how to fix it ? Thank you very much "}, {"role": "system", "text": "\n\n\n mregnier:\n\n\u201cUnique.seqs\u201d. The following error message appairs : \"Fatal error 1 () with the message \u201cTERM environment variable not set\u201d.\n\n\nThis is a technical artifact and can be ignored as a non-specific warning.\n\n\n\n mregnier:\n\nWhen I look to the previous step (Screen.seqs), the contigs.report and the align.report contain 0 line.\n\n\nTo clarify: the input to Unique.seqs does not contain any data? This means that some upstream step was problematic (usage), or none of your data actually passed the filtering criteria applied by Screen.seqs (or possibly another upstream step/tool). Not all tools will \u201cfail\u201d (turn red) when given empty inputs \u2013 some instead just produce more empty inputs.\nTraceback through your analysis to find out where the problem was introduced \u2013 and it wasn\u2019t necessarily the most recent upstream steps. The metagenomics GTN tutorials that include Mothur tools can help with usage and example data/workflows that are known to work.\n\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: Metagenomics\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nMore help here, but to be clear, the issue you explain is a tool usage or data content issue, not a tool/server issue (if I am understanding correctly!). Why there are empty results from the upstream tool should be addressed first.\n\n  \n    \n    \n    Troubleshooting resources for errors or unexpected results usegalaxy.org support\n  \n  \n    Check to see if there is a known usegalaxy.* server issue. \n\nServer Status: https://status.galaxyproject.org/\n\n\nIf the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\n  \n\n\nThanks!"}], [{"role": "user", "text": "Hi, I got the DEG table with the limma package in R, I searched for the SNAI1 gene in the DEG table and now I want to compare my gene expression in the samples. I want to know in which samples SNAI1 is up-regulated and in which samples it is reduced. How can I do that? With what code? Does anyone know about this ? Thanks in advance\n   library(limma)\n    library(edgeR) \n    dge <- DGEList(rawdata)\n    keep <- filterByExpr(dge, design = design)\n    keep2 <- data.frame(keep)\n    filt <- dge[keep, ,keep.lib.size = F]\n    norm <- calcNormFactors(filt, method = \"TMM\")\n    v <- voom(norm, design = design, plot = T)\n    E <- v$E\n    ex <- 2^E\n    Ex <- log2(ex + 1) \n\n    fit <- lmFit(Ex, design = design)\n    contrast <- makeContrasts(cancer-normal, levels = design)\n    fit2 <- contrasts.fit(fit = fit,contrasts = contrast)\n    fit2 <- eBayes(fit = fit2)\n    diff <- topTable(fit = fit2, number = Inf, sort.by = \"P\" )"}, {"role": "system", "text": "Hi @maryam.klntri.2695\nThis forum is for help using the UseGalaxy.* resources: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\nFor help with Bioconductor packages command-line, see the package vignettes here http://bioconductor.org/ and their support forum here Bioconductor - Help.\nIf you want to try in Galaxy instead, or want to review those resources for general analysis guidelines/examples, see: Galaxy Training!\nThanks!"}], [{"role": "user", "text": "I have a hairpin dataset and i cant convert it with RNA to DNA convertor because this dataset has some things like Y or R in their sequence, So i cant remove them one by one, is there any tool which can do it all together.\nFor example clear all sequence except the ACTGactgNn ?"}, {"role": "system", "text": "You could use the \u2018Text transformation with sed\u2019 tool with this SED Program\n/^[^>]/s/[^ACTGNactgn]+//g\nThis will remove all non-ACTGnactgn characters from all non-header lines in FASTA."}], [{"role": "user", "text": "I keep getting this error for my fastq files when uploading to GalaxyTrakr. I\u2019ve kept trying for the past 2 weeks with no resolution. Any help would be appreciated!"}, {"role": "system", "text": "Hi \u2013 this was most likely related to the other Q&A here Uploads require you to log in -- Status: Resolved bug\nMost public Galaxy servers should have updated by now to capture the fix. If not, please open a new topic, and include the server URL along with the other context question in the linked post."}], [{"role": "user", "text": "sh run.sh\n\u2026/\u2026\nyarn install v1.15.2\n[1/4] Resolving packages\u2026\n[2/4] Fetching packages\u2026\nerror Couldn\u2019t find match for \u201c6a1028fb562f9aa68c451f0901f8cfeb43cad140\u201d in \u201crefs/heads/develop,refs/heads/gh-pages,refs/heads/master,refs/heads/namespaced-events,refs/heads/no-deps,refs/heads/refactor,refs/heads/separate-orphan-step-template,refs/tags/0.10.3,refs/tags/0.2.0,refs/tags/v0.10.0,refs/tags/v0.10.1,refs/tags/v0.10.2,refs/tags/v0.10.3,refs/tags/v0.11.0,refs/tags/v0.12.0,refs/tags/v0.2.0,refs/tags/v0.3.0,refs/tags/v0.4.0,refs/tags/v0.5.0,refs/tags/v0.5.1,refs/tags/v0.6.0,refs/tags/v0.6.1,refs/tags/v0.6.2,refs/tags/v0.7.0,refs/tags/v0.7.1,refs/tags/v0.7.2,refs/tags/v0.7.3,refs/tags/v0.8.0,refs/tags/v0.8.1,refs/tags/v0.9.0,refs/tags/v0.9.1,refs/tags/v0.9.2,refs/tags/v0.9.3\u201d for \u201chttps://github.com/sorich87/bootstrap-tour.git\u201d.\ninfo Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.\nERROR: Galaxy client dependency installation failed. See ./client/README.md for more information, including how to get help."}, {"role": "system", "text": "When I\u2019ve seen something similar before it was because git was unavailable, I believe.  Can you tell me more about the platform you\u2019re running Galaxy on?  And, does that error still happen when you attempt to re-run Galaxy?"}], [{"role": "user", "text": "We recently upgraded our HPC environment and can\u2019t seem to get our Galaxy server to properly submit jobs. The issue seems to be with slurm-drmaa not working properly with Slurm 23.02:\n[harvey@bisonnet-hpc ~]$ /software/apps/slurm-drmaa/current/bin/drmaa-run testscript.sh\nSegmentation fault (core dumped)\nHas anyone gotten slurm-drmaa to work with Slurm 23.02?"}, {"role": "system", "text": "Hi @mwharvey521\nLet\u2019s ask the admins working group for feedback at their chat. They may reply here or there, and feel free to join the chat.  You're invited to talk on Matrix\nRef:\n\nTutorials https://training.galaxyproject.org/training-material/topics/admin/\n\nDocs https://docs.galaxyproject.org/en/master/index.html\n\n"}, {"role": "user", "text": "Thankfully, the slurm-drmaa devs patched the code and the changes should be merged into the main branch soon."}], [{"role": "user", "text": "I want to use Kallisto for pseudo alignment, I need a reference transcriptome (homosapien), there is no option available in the tool.\nwhere can I find it?\n\n1753\u00d7489 16 KB\n"}, {"role": "system", "text": "Dear @azzahrae,\nIn which galaxy instance are you working? You can finde reference transcriptomes on usegalaxy.eu. It seems they are not supported on usegalaxy.org. So if you are working on .org, then you can download your reference transcriptome, e.g., from UCSC, GENCODE, or Ensembl.\nHave a good day and best wishes,\nFlorian"}], [{"role": "user", "text": "I want to use DESeq2 to compare between samples before treatment and after treatment  individually as paired samples.\nFor example\nPt.1 before vs Pt.1 after\nPt.2 before vs Pt.2 after\nPt.3 before vs Pt.3 after\n\u2026\nI followed this tutorial:\n  \n      \n\n      galaxyproject.github.io\n  \n\n  \n    \n\nGalaxy Training: Reference-based RNA-Seq data analysis\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBut I\u2019m not sure if the DESeq2 in the above tutorial it compares patients individually or as a group, and How to do DESeq2 for paired samples for each patients individually in usegalaxy?"}, {"role": "system", "text": "Dear @wmsalsah,\nDESeq2 would group them.\nYou would need to apply DeSeq2 to each pair individually. I am not sure, if there is a more handy approach to this.\nKind regards,\nFlorian"}], [{"role": "user", "text": "Hello @jennaj,\nI am having the same problem. I am the admin for the Galaxy instance that we use at our institution. I imported the RNA-seq deg-analysis workflow shared by you earlier in an earlier post from Galaxy workflows. It ran without any errors and generated outputs for the example dataset. However, I had someone double-check the workflow, with data from NCBI. I have another workflow designed to get counts and it ran on this data without any errors. However, when they ran these counts on the above workflow, it gave an error at the goseq stage.\nUsing manually entered categories.\nError in `[.default`(summary(map), , 1) : incorrect number of dimensions\nCalls: run_goseq ... goseq -> reversemapping -> [ -> [.table -> NextMethod\n\nIn the above answer, you indicated double-checking my inputs. I have downloaded and looked at it, but is there another way to double-check in Galaxy? The tag \u201cinput\u201d didn\u2019t give me many posts.\nI compared the input files that are given to goseq with the test dataset and the input files in the case of the example dataset that worked earlier. It looks very similar. Is there anything else I could do? Is there a way I can share the history and objects within so someone can have a look?\nThank you so much!\nPriyanka"}, {"role": "system", "text": "\n\n\ngoseQ Erorr based on Reference based rna seq tutorial\n\nI have downloaded and looked at it, but is there another way to double-check in Galaxy?\n\n\nHi @Priyanka_Bhandary\nFor this kind of data \u2013 I usually look at the common keys between the inputs first. Computers are literal: Chr1, chr1, and 1 all mean something different.\nAlso: extra whitespace, empty values, empty blank trailing list, values with .N where N is a version being compared to values without the version, values that contain whitespace (check your fasta > lines) \u2026 and more.\nIf the basic formats are intact, more check usually involve:\n\nGenome assembly/build \u2013 use the exact same source for all inputs or expect problems. Or, you can get fancy and convert between builds \u2013 and that would be extra, upstream data prep steps (common manipulations, but not really \u201csimple\u201d).\nChromosome names\nGene names\nTranscript names\nComputed values \u2013 scientific notation versus not, and consistently used\n\nThe error you got is probably about missing/empty values, or values that are not matching up (not unique, or become not unique during processing/data reduction).\nColumns of 0 values without a header describing the sample in a singleUniqueWord would be one example I\u2019ve seen. Maybe look at the data immediately upstream from GOSEQ first? Can you run the tool directly and it works? Or also fails, the same as the workflow did?\nOr, try what I usually do, instead check that the inputs make sense first, then review the workflow steps in order to make sure some stray setting isn\u2019t causing the problem. Why do I back all the way up? Trying to diagnose technical problems based on scientific results is so much harder, and might be missed right up until the final data reduction. Or, might be missed entirely! Not all problems will fail a tool and instead just produce weird results.\nMost of the Q&A at this forum involves some variation of the problems above. The solutions vary but are still similar, and the tools in this tutorial (more of a guide actually) can help to find content problems. Data Manipulation Olympics\n\n\n\ngoseQ Erorr based on Reference based rna seq tutorial\n\nIs there anything else I could do? Is there a way I can share the history and objects within so someone can have a look?\n\n\nAfter checking the above, and you are still stuck, try to reproduce the error with the smallest data possible at a public server. Then share the history back and we\u2019ll take a look \n \u2192 Troubleshooting errors"}, {"role": "user", "text": "Thank you so much for the detailed answer. I figured out the problem! The issue was with selecting the gene names in the drop-down menu in the goseq tool. The gene annotation file was using gene symbols and the default for goseq in the workflow was the Ensembl gene IDs. This inconsistency caused the problem. It was really hard to pin the error down though because the R error message wasn\u2019t helpful. Thank you so much @jennaj ! Your reply helped me think through what could be going wrong!\nThank you,\nPriyanka Bhandary"}], [{"role": "user", "text": "Hello,\nI used Salmon for quatification and now I\u2019m trying to run DESeq2. I\u2019m using a mapping file from Transcript ID to GeneID but I get the following error:\nreading in files with read.delim (install \u2018readr\u2019 package for speed up)\n1 2 3 4 5 6\nremoving duplicated transcript rows from tx2gene\nError in $<-.data.frame(*tmp*, \u201cTXNAME\u201d, value = character(0)) :\nreplacement has 0 rows, data has 997581\nCalls: get_deseq_dataset \u2192 $<- \u2192 $<-.data.frame\nI think my mapping file does not have header bur I do not know how to set header=false and I do not know if that is the real problem.\nWould anyone help me with this issue?"}, {"role": "system", "text": "Hi @caroeweldt\n\n\n\n caroeweldt:\n\n1 2 3 4 5 6\n\n\nThis implies that your mapping file has more than two columns if I am understanding the log.\nThe mapping file should contain:\n\nNo header line\nTwo columns separated by a single tab\nTranscriptIDs in the first column\nGeneIDs in the second column\nNo extra spaces, tabs, or empty lines\nThe datatype should be assigned as \u201ctabular\u201d. If Galaxy doesn\u2019t guess this datatype with the detecting-the-datatype-file-format function, then there is a format problem.\n\nGeneral help\n\nThis same mapping file should have been used when running Salmon (parameter = \u201cFile containing a mapping of transcripts to genes\u201d)\nIf you used a GTF file instead with Salmon, you should also use that with DESeq2.\n\nGFF3 annotation won\u2019t work with these tools \u2013 you need either GTF or a tabular mapping. working-with-gff-gft-gtf2-gff3-reference-annotation\n\nMake sure that you are inputting the correct output from Salmon == \u201cquant.genes.sf\u201d\n\nI added more tags to your post that link to prior Q&A that helped others, and tutorials are here:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you need more help \u2013 you can either create a share link to your history and post that back, or it might be enough to post back just the first few lines from each of your inputs.\nThanks!"}], [{"role": "user", "text": "I shared a history with a colleague and then purged it from my account since I had no space. Colleague gets error \u201cUser doesn\u2019t have permission to use dataset\u201d. I understand I should have given all permissions when sharing but is there anything that can be done now to allow use of the datasets?"}, {"role": "system", "text": ""}, {"role": "system", "text": "For anyone reading this later on \u2013 once a dataset is purged from a server it cannot be recovered.\nConsider backing up important work: download locally, or to a server or cloud storage you control"}], [{"role": "user", "text": "Hello,\nI am trying to generate MultiQC report from multiple QualiMap RNA-Seq QC files. I am using \u2018usegalaxy.org\u2019.\nThe MultiQC runs but does not give any output (0 bytes). I think the problem is because MultiQC is expecting genome_coverage to be the first file in the QualiMap RNA-Seq QC output. However, I don\u2019t see any such report. genome_coverage is present in the QualiMap report, but not in the raw data folder. I\u2019m not sure why this is happening.\nVersions:\nMultiQC v1.8\nQualiMap RNA-Seq QC v2.2.2d\nP.S. MultiQC works fine with STAR .log output (v2.7.1a) and FastQC (v0.72) raw data."}, {"role": "system", "text": "Hi,\nTry using the tool QualiMap Counts QC instead to generate a summary. That means you\u2019ll want to output the optional \u201ccounts\u201d output, not just \u201cstatistics\u201d, when running QualiMap RNA-Seq QC.\nThe version of MultiQC wrapped in Galaxy doesn\u2019t interpret QualiMap RNA-Seq QC reports (raw data). It expects QualiMap BamQC (raw data) input, only, despite the top menu label stating that the input could be from either tool. The lower menu does specify BamQC but we can see how that can be confusing. The tool wrapper could probably be updated to be clearer \u2013 we\u2019ll follow up on that for later potential changes.\nAppreciate your patience for a reply!"}], [{"role": "user", "text": "Hello, I can\u2019t reset my password (I\u2019m using Galaxy), i tried but i never received the mail to reset it (it\u2019s the righ adress). How can i reset it?"}, {"role": "system", "text": "I\u2019m having the same problem, I don\u2019t know what to do. Someone can help me?"}, {"role": "system", "text": "\n\n\n Nadine_B:\n\nGalaxy\n\n\nHi @Drgvicm & @Nadine_B\nThis looks like a private Galaxy server internal to your university.\nYou\u2019ll need to contact the administrators of the server to find out what can be done. No information is available unless one has an account, so we can\u2019t help you with who to contact here. Maybe try contacting whoever originally granted you access?\nHope that works out!"}], [{"role": "user", "text": "Hi everybody,\nI\u2019m writing the Python code for a tool that taking a matrix as input, it returns an heatmap as result.\nWhen running the tool on Galaxy I get this error:\nTraceback (most recent call last):\n  File \"/Users/andrea/Desktop/galaxy/tools/personal_tools/sys_workflow/sys_heatmap.py\", line 16, in <module>\n    s.savefig(sys.argv[3])\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/seaborn/axisgrid.py\", line 65, in savefig\n    self.figure.savefig(*args, **kwargs)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/figure.py\", line 3046, in savefig\n    self.canvas.print_figure(fname, **kwargs)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2259, in print_figure\n    canvas = self._get_output_canvas(backend, format)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2188, in _get_output_canvas\n    raise ValueError(\nValueError: Format 'dat' is not supported (supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff)\n\nI think that the problem is that Galaxy is passing a dat file as storage for the output.\nIn the xml file I specified that the output should be a png file but it seems to not be enough, how can I solve this?\nHere\u2019s the code:\nxml file:\n<tool id=\"sys_heatmap_tool\" name=\"Heatmap \" version=\"0.1.0\">\n  <description>of phenotype counts</description>\n  <requirements>\n    <requirement type=\"package\">seaborn</requirement>\n\t<requirement type=\"package\">pandas</requirement>\n  </requirements>\n  <command>\n\t  <![CDATA[python3 \"$__tool_directory__/sys_heatmap.py\" '$input' '$cluster' '$heatmap_out']]>\n  </command>\n  <inputs>\n          <param name=\"input\" type=\"data\" label=\"Input file:\" help=\"Enter the matrix with the counts of phenotypes\"/>\n\t\t  <param name=\"cluster\" type=\"select\" label=\"Indicate which method use for clustering:\">\n\t\t\t  <option value=\"single\">single</option>\n\t\t\t  <option value=\"complete\">complete</option>\n\t\t\t  <option value=\"average\">average</option>\n\t\t\t  <option value=\"weighted\">weighted</option>\n\t\t\t  <option value=\"centroid\">centroid</option>\n\t\t\t  <option value=\"median\">median</option>\n\t\t  </param>\n\t\t  <param name=\"dist\" type=\"text\" label=\"Distance metric to use for the data:\" help=\"Enter the desired metrics\"/>\n  </inputs>\n  <outputs>\n\t  <data format=\"png\" name=\"heatmap_out\" file=\"heatmap.png\" label=\"Pheno count heatmap\"/>\n  </outputs>\n  <help><![CDATA[\n\t  **What it does**\n\t  \n\t  Given a matrix with counts of phenotypes, the tools retrieves the corresponding heatmap with clustering.\n\t  There user can choose between different methods of clustering:\n\t  \n\t  -single\n\t  \n\t  -complete\n\t  \n\t  -average\n\t  \n\t  -weighted\n\t  \n\t  -centroid\n\t  \n\t  -median\n\t  \n\t  More info about those metods can be found at: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n\t  \n\t  The default distance used for clusters is the euclidean. The user can enter a different distance metric from here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html?highlight=pdist#scipy.spatial.distance.pdist\n  ]]></help>\n</tool>\n\npython code:\nimport sys\nimport seaborn as sns\nimport pandas as pd\n\nmat = pd.read_csv(sys.argv[1], sep=\"\\t\", index_col=0)\n\nmat.index.name = 'MGI_id'\nmat.columns.name = 'phen_sys'\n\nclust=str(sys.argv[2])\n\ns = sns.clustermap(mat,row_cluster=True,col_cluster=False, method=clust, cbar_pos=(0.01,0.01,0.01,0.2), cmap=\"mako\")\ns.savefig(sys.argv[3])\n\nThank you for the help."}, {"role": "system", "text": "You can do something like:\npython:\ns.savefig(\"output.png\")\n\nxml\n<data format=\"png\" name=\"output\" from_work_dir=\"output.png\" label=\"Pheno count heatmap\" />\n\nor\npython:\ns.savefig(\"output.png\")\nshutil.copyfile(\"output.png\", sys.argv[2])\n"}], [{"role": "user", "text": "Hello,\nI am going through the \u201cFrom peaks to genes\u201d tutorial and keep running into the same error after re-running the job and double checking that the input files are correct.\n  \n      \n\n      training.galaxyproject.org\n  \n\n  \n    \n\nGalaxy Training: From peaks to genes\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI am trying to add the promoter region to gene records (in the Analysis step of Part 1 Naive Approach) with the Get Flanks tool, but keep getting \u201cAn error occurred setting the metadata for this dataset\u201d. The information in the attributes \u201c/corral4/main/jobs/040/971/40971917/galaxy_40971917.sh: line 123: galaxy-set-metadata: command not found\u201d\nAny help to bypass this would be much appreciated\nThanks!"}, {"role": "system", "text": "Update Resolved March 8, 2022\nDetails in the linked topic below.\n\nRelated recent Q&A: Error: galaxy-set-metadata: command not found\nThere is a technical problem with this set of Galaxy tools at UseGalaxy.org. It is being looked into.\nPlease try running the tutorial at UseGalaxy.eu or UseGalaxy.org.au for now.\nApologies for the trouble!"}], [{"role": "user", "text": "We have WGS PacBio sequenced data of environmental samples. As the data is huge, we don\u2019t have any high spec machine to analyse it. Therefore, we thought we should try the Galaxy platform. Is there any specific tool in Galaxy that can do the assembly of PacBio WGS metagenomic reads? Or any other online tool / pipeline that may do it?\nLooking for kind suggestions and help. Any leads?"}, {"role": "system", "text": "Hi @ZAYNE\n\n\n\n ZAYNE:\n\nIs there any specific tool in Galaxy that can do the assembly of PacBio WGS metagenomic reads?\n\n\nYes, there is at least Flye. Find this and tools under the \u201cAssembly\u201d tool group at any of the UseGalaxy.* servers.\nFull options\n\n\nFind publications that cover analysis methods you\u2019d like to follow, use comparable tools that Galaxy hosts, and create a workflow for batch analysis.\n\n\nReview public Galaxy servers that have an analysis domain focus on Metagenomics. Search here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\nAdapt the methods in our tutorials here\n\n\nMetagenomics topic\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Metagenomics\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nPacBio search \u2013 the first currently listed covers assembly\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Search Tutorials\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nHope that helps to get your started. Please ask a new question if you need more help with any specific tool."}], [{"role": "user", "text": "Hi!\ni would like to calculate FRiP score for my CHiPseq analysis, does everyone has an idea on how to do it ?\nthanks a lot!"}, {"role": "system", "text": "Hi @Marine_Bergot,\nit can be computed by using the following tools:\n\nbedtools SortBed: tool\n\nbedtools intersect intervals: tool\n\nsamtools view: tool\n\nawk: tool\n\n\nYou can find the details about the parameters in the following post.\nIt requires to include a new parameter in one of the tools; I opened a Pull Request in order to modify it.\nRegards"}], [{"role": "user", "text": "I have data with rows that contain count data for each gene and columns that contain a specific cell line.  I wish to perform a clustering analysis to determine which cell lines are most similar.  I am unsure which tool is most useful for this as well as how to filter the data since some genes aren\u2019t expressed (or only expressed in some of the cell lines)."}, {"role": "system", "text": "Hi @Coltin_Albitz\nFind the Galaxy single-cell tutorials here for example tools/workflows:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you are not sure where to start, the training event from last March has a suggested program, and single-cell analysis is covered with supplementary videos. The second video in the series is seems like a good match for where you are currently at with your own analysis.\n\n  \n      \n\n      gallantries.github.io\n  \n\n  \n    \n\nSm\u00f6rg\u00e5sbord 2022: Tapas Edition\n\n  Free, Global, Online Week of Galaxy Training\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      gallantries.github.io\n  \n\n  \n    \n\nSingle-Cell Analysis\n\n  This module will guide you through single-cell transcriptomics analysis in Galaxy.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nNote: the Slack channel mentioned in this content is not active right now \u2013 so please use this forum or the GTN gitter chat instead for Q&A. FAQs:\n\nhow to ask a question others can help with\ntroubleshooting errors\nsharing your history\n"}], [{"role": "user", "text": "Hi, I am using ChIPseeker for yeast experiment, however I didn\u00b4t find the gtf file of yeast (sacCer3). Where can I find it?\nThank you."}, {"role": "system", "text": "Welcome @Rafael_Lema\niGenomes hosts a quality GTF file for UCSC\u2019s sacCer3 genome.\nHow to get it out of the archive and uploaded to Galaxy is covered in this prior post:\n\n\n\nRNA-STAR and hg38 GTF reference annotation\n\n\nFor iGenomes, the archive corresponding to the target genome/build needs to be locally downloaded, the tar archive unpacked, and then just the genes.gtf data uploaded to Galaxy (browse the local file, or use FTP). Find all available genome/builds here: https://support.illumina.com/sequencing/sequencing_software/igenome.html\n\n\nThanks!"}], [{"role": "user", "text": "Hi everyone,\nI have been following tutorials and have been trying to get admin acess to the galaxy that i installed locally in my PC.\nAs per the instructions, i renamed the  config/galaxy.yml.sample to galaxy.yml and added my email to the admin_users section. However, i have not been able to get admin access even after restarting my galaxy.\nI am very new with linux and the galaxy setup so any help would be highly appreciated.\nThank you"}, {"role": "system", "text": "Did you also logged in with the same email after that? By default you don\u2019t need to log in to use galaxy itself."}], [{"role": "user", "text": "Hello,\nI am new to bioinformatic.\nI expect to get the table summarizing SNP or indel variants out of reads mapped to the reference genome together with coverage information (number of reads). I used BCFtools mpileup and continued with BCFtools call. I got .bcf file and could not figure out how to open or view it.\nAppreciate all the help,\nThank you"}, {"role": "system", "text": "Dear @Prakit_Saingam,\nA bcf file is a compressed version of a vcf file (see here). You can look into the content of a bcf file by converting it into a vcf file with the tool bcftools view Convert, filter, subset VCF/BCF files.\nHave a good day and best wishes,\nFlorian"}], [{"role": "user", "text": "Hello,\nis it possible to upload files from google drive to galaxy? I tried generating direct link to the google drive file (which was public) and then using it on usegalaxy.eu, but it returned an error: \u201cThe uploaded file contains invalid HTML content\u201d. Any advice?\nThanks!"}, {"role": "system", "text": "Hi @vojtace,\nyes, it is possible, but a bit tricky. First, you need to get the file ID from the sharing link:\n\ndrive.google.moc/file/d/1ZaG5oZm-CUbHiej7Y3OdjoU8Z89gOsE7/view?usp=sharing\n\nThen you need to use the following address in Galaxy:\n\ndrive.google.moc/uc?export=download&id=1ZaG5oZm-CUbHiej7Y3OdjoU8Z89gOsE7\n\n\nRegards"}, {"role": "user", "text": "Hi @gallardoalba,\nthanks for the response. Unfortunately this doesn\u2019t work for me, but I finally figured out why: the files are too large for the google drive direct download link to work. Knowing where the problem was it was easy to find out the solution: Google Drive Direct Download Link for Large Files (2022) - it is necessary to use google API: https://www.googleapis.com/drive/v3/files/FileID?alt=media&key=APIKey so it is even more complicated, but it works, I finally managed to upload the data to Galaxy. "}], [{"role": "user", "text": "I am so sorry to bother you to help me solve this problem. I wanna do the Lefse analysis, the first step of format data seems failed .  I can\u2019t get the point about the error. I would be so appreciated if this will be solved. Thx!!\nDetails:\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 429, in \nclass_sl,subclass_sl,class_hierarchy = get_class_slices(zip(*cls.values()))\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 145, in get_class_slices\nprevious_class = data[0][0]\nIndexError: list index out of range"}, {"role": "system", "text": "Hi @Emma\n\n\n\nLEfSs uploded file looks different to 2018 \u2013 problem?\n\n\nYou are working at this public server, correct? Galaxy\nThe server is set up a bit differently than other public servers and hosts custom versions of tools, in particular the Lefse tool suite. Their support/admin team does not follow this forum, but there are other ways to contact them about odd or non-reproducible results.\n\n\nThat said, this error in technical terms means that the tool is looking for specific data in part of the input that isn\u2019t being found. There is probably some problem with the column or row selected for the option \u201cSelect which [row|column] to use as subclass\u201d.\nTry double-checking your tabular input (first option on the tool\u2019s form) to make sure there is content in the row or column you selected. Tutorials and example data are hosted on that public Galaxy server for comparison. Scoll down on that tool form for an example of input data organized by rows.\nHope that helps!"}], [{"role": "user", "text": "Dear all,\nWhile running bowtie2 I am getting following error continuously\n\"An error occurred while trying to schedule this job. Please retry it and if it continues to fail, report it to an administrator using the bug icon."}, {"role": "system", "text": "This error was caused by an upgrade problem that has now been resolved, please try your job again and let us know if you still encounter errors."}], [{"role": "user", "text": "Hello, can anyone tell me how to extract all the reads which have a certain sequence from a multi fasta file? \u2018Fasta extract sequence\u2019 seems to only be able to extract using the Name of the sequence, not the sequence itself. All reads have the same name/ID so that isn\u2019t usable for extracting a subset.\nMy situation is that I have amplicon-seq files each containing reads from 2 different PCR products, and just want to split them into 2 files using a sequence which is unique to either.\nHelp!"}, {"role": "system", "text": "Sounds like the Filter FASTA tool is what you are looking for?"}], [{"role": "user", "text": "I\u2019m trying to upload control sgRNA file (.txt) in MAGeCK count and MAGeCK test.\nAfter complete uploading in history, system showed that this file was unavalible in the selection list\nof control sgRNA file.\nI\u2019ve tried to upload different file format (.txt/.csv/.tsv), then all of them were failed.\nI do not know if anyone has experience with that problem and could help me.\nThanks a lot for the help in advance.\n\ncontrol1913\u00d7929 179 KB\n"}, {"role": "system", "text": "Welcome, @angel\nTools screen for specific datatypes when accepting inputs. For this input, the tool is expecting a dataset with the tabular datatype assigned.\n\nScreen Shot 2023-05-25 at 6.06.47 PM1646\u00d7870 99.1 KB\n\nHow to know which datatype a tool is expecting is in this topic and a few others.\n\n\n\nfastq unavailable -- Tool does not recognize inputs? How to check why\n\n\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:\n\nCreate a new empty history\nLoad up the tool form\nReview the input datatypes listed in the tool form\u2019s input area\nNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\nExample\n\nwhat-datatype-is-accepted-input2346\u00d7554 87.2 KB\n\n\n\nSearch: Search results for 'tool doesn't recognize input' - Galaxy Community Help\n\n\nSituations when the (unavailable) message can be generated\n\nDataset 72 was originally accepted by a tool run.\nIt had a tabular datatype assigned, or was drag-and-dropped.\nThe latter breaks some built-in checks \u2013 you can try this way but expect problems exactly like this.\nOr, the first was done, but the datatype assigned to dataset 72 was modified to be something other than tabular.\n \u2192 When the \u201crerun\u201d function is used to load up the tool form, it preloads what was originally input. If the metadata datatype was changed to be anything other than tabular, it won\u2019t be accepted.\n\nKeep in mind that none of this is about the file content. The data still needs to be in tabular format and have data in the columns it can understand.\nExamples are down in the help section of the tool form. The GTN also has tutorials.\nhttps://training.galaxyproject.org/training-material/search?query=MAGeCK\nThere is a training event going on all week if interested. It is not too late to join. The bonus is a dedicated chat per tutorial, and the authors and other Galaxy scientists are there to help (including me). Sm\u00f6rg\u00e5sbord 3: A week of Free, Online, Galaxy Training supported by the Global Galaxy Training Community\nHope that helps!   "}], [{"role": "user", "text": "I am unable to view html output from MultiQC on usegalaxy.org. Perhaps the tool has been updated and needs to be whitelisted? I can successfully download and view the local html report. Thank you."}, {"role": "system", "text": "Hi @mark_j\nYes, that was exactly the problem. Should be fixed now \u2013 please let us know if not.\nThanks!"}], [{"role": "user", "text": "Hye, I have a account in usegalaxy.org and was working on important project on it. I took break from work for a month and today when I tried to log in, I am not able to access my account. While ai am resetting my password, it is showing \u201cFailed to produce password reset token. User not found.\u201d.My user email id is (admin-redacted). Is anyone else has faced such issue. Kindly help me to retrieve my account. It has very important data and months of work."}, {"role": "system", "text": "@Devashish_Sen\nWe are discussing this privately via the UseGalaxy.org mailing list. That is best place to address administrative account issues, although if you search this forum you will find other posts related to this topic. For privacy reasons, I have redacted your email address from the original question.\nYou still have one active account at UseGalaxy.org. This meets the terms of service: one account per person per public Galaxy server. Please have/use one account at each, and never two or more at any. This information is clearly stated in plain language on the account registration form and the account activation email exactly where the link is located. Both also include the full terms of service with an explanation about why these terms are in place. In short, the free public resources have practical limits. We must ensures fair resource access for you, and for everyone else working at the same service.\nIf you need more resources than the combined resources of all public servers can support or if you simply want more control over how resources are scaled/allocated, private Galaxy server options are available. Several options require very little to no administration. These and other deployment choices are widely utilized globally by both technical and non-technical scientists. Some for short term analysis projects, and some for longer term analysis needs. You can of course still have one account at each public server while using private options. Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\nI\u2019m going to close this topic out. Please direct future administrative account concerns to the private communications channels. Such contact information is usually on the home page of the server or in the public directly listing above. If you can\u2019t find it, and there is not already a topic covering known contact methods for the server you are working at, please post a new question with the URL of the server and we can help."}], [{"role": "user", "text": "I ran roary on galaxy europe for 37 e coli genomes but the core genome alignment file output is empty, there is no sequences in there."}, {"role": "system", "text": "Duplicated post, see Roary troubleshooting (version 3.13.0+galaxy2 or later)\nIf you would like more help, please post back a shared history link to the original topic.\nBefore you do that, please try at least one more rerun. The UseGalaxy.eu server had some technical issues earlier this week that may have contributed to failures."}], [{"role": "user", "text": "Dear community,\nI read that control is important to get reliable peaks when using MACS2. However, I do not find any control for my treatment regarding the experiment that  I am analyzing. I am using one dataset found in Chip Atlas and they get peak calling reports for the sample. But I do not know how do they define this control. The paper mentioned they used MACS2 but no explicit input controls were given. How is usually the best way to proceed when you lack this information without biasing the results or doing too much data manipulation?  Thank you in advance for any helpful information."}, {"role": "system", "text": "Hi @Pablin\nGTN provides at least couple tutorials for MACS2\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: ATAC-Seq data analysis\n\n  DNA methylation is an epigenetic mechanism used by higher...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Identification of the binding sites of the T-cell acute l...\n\n  DNA methylation is an epigenetic mechanism used by higher...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nHope it helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I recieved two already aligned to hg19 and filtered BAM files and wanted to proceed with Variant Calling on VarScan Somatic for comparing normal and diseased samples. I came across the following error message:\n[W::hts_idx_load2] The index file is older than the data file: normal.bam.bai\n[E::mpileup] fail to parse region \u2018chr11_gl000202_random:\u2019 with normal.bam\nIs there something I can do to solve this problem and use these files in VarScan or should I restart the quality control and mapping with the respective FastQ files? Although there is a workflow for that on Galaxy Training, I\u00b4d rather use the BAM files because I have never mapped data before, I usually work with BAM and VCF files. Thanks in advance, any help would be most welcome!"}, {"role": "system", "text": "Hi Anna,\nfirst, please don\u2019t post issues in multiple places (VarScan error: index file older than data file and fail to parse certain regions \u00b7 Issue #1879 \u00b7 galaxyproject/galaxy-hub \u00b7 GitHub) - it won\u2019t help anyone.\nNow, in your error message E stands for error and W for warning so what causes your tool run to fail is the \u201cchr11_\u201d\u2026 issue. Most likely, your BAM file has been generated against a version of hg19 that contained only the canonical chromosomes, chr11 for example, but not incompletely assembled scaffolds like the one mentioned. Varscan is really strict about this and insists on identity between the reference genome sequence names and the names listed in the header of your BAM file (which you can check yourself by clicking on the eye icon of that dataset and looking for header lines starting with @SQ.\nYou didn\u2019t mention where you have been running this job, but on usegalaxy.eu, there is a reference genome called \u201chg19 Canonical\u201d, which is identical to the full hg19 for all the standard chromosomes, but is lacking the additional scaffolds, and which you may try as your reference.\nThe warning about the index file is of no relevance in Galaxy and you can safely ignore it.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hello everyone, could you please help me?\nWhy is there no \u201cinteractive_tool_simtext_app\u201d tool in Galaxy? I can not do the step after \u201cPMIDs to PubTator\u201d because this tool does not exist at all and gives me this error: \u201cDownload tool interactive_tool_simtext_app Failed: Tool with \u201cinteractive_tool_simtext_app\u201d ID not found\u201d.\nThank you in advance."}, {"role": "system", "text": "Hi @maryam.klntri.2695,\nyou can find that tool in the usegalaxy.eu instance: Interactive tool simtext app.\nRegards"}], [{"role": "user", "text": "I have 4 different groups named as group_1, group_2, group_3, and group_4. 6 samples in each group. The input data was a text file obtained from picrust 2 pathway abundance feature table. I encountered the following error;\n\"/galaxy_venv/local/lib/python2.7/site-packages/rpy2/rinterface/init.py:185: RRuntimeWarning: Error in lda.default(x, grouping, \u2026) :\nvariable 36 appears to be constant within groups\nwarnings.warn(x, RRuntimeWarning)\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/run_lefse.py\u201d, line 89, in \nif params[\u2018rank_tec\u2019] == \u2018lda\u2019: lda_res,lda_res_th = test_lda_r(cls,feats,class_sl,params[\u2018n_boots\u2019],params[\u2018f_boots\u2019],params[\u2018lda_abs_th\u2019],0.0000000001,params[\u2018nlogs\u2019])\nFile \u201c/export/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/lefse.py\u201d, line 189, in test_lda_r\nz = robjects.r(\u2018z \u2190 suppressWarnings(lda(as.formula(\u2019+f+\u2018),data=sub_d,tol=\u2019+str(tol_min)+\u2018))\u2019)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/init.py\u201d, line 359, in call\nres = self.eval(p)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/functions.py\u201d, line 178, in call\nreturn super(SignatureTranslatedFunction, self).call(*args, **kwargs)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/functions.py\u201d, line 106, in call\nres = super(Function, self).call(*new_args, **new_kwargs)\nrpy2.rinterface.RRuntimeError: Error in lda.default(x, grouping, \u2026) :\nvariable 36 appears to be constant within groups\"\nAny suggestions will be appreciated!\nThanks"}, {"role": "system", "text": "Hi @Ishanmanandhar\nTools sourced from the Test Toolshed may have technical problems and produce weird errors.\nInstall and use tools from the Main Toolshed here instead: https://toolshed.g2.bx.psu.edu/\nIf you run into problems at your own server, try running the same data through the same tool/same tool version at a public usegalaxy.* server to compare. That will help to isolate usage problems from technical/configuration problems (bugs). You can also first try an internet search using content from an error report if it seems to be produced by the underlying tool itself (unrelated to the Galaxy wrapper). Most tools, including this one, have a lot of discussion at the different public bioinformatics forums, and that discussion is usually helpful even if it is based on running the tool line command instead of Galaxy.\nExample: the immediate problem is probably similar to the one reported in this Lefse forum Q&A: I can't see why I am getting this error in lefse in conda? - LEfSe - The bioBakery help forum.\nThat was found with a search using this part of the error message \u201cError in lda.default\u201d + lefse. You can also do that same search to find more Q&A, at that same tool dedicated forum hosted by the tool authors, or just in general. That finds Q&A specific to this tool, if it exists, whether run in Galaxy or not.\nWhat to try: Fix your input data to be compatible/understood by the underlying tool. If that produces an error that has not been discussed at a public forum, then you can try updating the Galaxy tool to be a stable version and proceed from there with troubleshooting via comparisons with public servers running that same tool. If the problem can be reproduced at a public site, you can use that as an example to get more troubleshooting help at this forum. Sometimes sharing the error message is enough, other times including a share a link to the history and noting the datasets involved is better.\nFAQ with more advice: Understanding-input-error-messages\nHope that helps!"}], [{"role": "user", "text": "\nSPades1282\u00d7865 25.8 KB\n\nI am trying to run assembly of SARS-COV2 genome from data set SRR10971381. When it shows completion, I don\u2019t get the contig fand scaffold files. It displays 0 bytes. There is a bug that needs to be fixed.\nI get the log file Log File where no error spotted."}, {"role": "system", "text": "Hi @Ranjan_J_Sarma,\nI reported the problem to the Galaxy sysadmins in order to update SPAdes up to version 3.15.4; probably it will fix the problem.\nRegards"}], [{"role": "user", "text": "Hello,\nI am working on RNA seq data to get the differential gene expression data of sulfate-reducing bacteria. My bioinformatic pipeline is raw read \u2192 fastqc \u2192 trimmomatic \u2192 hisat2 \u2192 featurecounts \u2192 deseq2. I could get results until hisat2. But when I upload the hisat2 BAM file for featurecounts, I am getting an error \u201cAn error occurred with this dataset:\u201d I checked with three different genomes with their corresponding reference genomes and I am getting an error with the featurecounts. Could any one help me with this?\nThank you\nJawahar"}, {"role": "system", "text": "Hi @jawaharrajk,\ncould you share your history with me? My email is .\nRegards"}, {"role": "system", "text": "Hi @jawaharrajk,\nas I told you by email, you need to use paired-end lists, instead of collections of paired-end reads.\nRegards"}], [{"role": "user", "text": "Hi,\nI\u2019m trying to upload a very small txt.file of several bits to galaxy eu. I tried to upload from local files and using ftp. When uploading from local file it says: the job is waiting to run and it does not run at all. When I upload from ftp it says: curl: (7) Failed to connect to ftp.usegalaxy.eu port 21 after 6085 ms: Network is unreachable.\nHow can I upload my file?\nThanks for answering!"}, {"role": "system", "text": "Hi @bart_joosten,\nthis problem seems to be related to the maintenance tasks that are currently being carried out. Sorry for the inconvenience.\nRegards"}], [{"role": "user", "text": "Hello all,\nI am currently developing synthetic biology tools for Galaxy. We try to use standards of the field including SBML and SBOL (https://sbolstandard.org/). The former is supported by Galaxy but I could not find the latter. Is there a roadmap to include SBOL? It simply is an XML, and we are currently defining input/output of the tools to take XML. However this is not ideal.\nThank you!"}, {"role": "system", "text": "Hi @Melclic1,\nGalaxy filetypes are defined in this file:  https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/datatypes_conf.xml.sample#L748\nYou can create a pull request on GitHub to include the SBOL filetype. Here is a link to the documentation: https://docs.galaxyproject.org/en/latest/dev/data_types.html\nHope this helps, feel free to ask any further questions.\nSimon"}], [{"role": "user", "text": "Hi, I am new to galaxy. Just installed local galaxy in my computer and changed admin_users to myself. Yes, after login, I am able to have the admin tab. However, after I clicked on the admin, I realized that the interface of my Galaxy is different from what others showed. See below for a screen shot. It doesn\u2019t show me the Tool Shed option and many other buttons.\nWhat could be wrong? Thanks for help!\nFrankie!\n17|690x375"}, {"role": "system", "text": "Interface will generally only show you options relevant for your Galaxy. E.g. if you don\u2019t have quotas configured, it won\u2019t show you the UI for defining them. Similar when you have no toolshed tools installed, it will not show you \u2018manage installed tools\u2019 menu item.\nYou can install tools from toolshed using the \u2018install or uninstall\u2019 menu item. (However if this is the \u2018dev\u2019 branch of Galaxy, that won\u2019t work atm, please use the latest release version of Galaxy instead - http://getgalaxy.org)"}], [{"role": "user", "text": "Good day. Is there a workflow that I can use to convert fastq files directly to fasta. I need to analyse the raw reads in fasta format and not the consensus sequence. I tried a workflow called fastq to fasta but it doesn\u2019t seem to be working."}, {"role": "system", "text": "Have you tried FASTQ to FASTA tools (instead of a workflow)? You can search for them in the tool menu. I see two:\nFASTQ to FASTA converter from FASTX-toolkit\nFASTQ to FASTA converter"}], [{"role": "user", "text": "I have a install galaxy but when i run tools, i don\u2019t see it use multi- thread and more memory. How can I config to use more ram and thread ?\nI have cloned galaxy from github\nI have a computer with 64cpu and 150 gb ram."}, {"role": "system", "text": "I think most tools from the toolshed use the variable ${GALAXY_SLOTS:-1}. You can change this variable in the file galaxy/config/job_conf.xml. In this file you will see a line like <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/> You could increase the workers. If you dont have the file job_conf.xml you could use job_conf.xml.sample_basic by doing cp job_conf.xml.sample_basic job_conf.xml. You need to restart galaxy after this change."}], [{"role": "user", "text": "Hi all,\nI have a RNA-seq have no replicates, so I am using edgeR to do DE. and I got counts data from featurecount. I run edgeR, but keep turning red\u2026 the warning message see below.\nCould anyone help me with that?\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nWarning message:\nIn estimateDisp.default(y = y$counts, design = design, group = group,  :\nNo residual df: setting dispersion to NA\nError in plotMDS.default(y, top = top, labels = labels, pch = pch, cex = cex,  :\nOnly 2 columns of data: need at least 3\nCalls: plotMDS \u2192 plotMDS.DGEList \u2192 plotMDS \u2192 plotMDS.default\n"}, {"role": "system", "text": "hi @Jiahui\nit seems you have only two samples, while the latest versions of edgeR require replicates, hence \u201cOnly 2 columns of data: need at least 3\u201d. The best option is to have replicates, at least for one condition. Alternatives: old versions might support analysis without replicates. Try an earlier versions of DESeq2 and/or edgeR. Cuffdiff supports analysis without replicates, at least old versions did.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "The display option after running a tool doesn\u2019t function anymore. I can download the output, but sometimes it\u2019s more convenient to view it real time as the titles of the download are not very intuitive when you run multiple histories in one session."}, {"role": "system", "text": "We\u2019re struggling a bit to get usegalaxy.eu back to normal functionality after an overnight update, but we are aware of the issue.\nThanks for reporting it nonetheless."}, {"role": "system", "text": "Update: Displaying the content of datasets should be working again."}], [{"role": "user", "text": "Dear Admin,\nI am writing to request the addition of Plasmodium species (Plasmodium vivax, Plasmodium malaria, Plasmodium ovale and Plasmodium falciaparum) reference genome in the galaxy main webpage. Thank you."}, {"role": "system", "text": "Hi @maryrose31\nThese genomes can be used as custom reference genomes (fasta) files from the history. If you need a \u201cdatabase\u201d metadata key, promote each fasta to a custom build with a unique name.\nAt the very start of an analysis project, the best practise is to prepare each genome fasta in a standardized format along with the correctly paired and formatted reference annotation (if you plan to incorporate that kind of data).\nFAQs:\n\nworking-with-fasta-datasets\nreference-genomes\nadding-a-custom-database-build-dbkey\nworking-with-gff-gft-gtf2-gff3-reference-annotation\n"}], [{"role": "user", "text": "I have my personal WGS 100x data in such a big CRAM file (183 GB) that I am not able to work on using public Galaxy servers. So I tried to launch my own GVL instance on AWS.\nUnfortunately now everyone can register and use my GVL instance and I doubt I could allow this financial wise.\nSo I my Q is: how to prevent others from using my private GVL instance deployed on AWS?"}, {"role": "user", "text": "Solved\ninsert into Galaxy-app/config/galaxy.ini the following lines:\nallow_user_creation = False\nrequire_login = True"}], [{"role": "user", "text": "Hi, I am loading the Gemini load file into Gemini query in \u201cInsert Region filter\u201d section with chromosome number and Genomic coordinates.\nAll the queries are returned with columns name only.\n|gene|chrom|start|end|ref|alt|impact|impact_severity|max_aaf_all|rs_ids\nAny idea why this might be happening?\nAm I doing anything wrong?\nPlease advice.\nThank you.\nCheers,\nShuchi."}, {"role": "system", "text": "Hi @Shuchi,\nthanks for the report; you are right, it seems that there\u2019s a bug in that tool. I\u2019m reviewing it.\nRegards"}, {"role": "system", "text": "The fixed version will be available in the next days; could you meanwhile use Advanced query construction? You need to use an expression with the following pattern:\nSELECT chrom, start, end, ref, alt, gene, impact FROM variants WHERE chrom = 'chr1' AND  start >= 0 AND end <= 13327\nRegards"}], [{"role": "user", "text": "Hi, can someone help me with this? I am trying to perform a workflow to achieve motif discovery and genome ontology data about the motifs found. I have started with fasta files from a ChIP-seq experiment, then I run FastQ Groomer to my data, followed by Mapped with Bowtie for Illumina, then I used MACS - model-based analysis for ChIP-seq. Based on ABCAM chip-seq tutorial by Xi Cheng, I would be able to get the tabular data from the MACS, generate an interval for the peak summit found using this workflow, to run on MEME-ChIP (another website), and, then, also with the interval, to run on GREAT (genome ontology), however, my MACS are coming out with no results. Please, help me! I had already run 7 different workflows,  using 7 different tutorials, and I have not found how to get there! My public name is paulovga. Help is really appreciated!!!"}, {"role": "system", "text": "Hi @pauloalabarse,\ncould you share your Galaxy history with me? I\u2019ll have a look at the MACS results. My email is\n.\nRegards"}], [{"role": "user", "text": "I have followed all steps on find my mobile but my galaxyS20 5G will not give location, I can ring it but not locate it"}, {"role": "system", "text": "Hi, this is the help forum for the galaxy project (usegalaxy.org) not the Galaxy range from Samsung."}], [{"role": "user", "text": "Hello, how can I find out the highest and lowest expression values in a large matrix of single cell gene expression data whose number of columns and rows is greater than 1000?"}, {"role": "system", "text": "Hi @maryam-gh99\nYou could Sort the file by a specific column. Consider removing any header lines first, as you can always add one back in (exact or just parts of it) later on and some tools might complain about it.\nThis tutorial has a similar method described.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Galaxy 101\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAnd this tutorial includes a bunch of data manipulation methods. Most are \u201cUNIX utility\u201d or \u201cspreadsheet functions\u201d analogues and definitely work on larger tabular text files.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nHope that helps!"}], [{"role": "user", "text": "\nScreen Shot 2022-08-10 at 11.24.48 AM1239\u00d7347 43.4 KB\n"}, {"role": "system", "text": "Hi @ashleyf\nHopefully you have discovered this already, but public Galaxy servers usually do not include indexed reference annotation.\nFor the reference genome, try one of these:\n\n\nUse an available build-in reference genome index without annotation.\n\n\nUse a fasta file from the history that represents the reference genome.\n\n\nFor the reference annotation, try:\n\nUse a matching reference annotation provided by you from the history. GTF will work best with this tool and many others.\n\nFAQs. Skip the first three and just review the last one if using a built-in genome index.\n\n\n\nRequest for of Plasmodium species reference genome\n\n\n\nworking-with-fasta-datasets\nreference-genomes\nadding-a-custom-database-build-dbkey\nworking-with-gff-gft-gtf2-gff3-reference-annotation\n\n\n"}], [{"role": "user", "text": "Can anyone advice on how to upload a .embl file on galaxy for the purpose of running Maker? When I upload it as auto detect it uploads as txt and maker does not recognize this file. maker also doesn\u2019t seem to accept fasts or gff3 or gtf formats. How have others loaded this file?\nThanks in advance!"}, {"role": "system", "text": "Hi @Sowmya_Ramachandran,\nin order to use Maker, you should convert the .embl file in fasta format; it can be done with the seqret tool."}], [{"role": "user", "text": "Hi,\nI\u2019m looking for a tool/way to extract the barcode from fastq file and to split fastq file into separate fastq files based on the indices. The indices are placed in the sequence ID. Here is the example:\n@NB552014:62:HGCLWBGXJ:1:11101:8614:1055 1:N:0:ACTATAAC+TAAGATTA\nGATCGNAAGAGCACACGTCTGAACTCCAGTCACACTA\n+\nAAAAA#EEEAEEEEEEAEE/AEEEEEAE//EE</A//\nIncorrect index sequences were entered in the sample sheet and Illumina demultiplexing filed. I have the fastq files and try to demultiplex on my own\u2026any suggestions, please?"}, {"role": "system", "text": "Hi @agata_s,\ncurrently, there\u2019s not a tool able to perform such operation; I started working on demuxbyname (more information here), since I will provide that functionality. I will try to make it available from the next week.\nRegards"}], [{"role": "user", "text": "In the data libraries, afetr clicking on some datasets, it redirects to the covid 19 dataset. Please help in resolving the problem. These datasets ate unrestricted,even then this problem is arising. Please help"}, {"role": "system", "text": "We believe this has been fixed in [21.05] Fix breadcrumb in dataset details in libraries by OlegZharkov \u00b7 Pull Request #11998 \u00b7 galaxyproject/galaxy \u00b7 GitHub we will deploy an update in the coming days with this fix.\nThanks for reporting!"}], [{"role": "user", "text": "Hi everyone,\nI have a tabular file as output of my tool and I would like to use the first row as header (displayed in blue in the view tab).\nI read an old discussion where they suggested to use the \u2018#\u2019 on the first row to let galaxy knowing that this is the header row but it seems that it is not working since the header is still considered part of the data.\nThank you for the help"}, {"role": "system", "text": "Hi @Andrea_Furlani\n\n\n\n Andrea_Furlani:\n\nI read an old discussion where they suggested to use the \u2018#\u2019 on the first row to let galaxy knowing that this is the header row\n\n\nThat is correct. In the \u201cpeek view\u201d, it will look like this for a tabular datatype with one header row starting with a #.\n\ntabular-header-peek-view526\u00d7532 25.1 KB\n\nHow to interpret:\n\nColumns are numbered with a dark blue background.\nTechnically, and the header is counted up as a header line, other lines are counted as data lines.\n\ntabular is one of the most general datatypes.\n\nThe header line and data lines are passed to downstream tools.\nSome tools will have an option on the form where you can specify the input has a header line, or how many rows to consider as a header line. Other tools won\u2019t, and for these, the user would need to strip header lines out first.\nThere are other defined datatypes that have \u201cbuilt in\u201d headers. Example: bed. What is seen in the \u201cpeek view\u201d for that datatype, under the dark blue background, is defined in the bed datatype specification (built into Galaxy), and not included in the file contents.\n\n\n\nIf you know what tools are likely to consume the output of your tool, you can test out how those interpret your header. Why? Several of the tools under \u201cData Manipulation\u201d expect no header lines in any data. If you want people to be able to use those tools without extra formatting steps, consider not including a header or making it an optional output on your tool\u2019s form. Maybe put some examples in your tool\u2019s help section to alert users about how to use the output with example tools that interpret it differently."}], [{"role": "user", "text": "hey i deleted my account on accident please may you assist me and reactivate it i need it for school"}, {"role": "system", "text": "Hello @luyandamahlase\nYou still have one active account at UseGalaxy.org using your Gmail email address. The other was removed. If you want to swap which single account is active, see this prior topic for how-to: Account marked deleted - #2 by jennaj\nfaqs/galaxy/#account"}], [{"role": "user", "text": "Hello, I am having great difficulty reporting on alpha diversity for kraken2 results! In Galaxy there is no option for Kraken tools and there is no option to generate OTU tables from your results, which would make it possible to analyze alpha diversity using other programs. Does anybody know how to solve this? What is the best path to follow, that is, how to analyze alpha diversity from kkraken2 results?vvvv"}, {"role": "system", "text": "Hi @Hugo_Valerio_Correa\nPlease consider working at Metagenomics - Galaxy Community Hub. You\u2019ll find the full suite of available metagenomics tools that have been wrapped for Galaxy hosted there. Those include format conversion tools.\nIf you already have an account at UseGalaxy.eu, that same account will be your account at metagenomics.UseGalaxy.eu/, too. If you need to move data between other servers, that can be copied over by dataset URLs pasted into the Upload tool and/or see #transfer-entire-histories-from-one-galaxy-server-to-another\n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Metagenomics\n\n  Metagenomics is a discipline that enables the genomic stu...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I had a usegalaxy.org account and tried to log in, but it said \u201cUser not found.\u201d Please help me. i need my data\u2019s from this account."}, {"role": "system", "text": "Hi @Pratanukayet\nWe discussed this over email, but for any others that run into a similar problem\u2026\n\nAccounts at different public Galaxy servers are distinct.\n\n\n\nExample 1 these are all different servers with different accounts: usegalaxy.org.au, usegalaxy.eu, usegalaxy.org\n\n\nExample 2 your account at https://usegalaxy.eu and https://covid19.usegalaxy.eu will be the same. Why? Some servers host multiple domain-specific servers.\nFAQs faqs/galaxy/#account\n\n\n\nCheck the original account activation email (the one with the link) for details.\n\n\nserver URL\nthe exact email address used for registration (case sensitive)\nadministrative contact\n\n\nAll known public servers are in our directory listing.\n\n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\nData can be transferred from one server to another without a download step: workflows, histories, and individual datasets.\n\n\nfaqs/galaxy/#workflows\nfaqs/galaxy/#histories\nfaqs/galaxy/#datasets\n\n\nEach account will have distinct a distinct data quota and jobs will use resources specific to that server. The homepage of the server will usually have more details. Please ask questions if anything is unclear \n\n"}], [{"role": "user", "text": "My galaxy active 2 watch has muted colors, its not unbearable l, but it makes it realy hard to read.\n\n166265408815377412474651538983661920\u00d72560 298 KB\n\n\n166265414575691729090521960157371920\u00d72560 277 KB\n\nI\u2019ve checked the settings and i couldnt find anything that could jave been causing it."}, {"role": "system", "text": "Hello, this forum is made for the scientific open source galaxy project. This is a webapplication that makes bioinformatics tools easily accessible. It looks like you have question about a samsung galaxy product. This forum has nothing to do with samsung products. "}], [{"role": "user", "text": "Hello, I am analyzing sequences obtained from leukemic cells and I am comparing them with hematopoietic stem cells, but the most used reference genes show a lot of variation \u2026 there is a Galaxy tool that allows me to determine which or these genes are the most suitable \u2026"}, {"role": "system", "text": "Hi @Alejandra1819,\nI suggest you perform a differential expression analysis (e.g. by using DESeq2), filtering those genes whose fold change is near to zero by using the Filter data on any column tool and sort the output by descending order based on the Standart Error. Then you can select as the reference one of the genes whose mean expression is significant, and with low dispersion.\nRegards"}, {"role": "user", "text": "Hello @gallardoalaba, I thank you for your suggestion, it was very useful to me."}], [{"role": "user", "text": "Hello,\nI am working RNA-seq dataset in influenza virus infection. I am doing differential expression using DeSeq2 in usegalaxy. I am keep getting this error message\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in DESeqDataSet(se, design = design, ignoreRank) :\nCould anyone please give me advice on how to resolve this? This is my first time analyze RNAseq\nThanks"}, {"role": "system", "text": "Hi @Vu_Ngo\nThis is a technical error.\nFirst, try a rerun.\nNext, check your inputs/parameters. Content and format both matter. The tool form has many details, including a link out to the Bioconductor forum. Galaxy tutorials and this forum are also good places to review. All can be searched with keywords like tool names.\nIf you cannot solve the problem and are working at a public Galaxy server, please share back the server URL and the full error report.\nLet\u2019s start there "}], [{"role": "user", "text": "Hi,\nI need to use obitools, and it is available in the toolshed. How do I do to access this within galaxy?\nbest,\nPeter"}, {"role": "system", "text": "Hi @hamback,\nthe obitools package is available in usegalaxy.eu.\nRegards"}], [{"role": "user", "text": "Hi,\nSome days ago I realised that the testing toolshed was down. Yesterday it worked again, but it still fails when trying to install any tool from it into a Galaxy instance. Do anyone knows about this error?\nGreetings,\nCarlos"}, {"role": "system", "text": "Hi @CarlosHorro\nThe Test ToolShed (TTS) has been undergoing some changes in the last few days. It appears to be up now. Maybe try again?\n(( I\u2019m assuming that you already know to use the Main ToolShed (MTS) instead for \u201cproduction-ready\u201d tools (where most people should source most tools). ))\nThe current status for these and other core usegalaxy.* servers, plus associated key resources, can always be found here: https://galaxyproject.statuspage.io/\nLonger or planned events will usually have some kind of notation added. But you can always ask for more details at this forum \u2013 or for even quicker Q&A suitable for a chat, reach us at Gitter https://gitter.im/galaxyproject/Lobby.\nWe can try to clarify at either.\nThanks!"}], [{"role": "user", "text": "I just downloaded a 90 Gb file to use as a database and now galaxy won\u2019t open.\nError message is below:\nThis page isn\u2019t working\nusegalaxy.org  is currently unable to handle this request.\nHTTP ERROR 500"}, {"role": "user", "text": "Side note - typing usegalaxy.org into the site isn\u2019t working, but I tried usegalaxy.org/login and it worked! The page opened and allowed me to log in! (In case anyone is having a similar issue)"}], [{"role": "user", "text": "I am trying to use DESEQ2 dor DGE analysis.  I am using paired datasets uploaded from GEO, aligned with Hisat2 and counted with HTseq-counts.  I am selecting the input files, Have the source set as HTseq-counts, but I keep getting the error below.\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: rownames<- \u2026 row.names<- \u2192 row.names<-.data.frame \u2192 .rowNamesDF<-\nWarning message:\nnon-unique value when setting \u2018row.names\u2019: \u2018SRR16947329\u2019\nI am not manipulating the dataframe from the counting program at all prior to input.  It has worked before.  At a loss on what to correct.\nThanks for any help!"}, {"role": "user", "text": "Nevermind.  For all other newbies bumbling around out here  I will share my mistake\u2026  One of my input files was a duplicate right from the start.  I used tags etc on the downloads but must have accidently double downloaded it right in the beginning, so the underlying file names were the same.  I merged the collections of paired end runs and it dropped the duplicate.  When I ran with the new merged collections worked fine."}], [{"role": "user", "text": "I run the bowtie2 to map reads against reference genome,It\u2019s show:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method:\n{\n\u201chistory_id\u201d: \u201c87610437856c28e6\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.3+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c2.3.4.3+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201clibrary|type\u201d: \u201csingle\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33a134933e9419880e8\u201d,\n\u201chid\u201d: 21,\n\u201cname\u201d: \u201cCaenorhabditisbriggsae_AD859-R01_good_1.fq\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201clibrary|unaligned_file\u201d: \u201cfalse\u201d,\n\u201clibrary|aligned_file\u201d: \u201cfalse\u201d,\n\u201creference_genome|source\u201d: \u201chistory\u201d,\n\u201creference_genome|own_file\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33ae87e4fc510d80ae4\u201d,\n\u201chid\u201d: 3,\n\u201cname\u201d: \u201cWS220.64.fa\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201crg|rg_selector\u201d: \u201cset\u201d,\n\u201crg|read_group_id_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_id_conditional|ID\u201d: \u201c001\u201d,\n\u201crg|read_group_sm_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_sm_conditional|SM\u201d: \u201cyq565\u201d,\n\u201crg|PL\u201d: \u201cILLUMINA\u201d,\n\u201crg|read_group_lb_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_lb_conditional|LB\u201d: \u201c\u201d,\n\u201crg|CN\u201d: \u201c\u201d,\n\u201crg|DS\u201d: \u201c\u201d,\n\u201crg|DT\u201d: \u201c\u201d,\n\u201crg|FO\u201d: \u201c\u201d,\n\u201crg|KS\u201d: \u201c\u201d,\n\u201crg|PG\u201d: \u201c\u201d,\n\u201crg|PI\u201d: \u201c\u201d,\n\u201crg|PU\u201d: \u201c\u201d,\n\u201canalysis_type|analysis_type_selector\u201d: \u201csimple\u201d,\n\u201canalysis_type|presets\u201d: \u201cno_presets\u201d,\n\u201csam_options|sam_options_selector\u201d: \u201cno\u201d,\n\u201csave_mapping_stats\u201d: \u201cfalse\u201d\n}\n}"}, {"role": "system", "text": "Dear @dkbryant513,\nThere might be different reasons for that. Please try to run bowtie2 again on your datasets and maybe wait half an hour and tr again, to see if it is a server error.\nIf the error persists, then check if you data is correct, meaning, your input data might be corrupted or not in the correct format. Or an option in the tool is wrong.\nHave a good weekeend and best wishes,\nFlorian"}], [{"role": "user", "text": "Hi,\nWhen I try to browse through my files to select input files for a tool, I get the error message \u2018Internal Server Error (500)\u2019.\nKind regards,\nLou"}, {"role": "system", "text": "Hello @lou\nThe UseGalaxy.eu server is up and working without known issues: https://status.galaxyproject.org/\nPlease try again. Sometimes a server will reject connections temporarily due to being busy, or restarting.\nIf that is not enough, check the input datasets. Is there anything special about them? Can you expand the datasets and review the contents? Were the data fully uploaded with a datatype assigned? If you directly assigned that datatype, instead of allowing Galaxy to detect it, you could try re-autodetecting the datatype. You might want to do that anyway to see if it resolve the problem. #detecting-the-datatype-file-format\nLet\u2019s start there. If you still need more help after checking on your own, please post back a share link to your history publicly as a reply or ask for a moderator to set up a direct message to share it in privately. #sharing-your-history"}], [{"role": "user", "text": "Dear Galaxy,\nI noticed that HTML outputs from QUAST and antismash can\u2019t be viewed in a browser (Safari or Chrome) while it works for FastQC. It might be the case for other tools, I didn\u2019t test (e.g, MultiQC).\nCould be a whitelisting issue.\nIt would be cool if Galaxy would manage this automatically.\nThanks for you fantastic work.\nLaurent"}, {"role": "system", "text": "Hi @Laurent_Falquet\nYes, those two needed to be added to the AllowList. I just did that at UseGalaxy.org. Please test it out. Should work on data that is already generated \u2013 but you might need to reload the browser page first.\n\n\n\n Laurent_Falquet:\n\nIt would be cool if Galaxy would manage this automatically\n\n\nAgree! But we like to have a person review even if sometimes these get missed. Just let us know if you run into this again. Including/confirming the server URL is super helpful.\nThanks "}], [{"role": "user", "text": "Dear support team at GALAXY,\nI am trying to perform a RNA-seq analysis for D. melanogaster. I did the mapping with Bowtie2, using the genome assembly: dmelo_r6.32 and all were fine, nice mapping stats etc. Then, I tried to generate the necessary matrices for the DEseq tool via using the tool HTseq:\nWhen I used the corresponding .GFF ( dmel-all-r6.32.gff.gz ) for this particular assembly, I got the error below:\n\nWhen I used the .GFF for the assembly 6 from the NCBI, then I got the error below:\n\nimage770\u00d7272 38.4 KB\n\nAny input on how to overcome this obstacle in my analysis will be highly appreciated.\nManolis"}, {"role": "system", "text": "Hi @Manolis1,\ncould you share the history with me? I\u2019ll have a look. My email is: \nRegards"}, {"role": "user", "text": "Hi,\nThank you very much for your response and my apologies for my delayed answer.\nI have resolved the issue by using a different (an older) assembly genome file and its corresponding GFF.\nGreetings,\nManolis"}], [{"role": "user", "text": "I keep getting errors when trimming my data with Cutadapt. Please find the error below.\nI ran exactly the same thing last week with the galaxy.org server and there it worked.\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\n{\n\u201chistory_id\u201d: \u201cff75df6a1c92239a\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.0+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c4.0+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201clibrary|type\u201d: \u201cpaired_collection\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c72822dfb6c647067\u201d,\n\u201chid\u201d: 21,\n\u201cname\u201d: \u201cPaired_List_NAPC2_FASTA\u201d,\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: ,\n\u201cmap_over_type\u201d: \u201cpaired\u201d\n}\n],\n\u201cbatch\u201d: true\n},\n\u201clibrary|r1|cut\u201d: \u201c0\u201d,\n\u201clibrary|r2|cut2\u201d: \u201c0\u201d,\n\u201clibrary|r2|quality_cutoff2\u201d: \u201c\u201d,\n\u201cadapter_options|action\u201d: \u201ctrim\u201d,\n\u201cadapter_options|internal\u201d: \u201c\u201d,\n\u201cadapter_options|error_rate\u201d: \u201c0.1\u201d,\n\u201cadapter_options|no_indels\u201d: \u201cfalse\u201d,\n\u201cadapter_options|times\u201d: \u201c1\u201d,\n\u201cadapter_options|overlap\u201d: \u201c3\u201d,\n\u201cadapter_options|match_read_wildcards\u201d: \" \",\n\u201cadapter_options|revcomp\u201d: \u201cfalse\u201d,\n\u201cfilter_options|discard_trimmed\u201d: \u201cfalse\u201d,\n\u201cfilter_options|discard_untrimmed\u201d: \u201cfalse\u201d,\n\u201cfilter_options|minimum_length\u201d: \u201c20\u201d,\n\u201cfilter_options|maximum_length\u201d: \u201c\u201d,\n\u201cfilter_options|length_R2_options|length_R2_status\u201d: \u201cFalse\u201d,\n\u201cfilter_options|max_n\u201d: \u201c\u201d,\n\u201cfilter_options|pair_filter\u201d: \u201cany\u201d,\n\u201cfilter_options|max_expected_errors\u201d: \u201c\u201d,\n\u201cfilter_options|discard_cassava\u201d: \u201cfalse\u201d,\n\u201cread_mod_options|quality_cutoff\u201d: \u201c20\u201d,\n\u201cread_mod_options|nextseq_trim\u201d: \u201c0\u201d,\n\u201cread_mod_options|trim_n\u201d: \u201cfalse\u201d,\n\u201cread_mod_options|strip_suffix\u201d: \u201c\u201d,\n\u201cread_mod_options|shorten_options|shorten_values\u201d: \u201cFalse\u201d,\n\u201cread_mod_options|length_tag\u201d: \u201c\u201d,\n\u201cread_mod_options|rename\u201d: \u201c\u201d,\n\u201cread_mod_options|zero_cap\u201d: \u201cfalse\u201d,\n\u201coutput_selector\u201d: [\n\u201creport\u201d\n]\n}\n}"}, {"role": "system", "text": "Hi @Jens_Posma,\ncould you try again by using the tool version 3.7? It is the version available in usegalaxy.org. Perhaps there\u2019s a bug in the last update (version 4.0).\nRegards"}], [{"role": "user", "text": "I\u2019m doing the NGS tutorial from NGS data logistics.\nAt one of the steps, I have to import a genome NC_045512.2 from Genbank via a link:\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.fna.gz\n\nI get a warning: \u201cUnsupported Media type (415)\u201d and the upload doesn\u2019t happen. It looks like this:\n\nimage899\u00d7549 22.2 KB\n\nI check and the link itself works if I just put it into my browser.\nMy history is here: Galaxy | Europe | Accessible History | Galaxy Introduction Part 2\nThank you,\nPaul"}, {"role": "system", "text": "This looks like .*gz files are failing to download under Chromium \u00b7 Issue #12330 \u00b7 galaxyproject/galaxy \u00b7 GitHub.\nThis is usually temporary, but of so far unknown cause. You could actually help us debug this if you could go to the developer console of your browser (with F12) and get us query and response details while you have this error from the \u201cNetwork\u201d tab.\nThanks in advance!"}], [{"role": "user", "text": "Hi all,\nI\u2019m getting an error when using the last step of the LEfSe galaxy module ( Plot LEfSe results). Error text posted below\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 151, in\nelse: plot_histo_hor(params[\u2018output_file\u2019],params,data,len(data[\u2018cls\u2019]) == 2)\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 54, in plot_histo_hor\nif bcl: data[\u2018rows\u2019].sort(key=lambda ab: fabs(float(ab[3]))*(cls.index(ab[2]) 2-1))\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 54, in\nif bcl: data[\u2018rows\u2019].sort(key=lambda ab: fabs(float(ab[3])) (cls.index(ab[2])*2-1))\nValueError: invalid literal for float(): 3wo\nI\u2019ve encountered the same error before, but that was due to having more than 2 classes in the data. This set only has 2, and otherwise worked for a different subset that I entered just a few minutes prior.\nAny help would be great, thank you so much!"}, {"role": "system", "text": "Hi @Kevin_Rey\n\n\n\n Kevin_Rey:\n\nValueError: invalid literal for float(): 3wo\n\n\nThis part of the error message indicates that there is a problem with the input data to this step. Maybe check your data/settings again? Maybe the wrong column of data was selected for plotting. Or, there is an unexpected header?\nYou are also using a version of the tool that was sourced from the Test ToolShed, which means that it might not be \u201cproduction-ready\u201d or have unresolved development bugs. A better choice would be to install and use the versions of the tool suite from the Main ToolShed: https://toolshed.g2.bx.psu.edu/\nHope that helps!"}], [{"role": "user", "text": "Dear all,\nhow can I find the read length in a fq.gz file for RNA Star?\nThanks in advance,\nBernhard"}, {"role": "system", "text": "@Bernhard , are you talking about Compute sequence length ?"}], [{"role": "user", "text": "I have files in .loom format from a very large single cell RNA seq experiment, and I would like to try to analyse it in Galaxy. I was happy to see that in the upload page, it asks what kind of file you are uploading, and the dropdown menu has .Loom in the list. However, there are absolutely no built-in tools to do anything with it.\nHas anyone ever used Galaxy for loom files, and if so, can you provide any guidance?"}, {"role": "system", "text": "If there are no tools you can use on usegalaxy.org there might be tools available on Galaxy toolshed: https://toolshed.g2.bx.psu.edu/ which you can install in any Galaxy you are an admin of.\nMaybe checkout https://bio.tools/?page=1&q=loom&sort=score to explore what is available. Is there a specific software you have in mind?"}, {"role": "system", "text": "Hi there,\nI am currently incorporating LOOM support into a single-cell RNA tool RaceID here.\nThere has been much talk of incorporating LOOM support into many of the scRNA tools that will be present in galaxy, as evidenced by this thread here.\nPlease stay tuned for updates throughout the year!"}], [{"role": "user", "text": "Hi, I am trying to have DEGs but I put all files .sf and I executed that but these process don\u2019t started to run. I deleted all information that I don\u2019t need to free space but is the same, could you help me please?\nthank you very much"}, {"role": "system", "text": "Update\nDelays are confirmed. See the post below for details.\n\nHi @Guerande29\nAt least two of the public servers have been busy for other people.\nFor usegalaxy.org, we\u2019ll post any updates here: Fastp not running: Confirmed job delays at UseGalaxy.org 09/23/2022. Solution: allow queued jobs to process\n\n\n\n Guerande29:\n\nI deleted all information that I don\u2019t need to free space but is the same\n\n\nHow much free storage space an account has is unrelated to how jobs queue and process.\n\nmy-jobs-arent-running\nunderstanding-job-statuses\n"}], [{"role": "user", "text": "Hi,\nI am trying to configure Slurm following the tutorial from may 2022 (I am using an older version of the tutorial since I am using an older version, version 22.01, of Galaxy as well) (admin tutorial: Connecting Galaxy to a compute cluster). Here I saw that the tutorial for connecting to a compute cluster contained some setting for Singularity, but Singularity wasn\u2019t part of the requirements and isn\u2019t installed on my server. I am wondering if I could just skip these lines containing the setting for singularity or if it is necessary to install singularity.\nBesides this, I am quite lost on how to exactly set the usage of CPU and RAM for each tool, since this is my first time using Slurm (or any workload manager/job scheduler for that matter). Can I just create new partitions and set these partitions for the destination id together with slurm as the runner? And what is the param id? I had the following settings in mind for Slurm, but am wondering if this would actually work or not.\nAny feedback/help is appreciated! Thanks in advance!\nParts of the files containing my settings:\nrequirements.yml\n- src: galaxyproject.repos\n  version: 0.0.2\n- src: galaxyproject.slurm\n  version: 0.1.3\n\ngalaxy.yml\n roles:\n    # SLURM\n    - galaxyproject.repos\n    - galaxyproject.slurm\n\ngalaxyservers.yml\n# Slurm\nslurm_roles: ['controller', 'exec']\nslurm_nodes:\n- name: localhost \n  CPUs: 14 # Host has 16 cores total\n  RealMemory: 110000 # 110000MB, viewed with command 'free --mega' which shows 135049MB total free\n  ThreadsPerCore: 1\n# slurm_partitions:\n#  - name: main\n#    Nodes: localhost\n#    Default: YES\nslurm_config:\n  SlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2\n  SelectType: select/cons_res\n  SelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node\n\njob_conf.xml.j1\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"local_plugin\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\"/>\n            <param id=\"drmaa_library_path\">/usr/lib/slurm-drmaa/lib/libdrmaa.so.1</param>\n    </plugins>\n    <destinations default=\"slurm\">\n        <destination id=\"local_destination\" runner=\"local_plugin\"/>\n        <destination id=\"partition_1\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=1 --mem=4000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n        <destination id=\"partition_2\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=2 --mem=6000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n        <destination id=\"partition_3\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=4 --mem=16000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n    </destinations>\n    </limits>\n     <tools>\n        <tool id=\"tool_example_1\" destination=\"partition_1\"/>\n        <tool id=\"tool_example_2\" destination=\"partition_2\"/>\n        <tool id=\"tool_example_3\" destination=\"partition_3\"/>\n     </tools>\n</job_conf>\n"}, {"role": "system", "text": "Hi @cbass \u2013 it looks like you are getting help already.\nFor others reading, this chat is a great place to reach other Galaxy admins for quick discussion: https://matrix.to/#/#galaxyproject_admins:gitter.im\nRelated Jobs in workflow sometimes run, sometimes don't, on local galaxy 22.01 using slurm cluster"}, {"role": "system", "text": "\n\n\n cbass:\n\nbut Singularity wasn\u2019t part of the requirements and isn\u2019t installed on my server. I am wondering if I could just skip these lines containing the setting for singularity or if it is necessary to install singularity.\n\n\nYes you can just skip that then. It\u2019s not necessary, it\u2019s an additional thing.\n\n\n\n cbass:\n\nBesides this, I am quite lost on how to exactly set the usage of CPU and RAM for each tool, since this is my first time using Slurm (or any workload manager/job scheduler for that matter). Can I just create new partitions and set these partitions for the destination id together with slurm as the runner? And what is the param id? I had the following settings in mind for Slurm, but am wondering if this would actually work or not.\n\n\nyou are very much on the right path. Setting multiple destinations, each with their own submission parameters is the right answer. You\u2019re looking for the next tutorial in the series: Mapping Jobs to Destinations\nyour id=\"?\" should be \u201cnativeSpecification\u201d:\n<param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=2</param>\n\nAnd then it should be fine. Next you\u2019ll have to map specific tools to those specific destinations as you\u2019ve started doing. Good luck! Apologies for the delayed response."}], [{"role": "user", "text": "Hello, I\u2019m beginner of this Seq analysis starting with Galaxy.\nI have studied this about 2 months.\nI learned rna-seq analysis using galaxy through tutorial.\nReference-based RNA-Seq data analysis (galaxyproject.org)\nI followed the tutorial perfectly using data set provided by tutorial, not any error.\nbut I can not execute \u201cGoseq\u201d process with my data, before this process everything is fine.\n\nI checked my two input data ( [geneid, logic], [gene id, length])\nBut I checked my logic data after \u201ccompute (bool<0.05)\u201d became short line (55402 ->17081).\n\nI found the reason is my adj-p value of lots of genes are NA.\nI am not sure It is reason of error.\nbut, I downloaded my data to my computer as a excel format.\nand I intentionally changed NA to False and upload this file to galaxy history again.\n\nbecause I think the reason of Goseq error is not the same number of row between two input data.\nBut It is also not excuted.\nPlease help me."}, {"role": "system", "text": "\n\n\n Seungyeong:\n\nI found the reason is my adj-p value of lots of genes are NA.\n\n\nI have sort of the same problem. Trying with my data pin GOSEQ, and there is always an error, no matter what variation I try. I don\u2019t know why."}, {"role": "system", "text": "Hi @Brists and @Seungyeong\n\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: run_goseq \u2026 goseq \u2192 reversemapping \u2192 [ \u2192 [.table \u2192 NextMethod\n\nThe error means that the tool couldn\u2019t construct a matrix of the two inputs correctly. Check to make sure that the identifiers in the \u201cDifferentially expressed genes file\u201d are all contained in the \u201cGene lengths file\u201d and that the identifiers in the same sort order.\nAll this should already be correct if the steps in either of the tutorials below were all done. If for some reason you need to do that again, the files could be joined and split up again using the original tutorial methods or using manipulations explained in the last tutorial.\nExamples of accepted formats and manipulations are here. The pre-filters remove any rows with lower scores (and NA values) before the compute step: 3: RNA-seq genes to pathways and here Reference-based RNA-Seq data analysis\nThere should never be a need to download data, manipulate it, then load it back to Galaxy. Especially not using Excel as that can introduce many format issues including hidden characters. If you are not sure how to do text manipulations in Galaxy, see this tutorial  Data Manipulation Olympics"}], [{"role": "user", "text": "Hi,\nI would like to combine samples from different studies to do differential gene expression. For example, paper 1 has published 10 nasal samples from healthy children and 10 nasal samples from children with COVID infections; and paper 2 from a different research group has published 10 nasal samples with influenza infections. They may or may not have used the same sequencing platform/kits and seq depths.\nCan I download the samples from both papers and do the the following DEG/pathway analyses etc on the dataset:\nhealthy children from paper 1 (as Factor 1), COVID from paper 1 (as factor 2)\nhealthy children from paper 1 (as Factor 1) and influenza from paper 2 (as Factor 2)?\nCOVID from paper 1 (as Factor 1) and influenza from paper 2 (as factor 2)\nHealthy children (factor 1) vs COVID (factor 2) vs. Influenza (factor 3)\nSince I will be using the same bioinformatic tools and commands to process both sets in the same analysis and all transcripts will be aligned to the same assembly, will this normalize data correctly and allow for valid comparisons?\nThanks,\nSwati"}, {"role": "system", "text": "Hi @seq2022\nmaybe ask people at a bioinformatics forum or DGE software developers. How do you distinguish between batch effect and potential biological effect in the described scenario with one condition coming from a different source?\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello, I am unable to sign in on the GalaxyTrakr platform since it states that I have an invalid password. When I want to change it i get an error:\nFailed to submit an email. Please contact the administrator: (535, b__sq__5.7.8 Username and Password not accepted. Learn more atXn5.7.8 Check Gmail through other email platforms - Gmail Help ay34-20020a05620a17a200b006bad7a2964fsm9454685qkb.78 - gsmtp__sq__)\nIs it possible to fix the problem?\nThank you for your help.\nKatarina B"}, {"role": "system", "text": "I think you need to email them directly, see:\n  \n\n      galaxy-genometrakr-docs.s3.amazonaws.com\n  \n\n  \n    \n\nFAQs+for+GalaxyTrakr.pdf\n\n  476.13 KB\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Dear All,\nPlease inform,\nI have stringtie reads count table (-B from Ballgown function) generated of which I used to perform deseq2 analysis. Please inform me how to use read count as input in deseq2 in galaxy. Tximport is not working for data.\nThanks"}, {"role": "system", "text": "Hi @KMS_KMS,\nthe output generated when using the -B option in Stringtie cannot be used as input for DESeq2. The help section of the Strintie tool includes the instructions for using Stringtie + DESeq2:\n\nDESeq2, edgeR and limma are three popular Bioconductor packages for analyzing differential expression, which take as input a matrix of read counts mapped to particular genomic features (e.g., genes). This read count information can be extracted directly from the files generated by StringTie (run with the -e parameter) by selecting DESeq2/edgeR/limma-voom under Output files for differential expression? above. This uses the StringTie helper script prepDE.py to convert the GTF output from StringTie into two tab-delimited (TSV) files, containing the count matrices for genes and transcripts, using the coverage values found in the output of StringTie -e.\n\nRegards"}], [{"role": "user", "text": "Dear All,\nI am attempting to analyze a genic locus starting from Whole genome sequencing data. I am a newbie in this field (to date I have always used specific software that provided directly to give a final result.\nNow I need to analyze a specific genic sequence that I am not able to obtain with previously cited software, but I should use Galaxy (easier than linux with command line scripts).\nI have fastQ files (by Illumina paired end) that I have firsly analyzed by using Trimmomatic and then align (with BWA) to a reference genome. So, I have obtained a .bam file. I am attempting to obtain the consensus fasta sequence from this file\u2026.is it possible? o I am skipped some steps???\nI have been using Galaxy tools.\nThank you in advance for you help"}, {"role": "system", "text": "Hi - Is your goal to explore variation?\nIf so, please review the tools below and see these Galaxy tutorials for examples of complete analysis workflows that include intermediate tools.\n\n\nCommon tools to use from the step you are at now can be reviewed/used at Public Galaxy servers such as Galaxy Main https://usegalaxy.org, Galaxy EU https://usegalaxy.eu, and Galaxy AU https://usegalaxy.org.au (All Public Galaxy servers: https://galaxyproject.org/use/). Try searching for tools by these keywords: pileup or variant or vcf.\n\n\nThe Galaxy ToolShed https://usegalaxy.org/toolshed can also be searched for tools that can be used in your own Galaxy instance https://galaxyproject.github.io/.\n\n\nGTN Tutorials: Variant Analysis\n\n\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Variant Analysis\n\nGenetic differences (variants) between healthy and diseas...\n\n\n  \n  \n    \n    \n  \n  \n\n\n\nMore about how/where to use Galaxy GTN tutorials and the included tools/workflows for your own work.\n\n\n\n\nTroubleshooting resources for errors or unexpected results\n\n\nGalaxy Training Network Tutorials : Some GTN  tutorials are appropriate for Galaxy Main  and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon  to review the Public Galaxy server choices. If a tutorial is supported by a pre-configured Galaxy Docker training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n\nThanks!"}], [{"role": "user", "text": "Hi everyone,\nI would like to do some modification to a vcf (changing one column name in the vcf header starting with #). Can I just do this modification with TextEdit and save the vcf in its .vcf format or will this be problematic for downstream analysis?\nIn order to have the most privacy for my data possible I would like to set the settings accordingly. I read the the information about data privacy in Galaxy (https://galaxyproject.org/learn/privacy-features/) and I am wondering if this \u201cMake all data private\u201d in user preferences is what I would select and if there are more possible safety settings to change?\nThanks a lot.\nRose"}, {"role": "system", "text": "\n\n\n roselucia:\n\nI would like to do some modification to a vcf (changing one column name in the vcf header starting with #). Can I just do this modification with TextEdit and save the vcf in its .vcf format or will this be problematic for downstream analysis?\n\n\nIt depends on what exactly you are changing in the \u201c#\u201d header line. The first eight (or nine) columns are required metadata. If you are just updating a sample name, that should be fine. VCF format: https://vcftools.github.io/specs.html\n\n\n\n roselucia:\n\nIn order to have the most privacy for my data possible I would like to set the settings accordingly. I read the the information about data privacy in Galaxy (https://galaxyproject.org/learn/privacy-features/) and I am wondering if this \u201cMake all data private\u201d in user preferences is what I would select and if there are more possible safety settings to change?\n\n\nYes, that is how to make all data private, existing and newly uploaded."}], [{"role": "user", "text": "Hi all,\nit seems that the testtoolshed is still since a week and I need a repository that is on it.\nDo you know if a mirror exists somewhere please ?\nThank you"}, {"role": "system", "text": "Afaik there is no mirror, but we\u2019ll get it up shortly. Sorry for the inconvenience.\nedit: up now"}], [{"role": "user", "text": "Hi,\nI am creating a galaxy tool and I need to get as output a number of collections that depends on a value contained in an input file. I got the value from my file and did a for loop using this value. However, I still get only one collection as output. Is it possible to get a variable number of collections? And if it is possible, how can I do it?"}, {"role": "system", "text": "Hi  @aepetit,\nunfortunately, this is not possible.\nRegards"}, {"role": "user", "text": "Thanks @gallardoalba, I managed to work around the problem by using nested collections but the solution is not perfect."}], [{"role": "user", "text": "Hi,\nAfter successfully installing Ansible I have tried setting up my own Galaxy server on a VM. During the tutorial: \u201cGalaxy Installation with Ansible\u201d (Galaxy Installation with Ansible), I came across an error.\nI have added the uWSGI to the playbook (galaxyservers.yml). However, when I run the playbook (\"Installing Galaxy; Galaxy; point 5: Run the playbook: ansible-playbook galaxy.yml), I receive the following error message:\nubuntu@packer-Ubuntu-18:~/galaxy/group_vars$ ansible-playbook galaxy.yml\nPLAY [galaxyservers] **********************************************************************************************************************************************************************************************************************************************************\nTASK [Gathering Facts] ********************************************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [Install Dependencies] ***************************************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nincluded: /home/ubuntu/galaxy/group_vars/roles/galaxyproject.postgresql/tasks/debian.yml for 145.100.59.212\nTASK [galaxyproject.postgresql : Install pgdg package signing key (Debian/pgdg)] **********************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install pgdg repository (Debian/pgdg)] *******************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install PostgreSQL (Debian)] *****************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Get installed version] ***********************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set version fact] ****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set version fact] ****************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set OS-specific variables] *******************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set pgdata fact] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set conf dir fact] ***************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Create conf.d] *******************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Check for conf.d include in postgresql.conf] *************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set conf.d include in postgresql.conf] *******************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Include 25ansible_postgresql.conf in postgresql.conf] ****************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set config options] **************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install pg_hba.conf] *************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Ensure PostgreSQL is running] ****************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [natefoo.postgresql_objects : Revoke extra privileges] *******************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Drop databases] ****************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create and drop users] *********************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create groups] *****************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Add or remove users from groups] ***********************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Drop groups] *******************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create databases] **************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Grant user database privileges] ************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Grant extra privileges] ********************************************************************************************************************************************************************************************************************\nTASK [geerlingguy.pip : Ensure Pip is installed.] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [geerlingguy.pip : Ensure pip_install_packages are installed.] ***********************************************************************************************************************************************************************************************************\nTASK [galaxyproject.galaxy : Ensure that mutually exclusive options are not set] **********************************************************************************************************************************************************************************************\nok: [145.100.59.212] => {\n\u201cchanged\u201d: false,\n\u201cmsg\u201d: \u201cAll assertions passed\u201d\n}\nTASK [galaxyproject.galaxy : Set privilege separation default variables] ******************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.galaxy : Include layout vars] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.galaxy : Set any unset variables from layout defaults] ****************************************************************************************************************************************************************************************************\nfatal: [145.100.59.212]: FAILED! => {\u201cmsg\u201d: \u201cThe task includes an option with an undefined variable. The error was: \u2018galaxy_server_dir\u2019 is undefined\\n\\nThe error appears to be in \u2018/home/ubuntu/galaxy/group_vars/roles/galaxyproject.galaxy/tasks/layout.yml\u2019: line 7, column 3, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n- name: Set any unset variables from layout defaults\\n  ^ here\\n\u201d}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\n145.100.59.212             : ok=18   changed=0    unreachable=0    failed=1    skipped=17   rescued=0    ignored=0\nindent preformatted text by 4 spaces\nI speculate that it may have something to do with either the loop (with_items)  or the set_fact line in the layout.yml file. The variable galaxy_server_dir, however, is undefined whilst galaxy_venv_dir is not (see the added contents of the .yml files). Do you maybe have an idea what causes this error or how I am able to fix it?\nFor any information about the contents of the .yml files:\n\nPath to galaxy.yml\n\n#~/galaxy/groups_vars/galaxy.yml\n\nhosts: galaxyservers\nbecome: true\npre_tasks:\n\nname: Install Dependencies\npackage:\nname: [\u2018git\u2019, \u2018make\u2019, \u2018python3-psycopg2\u2019, \u2018virtualenv\u2019]\nroles:\ngalaxyproject.postgresql\nrole: natefoo.postgresql_objects\nbecome: true\nbecome_user: postgres\ngeerlingguy.pip\ngalaxyproject.galaxy\nrole: uchida.miniconda\nbecome: true\nbecome_user: galaxy\n\n\n\n\nPath to galaxyservers.yml\n\n~/galaxy/groups_vars/galaxyservers.yml\n\npython3 support\npip_virtualenv_command: /usr/bin/python3 -m virtualenv # usegalaxy_eu.certbot, usegalaxy_eu.tiaas2, galaxyproject.galaxy\ncertbot_virtualenv_package_name: python3-virtualenv    # usegalaxy_eu.certbot\npip_package: python3-pip                               # geerlingguy.pip\n\nPostgreSQL\npostgresql_objects_users:\n\nname: galaxy\npostgresql_objects_databases:\nname: galaxy\nowner: galaxy\n\n\nGalaxy\ngalaxy_create_user: true\ngalaxy_separate_privileges: true\ngalaxy_manage_paths: true\ngalaxy_layout: root-dir\ngalaxy_root: /srv/galaxy\ngalaxy_user: {name: galaxy, shell: /bin/bash}\ngalaxy_commit_id: release_20.01\ngalaxy_config_style: yaml\ngalaxy_force_checkout: true\nminiconda_prefix: \u201c{{ galaxy_tool_dependency_dir }}/_conda\u201d\nminiconda_version: \u201c4.6.14\u201d\ngalaxy_config:\ngalaxy:\nbrand: \u201c@@@@@\u201d\nadmin_users: @@@@@@@@@@@@@@@@@@@\ndatabase_connection: \u201cpostgresql:///galaxy?host=/var/run/postgresql\u201d\nfile_path: /data\ncheck_migrate_tools: false\ntool_data_path: \u201c{{ galaxy_mutable_data_dir }}/tool-data\u201d\nuwsgi:\nhttp: 0.0.0.0:8080\nbuffer-size: 16384\nprocesses: 1\nthreads: 4\noffload-threads: 2\nstatic-map:\n- /static/style={{ galaxy_server_dir }}/static/style/blue\n- /static={{ galaxy_server_dir }}/static\n- /favicon.ico={{ galaxy_server_dir }}/static/favicon.ico\nstatic-safe: client/galaxy/images\nmaster: true\nvirtualenv: \u201c{{ galaxy_venv_dir }}\u201d\npythonpath: \u201c{{ galaxy_server_dir }}/lib\u201d\nmodule: galaxy.webapps.galaxy.buildapp:uwsgi_app()\nthunder-lock: true\ndie-on-term: true\nhook-master-start:\n- unix_signal:2 gracefully_kill_them_all\n- unix_signal:15 gracefully_kill_them_all\npy-call-osafterfork: true\nenable-threads: true\nmule:\n- lib/galaxy/main.py\n- lib/galaxy/main.py\nfarm: job-handlers:1,2\n\nPath to layout.yml\n\n~//galaxy/groups_vars/roles/galaxyproject.galaxy/tasks/layout.yml\n\n\nConfigure any unset variables using layout defaults\n\n\nname: Include layout vars\ninclude_vars: \u201clayout-{{ galaxy_layout | default(\u2018legacy\u2019) }}.yml\u201d\n\n\nname: Set any unset variables from layout defaults\nset_fact:\n\u201c{{ item }}\u201d: \u201c{{ lookup(\u2018vars\u2019, \u2018__\u2019 ~ item) }}\u201d\nwhen: item not in vars\nwith_items:\n\ngalaxy_venv_dir\ngalaxy_server_dir\ngalaxy_config_dir\ngalaxy_mutable_data_dir\ngalaxy_mutable_config_dir\ngalaxy_shed_tools_dir\ngalaxy_cache_dir\ngalaxy_local_tools_dir\n\n\n\nname: Check that any explicitly set Galaxy config options match the values of explicitly set variables\nassert:\nthat:\n- lookup(\u2018vars\u2019, \u2018galaxy_\u2019 ~ item) == galaxy_config[galaxy_app_config_section][item]\nmsg: \u201cValue of \u2018{{ \u2018galaxy_\u2019 ~ item }}\u2019 does not match value of \u2018{{ item }}\u2019 in galaxy_config: {{ lookup(\u2018vars\u2019, \u2018galaxy_\u2019 ~ item) }} != {{ galaxy_config[galaxy_app_config_section][item] }}\u201d\n\nTODO: requires Ansible 2.7\n#success_msg: \u201cValue of \u2018{{ \u2018galaxy_\u2019 ~ item }}\u2019 matches galaxy_config value of \u2018{{ item }}\u2019 in galaxy_config\u201d\nwhen: \u201c\u2018galaxy_\u2019 ~ item in vars and item in ((galaxy_config | default({}))[galaxy_app_config_section] | default({}))\u201d\nwith_items:\n\ntool_dependency_dir\nfile_path\njob_working_directory\nshed_tool_config_file\n\n\n\nname: Set any unset variables corresponding to Galaxy config options from galaxy_config or layout defaults\nset_fact:\n\u201c{{ \u2018galaxy_\u2019 ~ item }}\u201d: \u201c{{ ((galaxy_config | default({}))[galaxy_app_config_section] | default({}))[item] | default(lookup(\u2018vars\u2019, \u2018galaxy\u2019 ~ item)) }}\"\nwhen: \"'galaxy\u2019 ~ item not in vars\u201d\nwith_items:\n\ntool_dependency_dir\nfile_path\njob_working_directory\n\n\n\nThank you in advance!"}, {"role": "system", "text": "Hi,\nI did the same Galaxy tutorial as you and I got the same error message.\nI simply added the galaxy_server_dir variable declaration in the galaxy.yml file :\n- hosts: galaxyservers\n\u2002vars:\n\u2002 \u2002 galaxy_server_dir: /srv/galaxy\n\u2002become: true\n\u2002pre_tasks:\n\u2002 \u2002 - name: Install Dependencies\n\u2026\nand it is working.\nHave a good day,\nL. Burlot"}], [{"role": "user", "text": "Hi,\nI am in the process of mapping my reads to the genome using Tophat in galaxy. However, I see that the Tophat tool has been deprecated in the galaxy platform. Can I still use it to perform my analysis or should I use HISAT2?"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI,\nTophat developers recommend using HISAT2 (developed by the same group), as they state that it is more accurate and much more efficient.\nRegards"}], [{"role": "user", "text": "Hi everyone,\nI am using snpsift rminfo on EU and annotate the variants with snpeff eff annotate afterwards. Before I use vcftotab-delimited in order to parse my data  (the great EU Team made this tool working for my data on Galaxy EU- MANY THANKS!), I would like to get rid of some vcf headerlines with the info tags from the original annotation, where there is no data for the info tag existent anymore. Otherwise I would end up with various empty columns using vcftotab-delimited. I tried a couple thinks and it is a possibility to use use the tool Cut  columns from a table (cut) (Galaxy Version 1.1.0) AFTER using vcftotab-delimited. However, as in my workflow I annotate my variants with lots of further information I would prefer to keep the vcf clean and amount of headerlines small. So I would rather find a tool to remove specific header lines BEFORE I used vcftotab in the vcf itself.\nBelow is  a list of the headers I would like to remove!\nMany thanks!\nAll the best,\nRose\n##SnpSiftVersion=\u201cSnpSift 4.2 (build 2015-12-05), by Pablo Cingolani\u201d\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/common_all_20160601.vcf.gz 5BC_S15.temp0.vcf\u201d\n##INFO=<ID=CDA,Number=0,Type=Flag,Description=\u201cVariation is interrogated in a clinical diagnostic assay\u201d>\n##INFO=<ID=OTH,Number=0,Type=Flag,Description=\u201cHas other variant with exactly the same set of mapped positions on NCBI refernce assembly.\u201d>\n##INFO=<ID=S3D,Number=0,Type=Flag,Description=\u201cHas 3D structure - SNP3D table\u201d>\n##INFO=<ID=WTD,Number=0,Type=Flag,Description=\u201cIs Withdrawn by submitter If one member ss is withdrawn by submitter, then this bit is set.  If all member ss\u2019 are withdrawn, then the rs is deleted to SNPHistory\u201d>\n##INFO=<ID=dbSNPBuildID,Number=1,Type=Integer,Description=\u201cFirst dbSNP Build for RS\u201d>\n##INFO=<ID=SLO,Number=0,Type=Flag,Description=\u201cHas SubmitterLinkOut - From SNP->SubSNP->Batch.link_out\u201d>\n##INFO=<ID=NSF,Number=0,Type=Flag,Description=\u201cHas non-synonymous frameshift A coding region variation where one allele in the set changes all downstream amino acids. FxnClass = 44\u201d>\n##INFO=<ID=R3,Number=0,Type=Flag,Description=\u201cIn 3\u2019 gene region FxnCode = 13\u201d>\n##INFO=<ID=R5,Number=0,Type=Flag,Description=\u201cIn 5\u2019 gene region FxnCode = 15\u201d>\n##INFO=<ID=NSN,Number=0,Type=Flag,Description=\u201cHas non-synonymous nonsense A coding region variation where one allele in the set changes to STOP codon (TER). FxnClass = 41\u201d>\n##INFO=<ID=NSM,Number=0,Type=Flag,Description=\u201cHas non-synonymous missense A coding region variation where one allele in the set changes protein peptide. FxnClass = 42\u201d>\n##INFO=<ID=G5A,Number=0,Type=Flag,Description=\">5% minor allele frequency in each and all populations\">\n##INFO=<ID=COMMON,Number=1,Type=Integer,Description=\u201cRS is a common SNP.  A common SNP is one that has at least one 1000Genomes population with a minor allele of frequency >= 1% and for which 2 or more founders contribute to that minor allele frequency.\u201d>\n##INFO=<ID=RS,Number=1,Type=Integer,Description=\u201cdbSNP ID (i.e. rs number)\u201d>\n##INFO=<ID=RV,Number=0,Type=Flag,Description=\u201cRS orientation is reversed\u201d>\n##INFO=<ID=TPA,Number=0,Type=Flag,Description=\u201cProvisional Third Party Annotation(TPA) (currently rs from PHARMGKB who will give phenotype data)\u201d>\n##INFO=<ID=CFL,Number=0,Type=Flag,Description=\u201cHas Assembly conflict. This is for weight 1 and 2 variant that maps to different chromosomes on different assemblies.\u201d>\n##INFO=<ID=GNO,Number=0,Type=Flag,Description=\u201cGenotypes available. The variant has individual genotype (in SubInd table).\u201d>\n##INFO=<ID=VLD,Number=0,Type=Flag,Description=\u201cIs Validated.  This bit is set if the variant has 2+ minor allele count based on frequency or genotype data.\u201d>\n##INFO=<ID=ASP,Number=0,Type=Flag,Description=\u201cIs Assembly specific. This is set if the variant only maps to one assembly\u201d>\n##INFO=<ID=ASS,Number=0,Type=Flag,Description=\u201cIn acceptor splice site FxnCode = 73\u201d>\n##INFO=<ID=REF,Number=0,Type=Flag,Description=\u201cHas reference A coding region variation where one allele in the set is identical to the reference sequence. FxnCode = 8\u201d>\n##INFO=<ID=U3,Number=0,Type=Flag,Description=\u201cIn 3\u2019 UTR Location is in an untranslated region (UTR). FxnCode = 53\u201d>\n##INFO=<ID=U5,Number=0,Type=Flag,Description=\u201cIn 5\u2019 UTR Location is in an untranslated region (UTR). FxnCode = 55\u201d>\n##INFO=<ID=WGT,Number=1,Type=Integer,Description=\u201cWeight, 00 - unmapped, 1 - weight 1, 2 - weight 2, 3 - weight 3 or more\u201d>\n##INFO=<ID=MTP,Number=0,Type=Flag,Description=\u201cMicroattribution/third-party annotation(TPA:GWAS,PAGE)\u201d>\n##INFO=<ID=LSD,Number=0,Type=Flag,Description=\u201cSubmitted from a locus-specific database\u201d>\n##INFO=<ID=NOC,Number=0,Type=Flag,Description=\u201cContig allele not present in variant allele list. The reference sequence allele at the mapped position is not present in the variant allele list, adjusted for orientation.\u201d>\n##INFO=<ID=DSS,Number=0,Type=Flag,Description=\u201cIn donor splice-site FxnCode = 75\u201d>\n##INFO=<ID=SYN,Number=0,Type=Flag,Description=\u201cHas synonymous A coding region variation where one allele in the set does not change the encoded amino acid. FxnCode = 3\u201d>\n##INFO=<ID=KGPhase3,Number=0,Type=Flag,Description=\u201c1000 Genome phase 3\u201d>\n##INFO=<ID=CAF,Number=.,Type=String,Description=\u201cAn ordered, comma delimited list of allele frequencies based on 1000Genomes, starting with the reference allele followed by alternate alleles as ordered in the ALT column. Where a 1000Genomes alternate allele is not in the dbSNPs alternate allele set, the allele is added to the ALT column.  The minor allele is the second largest value in the list, and was previuosly reported in VCF as the GMAF.  This is the GMAF reported on the RefSNP and EntrezSNP pages and VariationReporter\u201d>\n##INFO=<ID=VC,Number=1,Type=String,Description=\u201cVariation Class\u201d>\n##INFO=<ID=MUT,Number=0,Type=Flag,Description=\u201cIs mutation (journal citation, explicit fact): a low frequency variation that is cited in journal and other reputable sources\u201d>\n##INFO=<ID=KGPhase1,Number=0,Type=Flag,Description=\u201c1000 Genome phase 1 (incl. June Interim phase 1)\u201d>\n##INFO=<ID=NOV,Number=0,Type=Flag,Description=\u201cRs cluster has non-overlapping allele sets. True when rs set has more than 2 alleles from different submissions and these sets share no alleles in common.\u201d>\n##INFO=<ID=VP,Number=1,Type=String,Description=\u201cVariation Property.  Documentation is at ftp://ftp.ncbi.nlm.nih.gov/snp/specs/dbSNP_BitField_latest.pdf\u201d>\n##INFO=<ID=SAO,Number=1,Type=Integer,Description=\u201cVariant Allele Origin: 0 - unspecified, 1 - Germline, 2 - Somatic, 3 - Both\u201d>\n##INFO=<ID=GENEINFO,Number=1,Type=String,Description=\u201cPairs each of gene symbol:gene id.  The gene symbol and id are delimited by a colon ( and each pair is delimited by a vertical bar (|)\u201d>\n##INFO=<ID=INT,Number=0,Type=Flag,Description=\u201cIn Intron FxnCode = 6\u201d>\n##INFO=<ID=G5,Number=0,Type=Flag,Description=\">5% minor allele frequency in 1+ populations\">\n##INFO=<ID=OM,Number=0,Type=Flag,Description=\u201cHas OMIM/OMIA\u201d>\n##INFO=<ID=PMC,Number=0,Type=Flag,Description=\u201cLinks exist to PubMed Central article\u201d>\n##INFO=<ID=SSR,Number=1,Type=Integer,Description=\u201cVariant Suspect Reason Codes (may be more than one value added together) 0 - unspecified, 1 - Paralog, 2 - byEST, 4 - oldAlign, 8 - Para_EST, 16 - 1kg_failed, 1024 - other\u201d>\n##INFO=<ID=RSPOS,Number=1,Type=Integer,Description=\u201cChr position reported in dbSNP\u201d>\n##INFO=<ID=HD,Number=0,Type=Flag,Description=\u201cMarker is on high density genotyping kit (50K density or greater).  The variant may have phenotype associations present in dbGaP.\u201d>\n##INFO=<ID=PM,Number=0,Type=Flag,Description=\u201cVariant is Precious(Clinical,Pubmed Cited)\u201d>\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/CosmicAllMuts_v69_20140602.vcf.gz 5BC_S15.temp1.vcf\u201d\n##INFO=<ID=CDS,Number=1,Type=String,Description=\u201cCDS annotation\u201d>\n##INFO=<ID=AA,Number=1,Type=String,Description=\u201cPeptide annotation\u201d>\n##INFO=<ID=GENE,Number=1,Type=String,Description=\u201cGene name\u201d>\n##INFO=<ID=CNT,Number=1,Type=Integer,Description=\u201cHow many samples have this mutation\u201d>\n##INFO=<ID=STRAND,Number=1,Type=String,Description=\u201cGene strand\u201d>\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/clinvar_20160531.vcf.gz 5BC_S15.temp0.vcf\u201d\n##INFO=<ID=CLNSIG,Number=.,Type=String,Description=\u201cVariant Clinical Significance, 0 - Uncertain significance, 1 - not provided, 2 - Benign, 3 - Likely benign, 4 - Likely pathogenic, 5 - Pathogenic, 6 - drug response, 7 - histocompatibility, 255 - other\u201d>\n##INFO=<ID=CLNALLE,Number=.,Type=Integer,Description=\u201cVariant alleles from REF or ALT columns.  0 is REF, 1 is the first ALT allele, etc.  This is used to match alleles with other corresponding clinical (CLN) INFO tags.  A value of -1 indicates that no allele was found to match a corresponding HGVS allele name.\u201d>\n##INFO=<ID=CLNORIGIN,Number=.,Type=String,Description=\u201cAllele Origin. One or more of the following values may be added: 0 - unknown; 1 - germline; 2 - somatic; 4 - inherited; 8 - paternal; 16 - maternal; 32 - de-novo; 64 - biparental; 128 - uniparental; 256 - not-tested; 512 - tested-inconclusive; 1073741824 - other\u201d>\n##INFO=<ID=CLNSRC,Number=.,Type=String,Description=\u201cVariant Clinical Chanels\u201d>\n##INFO=<ID=CLNREVSTAT,Number=.,Type=String,Description=\u201cno_assertion - No assertion provided, no_criteria - No assertion criteria provided, single - Criteria provided single submitter, mult - Criteria provided multiple submitters no conflicts, conf - Criteria provided conflicting interpretations, exp - Reviewed by expert panel, guideline - Practice guideline\u201d>\n##INFO=<ID=CLNDSDB,Number=.,Type=String,Description=\u201cVariant disease database name\u201d>\n##INFO=<ID=CLNACC,Number=.,Type=String,Description=\u201cVariant Accession and Versions\u201d>\n##INFO=<ID=CLNDBN,Number=.,Type=String,Description=\u201cVariant disease name\u201d>\n##INFO=<ID=CLNDSDBID,Number=.,Type=String,Description=\u201cVariant disease database ID\u201d>\n##INFO=<ID=CLNHGVS,Number=.,Type=String,Description=\u201cVariant names from HGVS.    The order of these variants corresponds to the order of the info in the other clinical  INFO tags.\u201d>\n##INFO=<ID=CLNSRCID,Number=.,Type=String,Description=\u201cVariant Clinical Channel IDs\u201d>"}, {"role": "system", "text": "Hi Rose,\nit shouldn\u2019t surprise you at this point that the answer lies in yet another regexp and the Select lines tool https://usegalaxy.eu/root?tool_id=Grep1\nYou want to keep all lines NOT Matching a pattern like:\n^##INFO=<ID=((ANN)|(DP)|(WHATEVER)|(AND_SO_ON))"}], [{"role": "user", "text": "Hi everyone,\nI can\u2019t upload all of my .gff local data. Some dataset could not be read and it seems to occur accidentally.\nThe error info is\nDataset Error Report\nAn error occurred while running the tool DATA_FETCH.\nAmong 246 files, only half past of the data are correctly uploaded by the server, whereas the remaing failed. I don\u2019t understand the reason because the gff files are in the same format and were downloaded from the same repository. The correct and uncorrect upload seems to be random.\nHow can I do?\nThank you"}, {"role": "system", "text": "Hi @laura_95\nmaybe try upload in small(er) batches or upload datasets from failed jobs again.\nHope that helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all,\nWhen I mapped my RNAseq fastQ files using STAR in Galaxy, I got only one mapped.bam file. I was wondering if there is any way I could also get a  Aligned.toTranscriptome.out.bam, which you would get from the STAR mapping using a command line platform.\nMany thanks,\nKhanh"}, {"role": "system", "text": "Hi Khanh,\nthe Galaxy tool version of STAR installed on the big public servers does not let you produce that file.\nTheir is an updated version available from the toolshed (so you could install it into your own instance of Galaxy right now), which supports this, but it will still take a bit of time before the new version makes it to public servers (the new version is based on STAR 2.7 so needs new genome indices built before people can really use it productively).\nBest,\nWolfgang"}], [{"role": "user", "text": "Hi there,\nI have a dataset of 3Rad data, which have been created by using two pairs of enzymes (pstI/mspI and NsiI/MspI). I used to use Stacks but I now realize it only allows a maximum of 2 enzymes in process radtags. Does anyone know of an other tool or a way to pass by this problem? My first thought was to do 2 consecutive runs in process radtags, one for each pairs, but I\u2019m afraid it will leave residues in my dataset.\nThank you!"}, {"role": "system", "text": "Hi @Joelle99,\nI checked the Stacks documentation, and I think that it is not possible to provide three enzymes as input.\nRegards"}], [{"role": "user", "text": "Hi everyone,\nI have a BAM file in the following format (some of the data).\n\ucea1\ucc9811088\u00d7675 150 KB\nI would like to subset my BAM via specific cell barcode sequences (e.g, AAAGAACCAGGACATG, AACCAACAGAGATTCA, GCCATTCTCACAGTGT) into a new BAM.\nIs there any way to do so efficiently?\nIt would be great if someone knew how to do this.\nStay healthy.\nMany thanks,\nMiae"}, {"role": "system", "text": "Hi Miae\nIf you are just interested in a particular cell barcode, have you tried \u201cSelect lines that match an expression\u201d (https://usegalaxy.eu/root?tool_id=Grep1), assuming your file is actually in SAM format.\nRegards, Hans-Rudolf"}], [{"role": "user", "text": "Hi all,\nI am experiencing a problem with the tool Pureclip on the Galaxy / CLIP-explorer server. The problem is that the tool has been running for 4-5 days now with the notice: Job currently running. Previously I have already restarted the job after 1 week of waiting with job currently running.  I was wondering why it could take so long and what I can do to speed things up.\nI have uploaded .BAM files for the CLIP and input data and also a reference genome Hg19 in the Fasta format. Could someone please help me with this issue?\n\nimage1396\u00d7744 35.2 KB\n"}, {"role": "system", "text": "Hey. PureCLIP sometimes takes really long to train the peak calling model. Try to reduce the training on important chromosomes. There is a field where you can do that: Genomic chromosomes to learn HMM parameters. Reduce it maybe to three chromosomes.\nSo try this first and really pick chromosomes which are important for your study."}], [{"role": "user", "text": "Hi,\nI am trying to install some tools into my local galaxy instance (v22.01) with Ephemeris. For the installation I followed the Ephemeris tutorial from the Galaxy Training Network (Galaxy Tool Management with Ephemeris).\nWhenever I try to install tools from a list it keeps telling me: Timeout during install of tool_name, extending wait to 1h.\nI used (a personalized version of) the following command to install the tools from a list:\n\n\nshed-tools install -g https://your-galaxy -a  -t workflow_tools.yml\n\n\nWhen I look at the output of journalctl -f -u galaxy it it keeps giving the same output of uwsgi generating bytes.\nI also stopped Ephemeris when it took more than an hour fornothing to happen. I then tried to manually install the tools from the admin tab in galaxy, but this also didn\u2019t work.\nAny help would be appreciated!\nThanks!"}, {"role": "user", "text": "The problem was ultimately fixed by downloading another version of the tool which was first to be installed, which was NanoPlot. The latest version of this tool was however compatible with the server (tool: >=20.01, server: 22.01)."}], [{"role": "user", "text": "Dear Galaxy Community,\ni have problem to analyze the simply fastqsanger file made with HiSeq illumina on custom oncological panel qiagen. is if possible to have a workflow analysis for those file?.\nthanks a lot\ncarlo"}, {"role": "system", "text": "Hi @Carlo_Ceglia\n\n\n\n Carlo_Ceglia:\n\nproblem to analyze the simply fastqsanger file made with HiSeq illumina\n\n\nReads from that protocol should already be in \u201cfastqsanger\u201d format. Upload the data to Galaxy and let the process guess the datatype using the default option \u201cautodetect\u201d.\nWhat to do next depends on your analysis goals. The GTN tutorials can be navigated by topic or searched with keywords. Many tutorials include a workflow, and there are tutorials that cover how to use, modify, and create workflows.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you need more help, please be specific when you ask a new question so we can help you. What are the analysis goals? What steps have you done so far? If you ran into job errors: what did you check already, what did the error logs report, and can you share a link to your work for context?\nFAQs:\n\nhow to ask a question others can help with\ntroubleshooting-errors\nsharing-your-history\n"}], [{"role": "user", "text": "Hi, I tried several times to run jobs on Galaxy main but I can not get any job to run. I tried different tools with no success. There is an announcement that there are reduced resources to allow for the Smorgasbord training but I am unable to run job for the last 2 days. I do not know if this is related to the training or there is something wrong with my account. Thanks."}, {"role": "system", "text": "Welcome, @aa_aa\n\n\n\n aa_aa:\n\nI can not get any job to run\n\n\nDo you mean that jobs are queued (gray)? That is expected, even outside of training events. I added a tag to this topic that explains how the processing works at public servers. The warning on the server is stating that jobs may queue longer than usual.\nIf jobs are failing (red), please send in a bug report directly from the red error dataset. Please include a link to this topic in the comments so we can link the two together. The warning is also stating that very large jobs might not have the chance to execute for as long as usual. This situation should be somewhat rare.\nThe alternative is to share back a link to your history and we can take a closer look. You can start up a rerun to eliminate transient failures at the same time. Put that all in the same history, or share back links to multiple histories if needed.\nHow to do all of the above is here: Troubleshooting errors\nWith UseGalaxy.org specific error types listed here: Galaxy Training!\nLet\u2019s start there, thanks!"}], [{"role": "user", "text": "Trying to import a workflow created in Galaxy 18.09.  No log entries as to why the Import failed.  GA file is below:\n\n{\n\u201ca_galaxy_workflow\u201d: \u201ctrue\u201d,\n\u201cannotation\u201d: \u201c\u201d,\n\u201cformat-version\u201d: \u201c0.1\u201d,\n\u201cname\u201d: \u201cLow coverage module\u201d,\n\u201csteps\u201d: {\n\u201c0\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: null,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 0,\n\u201cinput_connections\u201d: {},\n\u201cinputs\u201d: [],\n\u201clabel\u201d: \u201cBAM file\u201d,\n\u201cname\u201d: \u201cInput dataset\u201d,\n\u201coutputs\u201d: [],\n\u201cposition\u201d: {\n\u201cleft\u201d: 145.5,\n\u201ctop\u201d: 262\n},\n\u201ctool_id\u201d: null,\n\u201ctool_state\u201d: \u201c{}\u201d,\n\u201ctool_version\u201d: null,\n\u201ctype\u201d: \u201cdata_input\u201d,\n\u201cuuid\u201d: \u201c84fd58a9-9317-4517-8afe-432a3cef699d\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: null,\n\u201coutput_name\u201d: \u201coutput\u201d,\n\u201cuuid\u201d: \u201c4bc20bcb-a432-4c77-82c2-89adcd1d8ee5\u201d\n}\n]\n},\n\u201c1\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: null,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 1,\n\u201cinput_connections\u201d: {},\n\u201cinputs\u201d: [],\n\u201clabel\u201d: \u201cCoverage reporting regions\u201d,\n\u201cname\u201d: \u201cInput dataset\u201d,\n\u201coutputs\u201d: [],\n\u201cposition\u201d: {\n\u201cleft\u201d: 147.5,\n\u201ctop\u201d: 391\n},\n\u201ctool_id\u201d: null,\n\u201ctool_state\u201d: \u201c{}\u201d,\n\u201ctool_version\u201d: null,\n\u201ctype\u201d: \u201cdata_input\u201d,\n\u201cuuid\u201d: \u201caf39455e-ebcf-4a45-a3e9-7324e71f45e9\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: null,\n\u201coutput_name\u201d: \u201coutput\u201d,\n\u201cuuid\u201d: \u201c32434cbf-71d6-4710-b1a5-5129c349036e\u201d\n}\n]\n},\n\u201c2\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.27.0.0\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 2,\n\u201cinput_connections\u201d: {\n\u201cinput_type|input\u201d: {\n\u201cid\u201d: 0,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool Genome Coverage\u201d,\n\u201cname\u201d: \u201cinput_type\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cGenome Coverage\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201coutput\u201d,\n\u201ctype\u201d: \u201cbedgraph\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 366.5,\n\u201ctop\u201d: 238\n},\n\u201cpost_job_actions\u201d: {\n\u201cHideDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {},\n\u201caction_type\u201d: \u201cHideDatasetAction\u201d,\n\u201coutput_name\u201d: \u201coutput\u201d\n},\n\u201cRenameDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {\n\u201cnewname\u201d: \u201c{SAMPLE_ID}.final.bedgraph\"\n                }, \n                \"action_type\": \"RenameDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.27.0.0\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"__page__\\\": null, \\\"d\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"input_type\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 1, \\\\\\\"input\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"input_type_select\\\\\\\": \\\\\\\"bam\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"three\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"five\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"report\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"report_select\\\\\\\": \\\\\\\"bg\\\\\\\", \\\\\\\"scale\\\\\\\": \\\\\\\"1.0\\\\\\\", \\\\\\\"zero_regions\\\\\\\": \\\\\\\"true\\\\\\\"}\\\", \\\"dz\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"2109aa97-4fb3-4a80-9c3d-748486476b6c\", \n        \"workflow_outputs\": []\n    }, \n    \"3\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 3, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 1, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 366.5, \n            \"top\": 456\n        }, \n        \"post_job_actions\": {\n            \"ChangeDatatypeActionout_file1\": {\n                \"action_arguments\": {\n                    \"newtype\": \"bed\"\n                }, \n                \"action_type\": \"ChangeDatatypeAction\", \n                \"output_name\": \"out_file1\"\n            }, \n            \"HideDatasetActionout_file1\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Cut1\", \n        \"tool_state\": \"{\\\"columnList\\\": \\\"\\\\\\\"c1,c2,c3,c4\\\\\\\"\\\", \\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"delimiter\\\": \\\"\\\\\\\"T\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"f94970a8-e554-49f1-815c-90072afe1595\", \n        \"workflow_outputs\": []\n    }, \n    \"4\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Filter1\", \n        \"errors\": null, \n        \"id\": 4, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 2, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Filter\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Filter\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 598.5, \n            \"top\": 242\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionout_file1\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Filter1\", \n        \"tool_state\": \"{\\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"header_lines\\\": \\\"\\\\\\\"0\\\\\\\"\\\", \\\"cond\\\": \\\"\\\\\\\"c4<30\\\\\\\"\\\", \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.1.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"d749fb31-e8d3-4a0d-bc74-bdb8a59aaa96\", \n        \"workflow_outputs\": []\n    }, \n    \"5\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.27.0.3\", \n        \"errors\": null, \n        \"id\": 5, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 4, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Compute both the depth and breadth of coverage\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Compute both the depth and breadth of coverage\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Compute both the depth and breadth of coverage\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"bed\"\n            }\n        ], \n        \"position\": {\n            \"left\": 684.5, \n            \"top\": 562\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.27.0.3\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"__page__\\\": null, \\\"overlap_a\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"d\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"a_or_b\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_b\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"hist\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"strandedness\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"reciprocal_overlap\\\": \\\"\\\\\\\"false\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.3\", \n        \"type\": \"tool\", \n        \"uuid\": \"b80c314f-649a-4fe8-a165-240e3e4e34f5\", \n        \"workflow_outputs\": []\n    }, \n    \"6\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"errors\": null, \n        \"id\": 6, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 4, \n                \"output_name\": \"out_file1\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Intersect intervals\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 814.5, \n            \"top\": 368\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"count\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"__page__\\\": null, \\\"reciprocal\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_mode\\\": \\\"null\\\", \\\"invert\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"header\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"fraction\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"once\\\": \\\"\\\\\\\"true\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"792f3eb0-3c6c-4fb0-8941-8ca962cc7fe1\", \n        \"workflow_outputs\": []\n    }, \n    \"7\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 7, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 5, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 964.890625, \n            \"top\": 704.84375\n        }, \n        \"post_job_actions\": {\n            \"RenameDatasetActionout_file1\": {\n                \"action_arguments\": {\n                    \"newname\": \"{SAMPLE_ID}.allregions.coverage.txt\u201d\n},\n\u201caction_type\u201d: \u201cRenameDatasetAction\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201ctool_id\u201d: \u201cCut1\u201d,\n\u201ctool_state\u201d: \u201c{\u201ccolumnList\u201d: \u201c\\\u201cc1,c2,c3,c4,c6,c8\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cdelimiter\u201d: \u201c\\\u201cT\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cpage\u201d: null}\u201d,\n\u201ctool_version\u201d: \u201c1.0.2\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201ccf8609df-6454-48b8-ab74-03991b88baa2\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: \u201cLow coverage, all regions table\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d,\n\u201cuuid\u201d: \u201cb0b8afdc-8190-4e10-a7b2-0ff72049ee67\u201d\n}\n]\n},\n\u201c8\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.27.0.0\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 8,\n\u201cinput_connections\u201d: {\n\u201cinput\u201d: {\n\u201cid\u201d: 6,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool MergeBED\u201d,\n\u201cname\u201d: \u201cinput\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cMergeBED\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201coutput\u201d,\n\u201ctype\u201d: \u201cbed\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 1051.5,\n\u201ctop\u201d: 380\n},\n\u201cpost_job_actions\u201d: {\n\u201cHideDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {},\n\u201caction_type\u201d: \u201cHideDatasetAction\u201d,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.27.0.0\u201d,\n\u201ctool_shed_repository\u201d: {\n\u201cchangeset_revision\u201d: \u201ce19bebe96cd2\u201d,\n\u201cname\u201d: \u201cbedtools\u201d,\n\u201cowner\u201d: \u201ciuc\u201d,\n\u201ctool_shed\u201d: \u201ctoolshed.g2.bx.psu.edu\u201d\n},\n\u201ctool_state\u201d: \u201c{\u201cpage\u201d: null, \u201cdistance\u201d: \u201c\\\u201c0\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cheader\u201d: \u201c\\\u201cfalse\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cc_and_o_argument_repeat\u201d: \u201c[]\u201d, \u201cstrand\u201d: \u201c\\\u201d\\\u201d\"}\",\n\u201ctool_version\u201d: \u201c2.27.0.0\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201ca7054b80-047d-45cd-a61a-597dc6674467\u201d,\n\u201cworkflow_outputs\u201d: []\n},\n\u201c9\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201cFilter1\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 9,\n\u201cinput_connections\u201d: {\n\u201cinput\u201d: {\n\u201cid\u201d: 7,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool Filter\u201d,\n\u201cname\u201d: \u201cinput\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cFilter\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201cout_file1\u201d,\n\u201ctype\u201d: \u201cinput\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 1188.5,\n\u201ctop\u201d: 747\n},\n\u201cpost_job_actions\u201d: {\n\u201cRenameDatasetActionout_file1\u201d: {\n\u201caction_arguments\u201d: {\n\u201cnewname\u201d: \u201c{SAMPLE_ID}.lessthan30x.coverage.txt\"\n                }, \n                \"action_type\": \"RenameDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Filter1\", \n        \"tool_state\": \"{\\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"header_lines\\\": \\\"\\\\\\\"0\\\\\\\"\\\", \\\"cond\\\": \\\"\\\\\\\"c5>0\\\\\\\"\\\", \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.1.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"d0ff2e13-8ae9-4612-a3ea-ecfe0288b182\", \n        \"workflow_outputs\": [\n            {\n                \"label\": \"Low coverage, regions of concern table\", \n                \"output_name\": \"out_file1\", \n                \"uuid\": \"3b2df0a9-788e-4b85-9f4d-141d992903be\"\n            }\n        ]\n    }, \n    \"10\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"errors\": null, \n        \"id\": 10, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 8, \n                \"output_name\": \"output\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Intersect intervals\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 1101.5, \n            \"top\": 561\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"count\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"__page__\\\": null, \\\"reciprocal\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_mode\\\": \\\"[\\\\\\\"-wao\\\\\\\"]\\\", \\\"invert\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"header\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"fraction\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"once\\\": \\\"\\\\\\\"false\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"e4c50b5a-1d04-4f79-95bc-51cc7a5fded4\", \n        \"workflow_outputs\": []\n    }, \n    \"11\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 11, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 10, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 1328.5, \n            \"top\": 586\n        }, \n        \"post_job_actions\": {\n            \"ChangeDatatypeActionout_file1\": {\n                \"action_arguments\": {\n                    \"newtype\": \"bed\"\n                }, \n                \"action_type\": \"ChangeDatatypeAction\", \n                \"output_name\": \"out_file1\"\n            }, \n            \"RenameDatasetActionout_file1\": {\n                \"action_arguments\": {\n                    \"newname\": \"{SAMPLE_ID}.lessthan30x.bed\u201d\n},\n\u201caction_type\u201d: \u201cRenameDatasetAction\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201ctool_id\u201d: \u201cCut1\u201d,\n\u201ctool_state\u201d: \u201c{\u201ccolumnList\u201d: \u201c\\\u201cc1,c2,c3,c7\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cdelimiter\u201d: \u201c\\\u201cT\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cpage\u201d: null}\u201d,\n\u201ctool_version\u201d: \u201c1.0.2\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201cf3b35bac-09ee-4fe2-97be-a0e2d0c754f7\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: \u201cLow coverage, intervals\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d,\n\u201cuuid\u201d: \u201cf45ee41a-bdee-495c-9adc-a498199cfabf\u201d\n}\n]\n}\n},\n\u201ctags\u201d: [],\n\u201cuuid\u201d: \u201c54492bd2-73be-4bbb-89df-dc2a4a28d1d5\u201d,\n\u201cversion\u201d: 14\n"}, {"role": "system", "text": "Same problem here, did you ever figure out what was going on?"}, {"role": "system", "text": "Update\nMore help for verifying workflows Non-specific Error on Malformed gxformat2 Import - #3 by mvdbeek\n\nHi @jhl667\nIf you can get your workflow uploaded to a server (try a usegalaxy.* server), open it in the editor to review the warnings. If it won\u2019t load at all, the web browser logs might have some clues."}], [{"role": "user", "text": "Hi,\nI am trying to extract loci sequence from the genome using bed file. I have uploaded the bed file and the fasta file of ref genome. But I am getting the error of unspecified build. Kindly help me sort this out\nhttps://usegalaxy.org/u/bhavani_puttaswamygowda/h/unnamed-history"}, {"role": "system", "text": "Hi @bhavani,\nyou need to specify the reference genome database also in the BED file; it can be done by clicking in the Edit attributes menu  (pencil icon), and then select the reference genome in the Database/build field.\nRegards"}], [{"role": "user", "text": "Hi\nCan anybody shed some light on the concept of forms ? In the administration panel, user management section there is a menu entry called forms. What can you do with it ? How to use it ?\nThanks and regards,\nMarc"}, {"role": "system", "text": "It has no real application in modern Galaxies afaik, it is rather a remnant of removed features, we should get rid of it, sorry for the confusion."}], [{"role": "user", "text": "Hi!\nI have 12 samples in total - 2 genotypes, and 4 treatments, 3 replicates each. After running DESeq2, I see with the PCA plots that there is a strong batch effect - the samples from one genotype aggregate according to the batch of cells and not the treatments.\nI am trying to use RUVSeq in usegalaxy.org but I am not sure how. As factors, should I input genotype, treatment and the batches as well?\nThank you!!!\nBest,\nCarolina"}, {"role": "system", "text": "Hi @CarolMCam,\nthese kind of known or easily inferred batch effects can be accommodated by DESeq2 without using RUVseq.\nYou can add multiple factors (not factor levels) in the DESeq2 wrapper by clicking on Insert Factor. The first factor is the primary factor, so should probably be treatment. You can then add a genotypes and batch information as secondary and tertiary factors.\nIf you want to use RUVseq in Galaxy you would only use the primary factor levels and let the algorithm find the batch factors that you can then use in DESeq2.\nHope that helps,\nMarius"}], [{"role": "user", "text": "Dear all,\nI am using PROKKA to annotate my bacterial assembly.fasta file\nAs I read in several discussions, PROKKA enables active annotation against  HAMAP database of HMMs, but not Pfam and TIGRfams.\nIs there any method to direct transfer the 2 databases, below, into galaxy, and use them in the annotation process?\nTIGRFAMs: ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.LIB.gz\nPfam: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\n\nThank you,\nMD"}, {"role": "system", "text": "Hi @Seraph\n\n\n\n Seraph:\n\nAs I read in several discussions, PROKKA enables active annotation against HAMAP database of HMMs, but not Pfam and TIGRfams.\n\n\nCorrect.\n\n\n\n Seraph:\n\nIs there any method to direct transfer the 2 databases, below, into galaxy, and use them in the annotation process?\n\n\nNot those specific files directly with Prokka. The first has a permissions problem (URL is incorrect?). The second could probably be used with other tools \u2013 search the tool panel at the EU servers for available choices. One example is PfamScan. Or, search the GTN tutorials with tool names or datatypes. Each has a listing of the public Galaxy servers that host a specific tutorial\u2019s tools.  Galaxy Training!\nNote: Galaxy doesn\u2019t modify the original underlying tool. Tools are instead \u201cwrapped\u201d in a way that makes it easier to use them all together all in one place: to build a workflow or to share exact methods/data with others. Or if you just prefer using a GUI . The usage is (mostly) otherwise the same as the command-line.\nRelated: if you can find the fasta for other protein database sources, those can be included with Prokka using this option: Optional FASTA file of trusted proteins to first annotate from\nHope that helps!"}], [{"role": "user", "text": "Hi,\nI am new here and I am using usegalaxy.org, I started with interested tutorials for me in metagenomics, in this tutorial \u201cAnalyses of metagenomics data - The global picture\u201d I faced 2 problems first with visualization data by Phinch although I already changed the file name form .biom1 to .biom and tried many times,but the error message that the file is not valid. Second, the tool \u2018MetaPhlAN2\u2019 is not available in the list of tools. I will be appreciated for your help\nthank you"}, {"role": "system", "text": "\n\n\n Jalal:\n\nAnalyses of metagenomics data - The global picture\n\n\nNot every public Galaxy is set up for this tutorial. The Galaxies that made sure everything for it is available are listed on the trainings page: "}], [{"role": "user", "text": "I\u2019ve used CloudLaunch to run GVL/Galaxy on AWS but I\u2019ve been having trouble getting files to upload. Actually, they upload fine but then when the go to History, the wheel spins forever\u2026 I\u2019ve even tried small text files. I\u2019ve even tried pasting in data. Anyone have any thoughts?"}, {"role": "system", "text": "Hi @mitchellgc, did you get this issue resolved? You had also mentioned a featurecounts issue (featureCount doesn't work on GVL/Galaxy) so perhaps these are both tied together. I would propose continuing the discussion there."}], [{"role": "user", "text": "Hello dear community, I have just started using galaxy on a local server, followed a few guides, and stopped at one of them where I needed to use join tool\u2026\nAll good, except 2 \u201cerrors\u201d, one a lot more important than other.\nWhen running join tool for the \u201cData Manipulation Olympics\u201d - training I have encountered this error, I have tried other variations with this tool - same error\u2026\nJoin two Datasets side by side on a specified field - error\nTraceback (most recent call last):\nFile \u201c/home/arian/galaxyproject/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nWith Sort data in ascending or descending order (built in tool) \u2013 is simply loading indefinitely without an error - but I have installed another type of sort tool (of the same kind), and it works flawlessly, but still I\u2019d like to understand why it\u2019s not working\u2026(unfortunately I don\u2019t have an output error)\nCould anyone help me?"}, {"role": "system", "text": "Hi @apolitics,\nthanks for the report. We have been informed previously about those bugs:\n\n  \n    \n    \n    Error in Join two datasets since update usegalaxy.org support\n  \n  \n    In the last two weeks the tool join two datasets has been upgraded from version 2.1.2. to 2.1.3 and now I can\u2019t get the tool to run.it keeps failing with the error Traceback (most recent call last): \nFile \u201c/cvmfs/main.galaxyproject.org/galaxy/tools/filters/join.py\u201d, line 19, in  \nfrom galaxy.util import stringify_dictionary_keys \nModuleNotFoundError: No module named \u2018galaxy\u2019 \nIt also now says that This tool requires galaxy-util (Version 19.9.0). Click here for more information. \nCan any one plea\u2026\n  \n\n\nSorry for the inconvenience.\nRegards"}], [{"role": "user", "text": "Hello, we hope that this email will find you well. We are trying to assemble the genome from Pacbio SMRT sequence data. Those sequences are present in the recent history, however, when we try to proceed the genome assembly by Unicycler, we get the message that the data is unavailable. Kindly help us in this matter."}, {"role": "system", "text": "Hi @hareem!\nAre you seeing one of these on the tool form?\nShort read input:\n\nfastq-inputs.png890\u00d790 4.42 KB\n\nLong read input:\n\nfastq-fastq-inputs.png910\u00d790 4.76 KB\n\nThe short reads need to be in fastq format to use with most Galaxy tools, including this one. Long reads can be in fastq or fasta format. Galaxy will autodetect the correct format and assign the datatype upon upload. Or, if is guessed incorrectly, you can directly assign the datatype.\nPlease see these FAQs for more details: https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format?\nUnderstanding compressed fastq data (fastq.gz)\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\n\nThanks!"}], [{"role": "user", "text": "\nAlthough the installation seems to succeed and everything seems to work, we get the following error when installing v22.05 using ansible.\nTASK [geerlingguy.docker : Ensure docker users are added to the docker group.] ***\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: KeyError: \u2018getspnam(): name not found\u2019\nfailed: [galaxy.classe.cornell.edu] (item=galaxy) => {\u201cansible_loop_var\u201d: \u201citem\u201d, \u201cchanged\u201d: false, \u201citem\u201d: \u201cgalaxy\u201d, \u201cmodule_stderr\u201d: \u201cTraceback (most recent call last):\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 107, in \\n    _ansiballz_main()\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 99, in _ansiballz_main\\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 47, in invoke_module\\n    runpy.run_module(mod_name=\u2018ansible.modules.user\u2019, init_globals=dict(_module_fqn=\u2018ansible.modules.user\u2019, _modlib_path=modlib_path),\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 207, in run_module\\n    return _run_module_code(code, init_globals, run_name, mod_spec)\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 97, in _run_module_code\\n    _run_code(code, mod_globals, init_globals,\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 3221, in \\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 3207, in main\\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 1055, in set_password_expire\\nKeyError: \u2018getspnam(): name not found\u2019\\n\u201d, \u201cmodule_stdout\u201d: \u201c\u201d, \u201cmsg\u201d: \u201cMODULE FAILURE\\nSee stdout/stderr for the exact error\u201d, \u201crc\u201d: 1}\n\nPLAY RECAP *********************************************************************\ngalaxy.classe.cornell.edu  : ok=111  changed=8    unreachable=0    failed=1    skipped=50   rescued=0    ignored=0\n\nOur python version is 3.8.11, and in group_vars/galaxyservers.yml we have:\ndocker_users:\n\n\u201c{{ galaxy_user.name }}\u201d\n\n\nWhere galaxy_user.name is galaxy.  Our galaxy user is in our central AD domain, and is in the docker group:\n(venv) [galaxy@lnx229 ansible]$ id\nuid=31292(galaxy) gid=6023(galaxy) groups=6023(galaxy),1500(docker)\nAny help or suggestions would be greatly appreciated.\nMany thanks,\nDevin"}, {"role": "system", "text": "Hi @Devin_Bougie\nHopefully you have solved the problem here '__galaxy_user_group' is undefined\".\nIf not, it might faster to reach out to the Galaxy Docker maintainers at this Gitter chat for more admin help bgruening/docker-galaxy-stable - Gitter"}], [{"role": "user", "text": "Hi and congrats on the new site! It leaves a really good first impression on me.\nOne question though: with the old biostars site it was possible to subscribe to the rss feed of latest posts. Does the new site come with an RSS feed, too, or is this planned for the future?\nBest,\nWolfgang"}, {"role": "system", "text": "Hi @wm75, add .rss to the url of any listing of posts to get the feed."}], [{"role": "user", "text": "Hello! I am a new user on Galaxy. I wanted to use Lefse to look for differences in my 16S dataset. When I ran the job through, it went well up until  B) LDA Effect Size, where I was met with the following error.\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, \u2018lefse_run.py\u2019)())\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse_run.py\u201d, line 90, in lefse_run\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse.py\u201d, line 222, in test_lda_r\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018rownames\u2019\nI then attempted to re-run the job with no luck. I proceeded to download the aerobiosis_small example file and run it through Lefse again. I was met with the same error:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, \u2018lefse_run.py\u2019)())\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse_run.py\u201d, line 90, in lefse_run\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse.py\u201d, line 222, in test_lda_r\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018rownames\u2019\nIs there anything I may be doing wrong? Thank you!"}, {"role": "system", "text": "Welcome, @capsola\nI\u2019m going to merge this question with another topic at this forum. It links to a topic at the forum hosted by the administrators of that server. There does seem to be a technical problem, and only they can fix it.\n\n  \n    \n    \n    How to fix errors with Lefse \n  \n  \n    Hi @Adelaide_Panattoni \nHave you tried to ask at forum the server hosts? It is linked from that server\u2019s homepage and is the only way to contact them as far as I know. \nThis threads and a few others have details. I\u2019ve also added a tag to this post that will find more (guessing that was the server where the original question asker was working). \n\n\n\n\nHope that helps, and if someone does make contact and gets feedback, posting back the solution would be helpful here, too  \nUpdate: I \u2026\n  \n\n"}], [{"role": "user", "text": "I modified the RNA-Seq protocol, so that 5\u2019 end adapters were ligated prior to RNA fragmentation and 3\u2019-end adapter ligation. I have mapped the reads to a reference bacterial genome. Now I would like to get a list of the 5\u2019 start positions (transcription start sites) along with the frequency of occurrence for each starting position. What is the best way to do this?\nChaitanya Jain"}, {"role": "system", "text": "Dear @cjain,\nThere is a tools at least on the usegalaxy.eu, which is called \u201cExtract alignment ends from SAM or BAM\u201d. With that you get the 5\u2019 and 3\u2019 ends in bed format. To count your start or ends sites your can use \u201cCount occurrences of each record\u201d. If you are operating on usegalaxy.org or other instances without the first tool, then what you can do is to convert the bam into a bed file with \u201cbedtools BAM to BED converter\u201d.\nI hope I could help.\nBest wishes,\nFlorian"}], [{"role": "user", "text": "Hi all I m trying to running Dist. seqs.But its taking too long to complete. Can you please help me with this. Thanks"}, {"role": "system", "text": "Generally, there is no reasonable way to speed up a running job. But you can deploy your own Galaxy on your resources that are faster: https://galaxyproject.org/choices/"}], [{"role": "user", "text": "Dear sir/ Madam,\nThe latest reference genome for rats is currently available. But it has been updated in galaxy. I downloaded and used for the mapping. But unfortunately, when I wanted to change my bedgraph to bigwig, I got errors that Build/database is unspecified. I could not continue my analysis. Could you please help me with that?\nCould you please let me know If it is possible to add an effective reference genome for rats?\nThank you\nFereshteh"}, {"role": "system", "text": "Please find the genome index at the UseGalaxy.eu and UseGalaxy.org.eu Galaxy servers.\nRelated Q&A: Mapping to a uploaded (large) reference genome dataset -- Platform Choices"}], [{"role": "user", "text": "Trying to evaluate values in columns 1 and 2. If value in column 2 is larger than value in column 1 then column 3 should be +. else if value is larger in column 1 it should be -.\nThen a second round where values in column 1 and 2 are swapped if column 3 is -.\nDont understand the syntax in the custom table operation. Need some help.\nAnyone have some tips on how to do this?"}, {"role": "system", "text": "Hi @Roger_Meisal\n\n\n\n Roger_Meisal:\n\ncustom table operation\n\n\nDo you mean the Datamash tool?\nThe tutorial below covers a bunch of data manipulation examples. Some include the Datamash tool plus other tool are covered. Often, similar manipulations can be done with different tools, so use these examples as a starting place.\nYour query will probably use more than one tool \u2013 or the same tool a few times \u2013 but those could be grouped into a workflow for reuse. And, if you \u201cbookmark\u201d that workflow on your Workflow listing, it will show up in your tool panel view.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}, {"role": "system", "text": "Hi @Roger_Meisal\ntry Text reformatting with awk.\nAdding + and -:\n{\nif ($2 > $1) {print $0,\u201c+\u201d} else {print $0,\u201c-\u201d}\n}\nSwapping columns and adding + and -:\n{\nif ($2 > $1) {print $1,$2,\u201c+\u201d} else {print $2,$1,\u201c-\u201d}\n}\nIf the same value is present in c1 and c2, the output gets \u201c-\u201d\nIf you are after intervals in BED format, make sure the start coordinate uses zero offset, and the is one based.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello,\nI would like to know if there is any tool installed to visualize progressive mauve alignments.\nThanks"}, {"role": "system", "text": "Hi @phbrito\nGood question! There are a few options now, and potentially more soon.\n\nThe CPT public Galaxy server hosts a visualization tool. If you run progressiveMauve in Galaxy, the gff3 and mxfa output will be created, but you\u2019ll need to provide the reference genome fasta yourself from the history (none are natively indexed).\n\n\nTool: https://cpt.tamu.edu/galaxy-pub/root?tool_id=edu.tamu.cpt.comparative.mauve\n\nGalaxy Platform Directory page with contact information, including how to request more space: https://galaxyproject.org/use/center-for-phage-technology-cpt/\n\nHow to format custom reference genomes used \u201cfrom the history\u201d. The primary goal is to ensure that the chromosome identifiers \u201cmatch up\u201d between all inputs:\n\nPreparing and using a Custom Reference Genome or Build\nMismatched Chromosome identifiers (and how to avoid them)\n\n\n\n\n\nNone of the usegalaxy.* servers incorporate that tool (yet). This is now under consideration. We will likely know more by mid-next week. Feel free to write back after then for an update if we don\u2019t write back here first.\n\n\nThose options are fine, but installing and using the Mauve browser directly is definitely an alternative for full visualization functionality specifically designed to work with these data (whether created directly with Mauve or in the Galaxy wrapped version). Find it here: http://darlinglab.org/mauve/user-guide/installing.html\n\n\nHope that helps!\nPs: Thank you @hxr for assistance and clarification for all of the above!!"}, {"role": "system", "text": "Hi @phbrito,\nI\u2019ve placed the tool on the usegalaxy.eu (maybe easier to use; much more popular/more tools/well maintained/bigger quota)\nHere is an example history where you can see the output. This was written by Eleni Mijalis at the CPT. It worked well enough for our purposes, basic structural rearrangement visualisation, but it is not being actively developed.\nThe X-Vis tool takes the XMFA file and a GFF3 file showing annotations, and visualises it:\nimage1078\u00d7625 260 KB\nEach contig is placed vertically, the blue boxes represent an LCB region, and the grey lines connect them. All features from the gff3 are visualised as black arrow boxes.\n\nClicking on a black feature will show the feature name at the bottom\nDouble clicking a blue LCB box will re-align the visualisation to center those LCBs across all contigs\nYou can scroll vertically to zoom in on a single region\nClick and drag to pan around.\n\nMauve is more full featured, but, this was a convenient way to include it into our comparative genomics workflows"}], [{"role": "user", "text": "Hi,\nI upgraded our Galaxy instance from 16.xy to 19.01.\nI succeeded to configure it with our deprecated galaxy.ini file.\nBut I face problems to migrate configuration to galaxy.yml file and set it up to use our SGE 8.1.8 cluster :\n\nwith galaxy.ini, galaxy jobs are correctly shifted and run by SGE\nbut with galaxy.yml, galaxy jobs keep waiting to be shifted but are not received by SGE.\n\nI believe I must have misconfigured something but can\u2019t find what.\nWould you have any hint ?\nHere are extracts from :\n\nour galaxy.yml :\n\nuwsgi:\n\n  http: :7090\n  buffer-size: 16384\n  processes: 2\n  threads: 4\n  offload-threads: 1\n  static-map: /static/style=static/style/blue\n  static-map: /static=static\n  static-map: /favicon.ico=static/favicon.ico\n  master: true\n  pythonpath: lib\n  module: galaxy.webapps.galaxy.buildapp:uwsgi_app()\n  thunder-lock: true\n  die-on-term: true\n  hook-master-start: unix_signal:2 gracefully_kill_them_all\n  hook-master-start: unix_signal:15 gracefully_kill_them_all\n  py-call-osafterfork: true\n  enable-threads: true\n\ngalaxy:\n\n  mule: lib/galaxy/main.py\n  farm: job-handlers:1\n\n\nour job_conf.xml :\n\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"sge\" type=\"runner\" load=\"galaxy.jobs.runners.drmaa:DRMAAJobRunner\">\n            <param id=\"drmaa_library_path\">/SGE/8.1.8/lib/lx-amd64/libdrmaa.so</param>\n        </plugin>\n    </plugins>\n    <destinations default=\"sge_default\">\n        <destination id=\"sge_default\" runner=\"sge\">\n            <param id=\"nativeSpecification\">-V -j n -q short.q</param>\n        </destination>\n    </destinations>\n\n\nour galaxy.log :\n\ngalaxy.jobs.command_factory INFO 2019-04-03 16:42:44,377 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] Built script [/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/tool_script.sh] for tool command [python '/gs7k1/home/galaxydev/galaxy_19.01/tools/data_source/upload.py' /gs7k1/home/galaxydev/galaxy_19.01 /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/registry.xml /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/upload_params.json 46441:/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/dataset_46441_files:/gs7k1/home/galaxydev/galaxy_19.01/database/files/046/dataset_46441.dat]\ngalaxy.jobs.runners DEBUG 2019-04-03 16:42:44,449 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) command is: rm -rf working; mkdir -p working; cd working; /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/tool_script.sh; return_code=$?; cd '/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532';\ngalaxy.jobs.runners.drmaa DEBUG 2019-04-03 16:42:44,483 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) submitting file /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/galaxy_18532.sh\ngalaxy.jobs.runners.drmaa DEBUG 2019-04-03 16:42:44,483 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) native specification is: -V -j n -q short.q\n\nRegards."}, {"role": "system", "text": "\n\n\n bertrand:\n\nmule: lib/galaxy/main.py farm: job-handlers:1\n\n\nShould be under uwsgi section in the config per Scaling and Load Balancing \u2014 Galaxy Project 21.09.1.dev0 documentation\nI am not sure that is the root cause but definitely firs thing to try."}], [{"role": "user", "text": "Hello,\nI am trying to upload files to the server:\n\nChose local file\nI managed to upload I think, but looking at the window on the right, where my datasets are, and it\u2019s written \u201cThis job is waiting to run\u201d. What does it mean, waiting for what?\nChose FTP file\nI cannot connect to usegalaxy.eu or usegalaxy.org through Filezilla (passive mode checked). |Response:|530 Login incorrect.|\n|Error:|Critical error: Could not connect to server|\n\nCould someone please help me spotting what is wrong?\nThank you."}, {"role": "system", "text": "Hi @gomette!\nOur FTP server is at a different address. You can find more information about it in our documentation.\nThere were some temporary problems with running jobs on usegalaxy.eu today, please see the notice in the center panel. These should be resolved now. If your uploads still do not succeed, please retry at your convenience.\nThanks for using UseGalaxy.eu! Sorry for the inconvenience."}], [{"role": "user", "text": "Hi,\nI\u2019m using the 16S Microbial Analysis with Mothur tutorial but I\u2019m having troubles with the step of visualization data. I have an biom file which I can visualize in Phinch server (link in the galaxy output of make.biom) (http://phinch.org/), the graphs looks great, even so when I try to export the graphs the server doesn\u2019t respond and never ends up exporting anything. There\u2019s a way I could export these files??"}, {"role": "system", "text": "Where to get help with the phinch service: https://www.phinch.org/Community"}], [{"role": "user", "text": "\nimage1920\u00d71080 459 KB\n\neven if I opened hidden datasets my account is still full.\nCould you please erase everything?\nbest"}, {"role": "system", "text": "Hi @pbrest\nThe issue seems to be that datasets have been deleted, but not permanently deleted (purged).  Data needs to be purged before it is actually removed (and no longer recoverable, even by an administrator).\nTry clicking on the \u201c186 deleted\u201d toggle at the top of the left history panel.\nIf a dataset has only been deleted but not permanently deleted (purged) you\u2019ll see a message within the dataset with note/link to purge.\nSince you have so many datasets and are removing all in the same history, individually clicking on those links, or even using the \u201cOperations on multiple datasets\u201d function, will take more time to do.\nTwo shortcuts, either should work:\n\nPermanently delete (purge) the entire History. Do this from the User > Histories view. The pull-down menu per history has an option for \u201cPurge History\u201d. Or, you can select multiple histories and use the \u201cDelete Permanently\u201d button at the bottom of your history list to purge in batch.\nPermanently delete (purge) all datasets that are deleted but not permanently deleted, in batch, using the History menu function \u201cPurge Deleted Datasets\u201d in the Analyze Data view (the view in your screenshot).\n\nNotes:\n\n\nHistories cannot be in a shared state if you want to permanently delete (purge) them. Unshare as needed first.\nPurging is a two-step process using any method. A pop-up box is the second step. Click to confirm the action.\nWhen purging a large amount of data, the server may take some time to update and accurately reflect quota usage. You can wait, or you can speed up the process by logging out then logging in again.\n\nFAQ with full details: https://galaxyproject.org/support/account-quotas/\nI also added some tags to your post. Click on them to review prior Q&A about the topics. You can also search all topics with the keyword \u201cpurge\u201d. Many have screenshots that may help even more.\nPlease give this a try and let us know if you need more help.\nThanks!"}], [{"role": "user", "text": "Hello,\nI am new to Galaxy and currently doing Galaxy 101 tutorial. When trying to get the data from UCSC main table browser and after sending the output to Galaxy, it does not send to the current open session/history. It logs me out and create a new history.\nThanks\nTrupti"}, {"role": "system", "text": "+1.  I also am getting logged out when performing this tutorial after the getting variation data.  Were you running this in Chrome?  I was and then switched to Firefox and it seemed to keep me logged in."}, {"role": "system", "text": "There was a prior issue that logged you out of Galaxy when downloading data from the UCSC Table Browser. It wasn\u2019t a consistent fail and we believe it was related to query-quotas at UCSC, or session identifiers in Galaxy (how UCSC knows where to download the data). Logging into Galaxy will add the newly downloaded data into a new history in your account, and you will need to consolidate datasets between histories. Copies of datasets in your own account do not consume extra quota space, but you can still purge the histories you don\u2019t need once you have a copy of the data in the history you want to work in.\nInconvenient, but we don\u2019t have another solution right now. This is the open ticket: Redirect from data sources sometimes loses session cookies \u00b7 Issue #11374 \u00b7 galaxyproject/galaxy \u00b7 GitHub"}], [{"role": "user", "text": "Hello,\nI\u2019m new to Galaxy, and especially new to using cloud resources. I set up a cloud instance of Galaxy, but am 100% completely lost. There are a few thing things that don\u2019t really seem to be adding up. I\u2019m very sorry for, what are probably trivial questions, but I would appreciate some help,\nSo in the cloudLaunch \u201cMy Appliances\u201d page, it says I\u2019m running a genomics virtual lab on the AWS cloud. When I click on the access link, it takes me to the GVL dashboard where it says that Galaxy isn\u2019t running, but Cloudman is. So I click on the Cloudman access link, and it tells me that I\u2019m running out of disk space. Now, I managed to get Galaxy working a few weeks ago (not sure how), and managed to download a fastq file that was probably about 6 GB. But the Cloudman console was telling me that I was running out of space. I deleted the file, and then the history pane on Galaxy showed that my history was taking up 12 GB of storage, but hte number on the top right of the console said I was using about 6.8 GB. Then, under cluster status in the cloudman console, it says I\u2019ve used 54 GB / 54 GB, but in the admin console, under file systems, it\u2019s telling me that the different fiel systems are taking up almost 150 GB of storage. So I\u2019m having trouble making sense of all this information. All I did was download a 6 GB file, which I then deleted. So why am I getting all this mixed information?\nI\u2019m sorry for the mess of information, but I would greatly appreciate it if someone could point towards something that could help me understand what\u2019s going on. How do I make sense of the memory usage?\nAlso, I tried restarting Galaxy from the cloudman admin console, but it seems to be taking a long time. Is this normal?"}, {"role": "system", "text": "Sorry to hear about the storage troubles. Hopefully together we can figure out what exactly is going on. So, by default, Galaxy does not really delete files from disk when a file is deleted from a history. It just marks a dataset as deleted in case you change your mind about wanting to un-delete later. To actually remove a dataset from disk, it is necessary to purge it, which is available under the history settings cog by clicking Purge Deleted Datasets.\nA more system-level approach would be to ssh to your instance and run the cleanup scripts, as described here: https://galaxyproject.org/admin/config/performance/purge-histories-and-datasets/\nOverall, the reason the CloudMan console is showing more disk usage than what you have uploaded to Galaxy is because of Galaxy and other supporting/accompanying software that is running on the same disk.\nFinally, if all you have done is uploaded a single file to your instance and realized the disk is too small, I would recommend you just delete that particular instance and launch a brand new one with a larger disk. Things will just go a lot quicker and smoother than fighting with the storage and/or waiting for a disk resize (which can take many, many hours to complete in the case of Amazon)."}], [{"role": "user", "text": "I finished an analysis and I would like to download a printable version of the workflow to keep for records.\nsomething similar to what is presented in \u201cview\u201d, but without printing or copy-pasting form browser that looks bad.\nI downloaded the *.ga but I dont know what to do with it.\nthanks!"}, {"role": "system", "text": "Hi @AgustinGV\nThere isn\u2019t a way to directly export a PDF of the Workflow \u201cview\u201d with formatting preserved, although that has been discussed and may be included in a future release. Or if you are interested in helping out, a development PR to add in this functionality would be welcomed. Many views could use this type of output.\nThe page can be saved in .htm format (\u201ccontrol+right-click+save-as\u201d on MAC OSX) which will remove a bit of the extra formatting. You\u2019ll end up with just the text, which may be enough.\nYou could also try importing into MyExperiment and see if their viewing/printing options work for you. How-to: http://wiki.myexperiment.org/index.php/Galaxy_Help#To_access_myExperiment_from_Galaxy_and_enable_the_Galaxy_perspective\nThe .ga file is a complete archive if that is your overall goal. It can be uploaded into any Galaxy server and viewed again, or shared, etc.\nNot a very satisfying answer, but hope that helps a bit.\nping @dannon \u2013 Is there a better/alternative way to do this I\u2019m missing?"}], [{"role": "user", "text": "Hello,\nI am using version 17.05. I am trying to disable automatic tool reloading. I know in galaxy.ini \u201cwatch_tools = False\u201d won\u2019t disable the automatic reload for the tools that are listed in the tool_conf.xml. Please let me know if there is a way to disable it. Thanks.\nBTW, the reason I want to disable reloading is: When there is a running job, if I edit any tool xml file, somehow Galaxy deletes a tmp file when reloading the tools. When the job finishes, it throw an error complaining missing a tmp file. This is annoying because I cannot edit any tool file when there are running jobs. Here is an error message:\nTraceback (most recent call last):\nFile \u201c/scratch/galaxy/job_working_directory/011/11813/set_metadata_LwWNda.py\u201d, line 1, in \nfrom galaxy_ext.metadata.set_metadata import set_metadata; set_metadata()\nFile \u201c/opt/galaxy/lib/galaxy_ext/metadata/set_metadata.py\u201d, line 85, in set_metadata\ndatatypes_registry.load_datatypes( root_dir=galaxy_root, config=datatypes_config )\nFile \u201c/opt/galaxy/lib/galaxy/datatypes/registry.py\u201d, line 106, in load_datatypes\ntree = galaxy.util.parse_xml( config )\nFile \u201c/opt/galaxy/lib/galaxy/util/init.py\u201d, line 214, in parse_xml\nroot = tree.parse( fname, parser=ElementTree.XMLParser( target=DoctypeSafeCallbackTarget() ) )\nFile \u201c/usr/lib/python2.7/xml/etree/ElementTree.py\u201d, line 647, in parse\nsource = open(source, \u201crb\u201d)\nIOError: [Errno 2] No such file or directory: \u2018/scratch/galaxy/tmp/tmp4u4oxV\u2019"}, {"role": "system", "text": "You can disable tool reloading in your (shed_)tool_conf.xml files by changing <toolbox monitor=\"true\"> to  <toolbox monitor=\"false\"> . 17.05 is extremely old though, this is a bug we fixed a very long time ago, I\u2019d strongly recommend updating Galaxy to a more recent version."}], [{"role": "user", "text": "Hello,\nI\u2019m trying to visualize genes from a gff3.gz file using JBrowse, with gff3.gz and fasta.gz files downloaded from NCBI into Galaxy. The gff3.gz file is not recognized by JBrowse, so it is displayed as a bed file, and as a result the \u201cType\u201d column (third column) is displayed instead of the gene names (which are in the Attributes column 9). If I try and convert the gff3.gz dataset to a gff3 datatype before displaying in Jbrowse, then no genes or labels appear. If I download the gff3.gz file to my laptop and decompress and reupload, then the gene names do appear. I\u2019m wondering if you can help me figure out how to display the gene names without downloading and decompressing first.\nHere are my steps:\n\n\ndownload genome and gff3 into Galaxy\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.gff.gz\n\n\nDisplay in Jbrowse by selecting \u201cgenome from history\u201d and adding a track \u201cGFF/GFF3/BED Features\u201d\n\n\nHere is the history showing the gff3.gz file downloaded from NCBI, which does not display gene names in JBrowas, and the one that was downloaded, decompressed and  reuploaded from my laptop, which does show gene names in JBrowse:\nhttps://usegalaxy.org/u/rbator01/h/intro-to-ngs\nThank you for any suggestions!\nRebecca"}, {"role": "user", "text": "OK, I figured it out! If you run the \u201cConvert\u201d->\u201cConvert compressed file to uncompressed file\u201d and use the uncompressed file in JBrowse, it is read as the correct datatype and gene names are displayed. I\u2019m wondering why it\u2019s not decompressed automatically, as the fasta is. In any case, this solution works for me."}], [{"role": "user", "text": "Hi there, I\u2019m trying to obtain a consensus sequence from sequences that I have already aligned.\nWhat has been done so far is that I have uploaded multiple data obtained from PCR on galaxy. I then used Bowtie 2 to map reads against a reference genome (obtained from Genbank). I then ran ivar consensus on each of the bowtie outputs to get the consensus and aligned them on MEGA11. How do I get a single consensus from the aligned data? I tried to repeat the whole process by reuploading the aligned dataset onto galaxy and running Bowtie 2 and ivar consensus again but this gave me a sequence with alot of Ns. The aligned data set I had consists of about 30 sequences and they all aligned very well on MEGA11.\nDoes anyone know what I am doing wrong/what I can do to circumvent this? Thanks!"}, {"role": "system", "text": "Hi @zach.low\nI believe MEGA can create a consensus sequence for an alignment.\nKind regards,\nIgor"}], [{"role": "user", "text": "I\u2019m having an issue getting PAM based user logins to work in v20.01. The problem has to do with Galaxy not having sufficient permissions to create a new user account folder.\nFor some unknown reason, it wants to append our org\u2019s domain to the user folder. This is unnecessary because the correctly named username folder (sans suffix) already exists in /home. I\u2019ve tested manually creating the fully qualified folder prior to login and it works.\ngalaxy.log:\ngalaxy.webapps.galaxy.controllers.user DEBUG 2020-05-29 13:15:34,638 [p:93475,w:1,m:0] [uWSGIWorker1Core2] trans.app.config.auth_config_file: /hpc/software/installed/galaxy/20.01/config/auth_conf.xml\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,639 [p:93475,w:1,m:0] [uWSGIWorker1Core2] use username: True use email False email None username test.user\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,639 [p:93475,w:1,m:0] [uWSGIWorker1Core2] PAM auth: will use external helper: False\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,951 [p:93475,w:1,m:0] [uWSGIWorker1Core2] PAM authentication successful for test.user\ngalaxy.auth.util DEBUG 2020-05-29 13:15:34,955 [p:93475,w:1,m:0] [uWSGIWorker1Core2] Email: test.user@domain.com, auto-register with username: test.user\ngalaxy.web.framework.decorators ERROR 2020-05-29 13:15:35,102 [p:93475,w:1,m:0] [uWSGIWorker1Core2] Uncaught exception in exposed API method:\nTraceback (most recent call last):\nFile \u201clib/galaxy/web/framework/decorators.py\u201d, line 282, in decorator\nrval = func(self, trans, *args, **kwargs)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 122, in login\nreturn self.__validate_login(trans, payload, **kwd)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 147, in __validate_login\nmessage, user = self.__autoregistration(trans, login, password)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 105, in __autoregistration\ntrans.handle_user_login(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 720, in handle_user_login\nself.user_checks(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 665, in user_checks\nself.check_user_library_import_dir(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 657, in check_user_library_import_dir\nsafe_makedirs(os.path.join(self.app.config.user_library_import_dir, user.email))\nFile \u201clib/galaxy/util/path/init.py\u201d, line 114, in safe_makedirs\nmakedirs(path)\nFile \u201c/hpc/software/installed/galaxy/20.01/.venv/lib64/python3.6/os.py\u201d, line 220, in makedirs\nmkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: \u2018/home/test.user@domain.com\u2019\nauth_conf.xml:\n<?xml version=\u201c1.0\u201d?>\n<auth>\n<authenticator>\n<type>PAM</type>\n<options>\n<auto-register>True</auto-register>\n<maildomain>domain.com</maildomain>\n<login-use-username>True</login-use-username>\n<pam-service>sshd</pam-service>\n</options>\n</authenticator>\n</auth>\nsssd.conf:\n[sssd]\ndomains = domain.com\nconfig_file_version = 2\nservices = nss, pam, sudo\n[domain/domain.com]\nad_domain = domain.com\nkrb5_realm = DOMAIN.COM\nrealmd_tags = manages-system joined-with-samba\ncache_credentials = True\nid_provider = ad\nkrb5_store_password_if_offline = True\ndefault_shell = /bin/bash\nldap_sasl_authid = galaxy1$\nldap_id_mapping = False\nuse_fully_qualified_names = False\nfallback_homedir = /home/%u\naccess_provider = simple\nsimple_allow_groups = Group1, Group2\nAnyone able to point me in the right direction?\nThanks."}, {"role": "user", "text": "Solution: Disable all user_library_import settings."}], [{"role": "user", "text": "I was working on usegalaxy.org\nI started working through the Galaxy training/Sequence analysis/Quality control hands-on training on Friday, July 12. At the time I had no problems. However, today (Tue, July 16) when I returned to the history and tried to view the results for \u2018FastQC on data_1:Webpage\u2019 I didn\u2019t see any of the graphical displays.\nThe green box underneath the step name contained: \u2018Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/239/24239388/_job_tmp -Xmx7g -Xms256m.\u2019\nWhen I clicked on the icon to view details and then on \u2018stderr,\u2019 it displayed the same message: 'Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxyrepl/main/jobdir/024/239/24239388/_job_tmp -Xmx7g  -Xms256m\nI am wondering if something needs to be reset as described in the response to a similar message 5 days ago \u2018FastQC file not visible - Add tool to HTML display \u201cwhitelist\u201d\u2019\nThank you!"}, {"role": "system", "text": "\n\n\n dsiegele:\n\nas described in the response to a similar message 5 days ago \u2018FastQC file not visible - Add tool to HTML display \u201cwhitelist\u201d\u2019\n\n\nexactly, thanks for the report, I think I fixed it now for good (until the next version of fastqc comes out :D)"}], [{"role": "user", "text": "Dear Sir,\nI\u2019m sorry because, earlier I wasn\u2019t aware of the galaxy policy and hence I created multiple accounts for my work.\nI, therefore, kindly request you to delete all my previous accounts and grant me permission to use one account for carrying out my research work.\nI apologise for my previous mistakes and assure this will not happen in future.\nThanking you\nYours sincerely\nVikas Gupta"}, {"role": "system", "text": "Hi @VIKAS_GUPTA\nThanks for understanding. If you don\u2019t have any accounts that are still active, you can create a new account right now \u2013 just use an email address that you haven\u2019t used for an account at the server before (option 3 below). Should you remember any other stray accounts later on, please consolidate data and remove excess accounts yourself under User > Preferences (option 2 below).\n\n\n\nNot able to log in\n\n\nOptions\n\nIf you have one single active account at UseGalaxy.org, keep using that one.\nIf you have two or more active accounts at UseGalaxy.org, please manage your data into one account and delete the others under User > Preferences.\nCreating a single new account with a different email address is OK if you do not have any active account(s) at UseGalaxy.org.\n\n\n\nThank you! "}], [{"role": "user", "text": "Hello,\nI have run a microRNA analysis using BWA and HISAT2 as mapping tools and with both I used HTSeq-count and then tried using DESeq2. With both different runs (for the different aligners) I have gotten the same error multiple times and I am not sure why. (I used the UCSC genome browser microRNA database for both). I have also ran other aligners and have gotten DESeq2 to work so I am just confused. The HTSeq count has 2,315 lines of data so I am no sure why DESeq2 states that there are no counts.\nThis is the error code:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nError in DESeqDataSet(se, design = design, ignoreRank) :\nall samples have 0 counts for all genes. check the counting script.\nCalls: get_deseq_dataset \u2026 DESeqDataSetFromHTSeqCount -> DESeqDataSetFromMatrix -> DESeqDataSet\nThank you!"}, {"role": "system", "text": "Take a look at this post. It might help."}], [{"role": "user", "text": "Hello, hope everyone well .\nI have a project where I am given  57 sequences ( including control).\nI would not know what data there is there , it is related to the pituitary adenoma ( different forms ) and it is also including controls . What I need - UP & DOWN regulated genes. Can you recommend work flow or tutorial exists?\nThank you in advance ."}, {"role": "system", "text": "Hi @Lidia_Ryabova, there is a range of tutorials here: https://training.galaxyproject.org/training-material/topics/transcriptomics/. Most of them have workflows attached which you can use.\nI am not really that familiar with RNA-seq, but maybe this one is a good start? https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html"}], [{"role": "user", "text": "I currently have a tool that our team has developed to help with data analysis regarding genomic and proteomic datasets. I wanted to deploy this tool using Galaxy, so is there any way that I can add it to the website? By the way, our tool is currently written in Python."}, {"role": "system", "text": "Hi @psundara\nThanks for the clarification.\nResources for tool development:\n\nTools working group Galaxy Working Groups and Projects - Galaxy Community Hub\n\nPlanemo for tool development https://planemo.readthedocs.io/\n\nToolShed (the Galaxy \u201capp store\u201d) https://toolshed.g2.bx.psu.edu/\n\nGalaxy Training Network (GTN) https://training.galaxyproject.org/\n\n\nThe general idea is to:\n\nDevelop a Galaxy compatible tool repository\nAdd the tool to the ToolShed.\nThe tool/tool suite is then available for any Galaxy server to install and use.\nThe tool form itself can include external links to author created web resources and publications in the help section, along with summarized usage help.\nConsider creating a GTN tutorial for how to use the tool, or create your own documentation with a published workflow.\n\nOnce the core basics are all in place (up to the Toolshed step is the minimum), then the public Galaxy sites may choose to include it. The Tools working group can help with those details.\nHope that helps!"}], [{"role": "user", "text": "Hi,\nI am struggling to get a custom interactive tool working. From gitter/admin I got suggested by Bj\u00f8rn and Helena to base my attempt on existing tools. The panoply tool seemed most suitable to me.\n\n  \n\n      github.com\n  \n\n  \n    usegalaxy-eu/galaxy/blob/release_21.05_europe/tools/interactive/interactivetool_panoply.xml\n\n\n    <tool id=\"interactive_tool_panoply\" tool_type=\"interactive\" name=\"Panoply\" version=\"@VERSION@\">\n    <description>interative plotting tool for geo-referenced data</description>\n    <macros>\n    <token name=\"@VERSION@\">4.5.1</token>\n    </macros>\n    <requirements>\n        <container type=\"docker\">quay.io/nordicesmhub/docker-panoply:@VERSION@</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"Panoply on $infile.display_name\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n    mkdir output &&\n        mkdir /config/home  &&\n        mkdir /config/home/output &&\n        export HOME=/config/home &&\n        cp '$infile' '/config/home/$infile.display_name' &&\n\n\n\n  This file has been truncated. show original\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWhat I have done\n\n\nAdded the panoply tool as it is. Works as expected on our galaxy instance. I use the same job destination definition as my custom tool (interactive_slurm_general, see below).\n\n\nTesting custom tool outside galaxy:\n\n\ndocker run --rm -p 5800:5800  maikenp/comphep-ubuntu-gui\nRuns as expected. I see the comphep tool and can use it.\n\nScreenshot 2021-10-19 at 10.18.261920\u00d71343 231 KB\n\n\nImplementation in galaxy: First of all the tool exits right away. Putting a sleep 360 I can get it running. However I see: \u201cProxy target missing\u201d when I try to access the interactive tool running in galaxy. I use the same destination for my custom tool as for the panoply tool.\n\n\nScreenshot 2021-10-19 at 09.42.441305\u00d7210 20 KB\n\n\nThe docker run command that the galaxy produces for the comphep tool is the following:\n\ndocker run -e \"GALAXY_SLOTS=$GALAXY_SLOTS\" -e \"HOME=$HOME\" -e \"_GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR\" -e \"_GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR\" -e \"TMPDIR=$TMPDIR\" -e \"TMP=$TMP\" -e \"TEMP=$TEMP\" -p 5800 --name 88eba70b849646718ebe23e4cb668b88 -v /srv/galaxy/server:/srv/galaxy/server:ro -v /srv/galaxy/local_tools/interactive:/srv/galaxy/local_tools/interactive:ro -v /storage/galaxy/jobs_directory/001/1741:/storage/galaxy/jobs_directory/001/1741:ro -v /storage/galaxy/jobs_directory/001/1741/outputs:/storage/galaxy/jobs_directory/001/1741/outputs:rw -v /storage/galaxy/jobs_directory/001/1741/configs:/storage/galaxy/jobs_directory/001/1741/configs:rw -v /storage/galaxy/jobs_directory/001/1741/working:/storage/galaxy/jobs_directory/001/1741/working:rw -v /storage/galaxy/files:/storage/galaxy/files:rw -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v \"$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR:rw\" -w /storage/galaxy/jobs_directory/001/1741/working --net bridge --rm maikenp/comphep-ubuntu-gui:latest /bin/sh /storage/galaxy/jobs_directory/001/1741/tool_script.sh > ../outputs/tool_stdout 2> ../outputs/tool_stderr; return_code=$?; cd '/storage/galaxy/jobs_directory/001/1741'; \n\n\nThe docker run command for the panoply tool (which works) is the following:\n\ndocker run -e \"GALAXY_SLOTS=$GALAXY_SLOTS\" -e \"HOME=$HOME\" -e \"_GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR\" -e \"_GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR\" -e \"TMPDIR=$TMPDIR\" -e \"TMP=$TMP\" -e \"TEMP=$TEMP\" -p 5800 --name 49e8f15c4be9441593615daa32392a89 -v /srv/galaxy/server:/srv/galaxy/server:ro -v /srv/galaxy/local_tools/interactive:/srv/galaxy/local_tools/interactive:ro -v /storage/galaxy/jobs_directory/001/1742:/storage/galaxy/jobs_directory/001/1742:ro -v /storage/galaxy/jobs_directory/001/1742/outputs:/storage/galaxy/jobs_directory/001/1742/outputs:rw -v /storage/galaxy/jobs_directory/001/1742/configs:/storage/galaxy/jobs_directory/001/1742/configs:rw -v /storage/galaxy/jobs_directory/001/1742/working:/storage/galaxy/jobs_directory/001/1742/working:rw -v /storage/galaxy/files:/storage/galaxy/files:rw -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v \"$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR:rw\" -w /storage/galaxy/jobs_directory/001/1742/working --net bridge --rm quay.io/nordicesmhub/docker-panoply:4.5.1 /bin/sh /storage/galaxy/jobs_directory/001/1742/tool_script.sh > ../outputs/tool_stdout 2> ../outputs/tool_stderr; return_code=$?; cd '/storage/galaxy/jobs_directory/001/1742'; \n\nQuestions:\na) At the moment I dont need anything actually done in the tool except that it should run. Can the comand field of the tool wrapper be more or less empty as I have it now? It seems not, as the tool then exits right away? Or could the exit be due to other underlying problems?\nb) Can you spot other things that don\u2019t seem right with the Dockerfile/startapp.sh/tool-wrapper?\nDockerfile:\nFROM jlesage/baseimage-gui:ubuntu-18.04\nMAINTAINER Maiken Pedersen, maikenp@uio.no\n\nRUN apt-get update -y && \\\n     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n         build-essential \\\n\t libx11-dev \\\n\t libgfortran5 \\\n\t emacs \\\n\t wget \\ \n\t gfortran && \\\n     rm -rf /var/lib/apt/lists/*\n\nUSER root\nCOPY startapp.sh /startapp.sh\n\nRUN mkdir -p /opt/comphep && cd /opt/comphep/ && wget https://theory.sinp.msu.ru/lib/exe/fetch.php/comphep/download/comphep-4.5.2.tgz && tar -zxf  comphep-4.5.2.tgz && \\\n    cd /opt/comphep/comphep-4.5.2 && \\\n    ./configure && \\\n    make && \\\n    make setup WDIR=/opt/comphep_wd\n\nENV APP_NAME=\"COMPHEP\"\nRUN chown 1000:1000 -R /opt/comphep_wd\nWORKDIR /opt/comphep_wd\n\nstartapp.sh:\n#!/bin/sh\ncd  /opt/comphep_wd\nexec /opt/comphep_wd/comphep\n\ntool-wrapper:\n<tool id=\"interactive_tool_comphep_gui\" tool_type=\"interactive\" name=\"COMPHEP ubuntu-gui\" version=\"1.0.0\">\n    <requirements>\n      <container type=\"docker\">maikenp/comphep-ubuntu-gui:latest</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"COMPHEP\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    \n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n        mkdir output &&\n\tsleep 360\n    ]]>\n    </command>\n\n    <outputs>\n      <data name=\"outputs\" label=\"COMPHEP outputs\">\n\t<discover_datasets pattern=\"__name_and_ext__\"  directory=\"output\"/>\n      </data>\n    </outputs>\n    \n</tool>\n\njob_conf.xml relevant section for the destination:\n<destination id=\"interactive_slurm_general\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--time=72:00:00 --mem-per-cpu=8</param>\n            <param id=\"docker_enabled\">true</param>\n            <param id=\"docker_volumes\">$defaults</param>\n            <param id=\"docker_sudo\">false</param>\n            <param id=\"docker_net\">bridge</param>\n            <param id=\"docker_auto_rm\">true</param>\n            <param id=\"docker_set_user\"></param>\n            <param id=\"require_container\">true</param>\n        </destination>\n"}, {"role": "user", "text": "Thanks to Anne she pointed out that I was missing /init statement which is needed by galaxy.\nThat solved it all, everything seems to be working perfectly.\nSo my tool now looks like:\n<tool id=\"interactive_tool_comphep_gui\" tool_type=\"interactive\" name=\"COMPHEP ubuntu-gui\" version=\"1.0.0\">\n    <requirements>\n      <container type=\"docker\">maikenp/comphep-ubuntu-gui:latest</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"COMPHEP\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    \n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n        mkdir output &&\n\tcd - &&\n        /init ;\n\ttouch output/test.txt &&\n        echo \"Hi!\" > output/test.txt &&\n\tsleep 360\n    ]]>\n    </command>\n\n    <outputs>\n      <data name=\"outputs\" label=\"COMPHEP outputs\">\n\t<discover_datasets pattern=\"__name_and_ext__\"  directory=\"output\"/>\n      </data>\n    </outputs>\n   \n"}], [{"role": "user", "text": "Hello. I ran Cuffdiff on some RNAseq data last weekend, and it completed in about 48 hours. However, I discovered that I didn\u2019t select the option to generate the SQLite file for use in CummeRbund. So, I re-ran the same analysis, but it\u2019s been running for four days now. I have two questions:\n\nShould it take this long to run?\nHow consistent are Cuffdiff\u2019s outputs between runs? Will genes that came up as differentially expressed from the first time I ran Cuffdiff have the same results the second time around (ie does Cuffdiff use any random error to generate its results)? Or do I need to wait for it to finish re-running to  start my follow-up analysis?\n\nThank you!"}, {"role": "system", "text": "Hi @elizamccoll\nmaybe refresh Galaxy page by clicking at \u201cGalaxy\u201d banner at the top left corner. Is the job still running?\nFour days: do you mean run time or both queue time and run time?\nThe run time depends on resources allocation for a job.\nIn my experience Cuffdiff results are consistent, as long as you use the same version of the software.\nKind regards,\nIgor"}], [{"role": "user", "text": "I\u2019ve had issues just installing galaxy because it keeps stalling. I thought I had finally got it installed but when I change the config file- it wont make my account admin even when its changed. I tried run.sh again to see if I installed it correctly and I got this warning\n*** WARNING: you are running uWSGI without its master process manager ***\n\nand it stalls at\ngalaxy.model.database_heartbeat DEBUG 2020-05-21 10:43:15,574 [p:1239,w:1,m:0] [database_heartbeart_main.web.1.thread]\nmain.web.1 is config watcher     \n\nI am trying to set up galaxy to use conda if that makes a difference. I also just upgraded my memory since before I was told it may be stalling due to lack of memory so now I know that\u2019s not the issue."}, {"role": "system", "text": "Given your next related question (run.sh stalls after Toolbox index finished) could you please post the whole startup log, or at least couple of hundred lines from the end?"}], [{"role": "user", "text": "I\u2019m having trouble accessing usegalaxy.org. the problem is that in other computers it is possible to enter the service, but it simply does not load from mine"}, {"role": "system", "text": "Hi @r.papikyan\nThe usegalaxy.org service is up and running.\n\n  \n      \n\n      status.galaxyproject.org\n  \n\n  \n    \n\nThe Galaxy Project Status\n\n  Welcome to The Galaxy Project's home for real-time and historical data on system performance.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nIf you are having trouble connecting from some computers and not others, the problem likely involves your internet connection."}], [{"role": "user", "text": "\nUntitled1336\u00d7777 59.4 KB\n\nHello.\nI\u2019m attaching a snapshot of the fastQC output of a sample, I\u2019m debating how to QC this and similar samples, on the one hand it look alright as is, on the other hand I sent it to bioinformatic and he did the following:\nTrimming and filtering of raw reads\nRaw reads (fastq files) were inspected for quality issues with FastQC. Following that, the reads were quality-trimmed with cutadapt, using a quality threshold of 32 for both ends, poly-G sequences (NextSeq\u2019s no signal) and adapter sequences were removed from the 3\u2019 end, and poly-T stretches were removed from the 5\u2019 end (being the reverse-complement of poly-A tails). The cutadapt parameters included using a minimal overlap of 1, allowing for read wildcards, and filtering out reads that became shorter than 15 nt. Finally low quality reads were filtered out using fastq_quality_filter, with a quality threshold of 20 at 90 percent or more of the read\u2019s positions.\nhis QC caused the lost of 16% of the reads\nI also triad using fastp on default setting which only discard 1.2% of the reads\u2026\ni will be happy to hear your inputs\nThank you\nNetanel"}, {"role": "system", "text": "hi @Netanel_Cohen\nstringency of trimming depends on downstream analysis, so take settings from a recent paper with a similar analysis as yours or test different trimming setups on one sample or small number of samples. Number of reads is not really relevant without information on the project. 47M reads look like an overkill for differential gene expression in bacteria but might not be sufficient for whole genome variant calling, and excessive data might results in poor quality in some projects.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am new to Galaxy and bioinformatics so feel free to let me know if there is a better way of asking my question. The files were executed in a way that the MultiQC did not recognize the files existed. I downloaded SRA files to Galaxy. These files were transfered to a Paired-end data folder with each accession (sample) inside along with the forward and reverse fastq files. This general file format remains after running these Files through FastQC. Now, I am trying to run the files through MultiQC but the data is not recognized. Does anyone have suggestions on how to fix this?"}, {"role": "system", "text": "Please see this tutorial https://galaxyproject.org/tutorials/ngs/#organizing-and-qcing-multiple-datasets for:\n\nHow to create fastq dataset collections\nHow to run FastQC in batch on collections\nHow to generate a MultiQC report based on those FastQC results in a collection\n\nDouble checks: Make sure the MultiQC tool form is set up correctly for this use case (all covered by the video in the tutorial):\n\nFastQC reports are in a dataset collection\n\u201cFastQC\u201d is set as the report type.\n\u201cDataset Collections\u201d are set as the input type.\nPick the \u201craw\u201d results (text report). The \u201chtml\u201d results are not appropriate.\n\nMy guess is that either item 2 or 3 was not set properly on the FastQC tool form.\nadmin edit: Cross-posted at Gitter: https://gitter.im/galaxyproject/Lobby?at=5c0c5f88178d7860a1a84938"}], [{"role": "user", "text": "Hello!\nI made xml wrappers for some gatk4 tools. They receive and/or generate bam files and also corresponding bai files. So i made separate inputs/outputs via which they can pass that bai files.\nBut today i tried to download some bam from galaxy web interface (21.05), and it showed me menu in which i can also download bai file, and this bai is not the bai generated by my tools cos it weighs about 1.5 times less. I found it by size, and it was there:\n\n/home/transgen/galaxy/database/objects/_metadata_files/8/9/c/metadata_89ca5b70-7881-4cc1-bc7a-fcc701c18b8b.dat\n\n, whereas the bam file was there:\n\n/home/transgen/galaxy/database/objects/9/4/b/dataset_94bc20e7-2eaf-4345-9b1f-a2e7c0988b5d.dat\n\nIs there a way to use this bai in tool xmls so i could pass only one noodle for each bam file?\nThanks in advance."}, {"role": "user", "text": "The right answer is:\nln -s '$bam.metadata.bam_index' 'input.bam.bai'"}], [{"role": "user", "text": "First of all, can you tell me how necessary it is to generate read counts in an RNA-Seq differential expression analysis and what it is for? I\u2019m working with the reference genome of Arabidopsis thaliana and paired-end data.\nI checked the tools FeatureCounts and HTseq counts and both of them require a GTF file, but the Arabidopsis genome is not listed there, should I do my own GTF file with the whole genome or just with some genes?\nThanks!"}, {"role": "system", "text": "Hi Mario\nWithout a count table, you cannot do the differential gene expression analysis. You might want to read up on this in the training material:\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: Reference-based RNA-Seq data analysis\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: RNA-Seq reads to counts\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      \n      galaxyproject.github.io\n  \n  \n    \n\nGalaxy Training: RNA-seq counts to genes\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\nFor both htseq-count and featureCounts, you can provide your own GTF file. A possible source is here: http://plants.ensembl.org/info/website/ftp/index.html\nEven if you are interested in only a few genes, it is recommended to to the differential gene analysis on all genes, and select afterwards.\nI hope this helps, Hans-Rudolf"}], [{"role": "user", "text": "We have some custom datatypes (hep.mc.root for instance) defined very simply in datatypes_conf.xml like so:\n<datatype extension=\"hep.mc.root\" type=\"galaxy.datatypes.binary:Binary\" subclass=\"true\" display_in_upload=\"true\" description=\"ROOT binary file.\"/>\n\nIn a tool we are creating we wish to have as input-data a data collection which has data elements of this type. We have this data collection (a list with X number of hep.mc.root files ) in the history.\nHowever, in the input field of the tool, only collections with type png or empty lists are possible to select (we have other tools producing png files, and thus they are in the history). png is one of the standard types from galaxy. Could it be that for Dataset Collections the new custom hep.mc.root datatype does not exist, thus can not be selected?\nThe screenshot:\nThe collections suggested in the input field are either empty or contain png files. The MC Hists item 3895 is the one I would like to be able to select, but it is not suggested in the collection list in the input field to the left as you see.\n\nScreen Shot 2021-08-25 at 13.24.221263\u00d7534 60.8 KB\n\nThe tool wrapper looks like the following:\n<tool id=\"plotHistograms_maiken\" name=\"Plot Histograms MaikenTest\" profile=\"16.04\" version=\"1.0.0\">\n<description></description>\n<requirements>\n\t<requirement type=\"package\" version=\"1.0.0\">fys5555_py3</requirement>\n</requirements>\n  <command>\n    <![CDATA[\n\t     #set $origname_data = $dataFile.element_identifier\n\t     #set $orignames_mc = []\n\t     #set $galaxynames_mc = []\n\t     #for $d in $mcFileList\n\t     #set $tmpname=$d.element_identifier\n\t     #silent orignames_mc.append(\"%s\" % $tmpname)\n\t     #silent galaxynames_mc.append(\"%s\" % $d)\n\t     #end for\n\t     python '/storage/shared/software/Scripts/ATLASOpenData/13TeV/copyForMakePlots.py' '$dataFile' '$origname_data' '$galaxynames_mc' '$orignames_mc';\n\t     python '/storage/shared/software/Scripts/ATLASOpenData/13TeV/MakePlots.py' '$channel';\n    ]]> \n  </command>\n  <inputs>\n    <param name=\"channel\" type=\"select\" label=\"Channel\">\n      <option value=\"ee\" selected=\"true\">ee</option>\n      <option value=\"uu\">uu</option>\n    </param>\n    <param name=\"dataFile\" type=\"data\"  format=\"hep.data.root\"  label=\"Data root file\" />\n    <param name=\"mcFileList\" type=\"data_collection\"  label=\"MC root file(s)\" />\n  </inputs>\n  \n  <outputs>\n \t <collection name=\"output\" type=\"list\" label=\"Conventional Analysis Plots\"> \n\t\t         <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.png\"  directory=\"Histograms/Plots\"  ext=\"png\"  recurse=\"true\" visible=\"false\" from_tool_provided_metadata=\"true\" />\n\t \n\t\t </collection>\n  </outputs>\n</tool>\n\n"}, {"role": "user", "text": "Update: ?roblem solved. It was confusion related to the ext option in the discover_datasets parameter and the format option in the collection parameter in the tool that generated the list in the first place:\nWe had:\n<collection name=\"output2\" type=\"list\" format=\"hep.mc.root\"  label=\"MC Hists\">\n    <discover_datasets pattern=\"(?P&lt;designation&gt;.+.*MC.*)\\.root\" directory=\"Histograms/\"   ext=\"root\" recurse=\"true\"  />\n  </collection>\n\nAnd then changed to\n<collection name=\"output2\" type=\"list\"  label=\"MC Hists\">\n    <discover_datasets pattern=\"(?P&lt;designation&gt;.+.*MC.*)\\.root\" directory=\"Histograms/\"   ext=\"hep.mc.root\" recurse=\"true\"  />\n  </collection>\n\nSeems the format in the collection parameter is just ignored, and the ext is taken as the format. We had the wrong extention name \u201croot\u201d instead of \u201chep.mc.root\u201d.\nProblem solved!"}], [{"role": "user", "text": "Hi,\nI want to obtain 3\u2019UTR sequences from my assembled transcripts.\nWhat tool should I use?\nor is it possible to add the following tools on the online Galaxy server:\nExUTR (GitHub - huangzixia/ExUTR: ExUTR is a practical and powerful tool that enables rapid genome-wide 3'-UTR prediction from massive RNA-Seq data) or\nCodAn (GitHub - pedronachtigall/CodAn: CDS prediction in transcripts)."}, {"role": "system", "text": "Hi @s316052000\nmaybe have a look at TransDecoder. Activate all outputs. Try it with \u2018output only longest ORF option\u2019. It can produce GFF and BED files. GFF has annotation of UTRs, and you can get positions of UTRs from BED file, as well. Sequences can be extracted with any FASTA extraction tool, like getfastabed.\nYou probably will miss transcripts with short ORFs and some complicated cases, such as polycistronic transcripts, might be difficult to process correctly. Also, keep in mind that assembled transcripts from some tools, such as Trinity, can be in both strands, so if you use BED, make sure you consider orientation of ORF while selection 3\u2019 UTR.\nHope it does make sense.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear all,\nTrim Galore! can\u2019t actually recognize any fastq files or Cutadapt output files for Nextera Transposase trimming. Do you have any solutions?\nThanks in advance,\nBest\nBernhard"}, {"role": "system", "text": "Hi @Bernhard,\nI think it solved your problem Trim Galore! can't find fastq files - #2 by igor."}], [{"role": "user", "text": "Hi everyone,\nI had similar problem with vcftotab on Galaxy Main (Missing information VCFtoTab-delimited) and tried to workaround this problem using vcftotab on Galaxy EU and was asked to issue this similar problem.\nUsing vcftotab on my data leaves me with lots of missing information from the INFO field. Even when using bcftools norm (accordingly to the advice in the Galaxy Tutorial \u201csomatic variant calling\u201d), cleaning out all the information on the Info field with SnpSift rmInfo and annotating it with SnpEff again (which is the annotation tool used in the just mentioned tutorial) I keep having lots of missing information.\nI tried other tools in order to query my annotated variants, such as bcftools query. However, I encounter the problem, that for information in the Info field with the same INFO Tag (gnomad_exome and gnomad_genome for Non-finish are both annotated to my variants using annovar with AF_nfe) only one value is represented in the query output.\nEventually I could manage to install a Galaxy docker and vcftotab worked perfectly on the docker. However, snpeff does not work properly on my docker (SnpEff Error java.lang.OutOfMemoryError: Java heap space). So right know I cannot apply my workflow on Galaxy Eu or my own instance, because either way one of the tools used (snpeff or vcf2tsv) is not working.\nI am glad for any help!\nAll the best Rose."}, {"role": "system", "text": "answered in Missing information VCFtoTab-delimited"}], [{"role": "user", "text": "Just to confirm: Galaxy does not need Python 2.7, right? It will work fine with 3.6/3.7?\nJust because I read this on so many places\u2026"}, {"role": "system", "text": "Yes, recent Galaxy versions work fine with 3.5+ Galaxy (19.09 or newer). We still occasionally discover issues with python 3 support as more people start using Galaxy on python 3, but I would advise you to update.\nThere is currently a bug with uwsgi that prevents threading on python > 3.7.1, so your best bet now would be 3.6"}], [{"role": "user", "text": "Hi All,\nI have a question about enabling the fractions button in FeatureCounts. The subtext seems to contain a contradiction, because it states that the \u2018NH\u2019 value ( multimapped[MM] reads) will be used for the fractions (see image) of multimapped and overlapping reads. It does not say anything about how the overlapping genes are included in the fraction counts.\nOn the biostars forum (the --fraction option of featureCounts) it is explained that the MM reads are calculated by using the NH and the overlapping reads calculated by a separate variable ( which is missing from Galaxy:FeatureCounts)\nMy question: Is the NH value the only variable used for the calculation of fractional counts on Galaxy?\n\nimage774\u00d7122 4.51 KB\n\nBest regards,\nPatrick"}, {"role": "system", "text": "Hi @Patrick,\nthe tool works as expected, but there is an error in the description, which will be fixed. Thanks for your report!\nRegards"}], [{"role": "user", "text": "Hi everyone,\nThis is a question for the LEfSe galaxy module: I have a question about comparing more than two classes. Is this possible in the module? I tried this but the results were a bit unexpected.\nWhen I compare, for example, class A and class B or A and C or A and B, I get several distinct differences. However, when I compare A, B, and C together, I get only one difference that never appeared in the individual comparisons? Is that an expected result? Would you have any advice on this? Thank you!"}, {"role": "system", "text": "Hi @microbiome\nThere are a few versions of Lefse in the Galaxy community. I think Lefse only produces a single comparison per run for the versions hosted at ORG, EU, and AU. Not sure about Biobakery\u2019s version. Maybe that explains your observations?"}], [{"role": "user", "text": "Hey, My datasets are still in red and haven\u2019t changed in green since one week. I re-uploaded them but no success. What else can I do from my end."}, {"role": "system", "text": "Hello, The color red means that an error occurred. You can click the bug icon to see the error message. The color yellow would mean it is still running."}], [{"role": "user", "text": "Hello,  I am going through \u201cDe novo transcriptome reconstruction with RNA-Seq\u201d  tutorial. I can not cash mm10 reference genome at GFFCompare step: \u201cUse Sequence Data\u201d :  Yes\n\u201cChoose the source for the reference list\u201d :  Locally cached\n\u201cUsing reference genome\u201d* : \u2018Mouse (Mus Musculus): mm10\u2019\nHow do I import mm10 into Galaxy?  see snapshot: \n12%20PM1028\u00d7360 18 KB\n"}, {"role": "system", "text": "\n\n\n irinastl:\n\nGFFCompare\n\n\nWelcome, @irinastl\nThe list of available genomes is based on the assigned \u201cdatabase\u201d of the inputs. Maybe you need to assign the \u201cmm10\u201d database to those?\nThe database can be assigned during Upload. Or, for already loaded data, click on the pencil icon to reach the Edit attributes forms. For your case, the first tab has the option of assigning the genome/build \u201cdatatype\u201d. Type in \u201cmm10\u201d to search the list. Make sure your data really is from this same exact build, and that the assigned datatype is correct, or expect problems.\nFAQs:\n\nCommon datatypes explained\nThe tool I\u2019m using does not recognize any input datasets. Why?\nHow do I find, adjust, and/or correct metadata?\nMismatched Chromosome identifiers (and how to avoid them)\nExtended Help for Differential Expression Analysis Tools\n\nThis tool is technically from a deprecated tool suite but I don\u2019t know of any specific issues with it. And I just checked it at Galaxy Main https://usegalaxy.org and mm10 is indexed for it.\nHope that helps!"}], [{"role": "user", "text": "Hello,\nI am analyzing five Illumina paired-end ChIP-seq datasets, and have run out of Galaxy storage needed to perform my analyses, even after deleting unused histories. Is it possible to get more storage on my account?\nThank you!\nHolly"}, {"role": "system", "text": "Hi @Holly_Kleinschmidt,\nas indicated on Account quotas, if your work is based on academic research and additional space is needed in the short term on usegalaxy.org, send an email to galaxy-bugs@lists.galaxyproject.org from your registered email address, providing some details about your project, the amount of space that you need, and how long you need it.\nRegards"}], [{"role": "user", "text": "Hello, I was wondering if it is possible to have other versions for GROMACS production simulation. Available versions are 2018.2 and 2019.1 and I really need to use version 2021.4\nThank you"}, {"role": "system", "text": "Hello @desire_mtetwa\nUseGalaxy.eu hosts the current functional versions. Changing the tool version"}], [{"role": "user", "text": "Traing says .mzxml.  data is .mzml\nImage 3-10-21 at 10.10 AM691\u00d7555 89.4 KB\n"}, {"role": "system", "text": "Thanks! It will be fixed. Regards."}], [{"role": "user", "text": "Hi, I\u2019m new to Galaxy workflow, and I have a problem with Galaxy workflow. Usually, we use GIE to customize tools to handle our data. In there, we can use the if-else branch to decide the next step based on the results in previous steps.\nMay I ask if the Galaxy workflow can achieve the same if-else function? Because I just can connect the tools when using the workflow. Thanks!"}, {"role": "system", "text": "Check out the expression tools, there is a training that uses them here: https://training.galaxyproject.org/training-material/topics/galaxy-ui/tutorials/workflow-parameters/tutorial.html"}], [{"role": "user", "text": "Hello,\nI am following VGP workflow and have an issue with purging step. I am using purging mainly to decrease the number of contigs i have as when doing direct scaffolding from hifiasm i am ending up with big number of scaffolds.\nHowever, purging step appear to be removing bacterial DNA known to be present present in the sequenced insect chromosome. I also confirmed its presence in the HIFIASM assembly but it get wiped out from the purged assembly.\nKindly, any suggestion how to deal with this as i would like to maintain the bacterial DNA in the resulting purged sequences ?\nRegards"}, {"role": "system", "text": "Hello,\nThe purging step may be removing bacterial DNA if the coverage is too high. You can get the purged sequences from the purgedups workflow (dataset tagged with \u201cseq_purged_p2\u201d) if you want to collect the bacterial DNA that has been purged.  In addition to using the decontamination workflow after scaffolding you should be able to recover all the data from the symbiont.\nI hope it helps,\nBests"}], [{"role": "user", "text": "Hello -\nI keep getting an error reading as the following.\nError executing tool with id \u2018toolshed.g2.bx.psu.edu/repos/iuc/hisat2/hisat2/2.2.1+galaxy0\u2019: \u2018HistoryDatasetAssociation\u2019 object has no attribute \u2018forward\u2019\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\n{\n\u201chistory_id\u201d: \u201c23a225085c9279cd\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/hisat2/hisat2/2.2.1+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c2.2.1+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201creference_genome|source\u201d: \u201cindexed\u201d,\n\u201creference_genome|index\u201d: \u201chg38canon\u201d,\n\u201clibrary|type\u201d: \u201cpaired_interleaved\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201cf9cad7b01a472135251a6d4487ba53a9\u201d,\n\u201chid\u201d: 19,\n\u201cname\u201d: \u201cFASTQ interlacer singles from data 2 and data 1\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201clibrary|rna_strandness\u201d: \u201c\u201d,\n\u201clibrary|paired_options|paired_options_selector\u201d: \u201cdefaults\u201d,\n\u201csum|new_summary\u201d: \u201cfalse\u201d,\n\u201csum|summary_file\u201d: true,\n\u201cadv|input_options|input_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|alignment_options|alignment_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|scoring_options|scoring_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|spliced_options|spliced_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|reporting_options|reporting_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|output_options|output_options_selector\u201d: \u201cadvanced\u201d,\n\u201cadv|output_options|unaligned_file\u201d: true,\n\u201cadv|output_options|aligned_file\u201d: \u201cfalse\u201d,\n\u201cadv|sam_options|sam_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|other_options|other_options_selector\u201d: \u201cdefaults\u201d,\n\u201c__job_resource|__job_resource__select\u201d: \u201cno\u201d\n}\n}\nOK\nCan you please help me? I\u2019m not quite sure what the issue is here and I\u2019m still new to this line of analysis. Thanks!"}, {"role": "system", "text": "Hello @jmn97\nWe replied to your email directly. This was the advice last Friday:\n\n\nHISAT2 tends to work best with non-interleaved fastq inputs\nWhen you rerun, use distinct forward and reverse read datasets\nRun QA/QC on the read data before mapping if you have not already (FastQC then quality filtering/trimming tools if needed).\n\n\nResources:\n\nPerformance status: https://status.galaxyproject.org/\n\nTroubleshooting strategies: Galaxy Training!\n\n"}], [{"role": "user", "text": "Hello,\nThese past days I am trying to setup a AWS instance with galaxy following this tutorial (Getting started with Galaxy on the cloud).\nHowever, the CloudLaunch application link given in the tutorial and multiples documentations and trainings appears to be dead and not responding.\nSince there is no mention of it on galaxy help or anywhere, I suppose it is not a problem and people use a workaround. But I did not find it."}, {"role": "system", "text": "Hi @Nathanael\nCurrent options are available here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\nRelated prior Q&A Galaxy Cloudman or GVL?"}], [{"role": "user", "text": "Hello,\nAfter successfully using localhost:8080 for my local galaxy install for several months I am now receiving a localhost refused to connect error.  Nothing that I am aware of has changed in my environment, so I cleared my cache and even tried to connect with firefox\u2019s browser, but localhost\u2019s message is that it cannot receive any data.  I am using tunneling to define forward port to localhost.\nTroubleshooting questions:\nIs there any other way I can connect to Galaxy other than localhost when I have a local installation?  Does anyone have any other troubleshooting ideas?"}, {"role": "system", "text": "Not sure what this exactly means.\n\n\n\n bionewbie:\n\nI am using tunneling to define forward port to localhost.\n\n\nBut what you could do:\n\nStop galaxy\nrun the command sudo lsof -i tcp:8080\n\nKill the PID\u2019s of galaxy sudo kill 23001 (The PID of yourself, 23001 is an example)\nstart galaxy again\ntest if it solved your problem\n"}], [{"role": "user", "text": "I am new to Galaxy, and am trying to complete a differential gene expression analysis. I use two replicates of RNAseq data. Since I got the clean reads data, I started my workflow from HISAT2>>StringTie>>StringTie merge>>StringTie (using Stringtie merge as a reference genome) >> DESeq2. I found an error result of DESeq2, and then I checked my input, and I found that some of the StringTie gene counts of my data were showing 0 counts. I don\u2019t know why this happened, because the other data is good. Can anyone help me out with this problem? Thanks!"}, {"role": "system", "text": "Hi @nzrf\nmaybe consider following any established protocol, such as\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: 1: RNA-Seq reads to counts\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKind regards,\nIgor"}], [{"role": "user", "text": "Sorry for the dumb question - I have forgotten how to log in to my local instance of galaxy and apparently, I never set up the email server for \u201cforgot my password\u201d. Can I find the password for my email so I can log in?"}, {"role": "system", "text": "Passwords are not stored as plain text, so finding them in the database does not help you. Your best bet is to find Galaxy\u2019s configuration file (usually galaxy.ini) and add another admin account, then create that new account and then you are an admin and can change people\u2019s passwords (like the one of your old account)."}], [{"role": "user", "text": "I am trying to use Faster Download and Extract Reads in FASTQ to download single-cell RNA-seq raw reads. The specific SRR accession is SRR11821880, and the SRA run browser tells me that there are three reads per spot. The first two are technical reads (adapter: 8 bp, barcode+UMI: 28 bp), and the last one is a biological read (150 bp).\nMy settings for the tool are:\n\nSelect input type: SRR accession\nAdvanced options>Select how to split the spots: --split-files\nAdvanced options>Dump only biological reads: No\n\nCurrently, I am only getting the fastqs with the adapter sequences and the biological reads, but not the barcode+UMI. I\u2019m unsure of why this is happening, since I am able to browse and view all of the reads in the run browser. Is this tool restricted to getting only two files maximum, or am I overlooking a setting?"}, {"role": "system", "text": "Hi @cathy37,\nit seems that is not possible to get the 3 reads at all, and the ENA dump also just contains 2 fastq files.\nIf you go to https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR11821880,  under data access you can get the original files, that might be the easiest way to get them.\nRegards"}], [{"role": "user", "text": "I\u2019m trying to run a HTSEQ-COUNT function on a BAM and GTF file but it keeps failing and showing \u201cErrno 122: Disk Quota Exceeded\u201d but I\u2019ve only used 8% of my storage."}, {"role": "system", "text": "Update March 24 2023\nOver the last few days, a subset of jobs failed with \u201cErrno 122: Disk Quota Exceeded\u201d and a few other related messages (examples: file not retrieved, metadata generation failed, server could not complete)\nAny jobs or operations that failed in odd ways or that produced empty green outputs should be rerun.\nFor errors produced when using the Upload tool, reloading the data is the solution.\n\nWelcome, @Ari_Sysimaki\n\n\n\n Ari_Sysimaki:\n\n\u201cErrno 122: Disk Quota Exceeded\u201d\n\n\nJobs that failed with this message are due to a server issue at UseGalaxy.org.\nPlease rerun any jobs that ended this way."}], [{"role": "user", "text": "primary factor: Treatment\n\nError in data.frame(\u2026, check.names = FALSE) :\narguments imply differing number of rows: 8578, 11447\nCalls: get_deseq_dataset \u2026 eval \u2192 eval \u2192 eval \u2192 cbind \u2192 cbind \u2192 data.frame"}, {"role": "system", "text": "Hi @19B236_SATHYAPRAKASH,\nit seems that your replicates have different numbers of entries. If you need additional support, please, share your Galaxy history with me. My email is .\nRegards"}], [{"role": "user", "text": "Hi, I\u2019m trying to run galaxy in my local drive. However, it seems file uploading from my local drive doesn\u2019t work.\nHow can I fix it?\nthe error traceback is like this:\nTraceback (most recent call last): File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 550, in move os.rename(src, real_dst) FileNotFoundError: [Errno 2] No such file or directory: SafeStringWrapper(str:__lt__class __sq__NoneType__sq____gt__,__lt__class __sq__NotImplementedType__sq____gt__,__lt__class __sq__bool__sq____gt__,__lt__class __sq__bytearray__sq____gt__,__lt__class __sq__ellipsis__sq____gt__,__lt__class __sq__galaxy.security.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq____gt__) object at 7fbdc82aa480 on: __sq__/home/chansik/XXXX/galaxy/database/tmp/upload_params_99yg3r5a__sq__ -> '/home/chansik/\ubc14\ud0d5\ud654\uba74/galaxy/database/jobs_directory/000/26/upload_params.json' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"lib/galaxy/jobs/runners/__init__.py\", line 243, in prepare_job job_wrapper.prepare() File \"lib/galaxy/jobs/__init__.py\", line 1159, in prepare self.__prepare_upload_paramfile(tool_evaluator) File \"lib/galaxy/jobs/__init__.py\", line 1124, in __prepare_upload_paramfile shutil.move(tool_evaluator.param_dict['paramfile'], new) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 564, in move copy_function(src, real_dst) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 263, in copy2 copyfile(src, dst, follow_symlinks=follow_symlinks) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 120, in copyfile with open(src, 'rb') as fsrc: FileNotFoundError: [Errno 2] No such file or directory: SafeStringWrapper(str:__lt__class __sq__NoneType__sq____gt__,__lt__class __sq__NotImplementedType__sq____gt__,__lt__class __sq__bool__sq____gt__,__lt__class __sq__bytearray__sq____gt__,__lt__class __sq__ellipsis__sq____gt__,__lt__class __sq__galaxy.security.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq____gt__) object at 7fbdc82aa480 on: __sq__/home/chansik/XXXX/galaxy/database/tmp/upload_params_99yg3r5a__sq__\n"}, {"role": "system", "text": "which version of galaxy are you using and are you by any coincidence uploading a xlsx file?\n\n  \n\n      github.com/galaxyproject/galaxy\n  \n\n  \n    \n  \n\t  \n  \n\n  \n    \n      xlsx format is not recognize\n    \n\n    \n      \n        opened 10:15AM - 16 Jul 19 UTC\n      \n\n        \n          closed 01:32PM - 20 Aug 19 UTC\n        \n\n      \n        \n          \n          FredericBGA\n        \n      \n    \n\n    \n    \n  \n\n\n  \n    on galaxy 19_05 we can't upload an xlsx file and let galaxy sniff the datatype.\n\u2026\nit works when you set the datatype yourself.\n\nthe errors on galaxy.eu and galaxy.org are different...\nI have the same error as galaxy.org on my instance.\n\ngalaxy.eu\n> Fatal error: Exit code 1 ()\n> [Errno 2] No such file or directory: SafeStringWrapper(unicode:__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__galaxy.util.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq___\n\ngalaxy.org:\n> Traceback (most recent call last):\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 326, in <module>\n>     __main__()\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 319, in __main__\n>     metadata.append(add_file(dataset, registry, output_path))\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 128, in add_file\n>     convert_spaces_to_tabs=dataset.space_to_tab,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/upload_util.py\", line 54, in handle_upload\n>     convert_spaces_to_tabs=convert_spaces_to_tabs,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 724, in handle_uploaded_dataset_file_internal\n>     auto_decompress=auto_decompress,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 663, in handle_compressed_file\n>     sniffed_ext = run_sniffers_raw(filename, sniff_datatypes)\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 482, in run_sniffers_raw\n>     file_prefix = FilePrefix(filename_or_file_prefix)\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 541, in __init__\n>     compressed_format, f = compression_utils.get_fileobj_raw(filename, \"rb\")\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/util/compression_utils.py\", line 50, in get_fileobj_raw\n>     with zipfile.ZipFile(filename, mode) as zh:\n>   File \"/cvmfs/main.galaxyproject.org/python/lib/python2.7/zipfile.py\", line 729, in __init__\n>     raise RuntimeError('ZipFile() requires mode \"r\", \"w\", or \"a\"')\n> RuntimeError: ZipFile() requires mode \"r\", \"w\", or \"a\"\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi. First off, I am a complete noob, but I have managed to get Galaxy running on a M1 Mac. Unfortunately, I am not able to take advantage of the multiple cores. I know I need to edit the job_conf.xml file so that it will override the GALAXY_SLOTS default value for most of the tools, which is usually 1. I have an 10 CPU core mac with 32gb of memory and I want to try to optimise galaxy to use all of it when necessary.\nI have followed several other post and have tried to cobble together my own job_conf.xml file to have the following:\n<?xml version=\"1.0\"?>\n<job_conf>\n<plugins>\n  <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"10\"/>\n<plugin id=\"multilocal\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"10\"/>\n</plugins>\n<handlers>\n<handler id=\"main\"/>\n</handlers>\n\n<destinations default=\"local\">\n    <destination id=\"local\" runner=\"local\">\n    <destination id=\"local10threads\" runner=\"multilocal\">\n        <param id=\"request_cpus\">10</param>\n        <param id=\"memory\">28G</param>\n        <param id=\"embed_metadata_in_job\">False</param>\n        <param id=\"nativeSpecification\">-p Galaxy -n 10</param>\n        <env file=\"/data/galaxy/galaxy-central/galaxy_venv/bin/activate\" />\n    </destination>\n</destinations>\n<tools>\n    <tool id=\"bowtie2\" destination=\"local10threads\"/>\n    <tool id=\"bwa-mem\" destination=\"local10threads\"/>\n    <tool id=\"bwa-mem2\" destination=\"local10threads\"/>\n    <tool id=\"bowtie2\" destination=\"local10threads\"/>\n    <tool id=\"bowtie\" destination=\"local10threads\"/>\n    <tool id=\"bowtie-build\" destination=\"local10threads\"/>\n    <tool id=\"fastq_groomer_parallel\" destination=\"local10threads\"/>\n    <tool id=\"deeptools\" destination=\"local10threads\"/>\n    <tool id=\"cufflinks\" destination=\"local10threads\"/>\n    <tool id=\"cuffdiff\" destination=\"local10threads\"/>\n    <tool id=\"cuffmerge\" destination=\"local10threads\"/>\n    <tool id=\"tophat2\" destination=\"local10threads\"/>\n    <tool id=\"trimmomatic\" destination=\"local10threads\"/>\n</tools>\n</job_conf>\n\nHowever, when I run bwa-mem2 or any other tool in galaxy, it still reverts to only using one core or whatever the default value is  .\nCan someone please help me figure out what to do? I am out of ideas\nAs an example, this is what Galaxy shows is the resulting argument when I try to run bwa-mem2:\nset -o | grep -q pipefail && set -o pipefail;  ln -s '/Users/antoniolamb/galaxy/database/objects/e/1/c/dataset_e1c7b303-95f1-4185-9b51-a3e66cacde75.dat' 'localref.fa' && bwa-mem2 index 'localref.fa' &&    bwa-mem2 mem -t \"${GALAXY_SLOTS:-1}\" -v 1                 'localref.fa' '/Users/antoniolamb/galaxy/database/objects/5/d/1/dataset_5d1ed8fa-99aa-466c-b508-8fff2dfcc65b.dat' '/Users/antoniolamb/galaxy/database/objects/1/9/1/dataset_19188ad0-62b7-492c-9d7b-6263ff735bd0.dat'  | samtools sort -@${GALAXY_SLOTS:-2} -T \"${TMPDIR:-.}\" -O bam -o '/Users/antoniolamb/galaxy/database/objects/0/b/4/dataset_0b48873c-3cf7-4f80-add1-2a086f5237cb.dat'\nThis is what shows in the destination parameters. I am so confused and frustrated why it won\u2019t use all the cores available?\n\nScreenshot 2023-04-04 at 01.23.251808\u00d7656 42.7 KB\n\n\nScreenshot 2023-04-04 at 01.24.081840\u00d7282 54 KB\n"}, {"role": "user", "text": "Hello! After almost a week of troubleshooting, I finally got it working and I now have a local installation of galaxy that uses 9/10 cores and 32 GB of memory on my macbook pro . I left one for the OS so it doesn\u2019t crash. If anyone else has this problem, please use these job_conf.xml settings in your file, and change the number of cores\n(  9). It will make all tools default to using the number of cores specified.\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n    </plugins>\n    <handlers>\n        <handler id=\"main\"/>\n    </handlers>\n    <destinations default=\"multicore_destination\">\n        <destination id=\"multicore_destination\" runner=\"local\">\n            <param id=\"local_slots\">9</param>\n        </destination>\n        <tools>\n        <tool id=\"*\" destination=\"local\"/>\n    \t</tools>\n    </destinations>\n</job_conf>\n\nBelow is proof :). Trimmomatic using 46 threads running on multiple cores. I confirmed it works with BWA-mem as well.\n\nimage1768\u00d7212 30.5 KB\n"}], [{"role": "user", "text": "Hello there,\nI want to use CAT bins, but it requires a CAT database. I have seen that other galaxy server have a (pre-stored) CAT database available, but not on usegalaxy.org. I could use \u201cprepare CAT database\u201d, but I have seen posts of other users that this used up their quota.\nWould it be possible to have a recent CAT database buildt for the general user available from galaxy?\nThanks a lot! Cycle"}, {"role": "system", "text": "Welcome, @cycle\nThe indexes between servers will be synched up later this year.\nDid the tool not work at the EU server with the built-in indexes?\nRef Database generated from CAT prepare exceed the account quota"}], [{"role": "user", "text": "Using DANTE and Protein Domains Filter I noticed that sometimes there is no exact correspondence between the coordinates of the same domain between the gff and the fasta files. What is the reason for this?\n\nImmagine_galaxy1909\u00d7871 87.9 KB\n"}, {"role": "system", "text": "Hi @m_ventimiglia\nCorrect, these are not always expected to be an exact match. For this specific example, my guess is that the stop codon is being omitted from the fasta, but you could investigate that by examining the region in a genomic browser.\nQuote from the Protein Domains Filter tool form:\n\nOUTPUTS PRODUCED:\n\nFiltered GFF3 file\nTranslated protein sequences of the filtered domains regions of original DNA sequence in fasta format\n\nTranslated sequences are taken from the best alignment (Best_Hit attribute) within a domain region, however this alignment does not necessarily have to cover the whole region reported as a domain in gff file\n\nTo learn more about how this tool suite tool functions, including Galaxy usage, please see:\n\n\nRepeat Explorer documentation: http://repeatexplorer.org/\n\nGalaxy Repository: https://toolshed.g2.bx.psu.edu/view/petr-novak/dante/65a6fb89495d (includes links to the development and associated end-user resources)\n\nThanks!"}], [{"role": "user", "text": "Hi all, this morning I started to notice some problems runnings jobs. I launched three different jobs and 1 hour later all of them are still on queue\u2026I\u2019m using scanpy tools for a scRNAseq analysis and plotting some DESeq2 results with Heatmap2. Does anyone know if there has been any general problems?"}, {"role": "system", "text": "Update 3/31/2023\nSee Failed local file upload with 504 timeout: Resolved at UseGalaxy.eu March 31 2023 - #6 by marek\n\nWelcome, @David_Olivares_Osuna\nMost jobs will queue before executing. These are shared public resources. Leaving jobs queued is important since all new jobs are added back at the end of the queue, extending the wait time.\nThe UseGalaxy.eu database did have a small issue but seems to be up now. Rerun if a job happens to fail.\nhttps://status.galaxyproject.org/"}], [{"role": "user", "text": "I\u2019ve been trying to download files from my history in usegalaxy.eu using \u2018curl\u2019, following the instructions on the following page: https://galaxyproject.org/support/download-data/\nThis method works fine with the main Galaxy server at usegalaxy.org, just not with the European server. I\u2019m able to download files in a web browser using http and https.\nAs an example, using the verbose parameter, I get the following output (precise URLs redacted). Any advice would be much appreciated. Downloading ginormous BAM files through a web browser is not ideal.\ninfo-sml-113-x:SSD bramblepuss$ curl -JLOv https://usegalaxy.eu/datasets/redacted/display?to_ext=bam\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:\u2013:-- --:\u2013:-- --:\u2013:--     0*   Trying 192.52.32.126\u2026\n\nTCP_NODELAY set\nConnected to usegalaxy.eu (192.52.32.126) port 443 (#0)\nALPN, offering h2\nALPN, offering http/1.1\nCipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\n\nsuccessfully set certificate verify locations:\nCAfile: /etc/ssl/cert.pem\nCApath: none\nTLSv1.2 (OUT), TLS handshake, Client hello (1):\n} [512 bytes data]\nTLSv1.2 (IN), TLS handshake, Server hello (2):\n{ [93 bytes data]\nTLSv1.2 (IN), TLS handshake, Certificate (11):\n{ [2636 bytes data]\nTLSv1.2 (IN), TLS handshake, Server key exchange (12):\n{ [333 bytes data]\nTLSv1.2 (IN), TLS handshake, Server finished (14):\n{ [4 bytes data]\nTLSv1.2 (OUT), TLS handshake, Client key exchange (16):\n} [70 bytes data]\nTLSv1.2 (OUT), TLS change cipher, Client hello (1):\n} [1 bytes data]\nTLSv1.2 (OUT), TLS handshake, Finished (20):\n} [16 bytes data]\nTLSv1.2 (IN), TLS change cipher, Client hello (1):\n{ [1 bytes data]\nTLSv1.2 (IN), TLS handshake, Finished (20):\n{ [16 bytes data]\nSSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\nALPN, server did not agree to a protocol\nServer certificate:\nsubject: CN=usegalaxy.eu\n\nstart date: Dec  7 07:26:49 2018 GMT\nexpire date: Mar  7 07:26:49 2019 GMT\nsubjectAltName: host \u201cusegalaxy.eu\u201d matched cert\u2019s \u201cusegalaxy.eu\u201d\nissuer: C=US; O=Let\u2019s Encrypt; CN=Let\u2019s Encrypt Authority X3\nSSL certificate verify ok.\n\n\nGET /datasets/8fb8030d304f7807/display?to_ext=bam HTTP/1.1\nHost: usegalaxy.eu\nUser-Agent: curl/7.54.0\nAccept: /\n\n< HTTP/1.1 200 OK\n< Server: nginx/1.12.2\n< Date: Wed, 16 Jan 2019 20:16:17 GMT\n< Content-Type: text/html\n< Transfer-Encoding: chunked\n< Vary: Accept-Encoding\n< Set-Cookie: galaxysession=11ac94870d0bb33aef1bd67c23e76ec7d7d0aa26dbc781503fab4ecc9b5ac968cb576e82072afe8f; expires=Tue, 16-Apr-2019 22:16:17 GMT; httponly; Max-Age=7776000; Path=/; secure; Version=1\n< X-Content-Type-Options: nosniff\n<\n{ [4979 bytes data]\n100  4960    0  4960    0     0   9185      0 --:\u2013:-- --:\u2013:-- --:\u2013:--  9202\n\nConnection #0 to host usegalaxy.eu left intact\n"}, {"role": "system", "text": "Hi - Just ran a few tests and it appears that the history and objects need to be shared before curl can be used for download at the EU server. This can just be a share link (no need to \u201cpublish\u201d as well). However, it is important to check the box that \u201calso shares objects\u201d. If the history is already shared, unshare, check the box, then reshare.\nScreenshot after clicking object share box but before clicking on the share-by-link:\n\nshare-history-objects.png1284\u00d7272 33.7 KB\n\nHere is an example dataset I\u2019ll leave up for a bit so you can try it: tabular dataset, one line, super small\n$ curl -o outfile_after_sharing 'https://usegalaxy.eu/datasets/531ddf9f9edb68b8/display?to_ext=txt'\n$ more outfile_after_sharing\nhello   world\n\nI\u2019ll follow-up with the EU team about this and get these details added to the general Download FAQ if \u201csharing first\u201d is an intentional pre-requirement for that server (seems so from testing).\nHope that helps!"}], [{"role": "user", "text": "I\u2019m using circos to create a visual representation of genomic data, but I\u2019m unable to create cytogenic bands that show up on the image. I\u2019ve managed to create the basic ideogram used by: https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html to depict a cancer genome but I can\u2019t seem to decipher how the band data is stored in \u2018hg18_karyotype_bands.tsv\u2019, more specifically how the seventh column is used to generate bands. I\u2019d appreciate any help. Thank you."}, {"role": "system", "text": "Hi @Vignesh_P, I think I can provide some help here. The 7th column is just a named color that you want to use for that band. There are a number of colors built in, here the staining colours are used.\nband\tchr1\tp36.33\tp36.33\t0\t2300000\tgneg\nband\tchr1\tp36.32\tp36.32\t2300000\t5300000\tgpos25\nband\tchr1\tp36.31\tp36.31\t5300000\t7100000\tgneg\nband\tchr1\tp36.23\tp36.23\t7100000\t9200000\tgpos25\nband\tchr1\tp36.22\tp36.22\t9200000\t12600000\tgneg\n\nyou can read more about the karyotype table format on their website http://circos.ca/documentation/tutorials/ideograms/karyotypes/lesson"}], [{"role": "user", "text": "Is there a quick way to save an entire history to an external hard drive? I can go line by line and download but that would take hours\u2026"}, {"role": "system", "text": "Try History Options / Export History to File"}], [{"role": "user", "text": "I am having issues with DESeq2. I get this error message\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nImport genomic features from the file as a GRanges object \u2026 OK\nPrepare the \u2018metadata\u2019 data frame \u2026 OK\nMake the TxDb object \u2026 OK\n\u2018select()\u2019 returned 1:many mapping between keys and columns\nNote: importing abundance.h5 is typically faster than abundance.tsv\nreading in files with read.delim (install \u2018readr\u2019 package for speed up)\n1 2 3 4 5 6\nsummarizing abundance\nsummarizing counts\nsummarizing length\nusing counts and average transcript lengths from tximport\nWarning message:\nIn .get_cds_IDX(type, phase) :\nThe \u201cphase\u201d metadata column contains non-NA values for features of type\nstop_codon. This information was ignored.\nestimating size factors\nusing \u2018avgTxLength\u2019 from assays(dds), correcting for library size\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\nError in estimateDispersionsFit(object, fitType = fitType, quiet = quiet) :\nall gene-wise dispersion estimates are within 2 orders of magnitude\nfrom the minimum value, and so the standard curve fitting techniques will not work.\nOne can instead use the gene-wise estimates as final estimates:\ndds <- estimateDispersionsGeneEst(dds)\ndispersions(dds) <- mcols(dds)$dispGeneEst\n\u2026then continue with testing using nbinomWaldTest or nbinomLRT\nCalls: DESeq \u2026 estimateDispersions -> .local -> estimateDispersionsFit\nI\u2019m not exactly sure what to do to remedy this problem."}, {"role": "system", "text": "Hello,\nThere is much discussion about this error coming up at the DESee2/Bioconductor forum if you want to explore prior Q&A solutions (warning: not all will be available in Galaxy). The error would come up whether run in Galaxy or not.  Google will show the top hits for https://support.bioconductor.org/ with this search: \u201call gene-wise dispersion estimates are within 2 orders of magnitude\u201d\nIn summary, this is a data content-related error. It indicates that there is a data mixup upstream (the same sample processed more than once by accident) or the given samples do not really have enough expression variance to do the calculation. Biological replicates are expected; technical replicates will have the same problem, e.g. insufficient expression differences."}], [{"role": "user", "text": "Hi, I have a fastqsanger.gz paired-end data file with both forward and reverse reads combined into a single file. I want to use Trimmomatic to trim adapters and set a quality score. I don\u2019t see an option to input a single paired-end file. Would it be advisable to input this as a single-end file instead?\nI was originally going to use FASTQ Splitter to first split the data into two separate files for the forward and reverse reads, however my forward and reverse reads are not all the same length which is required for this tool. Also there is variable length for each read.\nI am new to this so I\u2019d really appreciate any help, thank you! "}, {"role": "user", "text": "I figured I\u2019d give an update in case this helps someone else in the future.\nI reimported my data which was on NCBI using the tool \u201cNCBI using Faster Download and Extract Reads in FASTQ\u201d. This imported my pair-end data into a separate file for the forward and reverse reads and eliminated needing to use FASTQ Splitter. I was then able to use Trimmomatic with the paired-end setting with my two files."}], [{"role": "user", "text": "Hi,\nI have been using Galaxy for a few months now, and lately I had to redo an old Trinity assembly and came back to the server. After setting everything i executed and the proccess has been queued up for more than one day now, I have tried setting new instances and always happens the same. Other programs seem to be working fine. Have trinity jobs been paused for now? Is there a fix for this problem?\nBest Regards\nMiguel"}, {"role": "system", "text": "Hi @miguel1cifu\nSome tools will queue longer than others, and how long any tool queues varies daily.\nClusters than executed assembly tools, including Trinity are usually busy. Why? The tools are computationally expensive so jobs run longer and the cluster nodes turn over for new jobs slower.\nThe best advice is to queue jobs up and let them run. Avoid deleting and rerunning, as that places the new jobs back at the end of the queue, extending wait time. If done often enough, jobs may take a very long time with all the restarts!\nIf you didn\u2019t know: you can set a notification on tool forms to email yourself once jobs are done  Also, consider using collections and workflows. Workflows queue up tools/jobs in batch (and also has an email notification feature) and collections group similar dataset together. If you are not sure how to use those, search with the keywords here: https://training.galaxyproject.org/\nThanks!"}], [{"role": "user", "text": "i put the fastQ data, at the end pops up this message in all files:\nThis is a new dataset and not all of its data are available yet.\nformat html database ?\nWhat now? how to proceed?\n\nfastq sem dados1366\u00d7768 78.8 KB\n"}, {"role": "system", "text": "Hi @Jaqueline_Rocha\n\n\n\n Jaqueline_Rocha:\n\ni put the fastQ data, at the end pops up this message in all files:\nThis is a new dataset and not all of its data are available yet.\n\n\nThose jobs are still queued.\n\n\n\n Jaqueline_Rocha:\n\nformat html database\n\n\nAn HTML report is one of the outputs of this tool.\n\n\n\n Jaqueline_Rocha:\n\nWhat now? how to proceed?\n\n\nLeave the jobs queued until completed. If you delete and rerun, any new jobs are placed back at the end of the queue, further extending wait time.\n\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/datasets_job_status.html\n"}], [{"role": "user", "text": "Hi! I\u2019m doing an RNAseq analysis. Do you know if it\u2019s correct to use an annotation file normalized with DESeq2 (and annotated with Annotate DESeq2/DEXSeq output tables) in limma-voom or EdgeR? Since they use different normalization methods, I\u2019m not sure if it will affect my results.\nDo you think it would be enough to turn off the normalization option in EdgeR, so it doesn\u2019t get normalized twice? Or what tool would you recommend to get a differential expression table if I want to use EdgeR? I\u2019ve seen in tutorials that AnnotateMyIDs is useful, but I\u2019m not working with any of the organisms it can be used for.\nI hope you can help! Thanks!"}, {"role": "system", "text": "Hi MarioG\nFor edgeR you need a count table with the raw read counts (i.e. not normalized data), See chapter 2.3 in the edgeR users guide (https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf)\nRegards, Hans-Rudolf"}], [{"role": "user", "text": "Hello, I am trying to use an .Rdata file for a mouse dataset in fgsea. The tool does not recognize the target file in my histories or in the dropdown. Thanks for any insight or solutions to running the tool!"}, {"role": "system", "text": "Welcome, @jclaudio\nThe dataset might just need to have a correct datatype assigned.\n\n\n\nfastq unavailable -- Tool does not recognize inputs? How to check why\n\n\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:\n\nCreate a new empty history\nLoad up the tool form\nReview the input datatypes listed in the tool form\u2019s input area\nNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\nExample\n\n\n\n\nEither of these resources can be search with keywords or navigated by topic.\nTutorials\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Search Tutorials\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nFAQs\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I\u2019ve followed the instructions to run Galaxy on my computer, however I\u2019m getting an error and it just stops working.\n\nScreen Shot 2022-08-09 at 5.09.43 PM1437\u00d71097 238 KB\n\nAny help on how to solve this?"}, {"role": "system", "text": "Hello @JessicaIsp\nDid you resolve the problem already?\nIf not, please include more details: about your server (OS), how you sourced Galaxy (where + version), and what you have done so far. Is this a new install or are you upgrading?\nGalaxy Admin resources:\n\nTraining: Galaxy Training!\n\nDocs: Galaxy Deployment & Administration \u2014 Galaxy Project 22.01.1.dev0 documentation\n\n\nI also cross-posted this to the Galaxy Admin gitter chat: galaxyproject/admins - Gitter. They may reply here or there, and feel free to join the chat."}], [{"role": "user", "text": "Hi there,\nI wonder how to do md5 checksum on Galaxy to make sure files downloaded from Galaxy are original?\nThanks in advance!"}, {"role": "system", "text": "Hi @tithuytrang\nTry this tool: Secure Hash / Message Digest on a dataset"}], [{"role": "user", "text": "Hi all,\nWe\u2019re working on getting OIDC Okta running with galaxy and we\u2019re hitting a bug.\nWe get:\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://myco.okta.com/oauth2/.well-known/openid-configuration?client_id=\u2026\nBut our well known openid config doesn\u2019t have oauth2 in the url\nhttps://myco.okta.com/.well-known/openid-configuration?client_id=\u2026\nOn the oidc_backends.config.xml, we have:\n<redirect_uri>https://galaxy.myco.org/authnz/okta/callback</redirect_uri>\n<api_url>https://myco.okta.com/oauth2</api_url>\nThanks!"}, {"role": "system", "text": "Hi @jdlamstein\nIf you would like to help us to develop that authentication method, please reach out to the developers. Galaxy Working Groups and Projects - Galaxy Community Hub"}], [{"role": "user", "text": "I started two RNA Star jobs yesterday and it has been almost 24 hours since I started the jobs and they are still showing as waiting to run.  It has been a little while since I used Galaxy for data analysis but I did not experience this type of wait times previously.  Is this normal or is there a way to submit these jobs where they would run faster."}, {"role": "system", "text": "Hi @mjgage\nThe UseGalaxy.org server has been very busy. The best strategy is to queued jobs then let them complete.\nWe did have a small cluster issue last week but the associated backlog is mostly resolved by now. There isn\u2019t an update as of now, but if something comes up in the next few days we\u2019ll keep posting here: Fastp not running: Confirmed job delays at UseGalaxy.org 09/23/2022. Solution: allow queued jobs to process - #2 by jennaj\nThe UseGalaxy.eu server has also been busy, as are most others with public access.\n\n\n\n mjgage:\n\nIs this normal or is there a way to submit these jobs where they would run faster.\n\n\nEveryone has equal priority when working at public Galaxy servers. Some wait time is expected, but more so for computationally expensive tools that run on our largest cluster nodes. Jobs queue, then process in the order submitted as shared resources become available appropriate for the tool. Using workflows can significantly speed things up.\nAll known Galaxy server options are covered here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub. It is common for people to set up one account at several public sites to maximize their access to the different resources available (storage, computational). Others decide to set up a private server for institutional, lab, or personal work where they can control the resources \u2013 and this can include attaching local cluster resources, cloud resources, etc."}], [{"role": "user", "text": "Hi,\nI have reached my quota limit.\nCan you please tell me for how long my history(ies) and datasets be accessible as I am submitting a manuscript and might have to revisit the dataset.\nThanks"}, {"role": "system", "text": "@akankshabafna\nGalaxy Europe provides a description of the current data policy at the front page. Here is the quote:\nUser data on UseGalaxy.eu (i.e. datasets, histories) will be available as long as they are not deleted by the user.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello Galaxy team,\nI am using RNA STARSolo from Galaxy which is really useful tool for people who are not familiar with the bioinformatics. I was using tutorial from the following link - Pre-processing of 10X Single-Cell RNA Datasets\nMost of time, it works great. However, some of our and others dataset has R1 barcode length that are 151bp long (R2 is also 151bp), and I was wondering if there is any way to do pre-processing of those data. Help will be really appreciated.\nThank you,\nYoung"}, {"role": "system", "text": "Dear @Young,\nFor a different bacode-(length) you can check out the option Configure Chemistry Options and pick Custom, then you can and have to define the barcode details yourself.\nKind regards,\nFlorian"}], [{"role": "user", "text": "Hi all,\nI have a quite stable running local Galaxy installation running on Centos 8.1\nFor user authentication I run an apache proxy on the same machine connecting to LDAP to an domain server. Galaxy and Apache have been configured as decribed in the Galaxy Tutorials and I use Basic Authentication on Apache.\nThis setup works fine.\nMore recently I wanted to put a Sophos UTM as Web Application Firewall (reverse proxy) in front of the Galaxy-Server.\nI did this to have a more shiny form-based login to the Galaxy Server. The Sophos UTM then passes the form login data to the Galaxy-Server using Basic authentication.\nThis setup works in most of the times also nice - however, from time to time (especially after deleting browser cookies or when first-time users log in) the Galaxy server throws a error 403001 (API authentication required for this request) on the web fronted (see below).\nI looked in the logs of Apache and Galaxy and could not find anything strange. For troubleshooting: can someone explain me why and when Galaxy triggers this error? There seems to be something happening when deleting cookies or new users are logging in that pushed Galaxy on the \u201capi\u201d track.\nHappy for any ideas/suggestions.\nThanks a lot and have a nice and healthy weekend!\n\nerror691\u00d71041 31 KB\n"}, {"role": "user", "text": "Alright, took me some hours, but I found the solution.\nThe error was triggered because Galaxy did not find its session cookie in the RequestHeader that the apache proxy (proxy 2, installed on the Galaxy machine) forwarded to the Galaxy instance when GET for any HTML page was send from a client to Galaxy.\nThe reason why Galaxy got confused was that the client passed both cookies in the header: the one for the Sophos UTM WAF session (reverse proxy 1) and the one for the Galaxy session. The Sophos UTM WAF then remove its session cookie from the RequestHeader BUT, at least in some situations, left behind a \"; \" (so the end of the cookie prior to the next cookie). This leading \"; \" confused the Galaxy script that searches for the \u201cgalaxysession\u201d cookie in the header (i.e. the script did not see the cookie-string even though it was there). Therefore a 403 was triggered.\nI now remove the leading \"; \" by a command in the httpd.conf on the apache RP (proxy 2):\n\nRequestHeader edit Cookie \u201c^(?:; *)\u201d \u201c\u201d\n\nThis solves the issue.\nJust in case there are other Sophos UTM users out there\u2026\n@Mods: Maybe you want to add this (Sophos UTM WAF in front of Galaxy) to the titel of my post\u2026\nCheers"}], [{"role": "user", "text": "I am experiencing \u201cInternal Server Error (500)\u201d messages whenever I try to run a job with Pilon. The error appears after a few hours of the job start. The detailed error message suggest Error 500, internal server error.\nWhat should I do to avoid/fix this error.\nI\u2019m pasting the head of the error message below.\n\n{\n  \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"21.09\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"\",\n    \"responseJSON\": null,\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"2022-01-19T06:13:15.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n    \"details\": \"bbd44e69cb8906b515e37ecb30744100\",\n    \"traditional\": true,\n    \"data\": {\n      \"details\": \"bbd44e69cb8906b515e37ecb30744100\",\n      \"order\": \"hid\",\n      \"v\": \"dev\",\n      \"q\": [\n        \"update_time-ge\",\n        \"deleted\",\n        \"purged\"\n      ],\n      \"qv\": [\n        \"2022-01-19T06:13:15.000Z\",\n        \"False\",\n        \"False\"\n      ]\n    },\n    \"emulateHTTP\": false,\n    \"emulateJSON\": false,\n    \"textStatus\": \"error\",\n    \"errorThrown\": \"Internal Server Error\"\n  },\n  \"url\": \"https://usegalaxy.org/api/histories/3d213f5e64af4795/contents?details=bbd44e69cb8906b515e37ecb30744100&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2022-01-19T06%3A13%3A15.000Z&qv=False&qv=False\",\n  \"model\": [\n    {\n      \"state\": \"running\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"FASTA from pilon on data 19 and data 7\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"3d213f5e64af4795\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 46,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"update_time\": \"2022-01-19T03:16:37.313Z\",\n      \"id\": \"bbd44e69cb8906b515e37ecb30744100\",\n      \"type_id\": \"dataset-bbd44e69cb8906b515e37ecb30744100\",\n      \"create_time\": \"2022-01-19T03:15:50.458Z\",\n      \"extension\": \"fasta\",\n      \"type\": \"file\",\n      \"url\": \"/api/histories/3d213f5e64af4795/contents/bbd44e69cb8906b515e37ecb30744100\",\n      \"dataset_id\": \"bbd44e69cb8906b5a067e46c3375b639\",\n      \"rerunnable\": true,\n      \"creating_job\": \"bbd44e69cb8906b5dc7e571a17ea8d06\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b515e37ecb30744100\",\n        \"download\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b515e37ecb30744100\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b515e37ecb30744100\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b515e37ecb30744100&metadata_name=\"\n      }\n    },\n    {\n"}, {"role": "system", "text": "Hi @Candidomolino\nThis looks like a server error that can come up when the server is restarted (and sometimes when it is really really busy) \u2013 but that should resolve with a rerun. I wouldn\u2019t expect this particular error from problematic inputs or outputs but that\u2019s possible too. Or maybe the tool has some corner-case configuration bug.\nIf a rerun hasn\u2019t worked by now, and since you were working at usegalaxy.org, I can help quicker and with more specificity if you send a bug report from one of the red error datasets. Please leave all of the inputs/outputs undeleted in the same history, plus any prior failures intact, and paste the URL to this help topic in the comments for context.\nIf you are using a workflow, consider one more rerun directly from the history instead to rule out problems with it. Or I can try that \u2013 but generate a share link to the workflow and also include that in the comments \u2013 or at least include the full name/version and I can find it that way. We want to be all reviewing the same exact processes that produced the error. This error tends to be technical when persistent.\nLet\u2019s start there, thanks!"}], [{"role": "user", "text": "I\u2019ve been running DeSeq2 for my data and an error message came up saying: \u201cUnable to run this job due to a cluster error, please retry it later\u201d. I think there is an issue with the Galaxy server at the minute, as this happened to me before, but was quickly resolved when the Galaxy server was fixed. I just thought I\u2019d give a heads up. The URL used is \u201cusegalaxy.org\u201d."}, {"role": "system", "text": "Hi Joe,\nThe server is up now.\nFor anyone else that ran into the same issues, please rerun any jobs that errored. Many failed due to a cluster issue that is now resolved.\nDetails: https://galaxyproject.statuspage.io/\n\nApr 18, 2019\nGalaxy Main job failures\nResolved - Jobs run between 8:30AM and 10:30AM EDT (UTC -0500) on Galaxy Main failed due to an out-of-space condition on the primary cluster (Slurm) server. This issue has been resolved.\nApr 18, 14:48 UTC\n\nThank you for posting the problem!"}], [{"role": "user", "text": "Please add new Reference genomes to the DeepVariant deep learning-based variant caller:\nMycobacterium tuberculosis\nPseudomonas aeruginosa\nKlebsiella pneumonia\nAcinetobacter baumannii\nE. coli\nPneumococcus\nSARS-CoV-2\nThank you"}, {"role": "system", "text": "\n\n\n player_777:\n\nDeepVariant\n\n\nHi @player_777\nThis tool accepts a reference genome fasta file from the history using the custom genome function. If you plan on incorporating reference annotation ( gtf or gff3 ), be sure to get that at the same time and prep the files to avoid tool errors.\nPrior Q&A topics with instructions for the how-to and expected data formats. I also added some tags to this topic that find even more related topics! (sort by \u201cLatest Posts\u201d).\n\n  \n    \n    \n    Reference genome for RNAseq analysis \n  \n  \n    Hello, \nI\u2019m new to using galaxy and RNAseq analysis in general. I wanted to perform reference based RNA-seq analysis on fastq raw read files for Pseudomonas aeruginosa, PA14. I was going through a tutorial to this and reached the mapping step using HISAT2. However, the reference genome file is unavailable in galaxy nor the UCSC bacterial genome database. Is there any way I could obtain the annotated genome fasta for the bacterium in question. Sorry if its a dumb question but I\u2019m fish out of wate\u2026\n  \n\n\n\n  \n    \n    \n    Uploading new reference genome \n  \n  \n    Hi all, \nI want to add a reference genome for RNA-seq analysis to use on HISAT2 and FeatureCounts. \nGenome assembly ARS-UI_Ramb_v2.0 \n\nHow do I go about doing this and what file formats do the they need to be for each program? \nThanks in advance.\n  \n\n\n\n  \n    \n    \n    new annotated bacterial genome usegalaxy.eu support\n  \n  \n    How can I incorporate an annotated bacterial genome for RNA sequencing analysis using galaxy?\n  \n\n\n\n  \n    \n    \n    Request for of Plasmodium species reference genome usegalaxy.org support\n  \n  \n    Dear Admin, \nI am writing to request the addition of Plasmodium species (Plasmodium vivax, Plasmodium malaria, Plasmodium ovale and Plasmodium falciaparum) reference genome in the galaxy main webpage. Thank you.\n  \n\n\nHope that helps "}], [{"role": "user", "text": "Hi Galaxy team,\nI would like to add rice genome as the reference genome to HISAT2, but when I load the fasta file, the page become unresponsive and snap notification appear. How can I solve this problem?\nThanks."}, {"role": "system", "text": "Hello @nzrf\nThe server was likely busy back when you were attempting to load the fasta with the Upload tool. Trying again was the solution, and hopefully that has worked for you already.\n  \n      \n\n      status.galaxyproject.org\n  \n\n  \n    \n\nThe Galaxy Project Status\n\n  Welcome to The Galaxy Project's home for real-time and historical data on system performance.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf HISAT2 jobs happened to fail while using a fasta that large, and the format is correct (FAQ) that might be due to the size of this plant genome versus the public computational resources available.\nYou could work at UseGalaxy.eu instead \u2013 they host a strain of this genome already built-in for HISAT2 which is far less computationally expensive to use."}], [{"role": "user", "text": "Hi everyone,\nI am working through the Galaxy training tutorial to analyse CLIPseq data on the Galaxy / CLIP-explorer server. (CLIP-Seq data analysis from pre-processing to motif detection)\n.\nI was wondering if someone knows why the Peak caller ( PEAKachu) does not call the peak (Chr17 8,364 Mb - 8,370 Mb region) that was published in the paper on which the training set is based on.  The fold enrichment was reported to be >8 for this which is well above the threshold set in the peak caller.\nI also analyzed one complete replicate (REP2) from yeolab (original data) and the Peak caller still did not return the expected RBFOX2 peak)\n\nimage1322\u00d732 2.07 KB\n\n\nimage1324\u00d7461 36.5 KB\n\n\n\nimage776\u00d7789 30.1 KB\n"}, {"role": "system", "text": "Dear Patrick,\nThe training set is just a tiny sub-sample of the original dataset and might not give you the peak you would find in the paper.\nHowever, with the original data you should obtain the peak. Few notes, since the new version of DESeq2, it is required that you have two replicates for CLIP and control. Since the data by Van Nostrand et al. 2016 only has one replicate for the control, PEAKachu cannot calculate pvalues for the peaks and that is why you only get foldchanges. So be careful if you filter for pvalues, this might result in a false peak set. PEAKachu relies on DESeq2 to call peaks. Another point could be that the parameters are not optimize. Maybe set the MAD to 2.0 and fine tune with the two parameters minuminum cluster expression fraction and minimum block expression.\nIf you still struggle to get the required peaks than maybe try a different Peakcaller such as PureCLIP.\nI hope I could help.\nHave a good day and best wishes,\nFlorian"}], [{"role": "user", "text": "I\u2019m trying to use the Infinium Human Methylation BeadChip workflow in Galaxy with idat files from an Illumina EPIC chip. It\u2019s based on Minfi, which I usually run in Rstudio but wanted to give it a try. It\u2019s ok with the idat files ant the phenotype table, but then I\u2019m asked for a GTF Genome Table (in the example wgEncodeHaibMethyl450Gm12878SitesRep1). What should this genome table be for an EPIC chip?"}, {"role": "system", "text": "Welcome, @ettoremeccia\nInstructions are in the tutorial associated with the workflow in this section:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Infinium Human Methylation BeadChip\n\n  DNA methylation is an epigenetic mechanism used by higher...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I just installed galaxy (release_19.05) on a local server (EL7) under the account galaxy and was able to install some tools.  But when I tried to download some sequences from NCBI SRA, I got problems.  The process got hung. When I checked processes of galaxy, I saw the following two processes.\ngalaxy    90075  90070  0 19:26 ?        00:00:00 prefetch -X 200G --ascp-path /usr/local/bin/ascp|/usr/local/bin/asperaweb_id_dsa.openssh SRR3541303\ngalaxy    90076  90075  0 19:26 ?        00:00:00 /usr/local/bin/ascp -i /usr/local/bin/asperaweb_id_dsa.openssh -pQTk1 -l 450m dbtest@sra-download.ncbi.nlm.nih.gov:data/sracloud/traces/sra40/SRR/003458/SRR3541303 /data/packages/galaxy/database/jobs_directory/000/21/working/sra/SRR3541303.sra.tmp.10369.tmp\n\nWhen I tried the second process on the commandline, I got prompted for Key passphrase, which I have no idea what it was. When I just pressed enter, I got a \u201cfailed to authenticate, exiting.\u201d message.  I\u2019ve extensively searched on google and here, but didn\u2019t get any info about such problems.  I didn\u2019t see any info about such issues in galaxy document (of course I could have missted it).  Any info is appreciaed."}, {"role": "user", "text": "I\u2019ve found out the cause and fixed the problem.  The private key file /usr/local/bin/asperaweb_id_dsa.openssh  didn\u2019t exist.  I downloaded the aspera connect from https://downloads.asperasoft.com/ copied the file asperaweb_id_dsa.openssh to /usr/local/bin/."}], [{"role": "user", "text": "I wanted to run Genes to transcript map output on my trinity assembly data, but I am not able to find that tool. Is there any alternative tool for this?"}, {"role": "system", "text": "Hi @RRR,\nthis tool is not available currently in usegalaxy.org; I opened a PR in order to install it; meanwhile, you can run your analysis in usegalaxy.eu, since in this instance this tool is available:  Genes to transcripts.\nRegards"}], [{"role": "user", "text": "Hi Dears, I processed my Pseudomonas Aeruginosa sequences with the reference sequence Pseudomonas Aeruginosa (str.PA7): 16720 on the Map with Bow Tie for Illumina tool.\nI need the NCBI accession number of the reference sequences Pseudomonas Aeruginosa (str.PA7): 16720 on the Galaxy Au platform. Can you help me?\nAlso, Pseudomonas Aeruginosa reference sequence is not available in Map with BWA-MEM, can you add it?"}, {"role": "system", "text": "Hello @Busrak\nThis is where the index was sourced: Index of /indexes/microbes/16720/\n\nThe accession is NC_009656.\nInside the /seq directory is the fasta: pseuAeru_PA7.fa\n\nAdd the fasta to a history (capture the URL and paste into the Upload tool) then use it as a custom reference genome with any tools that do not have it already indexed. How to use Custom Reference Genomes?\n\n"}], [{"role": "user", "text": "Dear all,\ni have cutadapt raw output files and want to run cutadapt to trim nextera transposase sequences. Trim galore can\u2019t find any files neither my original fastq.gz files. Could you please help me?\nThanks in advance,\nBest\nBernhard"}, {"role": "system", "text": "Hi @Bernhard,\nmost likely your files have \u2018fastq.gz\u2019 datatype in Galaxy, eg format: fastq.gz. Cutadapt recognizes this datatype, while Trim Galore works with datatype like fastqsanger.gz. Similar topic discussed on the forum, eg Unicycler failed - \"reads unavailable\" -- Inputs not recognized due to an incompatible datatype assignment - #3 by vanina.guernier\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all,\nI am currently analysing data coming from RNA sequence. My samples come from mice. I would like to use the goseq tool. The available genome is mm10. However the alignment was done with the previous version of the genome (GRCm38.p6). I don\u2019t think that I can use the mm10 version for the goseq. Do you know if I have any other possibility or I should use external tools for the GO analysis (profiler, DAVID etc) ?\nThank you!"}, {"role": "system", "text": "Welcome, @des_b\n\n\n\n des_b:\n\nHowever the alignment was done with the previous version of the genome (GRCm38.p6).\n\n\nGRCm38 and mm10 are the same genomic assembly. But the \u201cbuild\u201d can differ between data providers (chromosome attribute). Then, the annotation used can also differ between sources (gene + transcript attributes).\nSee mouse assemblies here for what is used in Galaxy for GRCm38/mm10 \u2192 UCSC Genome Browser Downloads\nIf your data is based on a non-UCSC source for GRCm38/mm10 you might be able to convert the chromosome labels using the tool Replace column by values which are defined in a convert file. Common convert file mappings can be obtained through the link in the down in the help section.\nMapping between annotation sources is a bit trickier since those are usually not 1-1 pairings. But, you can try using annotateMyIDs, before deciding to use an annotation that is directly supported by GOseq instead (or, before bothering to create a custom GO mapping input, see next).\n\n\n\n des_b:\n\nDo you know if I have any other possibility or I should use external tools for the GO analysis (profiler, DAVID etc) ?\n\n\nThe Galaxy wrapped GOseq tool allows for custom GO mappings. Examples of the file formats are down in the tool form help.\nThe other two tools are also wrapped in Galaxy.\nIf these tools are used in Galaxy or directly, both will involve making sure that the data inputs and the reference files are based on the same genomic assembly build and a common annotation source.\nTL;DR The idea is to make sure all data is from the same assembly (GRCm37, GRCm38) and the build identifiers \u201cmatch up\u201d as a technical consideration, then the scientific algorithms are applied. Computers are literal when processing important values.\n\n\nChr1 and chr1 and 1 all mean  \u201cChromosome 1\u201d to a person, but are considered different by tools.\nSame for feature annotations: GeneA and GeneA.2 and GeneA.20 are all different to tools but could all mean \u201cGene footprint A\u201d with no particular version to a person.\n\nResources\n\n3: RNA-seq genes to pathways\nData Manipulation Olympics\n"}], [{"role": "user", "text": "How to filter SNP in file (A)as region in defined range  in file (B) in usegalaxy?\nA:\nchr1 186 187 rs378896312\nchr1 703 704 rs136740087\nchr1 833 834 rs136710007\nchr1 864 865 rs43096727\nchr1 1610 1611 rs132965558\nchr1 1611 1612 rs134909289\nchr1 1628 1629 rs136933083\nB:\nchr1  185  190\nchr1  800  840\nchr1  1600 1620\nchr1  1625  1630\noutput:\nchr1 186 187 rs378896312\nchr1 833 834 rs136710007\nchr1 1610 1611 rs132965558\nchr1 1611 1612 rs134909289\nchr1 1628 1629 rs136933083"}, {"role": "system", "text": "I hope I understand your question correctly. You want to have the SNPs in file A that are to 100% in the ranges listed in your file B, and thus generating your desired output?\nThe tool you are searching for is bedtools Intersect intervals. Make sure to tick the option: Write the original A entry once if any overlaps found in B. and under Required overlap that you give Minimum overlap required as a fraction of A the value 1.0.\nA detailed explaination of the tool is given in Galaxy and here."}], [{"role": "user", "text": "Hi. I\u2019m trying to RNA-Seq de novo assembly with using Trinity. Australia server supports this tool. But in EU server, it is not working. It\u2019s been waiting like this since last night."}, {"role": "system", "text": "Hi @barbaros,\nthank you so much for the report. The problem is that currently, Trinity requires to allocate 512Gb of RAM in order to work, but there are not always computer nodes that can provide those resources. We are working in order to optimize memory usage. Sorry for the inconvenience.\nRegards"}], [{"role": "user", "text": "Hello, I am new to galaxy, so this question may sound simple\u2026\nI am following the 16S analysis tutorial (https://galaxyproject.github.io/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html), but instead of using the Silva V4 fasta file provided ( https://zenodo.org/record/800651/files/silva.v4.fasta), I want to use the latest release os SILVA (138), available from Mothur (https://mothur.org/wiki/silva_reference_files/).\nHowever, when using align.seqs, it shows an error when using SILVA 138, although it works with the V4 FASTA available from that tutorial.\nDoes anybody know what could be going wrong? Any tips to make align.seqs work using SILVA 138?\nThanks"}, {"role": "system", "text": "Hi @siuguy\nHere is some help from the Mothur team regarding data extraction and formatting: https://mothur.org/blog/2020/SILVA-v138-reference-files/\nI haven\u2019t personally tried it but that is where to start troubleshooting the data you have now versus what the tool authors had success using."}], [{"role": "user", "text": "Hi everyone!\nThis may seem as a odd question, but we realized that our Rad-Seq sequencing was affected by the fact that the library preparation had poor results : they used 2 enzymes (pstI and mspI), but for any reason a lot of them ended sticking to each other (eg we had sequences looking like this: pst1\u2026mst1-mst1\u2026mst1), creating chimeras. Is there a tool somewhere on Galaxy that could allow us to separate those sequences, or ultimately that could reject a sequence based on the presence of a restriction site in it?\nThank you! "}, {"role": "user", "text": "Hi everyone \nI think I cracked it, so I\u2019ll put my solution here in case anyone have a similar issue. So in the end I chose to reject my chimeric sequences altogether, as I saw no way to \u201cdigest\u201d them with any tools. So, in case you want to get rid of them to purify your data, here\u2019s how I worked :\n\nI converted my data from fastq to fasta via the Fastq.info tool. There are others available but this one allows you to produce a quality file as well as the Fasta, so if you want to keep all nucleotidic quality you may want to use this tool or an equivalent.\nI then used the tool Filter FASTA (on the headers and/or the sequences) twice, entering the sequence of my first then second digestion enzyme (CTGCAG for pstl and CCGG for mspI). Make sure to check the Output discarded FASTA entries as the discarded entries are the ones without any traces of the restriction site (that may results in chimeras), and thus the ones we want to work with.  So, you want to perform the second filtering on the data discarded from the first filtering to end with data exempt of any of these restriction sites.\nI finally converted my data back to Fastq with the tool Make.fastq. You can either use real a quality file or just let the tool add a 100% quality indication to each nucleotide, depending on if you might need it later or not.\n\nYou should then be able to proceed with your analysis as normal ! Be aware though that this method leaves you with uncompressed fastq files, which has not been a problem for me but may be worth noting.\nHope this helps someone one day!"}], [{"role": "user", "text": "My variant calling jobs are giving errors. I am posting the error messages as snaps. Can someone please help me out with the problem?\nIMG_20210821_1552101920\u00d71440 247 KB\n\n\nIMG_20210821_1549321920\u00d72560 384 KB\n"}, {"role": "system", "text": "Hello,\nThe tool is expecting a tag that is not present in the input BAM dataset. Specifically, the \u201csample\u201d attribute.\nThis information can be added when mapping (read group options) or adjusted after mapping with a tool like AddOrReplaceReadGroups add or replaces read group information.\nMore help:\n\nSearch the BAM format specification PDF with the keyword \u201csample\u201d to learn more about the attribute: https://samtools.github.io/hts-specs/SAMv1.pdf\n*GATK4 workflow documentation may also help: https://gatk.broadinstitute.org/hc/en-us\n\nThe tool form itself has additional usage/formatting help.\nTutorials: Galaxy Training!\n\n"}], [{"role": "user", "text": "I\u2019ve been trying to upload 59 fastq.gz files, each around 500 MB.  These are going into a new history, and I uploaded them from disk with  Type = \u201cfastq.gz\u201d and Genome = \u201cS. cerevisiae\u201d.  In the upload pop-up window, the files were successfully uploaded.  However, when I close the window and view this history, only 40 out of 59 are \u201cgreen\u201d, while 19 are grey and are flagged \u201cthis job is waiting to run\u201d.\nI\u2019ve waited 6 hours, and on occasion one more file gets processed.  I\u2019m not sure what is being processed to begin with, or why it is taking so long?  I\u2019m confused as to what is even being performed, as this was just an upload (which showed completed?) - not sure what is being processed here?\nI\u2019ve tried refreshing, and resuming the jobs.  I also tried deleting the history and starting over fresh - this didn\u2019t work.\nThanks for any help!!"}, {"role": "system", "text": "Hi @JVGen!\nQueued jobs should be left in the queued state. These will eventually finish.\nThis includes Upload jobs. Metadata checks and assignments are made during this process that moves loaded data into a history. This is usually quick, and sometimes not \u2013 much depends on the overall server load and a few other technical factors.\nStarting over or rerunning jobs will only result in those new jobs being added back at the end of the queue, extending the wait time.\nThe public server Galaxy Main https://usegalaxy.org has had more windows of extended wait times than usual over the last few days. We are actively monitoring the ongoing rates of job processing.\nFAQ: Datasets and how jobs execute\nThanks for reporting the issue."}], [{"role": "user", "text": "What are the Galaxy tools for the de novo assembled transcriptome annotation?"}, {"role": "system", "text": "Hi @player_777,\nit not really clear for me what you are searching for. Do you already have your assembled transcriptome? Are you searching for transcriptome annotation software?\nPlease have a look at https://training.galaxyproject.org especially under assembly and transcriptomics.\nCiao,\nBjoern"}], [{"role": "user", "text": "Greetings!  I am a long-time Circos user through the command line, and I\u2019d like to be able to utilize Circos within Galaxy.\nHowever, I have been unable to get Circos to work on usegalaxy.org, for any job regardless of how basic.  Notably, one of the test files I have been using to create a basic ideogram (hg18_karyotype_withbands.txt) is taken directly from the GTN tutorial for Circos, so this input file has already been vetted in terms of proper formatting.  I\u2019ve also been testing with another custom karyotype file (karyotype.human.hg38.txt) with the same results.\nThe \u201cview or report error\u201d button does not provide any additional information for these failed jobs, and doesn\u2019t allow me to submit a bug report for some reason.\nNotably, running the same exact jobs on usegalaxy.eu is successful!  I tested both on \u201cCircos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy8)\u201d (which is not available on usegalaxy.org) and Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy7) (which is the newest version found on usegalaxy.org) - and in both cases for both test files, it works as intended.\nLinks to both test histories:\nusegalaxy.org history link: Galaxy | Accessible History | Circos test history\nusegalaxy.eu history link: Galaxy | Europe | Accessible History | Circos test history (Europe)\n(It appears this note is in the same spirit as Problems with Circos, but in this case, I am using version 7\u2026)\nThanks for taking a look."}, {"role": "system", "text": "Hi @rerdmann\nThanks for reporting the problem! We were internally tracking the need to update the tool suite but I just make a public tracking ticket for that as well. Until it closes or you see the updated version on the usegalaxy.org server, please run the tool at usegalaxy.eu instead."}], [{"role": "user", "text": "Hello.  We have galaxy installed following the \u201cGalaxy Installation with Ansible\u201d tutorial (Galaxy Installation with Ansible ). Everything is working fine until we try to change the database_connection string to use our central Postgres server instead of localhost.\nWhen we change this to:\ndatabase _connection: \u201cpostgresql://galaxy:password@pg1/galaxy\u201d\nThe page fails to load with a \u201c502 Bad Gateway.\u201d  If we revert it to the original, everything works.\nWe have tested the database_connection string from the command line using \u201cpsql -Atx postgresql://galaxy:password@pg1/galaxy\u201d\nAny help or suggestions would be greatly appreciated.\nMany thanks,\nDevin"}, {"role": "user", "text": "I finally fixed this by updating pg_hba.conf on the remote server to give the galaxy user access to the postgres database in addition to the galaxy database.\nIs it sufficient for galaxy to have read-only access to this database as long as it has full privs over the galaxy database?  If not, any details on exactly what it needs to access and do from the postgres database would be greatly appreciated.\nMany thanks,\nDevin"}], [{"role": "user", "text": "Hi would it be possible to add this genome?\nAlso if I want to align using Hisat2 and use a file added to my local history in the meantime, can I just use a fasta file or do I need to generate the .ht2 files?\nthanks"}, {"role": "system", "text": "d\u2019Artagnan,\nIt would be possible to add that genome to hisat2\u2019s indexes, but in the meantime, you can get away with uploading the reference genome and using that as another input by selecting the Use a genome from history option. The tool will then generate its own .ht2 files for that analysis, so you wouldn\u2019t need to do anything in addition."}], [{"role": "user", "text": "Can I use the gencode v19 annotation gtf file or any genecode version annotation gtf file for GRCh37 assembly as the associated gene model file for splice junctions for the built-in hg19 reference genome?"}, {"role": "system", "text": "Hello @nash52\nYes, Gencode\u2019s human annotation for build GRCh37 is a match for UCSC\u2019s hg19 version of the assembly.\nFor the how-to, please see:\nQ&A (same formatting advice applies to hg19): RNA-STAR and hg38 GTF reference annotation\nFAQ: https://galaxyproject.org/support/diff-expression/\nThanks!"}], [{"role": "user", "text": "Hi,\nIn untrimmed fast QC it shows me errors :\nPer sequence GC content = !\nSequence Length Distribution = !\nSequence Duplication Levels = !\nSo how i can the improve the duplication level ??\nThis one is before trimming:\n\nimage1920\u00d71080 363 KB\n\nThis one is after trrimming:\n\nimage1920\u00d71080 368 KB\n"}, {"role": "system", "text": "Hi @Thara_Dharanidhar,\nI suggest you to remove the first 10 nucleotides form the 5\u2019 end by using PRINSEQ.\nRegards"}], [{"role": "user", "text": "Hello,\nI didn\u2019t see any topic about my problem.\nI tried to follow the steps to submit with planemo a new tool on my toolshed. Then, I wanted to remove the tool but with the admin interface nothing to do that. After some search, I removed the repository located in /database/community_files/000/repo_1 and\u2026 this is the drama.\nI understood that it\u2019s not just a simple directory but also mercurial managment and sqlite database\u2026\nAnd now impossible to restart the toolshed using the  using the galaxyToolshed_start.sh\nTraceback (most recent call last):\n  File \"lib/tool_shed/webapp/buildapp.py\", line 65, in app_factory\n    app = UniverseApplication(global_conf=global_conf, **kwargs)\n  File \"lib/tool_shed/webapp/app.py\", line 95, in __init__\n    self.repository_registry = tool_shed.repository_registry.Registry(self)\n  File \"lib/tool_shed/repository_registry.py\", line 18, in __init__\n    self.certified_level_one_clause_list = self.get_certified_level_one_clause_list()\n  File \"lib/tool_shed/repository_registry.py\", line 146, in get_certified_level_one_clause_list\n    certified_level_one_tuple = self.get_certified_level_one_tuple(repository)\n  File \"lib/tool_shed/repository_registry.py\", line 164, in get_certified_level_one_tuple\n    latest_installable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(self.app, repository)\n  File \"lib/tool_shed/util/metadata_util.py\", line 93, in get_latest_downloadable_changeset_revision\n    repository_tip = repository.tip()\n  File \"lib/tool_shed/webapp/model/__init__.py\", line 331, in tip\n    repo = self.hg_repo\n  File \"lib/tool_shed/webapp/model/__init__.py\", line 219, in hg_repo\n    WEAK_HG_REPO_CACHE[self] = hg.cachedlocalrepo(hg.repository(ui.ui(), self.repo_path().encode('utf-8')))\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/hg.py\", line 231, in repository\n    createopts=createopts,\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/hg.py\", line 193, in _peerorrepo\n    ui, path, create, intents=intents, createopts=createopts\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/localrepo.py\", line 3350, in instance\n    return makelocalrepository(ui, localpath, intents=intents)\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/localrepo.py\", line 546, in makelocalrepository\n    raise error.RepoError(_(b'repository %s not found') % path)\nmercurial.error.RepoError: b'repository /storage/galaxydev/aubi/galaxy21.05/database/community_files/000/repo_1 not found'\n\nI tried to re-create the folder and I add a copy of another folder repo_2 to mimic the path but now, the toolshed started but nothing happen\u2026"}, {"role": "user", "text": "Finally, I reset the sqlite database and associated files before completely restart a new toolshed"}], [{"role": "user", "text": "Dear Galaxy Team,\ncurrently there are multiple jobs in my waiting list to run in RNA Star and it is not going on for more than 24h. Is there a problem with the tool?"}, {"role": "system", "text": "Hi @Jenny19,\nthe jobs will run when resources become available. Public servers utilized common resources shared with everyone else working at the same place; please be patient.\nRegards!"}], [{"role": "user", "text": "Hello,\nCan some one help or tell me how I can check what goes wrong with the communication server. In my galaxy.yml I have:\n  # Galaxy real time communication server settings\n  enable_communication_server: true\n\n  # Galaxy real time communication server settings\n  communication_server_host: 0.0.0.0\n\n  # Galaxy real time communication server settings\n  communication_server_port: 7070\n\nI enabled it in preferences > Change communication settings > Enable communication\nIf I go to 0.0.0.0:7070 I see a chat screen where I can type in and send messages.\nThe problem is that the chat icon does not appear. I am currently not using a proxy.\nEDIT:\nIf I change this to my actual ip adres the icon appears but the chat window is empty (a white square)\ncommunication_server_host: 0.0.0.0\nEDIT 2:\nIt turned out that even I only see a white window if my I hover over it with my mouse the cursor changes in the \u201ctyping\u201d cursor and if I type something and press enter I see my message appear at the 0.0.0.0:7070 adress. Is it a CSS problem?\nEDIT 3:\nI solved my problem, I still hope if some one can confirm if this is the proper way.\nI simply need to add opacity: 1 !important; to base.css (I did !important because for now this is a quick fix)"}, {"role": "system", "text": "Thanks for updating the post with your fix \u2013 I\u2019ll make sure this makes its way into the codebase now."}], [{"role": "user", "text": "Hi, after fixing a previous problem concerning the installation of cvmfs (ERROR while trying to make CERNVM-FS key directories), I ran into the following problem:\n*\nfatal: [192.168.123.108]: FAILED! => changed=false\n  cmd:\n  - cvmfs_config\n  - chksetup\n  delta: '0:00:00.032705'\n  end: '2022-10-20 14:46:59.249437'\n  msg: non-zero return code\n  rc: 1\n  start: '2022-10-20 14:46:59.216732'\n  stderr: |-\n    Error: failed to load cvmfs library, tried: './libcvmfs_fuse3_stub.so' '/usr/lib/libcvmfs_fuse3_stub.so' '/usr/lib64/libcvmfs_fuse3_stub.so' './libcvmfs_fuse_stub.so' '/usr/lib/libcvmfs_fuse_stub.so' '/usr/lib64/libcvmfs_fuse_stub.so'\n    ./libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    /usr/lib/libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    /usr/lib64/libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    ./libcvmfs_fuse_stub.so: cannot open shared object file: No such file or directory\n    libcrypto.so.1.0.0: cannot open shared object file: No such file or directory\n    /usr/lib64/libcvmfs_fuse_stub.so: cannot open shared object file: No such file or directory\n\n    Failed to read CernVM-FS configuration\n  stderr_lines: <omitted>\n  stdout: ''\n  stdout_lines: <omitted>\n\nI found the same error on github (Crashes when running the role for the GAT \u00b7 Issue #30 \u00b7 galaxyproject/ansible-cvmfs \u00b7 GitHub), where I found out that for some reason it\u2019s written in xenial.\nxxx@xxx:~/galaxy$ cat /etc/apt/sources.list.d/cernvm.list.list\ndeb [allow-insecure=true] https://cvmrepo.web.cern.ch/cvmrepo/apt/ xenial-prod main\nxxx@xxx:~/galaxy$ grep VERSION /etc/os-release\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.1 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\n\nUnfortunately, the fix that was used there doesn\u2019t work for me. This might have to do with the fact that I have Ubuntu 22.04 installed as my OS instead of 20.04. I was wondering how to update the cvmfs role to include the 22.04 package, if this is not already done. And if so, should this be done by editing the playbook (if so where?) or by manually installing the package?\nThanks in advance!\nEdit: I also found out I already have the libcvmfs_fuse_stub.so file in the /usr/lib directory but the specific path to this file (/usr/lib/libcvmfs_fus_stub.so) is not listed under the paths where it tries to load the cvmfs library from, which might actually be causing the problem. I have been looking for where to edit/specify this path, but can\u2019t find the file. Any help regarding this is appreciated!"}, {"role": "system", "text": "Morning @cbass, you\u2019re on an LTS/Jammy so that\u2019s good. We added support to the role for that on September 8 which is quite recent (Add jammy to list of supported Ubuntu versions \u00b7 galaxyproject/ansible-cvmfs@6b9c746 \u00b7 GitHub) and it looks like we haven\u2019t released a new version of the role since then! (Whoops!)\nBy the time you read this comment the ansible role for CVMFS at version 0.2.18 should be available, if you update, that might fix it "}], [{"role": "user", "text": "Hello,\nI have built a dataset collection from existing files in my history but I missed a couple of files and now want to add these to my dataset collection. I cannot find a way to do this without making the collection again which I want to avoid as there are ~100 files and that is a lot of clicking. Does anyone know how to do this?"}, {"role": "system", "text": "Collections are supposed to be immutable unless very specific conditions occur.\nHowever you can use tools for mass operations to filter and select all datasets you are interested in, and then create a collection with just these inside. See the screenshot below for an example.\nGalaxy289\u00d7650 29.4 KB"}], [{"role": "user", "text": "How to filter FreeBayes output (vcf) file to specific region (SNP) bed file in usegalaxy?\njust want to look at some of the positions (SNP) in bed file.\ntx"}, {"role": "system", "text": "bedtools Intersect intervals should let you do this."}], [{"role": "user", "text": "I have used tool String tie to prepare reference assembly and got output data  as GTF file. For structural and functional annotation I need to convert it to FASTA format. suggest me with a tool which can be used to convert GTF to FASTA."}, {"role": "system", "text": "Hi @Vikas_Sharma\nThe transcript assembly that Stringtie generates is not a full assembly that leads to a fasta output.\nFor a fasta output, you\u2019ll need to assemble the reads with a tool like Trinity.\nThe GTN Tutorials cover the how-to. Examples below. The last one is a work-in-progress but should be useful even in the current state.\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: De novo transcriptome reconstruction with RNA-Seq\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: Reference-based RNA-Seq data analysis\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: De novo transcriptome assembly, annotation, and different...\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\nMore help for Trinity is available from the tool authors. The methods are described for the command-line version of the tool but the overall protocol is the same as when running the wrapped tools in Galaxy.\n\n  \n      \n      GitHub\n  \n  \n    \n\ntrinityrnaseq/trinityrnaseq\n\nTrinity RNA-Seq de novo transcriptome assembly. Contribute to trinityrnaseq/trinityrnaseq development by creating an account on GitHub.\n\n\n  \n  \n    \n    \n  \n  \n\n\nThanks!"}], [{"role": "user", "text": "\nimage930\u00d7213 5.11 KB\n"}, {"role": "system", "text": "Hi @Aparna_Baxi,\nyou need to upload the reference annotation in GTF format and select it from your history.\nRegards"}], [{"role": "user", "text": "I downloaded a custom install of galaxy. I\u2019ve tried following the example for creating a custom tool, as shown here: Adding custom tools to Galaxy. I\u2019ve modified the config/galaxy.yml file to point to my tool_conf.xml file. However, when I startup galaxy, I don\u2019t see my custom tool. It appears to simply be the default. What am I doing wrong?\nI\u2019ve downloaded release 21.05."}, {"role": "user", "text": "I figured it out. Appears that I had capitalization discrepancies between the directory and my config file."}], [{"role": "user", "text": "Hi\ni tried running Bismark on usegalaxy.eu but it didn\u2019t work (toolshed.g2.bx.psu.edu/repos/bgruening/bismark/bismark_bowtie2/0.22.1+galaxy4). it seems the all mapping worked fine but then the files couldn\u2019t be merged (see snapshot) .i am trying to aligned 2 paired end samples to TAIR10.\nany help or advice are welcome\nthanks a lot\n\nScreenshot 2021-04-20 at 22.47.421274\u00d7784 80.6 KB\n"}, {"role": "system", "text": "Hi @pej,\nthanks for reporting the error. There is a problem with the tool, could you try it tomorrow again?\nRegards.\nUpdate: the problem was solved in this PR: Bismark: Fix subprocess command by gallardoalba \u00b7 Pull Request #1110 \u00b7 bgruening/galaxytools \u00b7 GitHub"}], [{"role": "user", "text": "Hi,\nSo unfortunatelly our bioinfo guy is unavailable and I need to analyze my ChIP-seq data myself. I need some guidance for the analysis with multiple biological repeats and differential binding between the treatments.\nI am checking histone marks distribution in the rat tissue at different age. I have 10 biological repeats/  per timepoint, and 4 different timepoints. What I would like to get is a list of genes marked by given histone mark at each timepoint (relatively easy), but then also observe the change of the binding of histone marks at different timepoints, so do some differential analysis.\nWhat should I do? Merge the BAM files at this stage (quite problematic, since the files are big and using quite a lot of space) and call the peaks on the merged file? How to compare then the difference in binding between the timepoints?\nShould I rather call the peaks separately for each biological repeat and then merge the BED files together at the level of MACS2? Then once again, how to compare the difference between the timepoints.\nIs there any step by step tutorial? I have seen there are some tools like diffbind or multiGPS, I am reading the papers describing them, but honestly I am quite lost (or stupid) and need some easier description of the process."}, {"role": "system", "text": "Hello @dkomar\nHopefully you have found the tutorials already, but if not please find them here. The site can be searched with keywords like datatypes and tool names, or navigated by analysis domain.\nMany tools will also have a help section with examples plus link-outs for citations/publications and author resources.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I have 3 vcf files that I want to merge but one of them has \u201cchr\u201d in the chromosome identifiers and the other two don\u2019t. Is there a way to edit the identifiers so they match?"}, {"role": "system", "text": "Hi @numbergirl86\ntry text replacement tools. Maybe separate header and the data. For data replace values in 1st column. Concatenate header and data after replacement.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "\nphoto_2021-04-21_21-45-31576\u00d71280 17.9 KB\n\nWhich steps should I follow to answer this question?"}, {"role": "system", "text": "Hi @Shaimaa_Gamal, this history provides the response."}], [{"role": "user", "text": "Hi,\nI\u2019m running RNA-STAR on 2 datasets which would normally take only <1hr to run, but now it has taken 2 days and the status is still shown in \u2018orange\u2019/running mode. The track doesn\u2019t turn red/failure either. Please let me know what\u2019s going on with these 2 jobs. Both jobs were started on Aug19, 2022.\nThanks!"}, {"role": "system", "text": "Hi @lordyiffles\nIf the job is still executing, then it is usually best to let it complete. If it is too large to process or there is some input problem, the result will change to a red dataset with error logs you can troubleshoot.\nThe runtime for jobs is limited at most public servers, and no job will execute more than about 48-72 hours at UseGalaxy.org. Jobs may queue before that, it depends on how busy the server is.\nIf instead you already know there is some problem, and you want to run the job with different inputs/parameters, it is fine to delete + purge the current job and start up a new one with the changes. The \u201cdon\u2019t delete and rerun\u201d advice is for when there won\u2019t be any changes.\nFAQ understanding-job-statuses"}], [{"role": "user", "text": "I\u2019m using galaxy for my Genomics course. We are currently working on our annotation unit. Every time I try to run gffread the job looks like it will start, but then never actually updates in my history. I do not get an error message at all. I\u2019ve been able to run other tools without any issues."}, {"role": "system", "text": "You need to select one of the options that produces an output, e.g. the options under Feature File Output, or the options available if you specify a reference genome."}], [{"role": "user", "text": "Hello everyone\nI am working in Galaxy with miRNAs data.\nI am trying to pass my reads mapped with Bowtie to counts with the featureCounts app.\nI have not been successful, the process gives me an error because the miRBase annotation file is in gff3 format and featureCounts asks for this file in GTF file.\nHow can I solve this step?\nP.S. I have used the miRBase sequences to map my reads."}, {"role": "system", "text": "Hi @andres2r89,\nI suggest you use the miRDeep2 tool for quantifying miRNAs data; it is built on Bowtie and specifically designed for working on miRNA. You can find some information about how to quantify miRNAs in this tutorial.\nRegards"}], [{"role": "user", "text": "Hi all,\nIs it possible to use galaxy GOseq if I am dealing with RNA-seq analysis for bacterial specie, which is not included in the list of the Gene categories?\nI uploaded its gtf.file > Use a categories file from history, but I got an error after 3 minutes!!!\nHas anyone faced the same issue,? thanks"}, {"role": "system", "text": "Hi @Seraph\nGOseq will not understand how to use a GTF annotation. Plus, that file type is usually doesn\u2019t contain ontology information. But if yours does, that data can be parsed into the format the tool is expecting.\nSee the help section on the tool form to learn the content/format of accepted custom inputs. The first column should match the Gene ID used in the other inputs. The second column are the GO/KEGG terms.\n\nGene categories file\nThis tool can get GO and KEGG categories for some genomes. The three GO categories are GO:MF (Molecular Function - molecular activities of gene products), GO:CC (Cellular Component - where gene products are active), GO:BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products). If your genome is not available, you will also need a file describing the membership of genes in categories. The category file should have two columns with an optional header row. with Gene ID in the first column and category identifier in the second column. As the mapping between categories and genes is usually many-to-many, this table will usually have multiple rows with the same Gene ID and category identifier.\nExample:\nENSG00000162526   GO:0000003\nENSG00000198648   GO:0000278\nENSG00000112312   GO:0000278\nENSG00000174442   GO:0000278\nENSG00000108953   GO:0000278\n\nHelp for data manipulations: Data Manipulation Olympics\nHope that helps "}], [{"role": "user", "text": "how can I run FASTAQE and how to interpret ?"}, {"role": "system", "text": "Hi @walaa,\nI recommend you to have a look at the Quality control training:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Quality Control\n\n  Analyses of sequences\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nRegards"}], [{"role": "user", "text": "Hi, I\u2019m a newbie here\nI\u2019m currently working with fastq secuences from GEO and I\u2019m trying to obtain counts in Galaxy.\nIn the description of the database i\u2019m using, the researchers say \u201cReads filtering under criteria removing reads with 20% of the base quality lower than 13\u201d.\nMy question is,\nThis configuration with Trimmomatic will lead me to do what they did? or how I could do it?\nimagen_2023-06-27_151601989787\u00d7557 40.3 KB\nThanks"}, {"role": "system", "text": "Hi @anlcorteslo\nyou are after Filter by quality (in FASTQ/FASTQ section).\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello, I\u2019m new to galaxy and I wanted to know why this happens:\nI made a collection of 3 items and used different tools on it. The tools worked and the output of each of the items loaded correctly and appered in green within the collection. However, the collection containing the 3 items appears to still be loading and grey in the history. How should I interpret this?"}, {"role": "system", "text": "Hi @alvaropr13,\ndid your collection turn green finally?\nRegards"}], [{"role": "user", "text": "Hello, I have been trying to get a jupyter notebook running with Interactive Jupyter Notebook (Galaxy Version 0.3) to no avail. The job gets put onto my history, but never executes, and I can not find anything under my \u2018Active Interactive Tools\u2019.\nThanks in advance for any input on this matter.\n\nKazam_screenshot_000171848\u00d71053 90.4 KB\n"}, {"role": "system", "text": "Hi @Trevor_Millar\nAll of the interactive environments at UseGalaxy.org are problematic right now. We are working on it.\nFor now, please use these tools at UseGalaxy.eu or UseGalaxy.org.au."}], [{"role": "user", "text": "Hi,\nI would like to know whether getgalaxy.org can be used for commercial purposes? Currently, it is listed as academic free license V3.0\nIt will be great if get the mail ID of galaxy team."}, {"role": "system", "text": "Hello @Ganesh_P_Kumar\nGalaxy is open-source. You do not need our permission to use it  If you would like to make more direct contact with a specific Galaxy community or project area, please see the resources here: https://galaxyproject.org/\nThanks!\n\n  \n      github.com\n  \n  \n    galaxyproject/galaxy/blob/dev/LICENSE.txt\nCopyright (c) 2005-2016 Galaxy Contributors (see CONTRIBUTORS.md)\n\nLicensed under the Academic Free License version 3.0\n\n 1) Grant of Copyright License. Licensor grants You a worldwide, royalty-free, \n    non-exclusive, sublicensable license, for the duration of the copyright, to \n    do the following:\n\n    a) to reproduce the Original Work in copies, either alone or as part of a \n       collective work;\n\n    b) to translate, adapt, alter, transform, modify, or arrange the Original \n       Work, thereby creating derivative works (\"Derivative Works\") based upon \n       the Original Work;\n\n    c) to distribute or communicate copies of the Original Work and Derivative \n       Works to the public, under any license of your choice that does not \n       contradict the terms and conditions, including Licensor's reserved \n       rights and remedies, in this Academic Free License;\n\n\n\n  This file has been truncated. show original\n\n  \n  \n    \n    \n  \n  \n\n"}], [{"role": "user", "text": "Hello,\nI\u2019m new to using galaxy and RNAseq analysis in general. I wanted to perform reference based RNA-seq analysis on fastq raw read files for Pseudomonas aeruginosa, PA14. I was going through a tutorial to this and reached the mapping step using HISAT2. However, the reference genome file is unavailable in galaxy nor the UCSC bacterial genome database. Is there any way I could obtain the annotated genome fasta for the bacterium in question. Sorry if its a dumb question but I\u2019m fish out of water in these areas. Thank you."}, {"role": "system", "text": "Hi @Gracious_Donkor\nNCBI has choices: Pseudomonas aeruginosa PA14 - NCBI - NLM\nLoad just the genome fasta and the annotation GTF/GFF3 to Galaxy.\nThese FAQs can help with common reformatting changes that are needed. If not formatted properly, tools will fail or produce odd results.\n\n\n\nGetting error while using HtSeq Count\n\n\nYou can also review these FAQs for help addressing this and related problems that often come up together. These problems can present whether you are working in Galaxy or not.\n\n#working-with-gff-gft-gtf2-gff3-reference-annotation\n#working-with-very-large-fasta-datasets\n\n\n"}], [{"role": "user", "text": "Hi, I am tried to find ROSE to do super-enhancer analysis.\nAlthough this tool is very popular in this field, I cannot find this program in Galaxy.\nI wonder if it is possible to see this tool in galaxy soon.\nIt would be fantastic to do all my analysis on galaxy."}, {"role": "system", "text": "Hi @Sheng-Chieh_Hsu,\nI opened an issue about it in the IUC repository.\nRegards"}], [{"role": "user", "text": "Run kallisto with .fasq and CDS.fa\nand I failed with this fatal message\nFatal error: Exit code 255 ()\nFATAL: container creation failed: mount /cvmfs/main.galaxyproject.org/galaxy->/cvmfs/main.galaxyproject.org/galaxy error: while mounting /cvmfs/main.galaxyproject.org/galaxy: while getting stat for /cvmfs/main.galaxyproject.org/galaxy: stat /cvmfs/main\n/corral4/main/jobs/045/461/45461829/galaxy_45461829.sh: line 129: python: command not found\nHow can i solve this?"}, {"role": "system", "text": "Hi @kocastl9,\nthis error is caused by a temporal problem in the server; I suggest you to run your jobs in usegalaxy.eu until the problems have been fixed.\nRegards"}], [{"role": "user", "text": "Hi Galaxy Community Galaxy, we mapped sequence reads from our RNA-Seq experiment to the S. Pome genome (see for your reference  Galaxy | Accessible History | Gavin project 2). We are trying to calculate splicing efficiency for all introns in S. pombe. I am seeking community help if anyone can guide me on how to obtain sequence reads for each intron and its flanking exons. Thank you in advance for the input and help !"}, {"role": "system", "text": "Hi @Gavin_Wang\nmaybe follow any published protocol/workflow. If you are after read counts in exons, maybe look at DEXSeq tools or use tools like featureCounts without aggregation of counts. To get sequences have a look at interval tools, such as bedtools. You also can get positions of introns using bedtools, if you have 12 columns BED files.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all,\nis there a way to open and visualize a pdb model directly in Galaxy?\nAs far as I know the only way is to build a custom chart?\nMany thanks."}, {"role": "system", "text": "At least NGL (https://github.com/galaxyproject/galaxy/pull/3601) and https://biasmv.github.io/pv/ are available in Galaxy already."}], [{"role": "user", "text": "Dear Amazing eu team,\nAnswering to a direct e-mail concerning error a user has using Wallace Shiny app, I discover that he is using a local version because the IT wallace tool is not reachable on eu instance.\nAs we don\u2019t work on this IT since long time, maybe we need to make something to update it or other ? Please dont hesitate to say to me if there is issues with existing Wallace IT and / or if we have something to do on it.\nWhishing you a good week,\nBest,\nYvan"}, {"role": "user", "text": "It is now sloved! \u2026 But don\u2019t know how to close the issue "}], [{"role": "user", "text": "Is there any path in galaxy to convert Tab file to Bam ?"}, {"role": "system", "text": "Hi @amir\nCould you explain more about what you are doing?  Share a few sample lines?\nThe tabular datatype is very general.\nOne example: If the tabular dataset contained identifiers and sequence reads:\n\nconvert tabular > fasta > fastq. actual quality scores are optional \u2013 default scores can be added in.\nconvert fastq to an unmapped bam/sam\n\n\nNot very useful and an extra step \u2026 since most tools accept reads in fastq format as an input, even if they also accept reads in an unmapped bam/sam."}], [{"role": "user", "text": "Hello !\nI am facing a problem with stringTie\nI mapped my sequence with  HISAT2 after this I used stringTie but I\u2019m not getting results, what should I have to do\n\nimage1173\u00d7262 2.01 KB\n"}, {"role": "system", "text": "Hi @dev\nErrors with this tool are often due to problems with the tool settings versus the reference annotation provided at run, so check that first.\nThis is an FAQ and you can find more QA at this forum about the topic.\nThis is where to find tutorials:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf you cannot find the problem, then more information is needed to help. See this FAQ for how and where to investigate dataset errors plus how to include context when reporting a problem."}], [{"role": "user", "text": "Hi,\nI attempted to download my complete history, one file being approximately 200GB and the other around 20GB. Both generated a link and the sizes were correct. However, after downloading, neither file could be extracted successfully.\nI received the following error message for both files: \u201cData error - This file is broken.\u201d\nHas anyone encountered a similar issue and found a solution? Any assistance would be greatly appreciated.\nBest,\nMin"}, {"role": "system", "text": "Hi @mintang92\nhistory export was discussed several times, for example:\n  \n    \n    \n    Trouble downloading larger results? Try the \"History Archive\" option \n  \n  \n    Hello, \nI ran Kallisto and Dsse2 to get my list of DE genes. In the results of Dseq2, I can see there are about 20000 lines - I am assuming 1 line for each gene. However, when I download the file, I only get 9000 lines. Can you please let me know why this is happening? And how can I download the entire dataset of 20000 lines?\n  \n\n\nMaybe search the forum with \u2018Download history\u2019 or something similar.\nKeep in mind that some file systems support relatively small files, such as 4 Gb. Make sure your local infrastructure supports storage of large files. Your internet provider might have download limits.\nKind regards,\nIgor"}], [{"role": "user", "text": "I received the following error message while running edgeR, please address this issue, thanks!\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in glmFit.default(sely, design, offset = seloffset, dispersion = 0.05,  :\nnrow(design) disagrees with ncol(y)\nCalls: estimateDisp \u2026 estimateDisp \u2192 estimateDisp.default \u2192 glmFit \u2192 glmFit.default"}, {"role": "system", "text": "Hi @lordyiffles\nI replied to your bug report already but for others reading:\nThis type of error message from any Bioconductor or R tool means that there is a labeling problem or design problem. The details usually have some clues \u2013 this message is stating that the tool ran into a mismatch between the rows and columns of the inputs.\n\n\n\n lordyiffles:\n\nnrow(design) disagrees with ncol(y)\n\n\nClick into the  icon for a dataset to see what was used to produce that data (error or success). This is an easier view to review tool inputs and parameters in a summary format. Often, this alone reveals what the job problems were, along with the content of the logs on that same view (stderr, stdout). And, this is the same information to share when asking a question here.\nFor your data, it looks like there is an extra column in the count matrix that is not associated with a sample name in the factor matrix. The tool Cut can be used to remove that column.\nExamples of these data are on the tool form, plus at the resources below. You might also need to:\n\nadjust headers in the factor matrix\nremove Geneid rows from the count matrix if all sample counts are 0\nfind out why the \u201cdatabase\u201d metadata is set to hg38Patch11 instead of just hg38 for both of these inputs. Only hg38 is indexed at the usegalaxy.* servers. Avoid assigning native metadata keys unless you are certain they apply. Mixups can lead to content problems that are very hard to detect.\n\nFAQs\n\nanalysis\nunderstanding-input-error-messages\nmismatched-chromosome-identifiers-and-how-to-avoid-them\n\nGalaxy Tutorials\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nRelated Q&A at this Galaxy Forum\n\nSearch results for 'bioconductor' - Galaxy Community Help\nTopics tagged edger\n\nOther Forums \u2013 many existing public solutions will also apply to the Galaxy wrapped versions of tools. Searching with error codes is usually enough. If you can\u2019t find a solution, or need help translating a command-line function to a Galaxy tool form\u2019s options, please come back and ask your question here.\n\nhttps://www.biostars.org/\nhttps://support.bioconductor.org/\n\nThanks! "}], [{"role": "user", "text": "Hi\nI have been running a trinity assembly on a dataset for 4 days now. Its still waiting to run or running (no idea)? When I come in each morning there is a message that mentions somthing about the connection interruption and if it persists to to contact admin, but my assembly seems to still be waiting to run or running?"}, {"role": "system", "text": "Hi @bch\nPop-up message that state something about a \u201cconnection interruption\u201d mean that the server is busy with respect to the browser session (server had temporary trouble posting the webpage over your connection). Reloading the web page usually gets rid of those. It is unrelated to queued jobs.\nFor longer queued jobs (grey in color), that is expected right now. All of the usegalaxy.* servers are very busy. If the job is already executing it will be yellow in color. Allow both states to process or you will extend the wait time: lose your place in the queue, or abort an executing job and need to rerun it.\nThis assumes that you have already checked the upstream jobs/datasets that were input to the Trinity job. If you haven\u2019t yet, examine the inputs (expand the datasets) to make sure all are green and without any warnings. If you do find a problem, you\u2019ll need to correct it, then rerun Trinity with the fixed inputs."}], [{"role": "user", "text": "Hi,\nI forgot my password for my Galaxy account and I also no longer have access to that old E-mail. Is there a way that I can transfer account to new E-mail?"}, {"role": "system", "text": "Hello. If you forgot your password and you do not have access to the email address you used we have no means to verify the account is yours. Hence we cannot give you access to it. Sorry."}], [{"role": "user", "text": "Hi\nHow could i filter normalized read counts which for example exclude the genes with read counts under 20.?\nBest"}, {"role": "system", "text": "Hi @amir,\nyou can use  Filter data on any column using simple for that.\nRegards"}], [{"role": "user", "text": "Hello\nI want to use Bowtie not Bowtie2. So i saw a tool named(Map with bowtie with illumina). is that the same bowtie ?"}, {"role": "system", "text": "Yes, the tool Map with Bowtie for Illumina is a wrapper for bowtie 1, not 2.\nBowtie2, the one you don\u2019t want, is Bowtie2 - map reads against reference genome."}], [{"role": "user", "text": "hi\ni was working till yesterday(29/9/21).now its showing this error\n\nThis page isn\u2019t working\nusegalaxy.org is currently unable to handle this request.\nHTTP ERROR 500"}, {"role": "system", "text": "Hi @Shafna_Asmy\nYou probably noticed already, but the server is up.\nIt was out for a short time several hours ago now. Details: https://status.galaxyproject.org/\nThanks for reporting the problem and our apologies for the confusion!"}], [{"role": "user", "text": "Hi. I\u2019m kind of a new user of galaxy and I am trying to run trimmomatic to do some RNA-seq analysis. after trimmomatic, I run fastqc to check the quality of sequences but it seems adaptors are not cut. what am I doing wrong? could you please help?\nhere is the link of files after trimmomatic being run:\nhttps:/usegalaxy.org/api/datasets/f9cad7b01a472135fefff3665ea301f7/display?to_ext=fastqsanger\nlink of the following fastq:\nhttps:/usegalaxy.org/api/datasets/f9cad7b01a472135597b0ae93de397c9/display?to_ext=html"}, {"role": "system", "text": "Hi @Reyhaneh_Nasri,\ncould you share your history with me? Sorry for the late response.\nRegards"}], [{"role": "user", "text": "I have audited the quality of the FASTA file in the FASTQC program on GALAXY server. The program performed the analysis, but unfortunately I can not read the graphs from the analysis. In the upper part of the page appeared a message, which I am sending below. Could you tell me, what is the reason for not displaying graphs and what can you do to display them? I opened GALAXY in the latest version of the Mozilla browser.\nThank you.\nfq_gz.gz FastQC Report @media screen { div.summary { width: 18em; position:fixed; top: 3em; margin:1em 0 0 1em; } div.main { display:block; position:absolute; overflow:auto; height:auto; width:auto; top:4.5em; bottom:2.3em; left:18em; right:0; border-left: 1px solid #CCC; padding:0 0 0 1em; background-color: white; z-index:1; } div.header { background-color: #EEE; border:0; margin:0; padding: 0.5em; font-size: 200%; font-weight: bold; position:fixed; width:100%; top:0; left:0; z-index:2; } div.footer { background-color: #EEE; border:0; margin:0; padding:0.5em; height: 1.3em; overflow:hidden; font-size: 100%; font-weight: bold; position:fixed; bottom:0; width:100%; z-index:2; } img.indented { margin-left: 3em; } } @media print { img { max-width:100% !important; page-break-inside: avoid; } h2, h3 { page-break-after: avoid; } div.header { background-color: #FFF; } } body { font-family: sans-serif; color: #000; background-color: #FFF; border: 0; margin: 0; padding: 0; } div.header { border:0; margin:0; padding: 0.5em; font-size: 200%; font-weight: bold; width:100%; } #header_title { display:inline-block; float:left; clear:left; } #header_filename { display:inline-block; float:right; clear:right; font-size: 50%; margin-right:2em; text-align: right; } div.header h3 { font-size: 50%; margin-bottom: 0; } div.summary ul { padding-left:0; list-style-type:none; } div.summary ul li img { margin-bottom:-0.5em; margin-top:0.5em; } div.main { background-color: white; } div.module { padding-bottom:1.5em; padding-top:1.5em; } div.footer { background-color: #EEE; border:0; margin:0; padding: 0.5em; font-size: 100%; font-weight: bold; width:100%; } a { color: #000080; } a:hover { color: #800000; } h2 { color: #800000; padding-bottom: 0; margin-bottom: 0; clear:left; } table { margin-left: 3em; text-align: center; } th { text-align: center; background-color: #000080; color: #FFF; padding: 0.4em; } td { font-family: monospace; text-align: left; background-color: #EEE; color: #000; padding: 0.4em; } img { padding-top: 0; margin-top: 0; border-top: 0; } p { padding-top: 0; margin-top: 0; }\nError! Filename not specified. FastQC Report"}, {"role": "system", "text": "Hello,\nThis looks like a FastQC web page (HTML) output, is that correct? And you clicked on the \u201ceye\u201d icon to visualize it?\nTools with HTML display need to be checked off here (through the Galaxy web interface): Admin > Tool Management > Manage whitelist\nOnly administrators can do this.\nIf this is your own Galaxy, you can adjust the setting. The server may need to be restarted to take effect (cannot recall if needed or not but you\u2019ll be able to notice).\nIf working at the usegalaxy.org, usegalaxy.eu, or usegalaxy.org.au server (check the URL), then write back about which one. All three have admins that can be reached here to resolve the display problems. A new updated version of the tool may have been added and this step was missed.\nIf working somewhere else, contact the admins of that server and have them adjust it. Contact information is usually on the home page for public Galaxy servers and sometimes here: https://galaxyproject.org/use/. If at your place of work/institution, ask internally about contact info. You can share a link to this post so they\u2019ll know what to do, if don\u2019t already for some reason.\nThanks!"}], [{"role": "user", "text": "The links to the scanpy documentation here on usegalaxy.eu server need to be updated to https://scanpy.readthedocs.io/en/stable/ - the current links show a 404\np.s. is this the correct way to report this or can I create a PR someplace?"}, {"role": "system", "text": "Ofcourse there is no harm in reporting it here, but I think most developers are more active on the corresponding github pages. See:\nhttps://toolshed.g2.bx.psu.edu/repository?repository_id=3d319fa861b2fa1c\n  \n      \n\n      github.com\n  \n\n  \n    tools-iuc/tools/scanpy at main \u00b7 galaxyproject/tools-iuc\n\n  main/tools/scanpy\n\n  Tool Shed repositories maintained by the Intergalactic Utilities Commission\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nTo quickly find the github repository you can click the litle arrow on the top right of the tool.\n\nimage1309\u00d7197 9.69 KB\n"}], [{"role": "user", "text": "Hi,\nI have created a local galaxy server (version 22.05) by following the GetGalaxy tutorial (Get Galaxy - Galaxy Community Hub). I would now like to automate the server by making a ansible playbook. I, however, have no idea were to start in regards to the galaxy.yml and galaxyservers.yml files.\nI have already created an ansible.cfg and a hosts file and added them to the server, but am quite stuck on what to specifically put in the galaxy.yml and galaxyserver.yml files so it wouldn\u2019t clash with already existing files with similar names or contents and would still do what it is supposed to do.\nEdit: I also saw that the GetGalaxy server uses SQLite instead of PostgreSQL and was wondering if there were other important differences between the GetGalaxy and Ansible version.\nIf anyone would have some experience or knowledge about this topic, any help would be greatly appreciated!\nThanks in advance!"}, {"role": "system", "text": "Hi @cbass\nResources, these are our primary admin hubs with linkouts to and examples for topic-specific resources\n\nadmin documentation\n\nhttps://docs.galaxyproject.org/\n\nadmin tutorials\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Galaxy Server administration\n\n  Resources related to configuration and maintenance of Gal...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nrelated prior Q&A at this forum\n\nhttps://help.galaxyproject.org/search?q=ansible\n\n\n\n\n cbass:\n\nI have created a local galaxy server (version 22.05) by following the GetGalaxy tutorial (Get Galaxy - Galaxy Community Hub )\n\n\nThis guide is for a very simple install. There are other basic deployment types with more pre-configured. One example is the Docker Ansible Galaxy\n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\n\n\n cbass:\n\nI have already created an ansible.cfg and a hosts file and added them to the server, but am quite stuck on what to specifically put in the galaxy.yml and galaxyserver.yml files so it wouldn\u2019t clash with already existing files with similar names or contents and would still do what it is supposed to do.\n\n\nDefinitely backup/clone what you have before making changes, unless you decide to start over and use a different deployment baseline.\n\n\n\n cbass:\n\nEdit: I also saw that the GetGalaxy server uses SQLite instead of PostgreSQL and was wondering if there were other important differences between the GetGalaxy and Ansible version.\n\n\nThe database should be upgraded to Postgres for any use-case that isn\u2019t temporary. Why is explained the admin docs in the \"Administration` section, along with other strongly recommended basic configuration updates. You\u2019ll notice that the most useful advance configs are already in place (and can be custom modified) in containerized and cloud deployment choices.\nThis specific Q&A isn\u2019t summarized in a topic at this forum yet. So, let\u2019s leave this topic as a single reference specific for the \u201cansible\u201d overview of primary Galaxy options/resources. If you have another question, please start up a new topic. You can reference this topic by URL (or any other prior Q&A) for context.\nHope that helps "}], [{"role": "user", "text": "All of the Emboss tools are giving this error (replace tool name on the link I used extractseq as an example)\nLoading tool toolshed.g2.bx.psu.edu%2Frepos%2Fdevteam%2Femboss_5%2FEMBOSS%3A+extractseq35%2F5.0.0 failed: Could not find tool with id \u2018toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS:+extractseq35/5.0.0\u2019.\nThanks for your help"}, {"role": "system", "text": "Hi @Drosha,\nthank you for the report; a PR has been opened in order to update them.\nRegards"}], [{"role": "user", "text": "I got this after running cutadapt to trim adapters, but the report says 0 bp were filtered. Why could this be happening?\nTotal read pairs processed:        309,147,525\n  Read 1 with adapter:                 573,790 (0.2%)\n  Read 2 with adapter:             302,963,879 (98.0%)\n\n== Read fate breakdown ==\nPairs that were too short:             708,546 (0.2%)\nPairs that were too long:          308,438,979 (99.8%)\nPairs written (passing filters):             0 (0.0%)\n\nTotal basepairs processed: 57,184,734,130 bp\n  Read 1: 10,812,785,560 bp\n  Read 2: 46,371,948,570 bp\nTotal written (filtered):              0 bp (0.0%)\n  Read 1:             0 bp\n  Read 2:             0 bp\n"}, {"role": "system", "text": "Hi @biostaring,\nI think that the report indicates that no reads have passed the filter (all of them have been filtered).\nRegards"}], [{"role": "user", "text": "Hi. I want to change the bed style of my genes from flybase to UCSC format in galaxy using pygenometracks. Even though I try to set it to UCSC bed style the output is always flybase. I would appreciate any help. Thanks."}, {"role": "system", "text": "Hi @MzwaneleN\nThis could be an input content problem, a tool option problem, or a previously undetected tool bug.\nFirst, make sure you are using the most current version of the tool. Right now that is: pyGenomeTracks plot genomic data tracks (Galaxy Version 3.5)\n. If using a different version, try this one and see if that resolves the issue.\nIf the problem persists, a few more details would help. Where are you running the tool? A public Galaxy server? Which URL? Or explain if somewhere else \u2013 and maybe try at a public Galaxy server to see what happens as a comparison \u2013 if it works at a usegalaxy.* server but not your own server, that narrows down what may be going wrong.\nLet\u2019s follow up from there."}], [{"role": "user", "text": "Hi everyone,\nI\u2019m creating different tools with the goal of use them inside a workflow.\nI would like to know if using a list with some ids it would be possible to have a loop in the workflow that runs a tool over each id (I need their output to be separated and not inside a single table).\nThank you for the help."}, {"role": "system", "text": "Hi @Andrea_Furlani,\nIt sounds like you might be able to achieve that with collections. Have a look at this tutorial: Using dataset collections\nConcretely, you could split your list of IDs to a collection using this tool. If you need the IDs as tool parameters, rather than datasets, you could use the \u2018Parse parameter value from dataset\u2019 tool (only accessible in the workflow editor).\nHope this helps, feel free to respond with any further questions.\nBest wishes,\nSimon"}], [{"role": "user", "text": "Trying to do RNA seq data analysis denovo based but trinity giving error.\nPlease help.\nGalaxy Tool Error Report\nfrom https://usegalaxy.org/\nError Localization\nDataset\t79869624 (bbd44e69cb8906b5cf64175ab8c7a2f7)\nHistory\t5619023 (07bef24c7a90ef05)\nFailed Job\t121: Trinity on data 56 and data 55: Gene to transcripts map (bbd44e69cb8906b57bc5840c89cd6fe5)\nUser Provided Information\nThe user (admin redacted for privacy) provided the following information:\nNone\nDetailed Job Information\nJob environment and execution information is available at the job info page.\nJob ID\t42383847 (bbd44e69cb8906b58ccb29cce2790c64)\nTool ID\ttoolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2\nTool Version\t2.9.1+galaxy2\nJob PID or DRM id\t42383847\nJob Tool Version\t\nJob Execution and Failure Information\nCommand Line\nif [ -z \u201c$GALAXY_MEMORY_MB\u201d ] ; then GALAXY_MEMORY_GB=1 ; else GALAXY_MEMORY_GB=$((GALAXY_MEMORY_MB / 1024)) ; fi ;  if [ ! -z \u201c$TRINITY_SCRATCH_DIR\u201d ] ; then workdir=pwd ; scratchfolder=$(mktemp -d -p \u201c$TRINITY_SCRATCH_DIR\u201d); emptyfolder=$(mktemp -d -p \u201c$TRINITY_SCRATCH_DIR\u201d); cd \u201c$scratchfolder\u201d ; fi ;  ln -s \u2018/scratch1/03166/xcgalaxy/main/staging//42383847/inputs/dataset_6d325f88-9577-4499-9b5a-bc6044005616.dat\u2019 left_input0.fastqsanger.gz && ln -s \u2018/scratch1/03166/xcgalaxy/main/staging//42383847/inputs/dataset_641bd050-dc3a-4114-8811-acf13a33f5de.dat\u2019 right_input0.fastqsanger.gz &&  Trinity --no_version_check  --left \u2018left_input0.fastqsanger.gz\u2019  --right \u2018right_input0.fastqsanger.gz\u2019  --seqType fq       --min_contig_length 200  --min_kmer_cov 1  --CPU ${GALAXY_SLOTS:-4} --max_memory ${GALAXY_MEMORY_GB:-1}G --bflyHeapSpaceMax ${GALAXY_MEMORY_GB:-1}G --bfly_opts \u2018-V 10 --stderr\u2019   &&  if [ ! -z \u201c$TRINITY_SCRATCH_DIR\u201d ] ; then mkdir -p \u201c$workdir/trinity_out_dir\u201d; cp -p trinity_out_dir/Trinity* \u201c$workdir/trinity_out_dir\u201d; cd \u201c$TRINITY_SCRATCH_DIR\u201d; rsync -a --delete \u201c$emptyfolder/\u201d \u201c$scratchfolder/\u201d; rmdir \u201c$emptyfolder\u201d \u201c$scratchfolder/\u201d; cd \u201c$workdir\u201d; fi ;\nstderr\nstdout\n ______  ____   ____  ____   ____  ______  __ __\n|      ||    \\ |    ||    \\ |    ||      ||  |  |\n|      ||  D  ) |  | |  _  | |  | |      ||  |  |\n|_|  |_||    /  |  | |  |  | |  | |_|  |_||  ~  |\n  |  |  |    \\  |  | |  |  | |  |   |  |  |___, |\n  |  |  |  .  \\ |  | |  |  | |  |   |  |  |     |\n  |__|  |__|\\_||____||__|__||____|  |__|  |____/\n\nTrinity-v2.9.1\n\nLeft read files: $VAR1 = [\n\u2018left_input0.fastqsanger.gz\u2019\n];\nRight read files: $VAR1 = [\n\u2018right_input0.fastqsanger.gz\u2019\n];\nMonday, May 2, 2022: 02:41:37\tCMD: java -Xmx64m -XX:ParallelGCThreads=2  -jar /usr/local/opt/trinity-2.9.1/util/support_scripts/ExitTester.jar 0\nPicked up _JAVA_OPTIONS:  -Xmx7g -Xms256m\nMonday, May 2, 2022: 02:41:38\tCMD: java -Xmx4g -XX:ParallelGCThreads=2  -jar /usr/local/opt/trinity-2.9.1/util/support_scripts/ExitTester.jar 1\nPicked up _JAVA_OPTIONS:  -Xmx7g -Xms256m\nMonday, May 2, 2022: 02:41:38\tCMD: mkdir -p /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir\nMonday, May 2, 2022: 02:41:38\tCMD: mkdir -p /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir/chrysalis\n\n\n-------------- Trinity Phase 1: Clustering of RNA-Seq Reads  ---------------------\n\n\n------------ In silico Read Normalization ---------------------\n\u2013 (Removing Excess Reads Beyond 200 Coverage \u2013\n\nrunning normalization on reads: $VAR1 = [\n      [\n        '/scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz'\n      ],\n      [\n        '/scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz'\n      ]\n    ];\n\n\nMonday, May 2, 2022: 02:41:38\tCMD: /usr/local/opt/trinity-2.9.1/util/insilico_read_normalization.pl --seqType fq --JM 190G  --max_cov 200 --min_cov 1 --CPU 56 --output /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir/insilico_read_normalization --max_CV 10000  --left /scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz --right /scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz --pairs_together --PARALLEL_STATS\n-prepping seqs\nConverting input files. (both directions in parallel)CMD: seqtk-trinity seq -A -R 1  <(gunzip -c /scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz) >> left.fa\nCMD: seqtk-trinity seq -A -R 2  <(gunzip -c /scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz) >> right.fa\nCMD finished (73 seconds)\nCMD finished (74 seconds)\nCMD: touch left.fa.ok\nCMD finished (0 seconds)\nCMD: touch right.fa.ok\nCMD finished (0 seconds)\nDone converting input files.CMD: cat left.fa right.fa > both.fa\nCMD finished (11 seconds)\nCMD: touch both.fa.ok\nCMD finished (0 seconds)\n-kmer counting.\n\n----------- Jellyfish  --------------------\n\u2013 (building a k-mer catalog from reads) \u2013\nCMD: jellyfish count -t 56 -m 25 -s 27321675151  --canonical  both.fa\nCMD finished (95 seconds)\nCMD: jellyfish histo -t 56 -o jellyfish.K25.min2.kmers.fa.histo mer_counts.jf\nCMD finished (11 seconds)\nCMD: jellyfish dump -L 2 mer_counts.jf > jellyfish.K25.min2.kmers.fa\nCMD finished (31 seconds)\nCMD: touch jellyfish.K25.min2.kmers.fa.success\nCMD finished (0 seconds)\n-generating stats files\nCMD: /usr/local/opt/trinity-2.9.1/util/\u2026//Inchworm/bin/fastaToKmerCoverageStats --reads left.fa --kmers jellyfish.K25.min2.kmers.fa --kmer_size 25  --num_threads 28  --DS  > left.fa.K25.stats\nCMD: /usr/local/opt/trinity-2.9.1/util/\u2026//Inchworm/bin/fastaToKmerCoverageStats --reads right.fa --kmers jellyfish.K25.min2.kmers.fa --kmer_size 25  --num_threads 28  --DS  > right.fa.K25.stats\n-reading Kmer occurrences\u2026\n-reading Kmer occurrences\u2026\ndone parsing 81736069 Kmers, 81736069 added, taking 87 seconds.\nSequence: CCGTCTTGGACCAAACTAis smaller than 25 base pairs, skipping\nSequence: TGATGATGTASequence: GGCAAAAis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\nSequence:\nSequence: Sequence: Sequence: CTAGTGCCACAGCACAGAGCCCACGGSequence: AGGTGCATCAGTis smaller than 25 base pairs, skipping\nis smaller than is smaller than 25 base pairs, skipping25CGTGTTCTTGCTGAAGis smaller than AGGAGCCTGSequence: CTCTAGCACis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CTTAGGis smaller than\n25 base pairs, skipping\nSequence: 25Sequence: TTTCAAAATCAGTAGGCAAAAis smaller than  base pairs, skipping\nCGGCAAG25is smaller than is smaller than  base pairs, skippingSequence: CGGGGATGGATCAACACAAGCCis smaller than 25 base pairs, skipping2525\nbase pairs, skipping base pairs, skipping\nSequence: ATGATGACTGTCGAAAis smaller than 25 base pairs, skipping\nSequence: GTCCis smaller than 25 base pairs, skippingSequence:\nTAAGAAGGGTAis smaller than 25 base pairs, skipping\nSequence: TTCAAGGAAAGTGAGTCGis smaller than 25 base pairs, skippingSequence:\nSequence: GGCCACATCSequence: GTTTCATGGCTCAAis smaller than Sequence: is smaller than 25is smaller than CATCCCis smaller than  base pairs, skipping\n2525 base pairs, skipping base pairs, skipping25\nSequence: Sequence: CCACTTAGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: CTCCATTGCTCis smaller than 25 base pairs, skippingSequence: ACAAAis smaller than CTTGTTTCATGis smaller than 25 base pairs, skipping\nSequence: CCTGACTis smaller than 25Sequence: Sequence: 25 base pairs, skippingCCCGCGTCTAGTCGATTCCTTCis smaller than 25\nSequence: CAGCAGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: GTCCGAAGGGCAGTSequence: CTAGGACis smaller than 25 base pairs, skipping\nbase pairs, skippingis smaller than 25 base pairs, skippingGGCATTGGCACTGTGCis smaller than 25\nbase pairs, skippingSequence:\nSequence: CCCCGATCCAGTTCTCGCATCTGTis smaller than GCACT25is smaller than  base pairs, skipping25\nbase pairs, skippingSequence:\nCTGTCis smaller than 25 base pairs, skippingSequence:\nTTCAAATCTCAAAis smaller than 25 base pairs, skipping\nSequence: GCCCTCACAACCAis smaller than 25 base pairs, skipping\nSequence: CCGCTis smaller than 25 base pairs, skippingSequence: Sequence:\nATTGAAAGGAAATACAAATCTGATis smaller than 25is smaller than Sequence: CTGTTCG base pairs, skipping\nSequence: is smaller than 25 base pairs, skipping\nSequence: AGACCACCGTCTCCGAis smaller than 25 base pairs, skipping\nSequence: AACAGAAACAAGTAAACis smaller than 25 base pairs, skippingSequence: ACGGGGAAGAATAis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nCAGGTGACATGGTTis smaller than\nSequence: 25Sequence: GTGGCGC base pairs, skippingCAAGCATCCGAGGACis smaller than\nis smaller than 2525 base pairs, skipping base pairs, skipping\nSequence:\nSequence: CTGATCACCACGCTGTACTCGTACis smaller than 25Sequence: CTCAACTis smaller than 25 base pairs, skipping\nSequence: CGTATis smaller than 25 base pairs, skippingSequence: ATAAACTTTACCCTGCis smaller than 25 base pairs, skipping\nis smaller than Sequence: ATGAGATGTis smaller than 25 base pairs, skipping\nSequence: AAGGAATACGAATCTCAis smaller than 25 base pairs, skipping\nSequence: CGCGGTGis smaller than 25Sequence: CCTTAACATTACTTAATACCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping25\nbase pairs, skipping\nSequence: Sequence: Sequence: Sequence: TGTAATTGCTTTCACCAACAACAACGTAis smaller than 25is smaller than TGAAGGCTCTTGis smaller than  base pairs, skipping25 base pairs, skipping\nSequence:\nis smaller than 25GGCTGTCGAAGAGTCGTT25Sequence:  base pairs, skipping\nbase pairs, skippingGGCCAAis smaller than Sequence: CTCCCCCTGAATGAGis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\n25Sequence:\nbase pairs, skippingSequence: CGTTACACATGTis smaller than 25GGCGSequence: CCGGACGis smaller than  base pairs, skipping\nSequence:\n25is smaller than Sequence: CCAASequence: CACCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nCCTCAACACTTGCAGCis smaller than 25 base pairs, skippingis smaller than 25Sequence: CGGCGCGGGCAGAGGis smaller than 25 base pairs, skippingSequence: GCTACis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nSequence: ACCGAis smaller than 25\nSequence: AGGGC base pairs, skipping\nis smaller than  base pairs, skipping25 base pairs, skippingSequence: Sequence: CAGGCAGTGGTAGAGAAis smaller than 25 base pairs, skipping\nSequence: Sequence: Sequence: CGAGAGGGAGACAAis smaller than 25 base pairs, skipping\nCTACCTAATCAAis smaller than Sequence: TGTTGAGis smaller than 2525 base pairs, skipping\nGCCCGTTAGCCCTCTCTGCAA base pairs, skipping\nSequence: is smaller than 25 base pairs, skippingSequence: CCGACAAATCCATis smaller than 25 base pairs, skippingCCGGGTGTTTGACis smaller than 25\nCGCSequence:  base pairs, skippingis smaller than CCTGCCATCATASequence: TGGAGACGGCAACGAGGTGG25 base pairs, skipping\nis smaller than Sequence: is smaller than 25 base pairs, skipping25GGGGAAGTAACGCTGGTis smaller than Sequence:  base pairs, skipping\n25Sequence: CCCCis smaller than  base pairs, skippingCTCGGGTGGCGCACTTCTTGGGis smaller than 25\n25 base pairs, skipping\nSequence: Sequence: Sequence: GGCTGCCGCAAGGACCATCG base pairs, skippingAATCACTTGAGis smaller than 25\nis smaller than 25CACGTCSequence:  base pairs, skippingSequence: GCCGGTGCGGCCGATGCis smaller than 25 base pairs, skippingSequence: CGTTCCTTCACGTAis smaller than 25 base pairs, skipping\nbase pairs, skippingCCCCTCCTC\nSequence: GCTACCCTTCTTCTTCis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: CCCGCCGTCis smaller than 25 base pairs, skipping\nSequence: CTCTGCCGCAGTAGGTCTGACis smaller than 25 base pairs, skipping\nSequence: CCCACACGAACCACACCCTis smaller than 25 base pairs, skipping\nSequence: Sequence: CGCCAGATCACCGCCGAGGASequence: CGCCAAGAis smaller than 25Sequence: CCTGGATGTis smaller than 25 base pairs, skipping\nGCCis smaller than Sequence: GGACTATis smaller than 25 base pairs, skipping\nSequence: CGAis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: Sequence: ATCAATAAACATGTGis smaller than is smaller than 25 base pairs, skipping\n25CTCCAGCCGis smaller than 25Sequence: GATCTCis smaller than 25Sequence: CTCGGAis smaller than 25 base pairs, skipping\nbase pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nSequence: CCTGCGis smaller than 25Sequence: CGTGGTGGTGCTCGCCATCCTTCTis smaller than Sequence: Sequence: CGACAATATCGACAC25AGACCTTAGATTTTCATCCGis smaller than 25 base pairs, skipping\nbase pairs, skipping base pairs, skippingis smaller than\nSequence:\n25Sequence: AGAAGATAAGCGAis smaller than TCGTCCCCTTTis smaller than  base pairs, skipping25 base pairs, skipping25 base pairs, skippingSequence: GAAGTCGAAGGCCis smaller than 25 base pairs, skipping\nSequence:\nATTTTCTCis smaller than 25Sequence:  base pairs, skippingCTTCCTTGAGCTG\nSequence: is smaller than 25CACCTGCGCAGCCCTT base pairs, skippingSequence: is smaller than 25 base pairs, skippingCCGATTCTCGTTGCTCAAGGGTSequence: CCCGTAATACis smaller than 25\nSequence:  base pairs, skipping\nSequence: CTACTGis smaller than Sequence: TGGCCTTATTGCTGCAis smaller than 25CAACis smaller than 25 base pairs, skipping\n25 base pairs, skippingSequence: CGGGGAGTAAGCCGGAis smaller than\nis smaller than  base pairs, skipping\n25 base pairs, skipping\n25 base pairs, skipping\nSequence: Sequence: CTGis smaller than Sequence: GGGCGGGCAGACGCC25Sequence: GGAGis smaller than Sequence: CGTGGATGTGTCis smaller than 25 base pairs, skipping\nSequence: CTGTTGGTTCTGCTis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: Sequence: ATCCCATCCAATGAGAAATCAis smaller than CAAGSequence: AGATAAGAGTGGCGATGGCTis smaller than 25 base pairs, skipping\nCCTGC25 base pairs, skipping\nis smaller than is smaller than 25Sequence: TGTis smaller than 2525 base pairs, skipping base pairs, skipping base pairs, skipping\n25 base pairs, skippingSequence: Sequence: CTGCAGGTTCTTCTCGACAAis smaller than Sequence: CCTTis smaller than\n25AAGAGGGCGTCCAAGASequence: CGACCATATCCGTC base pairs, skippingis smaller than 25is smaller than 25 base pairs, skipping\nSequence:\n25 base pairs, skippingSequence: GCCSequence: AAGGCATAACCCCTTis smaller than 25is smaller than 25\nCACCG base pairs, skipping\nbase pairs, skippingis smaller than  base pairs, skipping\nSequence: TTCGTGTCGG25 base pairs, skipping\nis smaller than Sequence:\nSequence: CCATTGis smaller than 25ATTTTis smaller than 25 base pairs, skipping\n25Sequence: CCTCSequence: CTTGCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skippingis smaller than\nSequence: 25 base pairs, skipping\nSequence: Sequence: AGCGGAGCTTGTTis smaller than 25Sequence: GCCTGGGTSequence: CATTAATCCCTTAAAGis smaller than 25 base pairs, skipping\nCCAAGAAGis smaller than 25 base pairs, skipping\nSequence: Sequence: ATAAAACGCGGATCis smaller than  base pairs, skipping\nACCG25 base pairs, skipping\nSequence: CGACis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nis smaller than GTCACGATAACATACTGGAATCTis smaller than 25 base pairs, skipping\nSequence: CGACTGGAGATCis smaller than 25 base pairs, skipping\nSequence: Sequence: AAGGCATAACCCCTTATGACis smaller than 25 base pairs, skipping\nGGT25Sequence: CTCCGGAGis smaller than Sequence: CTTGCGGCAGCCT base pairs, skipping\nSequence: ACTGAis smaller than 25Sequence: TATTGCTCTis smaller than 25 base pairs, skipping\nis smaller than is smaller than 25 base pairs, skipping\n2525Sequence:  base pairs, skipping\nGTTCATCCTCC base pairs, skipping base pairs, skippingis smaller than\nSequence:\n25CAGACCG base pairs, skippingSequence: ACAAGATCACACAGCTCACGGATAis smaller than is smaller than\n2525Sequence: TCGGTAGGCTCTCGCTGC base pairs, skipping base pairs, skippingSequence: ATAGAACCGis smaller than\nis smaller than\n25Sequence: 25 base pairs, skippingCTCCATTCTCAGTCCCGGCAA base pairs, skipping\nis smaller than Sequence: CCTTGAis smaller than 25 base pairs, skipping\n25Sequence: GGAA base pairs, skipping\nis smaller than Sequence: CGCCATSequence: CAAAGCGCTACCis smaller than 25 base pairs, skipping\n25 base pairs, skippingis smaller than\n25 base pairs, skipping\nSequence: CGCTGGACTGGTis smaller than Sequence: CGTAGCTCCC25is smaller than 25 base pairs, skippingSequence:  base pairs, skipping\nCCGAAACCCTAACCSequence: TCTCTTCTC\nis smaller than 25is smaller than 25 base pairs, skipping\nSequence: CTAGGGCAGATTCTis smaller than 25 base pairs, skipping\nSequence: CTCCAis smaller than 25Sequence: GTTCGis smaller than 25 base pairs, skipping\nSequence: CCTCGCCCACTTis smaller than 25 base pairs, skipping\nSequence: CTGTTGis smaller than 25 base pairs, skipping\nbase pairs, skipping base pairs, skipping\nSequence: CTCCCCACGGAGACTTTCTTGis smaller than 25 base pairs, skipping\nSequence: TGCTCTTCCCAAGTTCCGCAAGCTis smaller than 25 base pairs, skipping\nSequence: TGGCACGGis smaller than Sequence: 25CGACCGSequence:  base pairs, skippingSequence: CGATCCTCGAis smaller than 25 base pairs, skipping\nis smaller than ACSequence: CTTCGAGCTTCTCATCCCGCAis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nis smaller than\nSequence: 25CTCAis smaller than 25 base pairs, skipping base pairs, skipping\nSequence: GTCAACGis smaller than Sequence: 25Sequence: AAGGATCTCACCAGCATG base pairs, skippingTAGAis smaller than Sequence:\nis smaller than 25ATGTCTACAATCTATGATGAis smaller than 25 base pairs, skipping base pairs, skipping\nSequence: 25 base pairs, skipping\nATGAAGCTG\nSequence: is smaller than GTTACGGT25is smaller than  base pairs, skipping25\nSequence:  base pairs, skippingGTAGGAGTT\nis smaller than 25 base pairs, skippingSequence: Sequence: Sequence: CTGGTCAAACTATACTGAACTCCGATTTATCCTTAis smaller than 25is smaller than\nis smaller than  base pairs, skipping25 base pairs, skipping\n25\nSequence:  base pairs, skippingCCCAGCTGAAACTAACAAACis smaller than\n25 base pairs, skipping\nSequence: TTGTCATCAGCTTTCAGTGATAGis smaller than 25 base pairs, skipping\nSequence: Sequence: AGACAGGCCGTTGGATTGCCCSequence: is smaller than is smaller than Sequence: CTCGATGGATCAAAGAAAGCCATTis\n\u2026\nthan 25 base pairs, skipping\nSequence: is smaller than 25GCTGGGTGSequence:  base pairs, skippingSequence: ATACGGTGTis smaller than 25 base pairs, skipping\nGTGGA\nis smaller than Sequence: Sequence: GGAAGATGG25 base pairs, skipping\nis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\nSequence: Sequence: GTTGATGTTGCTGAAGCCAAis smaller than 25CAAAGSequence: GGAAGACACis smaller than 25TGATG base pairs, skipping\nbase pairs, skippingis smaller than 25\nbase pairs, skipping\nis smaller than Sequence: 25 base pairs, skippingGGCCCAAACAT\nis smaller than 25 base pairs, skipping\nSequence: TGGAAGAGAASequence: is smaller than Sequence: TCCTTis smaller than ATCTAACCCA2525is smaller than  base pairs, skipping base pairs, skipping25\nbase pairs, skippingSequence:\nCTTCGAAACGGATCAis smaller than Sequence: 25Sequence: CGGAAis smaller than 25 base pairs, skipping\nSequence: Sequence: GTTCATACAis smaller than GACACGAATATGTACGGAACCTis smaller than 25 base pairs, skipping\nbase pairs, skipping\n25CAGGAAASequence: Sequence: CTTGCCCCTTis smaller than ATCGCC base pairs, skipping\n25 base pairs, skipping\nSequence: Sequence: is smaller than 25is smaller than 25 base pairs, skipping\nSequence: GTTTis smaller than 25 base pairs, skipping\nTTCCTis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: CACCACSequence: CCCAGis smaller than 25 base pairs, skipping\nCTAAACTTCAATCTis smaller than Sequence: Sequence: is smaller than 25CTATAAATGATTTGGCTis smaller than 25 base pairs, skipping\n25Sequence:  base pairs, skippingSequence: CGCCGis smaller than 25 base pairs, skipping\nCCACCATCAACAGCAATTGTCis smaller than  base pairs, skipping\nGAAGAGGG25\nis smaller than  base pairs, skipping\n25Sequence: CAAAGGTATGAGAAGAGGis smaller than  base pairs, skipping25\nbase pairs, skipping\nSequence: Sequence: GTGTASequence: CGGCis smaller than 25GCCAAATCGAATCGAGGAGACTTTis smaller than 25 base pairs, skippingSequence: CAAGGTTGis smaller than 25 base pairs, skipping\nSequence: GTTCAAACTTGATAAis smaller than  base pairs, skipping\nSequence: CTAGAAis smaller than 25\nis smaller than 25 base pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nSequence:\nCGTGCAGTAGCTGTGis smaller than 25 base pairs, skipping\nSequence: GGAGGAis smaller than Sequence: 25 base pairs, skipping\nSequence: Sequence: Sequence: CCCACAAACTAATAAis smaller than 25 base pairs, skipping\nCTGAAGATGGTCTCGGTCAGAGTATis smaller than is smaller than 25 base pairs, skipping25CAAGGCGGTSequence: CGCAGGTGTCCTAAGATis smaller than 25is smaller than 25 base pairs, skipping\nSequence:  base pairs, skipping\nCTCCTTTCACGCCTTGTCTTTTAT base pairs, skippingSequence: is smaller than 25 base pairs, skipping\nCCGTGCATATACTACTGCCTCTSequence: is smaller than GGCGAGGGTTCAis smaller than 2525 base pairs, skipping\nSequence: GCAACGTCis smaller than  base pairs, skippingSequence: ACGACTAGCTGCis smaller than\nSequence: AACTC25Sequence: Sequence: CCAATCAis smaller than 25 base pairs, skipping\nbase pairs, skipping\nis smaller than GCTCis smaller than 2525 base pairs, skipping base pairs, skipping\nSequence: AATCTCACAAACACATTATTCis smaller than 25 base pairs, skipping\n25Sequence: Sequence: CTCCACATGCTGATGAAACCSequence: GGCGGCCCGCAAis smaller than 25is smaller than 25 base pairs, skipping\nSequence: ATGCAAATGCis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CTAGTGTCGTCGTTCCTCCTCGis smaller than 25\nCGGCATAGTGGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skippingSequence: CCTGTGGAAGCCCCTCCSequence: TGAAGis smaller than 25 base pairs, skipping\nSequence: GGCCGTTGGAGis smaller than\nis smaller than 2525 base pairs, skippingSequence:  base pairs, skipping\nTTGTAAGTGGCTCGTAC\nis smaller than 25 base pairs, skippingSequence: CCGCAAGCACCAis smaller than Sequence: GTTTGGAis smaller than 25 base pairs, skipping\n25Sequence: Sequence: CCCAAATTGAGTACTCGGGAAC base pairs, skipping\nSequence: GTCTCCGGTAACGCTCTCAAis smaller than 25 base pairs, skippingis smaller than AGTTAAGACT25 base pairs, skipping\nSequence: CTCGGAis smaller than 25 base pairs, skipping\nSequence:\nSequence: is smaller than 25GACCGis smaller than 25 base pairs, skipping base pairs, skipping\nCTGATATCCGis smaller than 25 base pairs, skipping\nSequence: CGTAAT\nSequence: is smaller than Sequence: GGACis smaller than 25 base pairs, skipping\n25TGTTT base pairs, skippingis smaller than\n25Sequence: CGCCCTTGACCis smaller than Sequence: CTGTGATGAGTGTGGTATCis smaller than 25Sequence: ATCTCis smaller than 25 base pairs, skipping\nSequence: CTTCGCCGCCATCTCGCTCTGGis smaller than 25Sequence: GTACAAGCTGCTATACT base pairs, skipping\nbase pairs, skipping25 base pairs, skipping base pairs, skipping\nis smaller than\nSequence:\n25TCTACATTATAGTAA base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: Sequence: Sequence: GTTGTACGAAATACGis smaller than is smaller than is smaller than 252525 base pairs, skipping base pairs, skipping base pairs, skipping\nSequence:\nSequence: CCACGSequence: CAGGCGAAGGTGCCis smaller than TAAAAACTTCAis smaller than 25 base pairs, skipping\nSequence: Sequence: CTCCTCAGCACCCTCCTTTis smaller than 25 base pairs, skipping\nSequence: Sequence: 25 base pairs, skipping\nSequence: CCTAis smaller than 25CTAGTTTACAACTis smaller than 25 base pairs, skipping\nSequence: CCGCTis smaller than 25 base pairs, skipping\nCGCTis smaller than 25 base pairs, skippingis smaller than  base pairs, skipping\n25Sequence: GTACCGCGTTCACTGTGis smaller than 25CAAGAATGTTGCTGTCAAGGAis smaller than  base pairs, skipping\n25 base pairs, skipping base pairs, skipping\nSequence: CGAGCGATCACCGis smaller than 25 base pairs, skipping\nSequence: CTCCTTCATTCTCCTTGGCTTCCis smaller than 25 base pairs, skippingSequence: CCTCGTis smaller than Sequence: CCAACAATTAis smaller than 25 base pairs, skipping\n25Sequence:  base pairs, skippingSequence: AGCAGTTGCTTCAis smaller than GGACA\nSequence: Sequence: AAGAAAGGis smaller than 25 base pairs, skipping\n25CTGAis smaller than  base pairs, skippingis smaller than 25 base pairs, skipping25\nSequence:  base pairs, skipping\nSequence: AAAGGCTAis smaller than 25CTCAGTGCACGCCGCCAis smaller than 25 base pairs, skipping\nSequence: Sequence: GCAAAGCTCGATTTGGGSequence:  base pairs, skipping\nis smaller than Sequence: Sequence: TCCAGATACAAATTACTTATTTAis smaller than 25 base pairs, skipping\nSequence: CAGGGis smaller than 25 base pairs, skipping\nCAGTTCTCCGTGTTTCTGCAAis smaller than 25 base pairs, skipping\nCCTAGG25 base pairs, skippingCTGCTGis smaller than 25Sequence: CTGCis smaller than 25 base pairs, skipping\nSequence: is smaller than 25 base pairs, skipping\nCTCGAGGTTAGGGTSequence:  base pairs, skippingSequence: GCCTCTTCCCCCis smaller than 25 base pairs, skipping\nCGACCAGAGGAis smaller than 25 base pairs, skipping\nis smaller than 25\nSequence:\nbase pairs, skippingCGCGGACTGAATCTSequence: AACCGACTGTGis smaller than 25 base pairs, skipping\nis smaller than\nSequence: 25Sequence:  base pairs, skipping\nCCCCGAACGCCAATCCCAGTGATTCTCTGGis smaller than is smaller than 25 base pairs, skipping25 base pairs, skipping\nSequence:\nSequence: GTGGSequence: GGCTTGGAis smaller than 25is smaller than GCAGGTTTCCGTTTTTTCis smaller than 25Sequence:  base pairs, skipping\nbase pairs, skippingACACCis smaller than 25Sequence: 25\nbase pairs, skippingCAGAAACATGCCTGACis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence:\nGTTASequence: is smaller than CTCTis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nSequence: TGAGis smaller than 25 base pairs, skipping\nSequence: CGAAGCCAAAGACTGTTCCCAAis smaller than 25 base pairs, skipping\nSequence: CTGAATCTis smaller than 25Sequence:  base pairs, skippingCTTTGGA\nis smaller than 25 base pairs, skipping\nSequence: CTCATGGAACATTTACCis smaller than 25 base pairs, skipping\nSequence: CAGTACCis smaller than 25 base pairs, skippingSequence:\nSequence: CGGAGAGGGCGGGGGGGAAAGSequence: CTGAGis smaller than Sequence: CCCTGATGGTTACCTGTTTGAGis smaller than 25 base pairs, skippingis smaller than 25is smaller than 2525 base pairs, skipping\nbase pairs, skippingSequence:\nbase pairs, skipping\nTGTGATATATATGTCCCTTGATCASequence: is smaller than GCCCCTTATCTCGCTCTTTCTG25is smaller than  base pairs, skipping25Sequence:  base pairs, skipping\nSequence: CATGAATACTAGTACis smaller than 25Sequence:\nCCAA base pairs, skippingTGGGACTCCAACGCis smaller than\nSequence: is smaller than 25GGC25 base pairs, skippingis smaller than  base pairs, skipping\n25Sequence:\nbase pairs, skippingGGATGATCAGTAC\nis smaller than 25 base pairs, skipping\nSequence: GTCGACGCCGAAis smaller than Sequence: 25Sequence: ATCACATTCTCCAAis smaller than Sequence: CGAAGAGTGTATTTGAAGTGCis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: GCTATACTGCCCCAGTGTCCGACGTCTCCTGCC\n25Sequence: CTCGGAAis smaller than  base pairs, skipping\nis smaller than is smaller than 25 base pairs, skippingSequence: 25 base pairs, skipping\nCAACTTT25Sequence: CACAACATis smaller than  base pairs, skippingis smaller than Sequence: GGGAAATis smaller than 25 base pairs, skipping\nSequence:\n25Sequence: 25ATTGCCTGGCGCCis smaller than 25 base pairs, skippingSequence: CACCTCis smaller than  base pairs, skipping\nbase pairs, skippingGGATATTGGAATGGTis smaller than 25 base pairs, skipping\nSequence: GGAAGis smaller than 25 base pairs, skipping\n25\nSequence:\nSequence:  base pairs, skippingATCCCis smaller than 25Sequence: GGAGGCTis smaller than  base pairs, skipping\nAACGA\n25Sequence: GTTGCis smaller than is smaller than  base pairs, skipping25 base pairs, skipping\n25\nSequence: CCGTAGCGGTGCCTACATTGSequence: CGAACACTTCis smaller than 25is smaller than 25 base pairs, skippingSequence: CTGAACCTTATAis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nSequence: Sequence: Sequence:\nCGCCCTTGACCCGGTATis smaller than is smaller than 25 base pairs, skipping25GAAGAATSequence: AGCCGTTCGGGGTT base pairs, skippingis smaller than 25is smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CCGCAGATATCACAAGis smaller than Sequence: CGAGGTCAGTGCAGTis smaller than 25 base pairs, skipping\nSequence: GCCTAATACGGCCGACACis smaller than 25 base pairs, skipping\n25Sequence: CGGGGis smaller than 25 base pairs, skipping\nSequence: GGCGCACACATCGATAAAACCCA base pairs, skippingis smaller than\n25Sequence: CCGAAis smaller than  base pairs, skipping\nSequence: Sequence: GCTGTGGGTCTGCCTGCTATGis smaller than 25 base pairs, skipping\nis smaller than Sequence: AGCTCGGGAAis smaller than 25Sequence: 25 base pairs, skipping\nCATTCCTGAis smaller than  base pairs, skippingSequence: 25 base pairs, skipping\nSequence: GAAGAAGis smaller than 25CTACATGCTCGGCAGCCis smaller than 25 base pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nSequence: GTGGTCTis smaller than Sequence: Sequence: 25CAGGGTCGGAGACAAATCCAATGTis smaller than is smaller than 25Sequence: CTCAGTCTTGACCTGCGis smaller than 25Sequence: CCACATATis smaller than  base pairs, skipping base pairs, skipping\nbase pairs, skipping25\n25 base pairs, skipping base pairs, skipping\nSequence: Sequence: Sequence: GTTCGACCAAAAAAGCCTGAGCCCGGCGACGAGAGCCGGCTGTTACGACGGCis smaller than is smaller than is smaller than 2525 base pairs, skipping\n25 base pairs, skipping\nSequence: CATTAAGAis smaller than Sequence:  base pairs, skipping25CCACCTTAAATCCTCAAATTis smaller than Sequence:  base pairs, skipping\n25GCATCTATTGCTGATTCATis smaller than 25\nbase pairs, skipping base pairs, skippingSequence:\nCCCTGTCATCTAGTGTCGGTTGATSequence: TGGGis smaller than 25is smaller than 25 base pairs, skipping base pairs, skippingSequence: CCAATAis smaller than\nSequence: CCGGCCAAGCAAGTis smaller than Sequence: 25 base pairs, skipping\nCCGATATCTCAATTACAGCTTCCTSequence: 25is smaller than 25 base pairs, skippingGAGG base pairs, skipping\nis smaller than\n25Sequence:  base pairs, skippingCGGGATTCCTCA\nSequence: is smaller than TGAATGCTTACT25is smaller than Sequence: CCTCCCCCATCGAGTTATCAACis smaller than  base pairs, skipping25\n25 base pairs, skippingSequence: TGTAAATCTTCCGACTGSequence: ATTCis smaller than\nbase pairs, skippingis smaller than 25 base pairs, skipping\n25 base pairs, skippingSequence: ACGCCCCCGGCGCAGCCGCis smaller than 25 base pairs, skipping\nSequence: CTGCCTGCAAGTGTGGCAGTGis smaller than\nSequence: Sequence: Sequence: 25 base pairs, skippingGTCCTCGCTCTGTCCGCCGCCATTTis smaller than 25 base pairs, skippingSequence: ATCTCis smaller than 25\nGCCis smaller than 25 base pairs, skippingSequence: CCACAACCCACCGTGCCCCACTCis smaller than 25"}, {"role": "system", "text": "Hi @balrajsharma\nThis message repeating means that you have many reads that are too short to assemble.\n\n\n\n balrajsharma:\n\nSequence: ACGCCCCCGGCGCAGCCGCis smaller than 25 base pairs, skipping\n\n\nTry filtering out reads that are too short to be used, then rerun. The job is seems to be running out of resources when attempting to sorting out usable read pairs from those that are not. Pre-filtering by length will address that. Use a QA tool that will remove/separate both ends of a pair if one of the paired reads fails the filter. Trinity expects intact pairs as an input.\nIf it seems like that length filter eliminates too many reads:\n\nprior QA steps may have been too strict\nyou need to still run QA\nthe reads were originally too short\nthe read quality is actually low, and the QA steps are correct\n\nThe last two would be the most concerning re is this data even usable?\nTutorials: Search Tutorials\nThanks!"}], [{"role": "user", "text": "Hi,\nI recently upgraded my galaxy instance to v19.09 from v19.05. This went fine, so I decided to try moving from python2 to python3. I removed my .venv, and created a new one with\nvirtualenv --python=/usr/bin/python3 .venv\nI had to upgrade setuptools (pip install --upgrade setuptools) and pull this change into my instance before the server could make it through startup.\nThe I tried to access galaxy and I got the following:\nserving on 0.0.0.0:8080 view at http://127.0.0.1:8080\n\n10.50.135.82 - - [09/Dec/2019:09:34:47 -0700] \"GET / HTTP/1.1\" 500 - \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\"\n\nError - <class 'NameError'>: name 'unicode' is not defined\n\nURL: https://galaxy.lygos.com/\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/middleware/error.py', line 154 in __call__\n\napp_iter = self.application(environ, sr_checker)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/paste/recursive.py', line 85 in __call__\n\nreturn self.application(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/paste/httpexceptions.py', line 640 in __call__\n\nreturn self.application(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/base.py', line 143 in __call__\n\nreturn self.handle_request(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/base.py', line 222 in handle_request\n\nbody = method(trans, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/root.py', line 79 in index\n\nreturn self._bootstrapped_client(trans)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/base/controller.py', line 281 in _bootstrapped_client\n\nreturn self.template(trans, app_name, options=js_options, **kwd)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/base/controller.py', line 360 in template\n\nmasthead=masthead\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/webapp.py', line 936 in fill_template\n\nreturn self.fill_template_mako(filename, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/webapp.py', line 949 in fill_template_mako\n\nreturn template.render(**data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/template.py', line 476 in render\n\nreturn runtime._render(self, self.callable_, args, data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 883 in _render\n\n**_kwargs_for_callable(callable_, data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 920 in _render_context\n\n_exec_template(inherit, lclcontext, args=args, kwargs=kwargs)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 947 in _exec_template\n\ncallable_(context, *args, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/database/compiled_templates/js-app.mako.py', line 48 in render_body\n\n__M_writer(unicode( h.url_for( '/' ) ))\n\nNameError: name 'unicode' is not defined\n\nDuring Galaxy startup, I noticed a message \u2018The Galaxy client build is up to date and will not be rebuilt at this time.\u2019 I\u2019m guessing this is incorrect and I do need to rebuild, but I would love to get an opinion form someone who knows the Galaxy internals. And if I do need to rebuild, how do I kick that off?\nThanks,\n-Will"}, {"role": "user", "text": "Within moments of posting I managed to solve the issue using this:\n\n  \n      github.com/galaxyproject/galaxy\n  \n  \n    \n  \n\t  \n  \n\n  \n    \n      Mako on Python 3\n    \n\n    \n      \n        opened 04:26PM - 11 Sep 19 UTC\n      \n\n        \n          closed 08:47AM - 12 Sep 19 UTC\n        \n\n      \n        \n          \n          selten\n        \n      \n    \n  \n\n\n\n  I'm using Galaxy release 19.09 and Python 3.7.\nI'm trying to access the main page and get the following error:\nTraceback (most recent...\n\n\n\n\n\n  \n  \n    \n    \n  \n  \n\n\n-Will"}], [{"role": "user", "text": "Hi, I am currently working on my project and is using the tutorial \u201cExome sequencing data analysis for diagnosing a genetic disease\u201d. I could not find the tool called SnpEff eff and SnpEff Download. So where i can find this tool ?? Please help me out."}, {"role": "system", "text": "Both tools are available on Galaxy Australia in Annotation section. Sometimes search does not pick up tools.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear All\nhow to perform differential gene expression from 4 transcriptomic library a two from control and 2 from treated which are uploaded on NCBI. So need to do the differential gene expression for fpkm/rpkm value so to check out the expression of identified element. Anybody can help.\nhighly thank ful to you"}, {"role": "system", "text": "Hi @Anand_Mishra,\nif a reference genome and a gene annotation is available, please check Galaxy Training!\nIf the genome assembly is not available, assemble transcription with Trinity or rnaSPAdes and follow any transcriptome based protocol.\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Dear @jennaj I use STAR very often and I noticed that the fasta.gz genome file was used as uncompressed file, but the GTF.gz file wasn\u2019t used as a uncompressed file resulting in a error.\nI tried to use the same fasta.gz and gtf uncompressed file directly and run without any problem.\nThere is any chance set the option gtf/gff file to be used as an uncompressed file automatically?\nHave a nice day!"}, {"role": "system", "text": "\n\n\n Agustin_Baricalla:\n\nI use STAR very often and I noticed that the fasta.gz genome file was used as uncompressed file, but the GTF.gz file wasn\u2019t used as a uncompressed file resulting in a error.\nI tried to use the same fasta.gz and gtf uncompressed file directly and run without any problem.\nThere is any chance set the option gtf/gff file to be used as an uncompressed file automatically?\n\n\nHi @Agustin_Baricalla\nData that is in gtf.gz format is automatically uncompressed to gtf during Upload. There is no datatype for gtf.gz \u2013 so it wouldn\u2019t be possible to have a dataset with gtf.gz assigned as the datatype. This means there is nothing to implicitly convert compressed to uncompressed at runtime.\nIf you are not working at a usegalaxy.* server, maybe test out the upload of the data at one of those to better understand the default/supported behavior? Allow Galaxy to autodetect the datatype.\nOther Galaxy servers, public or private, may have different custom datatype configurations added, or be running a (much) older Galaxy release, or be running a customized version of this tool. Those admins/tool developers would be the best people to contact if you want a change that fits the custom server environment you are working in.\nhttps://galaxyproject.org/use/\nThanks!"}], [{"role": "user", "text": "Hi~ I see Deseq2 (Galaxy Version 2.11.40.7+galaxy1) added the \u201csize factor\u201d option in output, rather than version 2.11.40.6, which is a big improvement for me. Further more, I wonder could we have the option to use custom defined size factor in Deseq2? because it is useful for spike-in samples."}, {"role": "system", "text": "Hi @Cindy,\nI created an issue in order to request this functionality: DESeq2: enable custom defined size factor.\nRegards"}], [{"role": "user", "text": "Hello, I have a question related to this issue. I am using Hisat2 but in the built-in reference genome there is no mm28 only mm9 and mm10. My question Can I select mm10 during Hisat2 and in the next stingtie I use the earlier version mm39 which I uploaded?"}, {"role": "system", "text": "Hi @Hossam_Shawki\n\n\n\n Hossam_Shawki:\n\nCan I select mm10 during Hisat2 and in the next stingtie I use the earlier version mm39 which I uploaded\n\n\nYou will have technical and scientific problems if data based on different genome assemblies are used together in the same analysis. Choose a matching reference genome and reference annotation at the start of an analysis and use those throughout.\nPublic servers host different genome indexes. UseGalaxy.eu hosts mm39 for now.\nNote: next time, please consider asking a new question for new problems. You can reference other topics by URL for context."}], [{"role": "user", "text": "I am currently experiemcing some difficulies counting mapped reads: I have used featureCounts, all features show up as either Unassigned_Unmapped or Unassigned_NoFeatures.\nMapping the reads does not seem to be the issue as the reads could successfully be visualized using JBrowse.\nHowever, I think the issue might be with the GTF file I am using, as no visualization of the annotated reference (GTF from UCSC table browser,  converted to GFF3 using gffread) is possible via JBrowse. Where might I find alternative GTF files?"}, {"role": "user", "text": "In case anyone else encounters this problem, here is the solution that worked for me:\nThe gtf file I had used wasn\u2019t ideal which resulted in improper assignment. This could be fixed by trying out different gtf files in JBrowse (after gffread conversion to gff3) and using the one that showed us in the visualization via JBrowse.\nAdditionally, featureCounts had to be set to look for \u201cgene\u201d instead of \u201cexon\u201d, once again due to the formatting in the gtf file."}], [{"role": "user", "text": "I am having trouble subsetting a VCF by desired regions. I imported the following VCF file from NCBI directly into Galaxy (vcf.gz):\nftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/NA12878_HG001/NISTv4.2.1/GRCh38/SupplementaryFiles/HG001_GRCh38_1_22_v4.2.1_all.vcf.gz\nI added format = vcf_bgzip and db=hg38\nA sample section of this VCF, skipping all # comment lines, looks like this:\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tINTEGRATION\nchr1\t10108\t.\tC\tCT\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged,lowmappabilityall\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\nchr1\t10146\t.\tAC\tA\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged,lowmappabilityall\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\nchr1\t10157\t.\tTA\tT\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\n\n\nThen, I uploaded a list of regions of interest that I want to subset from that VCF as a bed file, for example:\nchr1\t11790551\t11790551\nchr1\t11790553\t11790553\nchr1\t11790559\t11790559\nchr1\t11790560\t11790560\nchr1\t11790562\t11790562\nchr1\t11790563\t11790563\nchr1\t11790565\t11790565\n\nI used the tool VCF-BEDintersect: to intersect the original HG001_GRCh38_1_22_v4.2.1_all.vcf.gz  with the bed file containing regions of interest (around 200k). However, I could only retrieve 4 regions interesecting my bed file.\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tINTEGRATION\nchr2\t108259008\t.\tAAAACTTGGTCAGGTGATGTTAT\tA\t50\tPASS\tplatforms=4;platformnames=Illumina,PacBio,CG,10X;datasets=4;datasetnames=HiSeqPE300x,CCS15kb_20kb,CGnormal,10XChromiumLR;callsets=6;callsetnames=HiSeqPE300xGATK,CCS15kb_20kbDV,CCS15kb_20kbGATK4,CGnormal,HiSeqPE300xfreebayes,10XLRGATK;datasetsmissingcall=IonExome,SolidSE75bp;callable=CS_HiSeqPE300xGATK_callable,CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable,CS_CGnormal_callable,CS_HiSeqPE300xfreebayes_callable\tGT:PS:DP:ADALL:AD:GQ\t1/1:.:646:0,263:82,417:362\nchr11\t640056\t.\tTCCCCGGGGTCCCTGCGGCCCCGACTGTGCGCCCGCCGCGCCCAGCCTC\tT\t5\tallfilteredanddisagree\tplatforms=2;platformnames=Illumina,PacBio;datasets=2;datasetnames=HiSeqPE300x,CCS15kb_20kb;callsets=3;callsetnames=,CCS15kb_20kbDV,CCS15kb_20kbGATK4,HiSeqPE300xGATK;callable=CS_CCS15kb_20kbDV_callable;filt=CS_HiSeqPE300xGATK_filt,CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=GRCh38_AllTandemRepeats_201to10000bp_slop5\tGT:PS:DP:ADALL:AD:GQ\t0|1:640056_TCCCCGGGGTCCCTGCGGCCCCGACTGTGCGCCCGCCGCGCCCAGCCTC_T:303:107,91:0,0:106\nchr15\t40407805\t.\tGTT\tG\t50\tPASS\tplatforms=3;platformnames=PacBio,Illumina,10X;datasets=3;datasetnames=CCS15kb_20kb,HiSeqPE300x,10XChromiumLR;callsets=5;callsetnames=CCS15kb_20kbDV,CCS15kb_20kbGATK4,HiSeqPE300xfreebayes,HiSeqPE300xGATK,10XLRGATK;datasetsmissingcall=CGnormal,IonExome,SolidSE75bp;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable,CS_HiSeqPE300xfreebayes_callable\tGT:PS:DP:ADALL:AD:GQ\t1/1:.:677:0,283:1,63:215\nchr22\t42126617\t.\tAG\n\nI highly doubt that there are only 4 regions of interest out of the 200k as bed I originally uploaded in the input VCF, so I suspect that there is probably something else going on but I cannot figure it. I appreciate any suggestions that you might have."}, {"role": "system", "text": "Hi,\nBED files are 0 based for the start and non-inclusive for the end position. The wiki page about it is quite nice BED (file format) - Wikipedia\nSo I could imagine that you need to adjust your BED file.\nCiao,\nBjoern"}], [{"role": "user", "text": "I locally installed the docker version with qiime2 (Quay). When I use get data \u2192 upload file \u2192 rule based and I paste remote urls (e.g. https://zenodo.org/api/xxx/xxx.fastqsanger.gz) I get this message: \u201cBuild rules for updating datasets - Access to this address in (sic!) not permitted by server configuration\u201d. Any suggestions? Thanks"}, {"role": "system", "text": "Please see: Troubleshooting Galaxy Docker -- QIIME2"}], [{"role": "user", "text": "While merging bam files (output from alignment) I got the error The tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\nBut I don\u2019t have any duplicate input dataset there. 3 dataset was aligned with my account, and my teammate did another three. Then she shared her history with me. I set the read groups using AddorReplaceReadGroups tool for mine together and those of her in a second run.\nHere is the params I used-\n\nScreenshot from 2021-09-02 11-18-231532\u00d7851 142 KB\n\nThen I tried to merge them but got the error-\n\nScreenshot from 2021-09-02 10-50-131515\u00d7747 167 KB\n"}, {"role": "user", "text": "Got the answer submitting the bug report. It was due to different alignment files using different reference genome."}], [{"role": "user", "text": "Nothing happens when the Upload File from your computer tool is clicked. It seems there is something with release_21.01. Hope someone can fix this issue as soon as possible.\n"}, {"role": "system", "text": "Hi @istevenshen,\nit has been reported; meanwhile, you can use the Upload Data button which is under the search tools bar.\nRegads"}], [{"role": "user", "text": "I\u2019ve been trying to read about which workflow language underlies Galaxy and I am confused.\nThis link seems to say \u201cGalaxy additionally allows design and execution of complex MapReduce-type workflows\u201d\nhttps://galaxyproject.github.io/\nHowever, this link (as well as Wikipedia), seem to suggest that Galaxy is currently implementing the use of CWL?\nhttps://www.commonwl.org/\nCould anyone clarify this to me? Which information is more up to date? Thanks"}, {"role": "system", "text": "Galaxy has its own workflow specification that predates most of what is actively developed elsewhere. The specification is rich in features but not in documentation. You can download any Galaxy workflow in json format, but we also support yaml to a large extent. There is Galaxy Workflow format2 in the making which should be convertible to cwl format.\nYou can explore the following links if you want to know more.\n\n  \n      github.com/galaxyproject/galaxy\n  \n  \n      \n    \n  \n\n\n  Allow API import/export of format 2 workflows.\n\n\n\n  by jmchilton\n  on 05:43PM - 26 Sep 18 UTC\n\n\n\n  8 commits\n  changed 11 files\n  with 555 additions\n  and 416 deletions.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      github.com/galaxyproject/galaxy\n  \n  \n      \n    \n  \n\n\n  Support for Enhancements to gxformat2.\n\n\n\n  by jmchilton\n  on 08:33PM - 30 Sep 18 UTC\n\n\n\n  4 commits\n  changed 11 files\n  with 100 additions\n  and 748 deletions.\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      \n      GitHub\n  \n  \n    \n\njmchilton/gxformat2\n\nGalaxy Workflow Format 2. Contribute to jmchilton/gxformat2 development by creating an account on GitHub.\n\n\n  \n  \n    \n    \n  \n  \n\n\nhttps://planemo.readthedocs.io/en/latest/commands.html#workflow-convert-command"}], [{"role": "user", "text": "Hello everyone,\nI am using the Galaxy Europe web application and I have imported some new data and am also working with data uploaded previously. My job run times have been very long recently and now whenever I start a job I get the message \u201cThis is a new dataset and not all of its data are available yet\u201d which I have never gotten before.\nPreviously, seeing \u201cThis job is waiting to run\u201d was common but I have not seen this new message before. Is this normal?\nEdit: Sorry I forgot to mention that I am running a makeblastdb tool, a Filter Fasta tool, as well as changing the datatype on uploaded data. All have long wait times."}, {"role": "system", "text": "Hi @R_J\nBoth of these states mean that the job is queued. The dataset will be grey in color.\n\n\n\u201cThis is a new dataset and not all of its data are available yet\u201d\n\nFirst staging state\nTechnically, the job is having its metadata interpreted\nWhy? To sort the job into the appropriate cluster queue\nThis may happen so fast that you don\u2019t see it in the UI. Or, if the server is busy, this sub-step may take longer.\n\n\n\n\u201cThis job is waiting to run\u201d\n\nSecond staging state\nTechnically, the job has been successfully added to a cluster queue\nWhy? This is a \u201cready\u201d state, meaning it is now waiting until the target cluster has an available node\nQueued time can vary (grey dataset), as can execution time (yellow dataset)\n\n\n\nIf you think the job is \u201cstuck\u201d at either step: check the server status https://status.galaxyproject.org/ and ask here or in a Gitter chat to let us know about a potential new problem. General chat (any server): galaxyproject/Lobby - Gitter and EU server chat: usegalaxy-eu/Lobby - Gitter. The EU admin team may also reply more here at this post.\nDifferent tools require different computational resources\n\nLower computational needs == individual jobs process quicker and as a result nodes are freed up quicker (to process the next job in their queue)\nHigher computational needs == individual jobs process longer and as a result nodes are freed up at a slower pace\n\nOther factors\n\nEach Public Galaxy server is using its own cluster resources. There is variability in the cluster number/type/size.\nHow many people are using a service at any particular time and the types of jobs they are running both impact processing performance.\nHow many jobs you are concurrently running also impacts your own processing speed. Some of your jobs will run, then some of the other people\u2019s jobs, then more of yours, repeat.\n\nIf you have work that is large or time-sensitive, the public Galaxy servers may not be appropriate.\n\n\nSome people use multiple public Galaxy servers to distribute the work/data. One account at each is fine and expected.\n\n\nSome people decide to use their own Galaxy, for short or long-term needs. When running your own Galaxy, you control how data/job quotas and job prioritization is set up, plus what resources are attached.\n\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub\n\n\nThe GVL Cloudman version is a single or multi-user choice and AWS offers grants. Amazon Web Services (AWS) - Galaxy Community Hub && AWS Programs for Research and Education\n\n\nThe AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. AnVIL - Galaxy Community Hub\n\n\nIf you think the job is \u201cstuck\u201d at either queued step: check the server status https://status.galaxyproject.org/ and ask here or in a Gitter chat to let us know about a potential new problem. General chat (any server): galaxyproject/Lobby - Gitter and EU server chat: usegalaxy-eu/Lobby - Gitter. The EU admin team may also reply more here at this post.\nI added a few tags to this topic that point to more details."}], [{"role": "user", "text": "Is there anyway to download a history completely ?"}, {"role": "system", "text": "Hi @amir,\nyou can download the history to a file from here:\n\nScreenshot from 2021-12-01 11-57-481922\u00d71031 151 KB\n\nRegards"}], [{"role": "user", "text": "Hi, I am new to Galaxy and would like to normalize FASTA dataset which I intend to use as a reference genome. I could not locate the tool NormalizeFASTA, please any idea. Thanks."}, {"role": "user", "text": "Got it. Thanks"}], [{"role": "user", "text": "When attempting to use the \u201cFunannotate predict annotation\u201d tool in the tool parameters it requires you to provide a funannotate database. However, when attempting to do this it shows there are \u201cno options avaliable\u201d and does not show any options when clicking the drop down menu. It will not let me run this tool since there is nothing selected, but the funannotate database does not allow me to choose an option\n\nimage1062\u00d7453 34.1 KB\n"}, {"role": "system", "text": "Hi @sarah-bennett\nThe \u201cFunannotate database\u201d input for this tool has some data restrictions, and is hosted at the UseGalaxy.eu server for now. Please try running the tool there instead.\nThanks for reporting the issue!"}], [{"role": "user", "text": "Hello, I am using STAR to do alignment on a collection of 24 RNA-seq samples. The jobs ran smoothly for 18 of them and are finished (it was quick). However the last 6 have been running for more than 12 hours now. I have run this alignment before and have never had this happen. Is there something up on the server side or is there a limit on number of concurrent jobs that one can submit? I have tried to do the alignment 2 time and once it was stuck after 10 and now after 18. Thanks!"}, {"role": "system", "text": "I see your jobs seemingly running smoothly.\nWe have no problems on our side and since they are running you have not reached any limit.\nI can suggest you delete them and launch them again if you think they have been running for too long.\nCheers"}], [{"role": "user", "text": "Does anyone know if the Jan 1 2020 deprecation of Python 2.7 will effect locally installed Galaxy? Also, does the software upgrade to Catalina have any bugs?"}, {"role": "system", "text": "Not until Apple stop supporting Python 2.7, assuming you are using macOS.\nAny way it should be pretty easy to port your local Galaxy to Python 3 by replacing the virtual environment that Galaxy uses (by default .venv) with a new one created with virtualenv -p python3 .venv ."}], [{"role": "user", "text": "Hi all - I have jobs submitted on monday that are still in waiting mode.  Is this happening to others?  Maybe I have used up all my resources?  I do have other long running jobs that are currently executing."}, {"role": "system", "text": "Hi @Michael_Thon\n\n\n\n Michael_Thon:\n\nHi all - I have jobs submitted on monday that are still in waiting mode. Is this happening to others?\n\n\nTry not to delete and rerun or all the new jobs end up at the back of the queue.\n\n\n\n Michael_Thon:\n\nMaybe I have used up all my resources?\n\n\nThe quota space in the account does not impact when jobs run \u2026 unless the output puts you over quota.\n\n\n\n Michael_Thon:\n\nI do have other long running jobs that are currently executing.\n\n\nThis will impact run times. Some of your jobs run, some from other people, repeat."}], [{"role": "user", "text": "Hi all,\nI\u2019m getting a strange permission error when running galaxy through docker.\nI\u2019m running a docker instance of galaxy locally on my centos 7 machine and it\u2019s having trouble accessing a folder I created on our NAS.\nFrom docker, I can create folders on the NAS, so I don\u2019t think the permission error is from docker.\nI thought it might be an selinux issue, but I ran sudo setenforce 0 and the permission error persisted.\nI feel like I\u2019m poking around in the dark on this error. Do galaxy permissions change when running from the docker container? Is there a way to run galaxy from the command line instead of the gui?\nThanks for your help,\nJosh\nThe traceback is here, the file is trying to create a folder:\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 393, in <module>\n    main()\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 307, in main\n    var_dict = make_results_folders(input_path, output_path)\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 33, in make_results_folders\n    utils.create_folder_hierarchy(output_subdirs, output_path)\n  File \"/export/galaxy-central/tools/dev_staging_modules/utils.py\", line 411, in create_folder_hierarchy\n    os.makedirs(directory)\n  File \"/tool_deps/_conda/lib/python3.7/os.py\", line 221, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/finkbeiner/imaging/smb-robodata/Josh/tmp_galaxy_testing/GXYTMP-03312021-SW-NSCLC-cas-Output/BackgroundCorrected'\n"}, {"role": "user", "text": "Hi all,\nThe problem was a permission on our NAS."}], [{"role": "user", "text": "Will running the tutorial on: Galaxy Installation with Ansible i get an error in running the step with the ansible-galaxy role: \u201cgalaxyproject.galaxy\u201d\nError is:\nERROR: Could not find a version that satisfies the requirement attmap==0.12.11 (from versions: 0.1, 0.1.1, 0.1.2, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.12.2, 0.12.3, 0.12.4, 0.12.5, 0.12.6, 0.12.7, 0.12.8, 0.12.9, 0.12.10, 0.12.11, 0.13.0)\\\\nERROR: No matching distribution found for attmap==0.12.11\n\nDoes anyone else have this same problem?"}, {"role": "user", "text": "Solved by using a higher galaxy_commit_id"}], [{"role": "user", "text": "Hi,\nI\u2019m conducting an RNA-Seq analysis on galaxy and I\u2019m trying to use limma-voom to identify differential gene expression. When I input both files, I get the following error:\nError in data.frame(sample = basename(filenamesIn), filename = filenamesIn,  :\nduplicate row.names: /galaxy-repl/main/files/033/345/dataset_33345502.dat\nExecution halted\nDoes anyone know why I would get this error message? My previous data is an output from kallisto and there are two columns per file I input: the transcripts ID and the est_counts. The transcript IDs are a bit odd looking and they look like : ENST00000631435.1. Perhaps this is the problem. I\u2019m unsure which headers limma-voom accepts. I\u2019m relatively new to galaxy.and any suggestions would be helpful.\nThank you!\nK"}, {"role": "system", "text": "with my experience of this, you cant DEG with limma when using kallisto\ntry Deseq2 and in \u201cchoose of input data\u201d select \u201cTPM value\u201d and then selet kallisto and put your GTF file then execute.\ntry this i hope it works"}], [{"role": "user", "text": "I would like to use AnnotatemyIDs and EGSEA to visualize the pathway enrichment for my significant genes. Is there a way to add compatibility with the Sus Scrofa species? Thank you in advance!"}, {"role": "system", "text": "Hi @rlynn01\nOnly a few species are supported with built-in indexes from the source.\nUse a reference annotation GTF input from the history instead. All UCSC genome builds with a gene annotation track will have these available in their Downloads area.\n\n\n\nStringtie merge error\n\n\nThe hg38 reference annotation GTF is now best sourced from UCSC. Not from the Table Browser but from the Downloads area here . There are a few Gene tracks to choose from. You can review what each of those represents then upload the selected GTF to Galaxy with the Upload tool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\n\n\nThe path to the data is similar for all genome builds: https://hgdownload.soe.ucsc.edu/goldenPath/<database>/bigZips/genes/\n\nFor pig genomes, start here UCSC Genome Browser Downloads. The database/dbkey used at UCSC the same as you will see as the assigned \u201cdatabase\u201d to your outputs from upstream steps (BAM) in Galaxy. Be sure not to mix different builds in the same analysis or expect weird results!\n\nFAQ:\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training\n\n  Collection of tutorials developed and maintained by the w...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello,\nI submitted 12 HISAT2 jobs a few hours ago. While two of them ran successfully within 10 minutes, the rest have not started running (they are all grey and \u201cwaiting to run\u201d). No other jobs are running.\nShould I keep waiting, or is there a problem on my end?\nThank you!"}, {"role": "system", "text": "Hi @ra2021\nAllow the jobs to stay queued. The jobs will run when resources become available again. Public servers utilized common resources shared with everyone else working at the same place; some of your jobs will run, then other people\u2019s, then more of yours, repeat.\nThanks!"}], [{"role": "user", "text": "Hello,\nI have several jobs waiting to run in RNA STAR for already for almost 2 days.\nIs there any trouble with this tool?\nI am using usegalaxy.org server and my username vngo2.\nThanks"}, {"role": "system", "text": "Hi @Vu_Ngo\nFor your case, the RNA-STAR jobs are completed and several downstream tools are now either queued or executing as expected.\nThe server was and still is very busy. The best strategy is to queue up jobs, then wait and allow them to execute as resources become available. The tool forms have an option to send an email notification \u2013 helpful when running batches. Workflows also have that notification option.\nTry to avoid deleting and rerunning unless you need to change or adjust inputs/parameters. I added a tag to your post that links to prior Q&A + FAQs that explain how the job queues work if interested.\n"}], [{"role": "user", "text": "Hi.\nHow can I extract uniquely and concordantly paired-end mapped reads (aligned concordantly exactly 1 time) from HISAT2?\nThanks"}, {"role": "system", "text": "I think you want samtools view:\nhttps://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8\ne.g.:\n\nimage.png392\u00d7799 41.9 KB\n"}], [{"role": "user", "text": "Hi. New to Galaxy here. I submitted a few jobs about 72 hours ago, and they still haven\u2019t started running. Is this typical or did I make an error somewhere?"}, {"role": "system", "text": "Update\nI took a look at one of your queued jobs (Blastp) \u2013 there are input problems that will cause the job to fail once it does run. You should fix these, then rerun.\n\nIncorrect datatype \u2013 both datasets are in fasta format, but one is labeled as fastq. FAQ\n\nBoth fasta datasets includes description content on the title line. This won\u2019t always lead to errors at the mapping step, but definitely can with many downstream tools. Standardizing the format at the start of an analysis is usually best. FAQ\n\n\nIn one of your other histories, there is a queued Busco job that also has an input that is in fasta format but is labeled with a fastq datatype. Fix that as well, then rerun.\nThat same history has another fasta dataset that is uncompressed and with what appears to be the correct datatype assigned. However, it seems odd that some data is uncompressed and some is compressed for similar jobs. You should confirm if any of your data is actually compressed or not, and assign the correct datatype. Running QA tools can help catch format problems, example: FastQC. Galaxy will guess the datatype when data is uploaded, or you can use the redetect function after upload/manipulations, but this tends to work best with uncompressed data for most data formats (BAM is one exception).\nHow to check for input problems (before a job is run, or after once it fails): FAQ\nHope that helps!\n\nHi @melT\nThe job queue for some tools has been very busy over the last few days at the UseGalaxy.org public Galaxy server. This includes mapping and other computationally intensive tools.\nThe best strategy is to leave queued jobs queued. Avoid deleting and rerunning, as all new jobs are placed back at the end of the queue extending the wait time."}], [{"role": "user", "text": "I am trying to create a clustered heat map of Cut and Run data using deeptools (computeMatrix \u2192 plotHeatmap).\nOutside of galaxy the kmeans assignment is simple in plotHeatmap, but I cannot find a way to apply this parameter in the context of galaxy.  Any suggestions?"}, {"role": "system", "text": "Hi @dan_foltz\nHopefully you found the option already. It is nested on the Galaxy form under the advanced options.\nScreenshot\n\nplotheatmap-kmeans1310\u00d7460 52.8 KB\n"}], [{"role": "user", "text": "Hello everyone,\nI\u2019m currently trying to perform RNA-Seq alignment with the help of RNA STAR. I performed the alignment and I would like to export t he Reads per gene file. My question is: If I click on the reads per gene file I get 3 columns without any IDS at the top (see attached 1st picture). Hence, what exactly do these columns represent? Furthermore, should I perform additionally feature counts and then export the file (2nd picture)? I have attached the outputs for both methods attached. Thank you very much for your help!\n\nrna star343\u00d71003 11.1 KB\n\n\nfeature counts356\u00d71014 10.3 KB\n"}, {"role": "system", "text": "Hello, STAR output reads per gene gives 4 columns, they are:\n\nGene ID\nCounts for unstranded RNA-seq\nCounts for the 1st read strand aligned with RNA\nCounts for the 2nd read strand aligned with RNA\n\nI found the details here,\nReference-based RNA-Seq data analysis\ngo to part Counting the number \u2026, on the Hands-on: Choose Your Own Tutorial, choose STAR"}], [{"role": "user", "text": "Hello ,\nI am using usegalaxy.eu and I have a problem with the tool intersect. I am a new user of Galaxy and I was following the instructions in this link https://galaxyproject.github.io/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html . I was at the step where we intersect the peak region in BED format and  the genes and I had this error : _CONDOR_JOB_PIDS=\nGALAXY_WEBAPOLLO_EXT_URL=https://usegalaxy.eu/apollo\nGALAXY_CONFIG_SERVER_NAME=handler_main_4\nSHELL=/bin/bash\nTMPDIR=/data/dnb02/galaxy_db/job_working_directory/006/018/6018455/tmp\n_CONDOR_SCRATCH_DIR=/var/lib/condor/execute/dir_2285248.\nThank you in advance"}, {"role": "system", "text": "Hi and welcome,\nwe are aware of this issue, have identified the cause and fixed it upstream in the Galaxy code base. However, you will have to wait for the next update of usegalaxy.eu before things will again work on the server as advertised.\nIn the meantime, you can try to switch the version of the tool to its previous one (0.0.1 instead of 1.0.0; click on the Versions button at the top of the tool interface to select it), which should be unaffected.\nHope that helps and sorry for the inconvenience."}], [{"role": "user", "text": "I am getting error with whatever tool i use. The data files are provided by courseera, so there\u2019s no problem with that. See the error\n"}, {"role": "system", "text": "Update: Fixed!\nIf you or anyone else runs into this problem, the solution is to either correct the metadata directly as described below or to rerun the impacted jobs.\nThanks to @nate for the quick fix!\n\n@Abhilash_Tripathi\nThanks for reporting the issue. We noticed it as well.\nTry clicking on the link, go to the Datatypes tab, and \u201cRedetect\u201d the datatype. You can also reassign the datatype directly. This will need to be done before inputting the data to downstream tools.\nWe are working on a correction. More feedback soon.\nThanks again!"}], [{"role": "user", "text": "Hey there\u2026 I have a bacterial genome NGS reads please help me to assemble these reads for further process"}, {"role": "system", "text": "Good day Waqas. I recommend using Torsten Seeman\u2019s Shovill tool - this is a wrapper around SPAdes and does a very good job with most microbial genomes. Also note that the Microbinfie Podcast discusses Microbial Genome Assembly in episodes 19, 20 and 22. All of these are worth a listen.\nFinally there are some tutorials on the Galaxy Training portal on Assembly (although I would not use Velvet as used in their examples, Shovill is much better) and Annotation with e.g. Prokka."}], [{"role": "user", "text": "Hello,\nI sometimes use FreeBayes tool and it always works well but today I am waiting for hours and it says \u201cThis job is waiting to run\u201d, all other tools that I tried, worked normally. Is something wrong with FreeBayes tool?\nThank you for your help,\nMaja"}, {"role": "system", "text": "Hi Maja,\n\n\n\n maja1234:\n\n\u201cThis job is waiting to run\u201d\n\n\nThis message means that the job is in the cluster queue. It will execute when resources are available. Avoid deleting/rerunning unless you already know there is something wrong with the original run (inputs, parameters) \u2013 otherwise, the new job is added back to the end of the queue, extending wait time.\n\n\n\n maja1234:\n\nall other tools that I tried, worked normally\n\n\nTools execute on different clusters that match the resources needed to process the work. It is expected that some tools will run faster than others. The clusters that can handle computationally intensive tools, like Freebayees, will often queue for a longer time than simpler tools.\nThe best advice is to allow the job to complete. If it fails, then you can start troubleshooting the inputs.\nFAQ: *How Jobs Execute\nPrior Q&A about this same topic can be found with a search at this forum like these:\nhttps://help.galaxyproject.org/tag/queued\nhttps://help.galaxyproject.org/tag/queued-gray-datasets\nI also added the \u201cqueued-gray-datasets\u201d tag to your post, so you can click on that instead.\nThanks!"}], [{"role": "user", "text": "Hello,\nI am trying to use featureCounts and keep getting this message: \u201cExecution of this dataset\u2019s job is paused because you were over your disk quota at the time it was ready to run\u201d. I am only using 3%.\nThanks"}, {"role": "system", "text": "Hi @pfzamora\nJust to double check that the quota usage is fully updated based on recent data additions/purges, please try this:\n\nLog into Galaxy\nGo to User \u2192 Preferences \u2192 Storage Dashboard\nClick on the \u201cRefresh\u201d button, and allow that to fully process\nReview the updated quota usage, and delete then purge more data as needed to free up space for new work. #quotas-for-datasets-and-histories\n\n\nIf that is not enough, which server are you working at? Reply with the URL for a public Galaxy or describe if other. As needed, a moderator can start a direct message to share the account\u2019s email address in. Never post your email publicly, and administrators will never need or ask for your password.\nLet\u2019s start there "}], [{"role": "user", "text": "Hi all,\nI\u2019m writing to you for an issue on vcftools merge tool. I\u2019m  trying to performe the merge of two vcf files with bcftools merge, and the tool seems to work; however, when open the resulting merged vfc file, I see the genomic positions, but NO the geniotypes and info related to the specific vcf files;\n\nDiapositiva11280\u00d7720 87.2 KB\n\nHow can I solve the the problem and have a merged corrected file?\nthank you in advance for your help"}, {"role": "system", "text": "Hi @Paola_Orsini,\nsorry for that question, but you have tried to scroll over to the right in that window. Have to ask this because I do not understand what you\u2019re trying to show with that screenshot.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hi, I am trying to download a large collection of BAM files through the command line. I have an API key, which is the last part of the download link needed to be specified; however I am lost on where to find the download link for the entire collection (links from each individual dataset are available). The \u201cdisk\u201d button does not retrieve/or is associated with a link (see screenshot)\nThanks in advanced,\nAlejandro\n\nimage572\u00d71168 106 KB\n"}, {"role": "system", "text": "Hi @alejandrogzi\nThanks for asking about this. We knew that the functionality was dropped in the new/beta history view but weren\u2019t sure if anyone was actually using it or not. Sounds like it is!\nI created an issue ticket to add in the function. Details are here along with available workarounds: New/beta history: add functionality to enable capture of a collection's URL \u00b7 Issue #14896 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nPlease feel free to comment/upvote that ticket plus you can follow it for updates "}], [{"role": "user", "text": "I did not find methylKit(R tool ) in the galaxy. Is it present or not?"}, {"role": "system", "text": "Hi @Anas_Jamshed\nI could not find this tool on usegalaxy.* servers. From a quick look, the last update of the wrapper was released in 2016. I also could not find any GTN tutorial with the tool, but I have not checked hard.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nI have a question about the data normalization in DESeq2.\nI have a matrix of gene counts from 30 samples belonging to the same group. I don\u2019t want to get DE genes obviously. I just need to normalize these counts. Is there a way to do this in DESeq2 without the need to specify two conditions?\nThanks\nBarbara"}, {"role": "system", "text": "\n\n\n Barbara_Mandriani:\n\nIs there a way to do this in DESeq2 without the need to specify two conditions?\n\n\nThe tool needs at least two conditions each with replicates or there is nothing to compare for generated statistical tests.\nDetails direct from the original tool authors are in this post and related posts at their forum: How to perform DEseq2 on datasets without replicates or merged replicates"}], [{"role": "user", "text": "Hello!\nWhere can i see the list of all supported data types in galaxy? I honestly searched this on plenty of galaxy resources but found exactly nothing.\nSpecifically i got \u201cWARNING:galaxy.model:Datatype class not found for extension \u2018tar.gz\u2019\u201d when i wanted to save many output files to tar.gz archive.\nThanks in advance."}, {"role": "system", "text": "Hi @wormball,\nyou can find the supported datatypes here galaxy/datatypes_conf.xml.sample at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub\nRegards"}], [{"role": "user", "text": "Hi there, I have a list of compounds for docking. How do I concatenate them all together (in SMILES) in a single file? Or else I have to dock one by one? Thank you in advance !!"}, {"role": "system", "text": "Hi @zschong,\nHave you seen our introductory tutorial on protein-ligand docking? Protein-ligand docking\nIt explains how to dock a list of compounds to a protein, starting from a list of SMILES, like you describe.\nTo concatenate multiple SMILES datasets, try searching for a tool like Concatenate datasets.\nBest wishes,\nSimon"}], [{"role": "user", "text": "Have you experienced this error in galaxy?:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nestimating size factors\nError in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc,  :\n  every gene contains at least one zero, cannot compute log geometric means\nCalls: DESeq ... estimateSizeFactors -> .local -> estimateSizeFactorsForMatrix\n"}, {"role": "user", "text": "Every gene has at least one zero for at least one sample of your four samples. The thing is that DESeq2 calculates a geometric mean for every gene over every sample. The geometric mean will be zero if at least one entry is zero, i.e., if one of the four samples has a zero count it will be zero and cannot be used. The geometric means are later important for the DESeq2 normalisation.\nExample:\n\nID S1 S2 S3 S4\nGene_1 1 22 3 0\nGene_2 30 0 12 5\nGene_3 0 0 1 6\n\nAll Genes above are invalid and cannot be used by DESeq2 because at least one entry is zero.\nWhat you need is something like:\n\nID S1 S2 S3 S4\nGene_1 1 22 3 1\nGene_2 30 1 12 5\nGene_3 0 0 1 6\n\nThat would be fine since Gene_1 and Gene_2 can be used later for the normalisation. Gene_3 will be filtered out by DESeq2.\nSolution:\n\nYou include more data\nYou add a pseudo-count of 1 to all counts. Just add 1 to every entry. Thats technically not cheating because you treat every entry and DESeq takes the log of the data anyway. If you do then make sure you remove genes beforehand that have zeros for all samples, e.g., Remove Gene_1 in this example:\n\n\nID S1 S2 S3 S4\nGene_1 0 0 0 0\nGene_2 30 0 12 5\nGene_3 0 0 1 6\n\n\nGalaxy (DESeq) has an option for that error as well. Look under  (Optional) Method for estimateSizeFactors. These methods are more sophisticated. Choose either \u2018poscounts\u2019 or \u2018iterate\u2019. Please  read the documentation in the tool description  for both these models, before you use one of them.\n"}], [{"role": "user", "text": "i am trying to do a simple tutorial to learn how to use galaxy but after i send the query to galaxy, nothing happens - no yellow/green. is there a usual time frame/estimate for this?"}, {"role": "system", "text": "We are currently having issues with running jobs, we are working on a resolution. Sorry for the inconvenience.\nedit: I think the problem has been fixed, the jobs will start running shortly."}], [{"role": "user", "text": "Dear specialists,\nGTN material for MaxQuant for TMT-MS does not work. MaxQuant repeatedly fails with a fatal error; MaxQuant and MSstats for the analysis of TMT data\nFatal error: Exit code 134 ()\nUnhandled Exception: System.Exception: Exception during execution of external process: 3803082\nat QueueingSystem.WorkDispatcher.ProcessSingleRunExternalProcess(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.DoWork(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.Work(Object threadIndex)\nat System.Threading.Thread.ThreadMain_ParameterizedThreadStart(Object parameter)\nat System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)\n\u2014 End of stack trace from previous location where exception was thrown \u2014\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n/usr/local/bin/maxquant: line 4: 3795703 Aborted                 (core dumped) dotnet $DIR/MaxQuantCmd.exe $@\nI would like to kindly invite you to resolve the issue and provide feed-back so I can use the GTN material and also analyse my TMT data.\nPlease find the history link below where MaxQuant tool failed with the GTN version and its latest version;\nhttps://usegalaxy.eu/u/qcsciphi/h/maxquant-and-msstats-for-the-analysis-of-tmt-data-kmt9-ko-inhibits-a549-proliferation-pmid32095117\nI already sent a bug report from the tool with a shared link to the history with failing datasets.\nThanks in advance."}, {"role": "system", "text": "Hello @qcsciphi\nThe tutorial is still a work in progress.\nYou can try running through it at UseGalaxy.eu instead (where the tutorial authors are also working) but it may still have a few problems even there until it is finalized (sometime before May)."}], [{"role": "user", "text": "Dear users,\nI\u2019m new at using Galaxy, colleagues have used it and showed it to me.\nI wanted to start with a very small and simple data set to see if it works for me.\nI have made a .fasta file from 34 NovaSeq reads and want to run it in the immune repertoire pipeline.\nHowever it keeps coming back with the same message;\n\u201c|Sample 1 of patient EVD is not a zip file so assuming fasta/fastq, using igBLASTn|\u201d\nIs anyone experiencing this same error and what do I need to change to avoid it and get the data?\nRegards, Renate LUMC Hematology"}, {"role": "system", "text": "Context (worked out in a direct email):\nGalaxy Server: ARGalaxy \u2013 Erasmus MC (https://bioinf-galaxian.erasmusmc.nl/argalaxy)\nContact information is on the homepage of the server.\nContact information is listed also under https://galaxyproject.org/use at https://galaxyproject.org/use/argalaxy/\nReport the problem and they should be able to help.\nThanks!"}], [{"role": "user", "text": "Hi,\nI\u2019m trying to upload a large GTF file of 1.5G which is actually a combination of two smaller GTF files (lets call them file A and file B) which can be seperately uploaded without problems but the combined file is too large. I want to use this file in htseq-count. I was wondering if it is possible to get the same results as the combined file by running htseq count seperately for file A and file B and if not, if it is possible in another way to upload my large file to htseq count.\nThanks!"}, {"role": "system", "text": "Hi,\nplease use our FTP service to upload large files\nhttps://galaxyproject.eu/ftp\nCheers"}], [{"role": "user", "text": "Hello. I am trying to use HTseq count for my data. I am using the bam file and a gff file of the human reference genome from ncbi. I have been getting this error. Any help on how to go about this?\n[bam_sort_core] merging from 64 files and 1 in-memory blocks\u2026\nError occured when processing GFF file (line 17484 of file /scratch/user/galaxy/maroon_galaxy/objects/2/5/9/dataset_259335c3-5a5c-4d34-8eef-876dfc014a1d.dat):\nunmatched quote\n[Exception type: ValueError, raised in _HTSeq.pyx:1576]"}, {"role": "system", "text": "Hi @larad\nit is hard to identify the issue without looking into data and the log file.\nHave you mapped reads in Galaxy? If yes, have you used \u201cbuilt-in\u201d reference genome in the mapping step? If yes, maybe try featureCounts and change Genome annotation file to featureCounts built-in. Try another annotation file, preferably from GenCode. Make sure you use the annotation file for the same version of genome assembly, as in the mapping step.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,  I am trying to convert a bedgraph file that used the mm10 genome assembly to a mm9.  I tried the liftover tool and I do not see a choice to convert to mm9.   I do not see any mmX assemblies.  I must be doing something wrong (or multiple things!).   Can someone point me in the right direction?\nThank you,\nScott"}, {"role": "system", "text": "Hi @hsstadler,\nI assume that you are using the usegalaxy.org instance but at the moment this genome assembly is not available there. However, you can find the mm9 genome assembly in the usegalaxy.eu instance."}], [{"role": "user", "text": "Hi! I am trying to run cutadapt on paired-end reads, but the jobs stay on waiting to run for a long time (longer than 15 minutes). I was running the same files as single-end reads earlier and it worked pretty fast, so I\u2019m confused about why it\u2019s not working through paired-end reads. Any help would be greatly appreciated! Thank you!"}, {"role": "user", "text": "Update: It\u2019s now working! It was most likely just having to wait for the server! "}], [{"role": "user", "text": "Hi, when I try to use hicbuildmatrix to generate a matrix, a bed file is required (please find attached the pictures). Can I skip this step? The tutorial does not include this part.\nbed file needed1224\u00d7648 12.9 KB\n"}, {"role": "system", "text": "Hi @Chen,\nyou can skip this step by selecting the tool version 3.4.0.\nRegards"}], [{"role": "user", "text": "Hi,\nI run the job 11ac94870d0bb33a69a8e2a8dad79bbc (simple compute) and it doesn`t finish (yellow).\nRerunning the job finished in minutes. Can it be that there is a problem with the node used?\n\nJob Information\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.0\n\n\n\n\nCommand Line\npython \u2018/opt/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/column_maker/6595517c2dd8/column_maker/column_maker.py\u2019 --column-types str,int,int,str,int,int,float,float,int,int,int --file \u2018/data/jwd05e/main/059/782/59782338/configs/tmp39wdjmom\u2019 --fail-on-non-existent-columns --fail-on-non-computable \u2018/data/dnb08/galaxy_db/files/f/9/d/dataset_f9d935ca-f5ca-4a27-924e-040d951c96b5.dat\u2019 \u2018/data/jwd05e/main/059/782/59782338/outputs/galaxy_dataset_4d9ef16f-da0c-4030-899c-05ed225a86b2.dat\u2019\n\n\nTool Standard Output\nempty\n\n\nTool Standard Error\nempty\n\n\nTool Exit Code:\n\n\n\nJob API ID:\n11ac94870d0bb33a69a8e2a8dad79bbc\n\n\n\nThanks\nRalf"}, {"role": "system", "text": "Welcome, @ralfgilsbach\n\n\n\n ralfgilsbach:\n\nCan it be that there is a problem with the node used?\n\n\nPossibly, but glad the rerun worked!\nOther potential differences involve the input dataset and parameters. Or, just timing. The public servers run a lot of jobs, and involve many clusters and nodes. By chance some nodes might be executing more than one job at any particular time, or have different base-line resources.\nYou probably know this already, but for others reading: To visually compare two tool runs: click on the Dataset Details  to review the Tool Parameters in a summary format. Find that right below what you pasted back. That summary is sometimes clearer than reviewing a tool rerun forms  since nested parameters might become unexpanded, hiding some of the original settings.\nIf this repeats or you think more is going on, please share more details. Troubleshooting errors\n\n\nReferences:\n\nTool: Compute on rows\n\nHow-to Tutorial: Data Manipulation Olympics\n\nIn-context Tutorials: Galaxy Training!\n\n\ncc @wm75 @gallardoalba"}], [{"role": "user", "text": "Is there a way to merge xml/blastxml outputs of blastx?\nI have two sets of blast results for the same cds dataset, one against the Swissprot database and another against the NCBI nr database. (I would have done them together, but I wanted to subset the nr database so it wouldn\u2019t have such a long run time.)\nThe two outputs don\u2019t overlap. I\u2019m searching for a way to combine them into a single xml/blastxml file in which the descriptions missing from one blast output can be replaced by those in the other.\nOn Biostars, I think I found a very old solution for python and another for java, but I wondered if there was now a simpler way on Galaxy. Any ideas would be very much appreciated."}, {"role": "user", "text": "In case anyone is confronted with this in the future, I found an easy solution to this.\nOn Blast2GO (or OmicsBox), I loaded my sequences and my blast results against Swissprot. Then I subset the dataframe to exclude the ones without blast results. I then selected all remaining rows and extracted them to a new dataframe. So I had a dataframe with only sequences accompanied by Swissprot hits, which I saved (.b2g or .box). I then started a new project by again loading all my sequences and the nr blast results, and saved that as well. Finally, I merged the two together by adding the sequence, GO description, and blast results for nr-blasted data to the Swissprot-blasted data if the data were missing. This procedure left me with all my sequences, their Swissprot results, and, when those were missing, the nr results, plus the sequences that had no results for either. (Later I also merged this with results from Interproscan.)\nI saw an old post that indicated that there may be a wrapper on Galaxy that merges blast xml outputs, but I haven\u2019t yet found it. Any other ideas are welcomed!"}], [{"role": "user", "text": "Hi, my disk quota reports it\u2019s at 100 % capacity when I only have less than 20 GB out of the total 250 GB occupied. I have deleted files to free up more room but it has no effect (I have also made sure to \u201cpermanently delete\u201d unused files on the account storage page). Is there a way I can solve this problem and avoid it in the future?\nThanks for your help,\nJulia"}, {"role": "system", "text": "If you click on your quota meter and go to the \u201cStorage Dashboard\u201d you will find a few options that can help you.  The first thing I\u2019d try is to request a quota refresh, which is the main button you see on that interface.  This will recalculate your current usage, and might resolve your issue.\nA direct link to that on usegalaxy.org is: https://usegalaxy.org/storage"}], [{"role": "user", "text": "\nPer base sequence GC content960\u00d7730 111 KB\n\n\nPer base sequence content885\u00d7656 76.9 KB\n\nDear All,\nI am new to galaxy server and trying to analyze the ChI-seq data files. I run FastQC analysis for chipped fastq file and found per base sequence content error and warning for per base sequence GC content (two peaks in the graph). I have attached both the graphs with this post.\nI wanted to understand what could be the cause for these errors and how can we resolve this?\nYour help is highly appreciated.\nBest,\nShashi"}, {"role": "system", "text": "Welcome, @Anand\nThe FastQC tool form has a reference link at the bottom to the tool authors website. They host a range of resources, including example graphs with descriptions about what they mean and considerations for different sequencing types.\nThis Galaxy tutorial has some parts based on that same data plus related content. Quality Control\nThe GTN also hosts tutorials specific for ChIP-seq. Use the search at the very top to search with keywords to find those. Some include extra QA help.\nPlease give those a review.\nIf you need more help:\n\nIf this is raw data, try running the data through a trimming program, then rerun this tool.\nDoes that also report odd results in these or other modules?\nDo any other samples from the same source have odd results?\nIs there anything special about the data? You could just list out some features: species, source, any prior manipulations (including merging fastq files together).\n\nYou can post back more screenshots or sometimes better, post back a shared history link. A double peak indicates there are two different \u201cpools\u201d of data in the sample. Recognized contamination would show up in other sections of the report. Your job didn\u2019t fail but this FAQ explains how to review the job details and share back a history link: Troubleshooting errors"}], [{"role": "user", "text": "Dear all,\nGalaxy server cannot be accessed in both Chrome and Firefox.\nIt is now 6 days that the problem exists.\nDoes anybody face with the same issue?\nHow can I solve this problem?\nRegards\nNazanin"}, {"role": "system", "text": "I can access them. Maybe this page can also help https://status.galaxyproject.org/"}], [{"role": "user", "text": "I am receiving a server for our laboratory with 128 GB of memory to work with prokaryotes.\nI am thinking of installing a Galaxy environment in it with a SLURM queue to share with other users, with the possibility of increasing the infrastructure with the addition of new nodes in the future.\nCan I install the local GVL project, other than on a Cloud server? Could you install on an infrastructure with Rancher for your management? Is there a tutorial for this procedure?"}, {"role": "system", "text": "Hey @Fabiano_Menegidio!\nIt is definitely possible, although fyi by default Galaxy within the GVL uses the Kubernetes Job runner rather than Slurm (although hybrid setups, running on k8s but dispatching to Slurm is definitely possible). Running the GVL locally will need some tinkering to the out-of-the box configuration which is intended for the cloud, but in theory the biggest blocker is just figuring out a proper StorageClass for the Kubernetes cluster. Traditionally for a local setup we\u2019ve used hostPath storageClass which just uses the local host storage. I haven\u2019t set it up locally in a few months, but can try it again soon and maybe add a bit of documentation for future cases.\nIf you have a lot of experience in this realm, you can likely figure it out on your own. The entry point for the boot mechanism is the cloudman-boot docker image which sets up Rancher and installs the base GVL infrastructure (CloudMan). If you are just starting with K8S, it might be a bit harder to find the right configuration on your own, but we could schedule a time to do an interactive session over Zoom to help you get started.\nFeel free to email at almahmoud@jhu.edu to set-up a time in the next few weeks, or send me a message on Gitter/Slack (@almahmoud) and I\u2019ll be happy to help however i can and get other people involved to help as well."}], [{"role": "user", "text": "Hello All,\nI am relatively new to this sort of analysis. I have paired-end reads from Oncopeltus tissue. I ran these under Salmon-quant to generate TPMs. I then used these as input files into the newest version of DESEQ2 using the gff3 annotation file from the oncopeltus genome. The program runs, but returns outputs with nothing in them.\nPlease help!\nA"}, {"role": "system", "text": "There is much Q&A about these tools at this forum. Please review what has worked for others. Focus on the reference data first \u2013 both content and format \u2013 since a mismatch between identifiers is the most common root issue.\nI added a few tags to your topic that will find these, plus you can also just search with tool names, datatypes, error messages, etc to get started."}], [{"role": "user", "text": "Hi\nI am using trinity and the error message is:\n\"Fatal error: Exit code 1 ()\ncp: -r not specified; omitting directory \u2018trinity_out_dir/Trinity.tmp.fasta.salmon.idx\u2019\n/ocean/projects/mcb140028p/xcgalaxy/main/staging/50922182/.cvmfsexec/mountrepo: line 70: cd: /ocean/projects/mcb140028p/xcgalaxy/main/staging/50922182/.cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/config.d: No such file or directory\"\nI don\u2019t understand what is the problem here. The fastq files are OK and there are \u201c1\u201d and \u201c2\u201d in the names of sequences.\nPlease help."}, {"role": "system", "text": "Hello @guser\nThat type of stdout error can include problems with read content. There is much QA at this forum about common solutions. Please give that a review first, and if you are stuck later, share back more details and we can try to help more.\nThis post from yesterday lists out the exact information we will need to help you as well. The stderr at a minimum, a copy/paste of the full job information page report if the stderr also does not include any meaningful information. Make sure all input datasets and both logs are expanded, or we will need to ask you for that again. An error occurred when I used Trinity to analysis my RNA-seq data (clean paired-end fq.gz files) - #4 by jennaj\nLet\u2019s start there, thanks!"}], [{"role": "user", "text": "Hey everyone,\ncurrently I am struggling with aligning my seq data to my reference sequence. The annotation file hasn\u2019t any exons, just CDS, gene and region and therefore I can\u2019t use STAR aligner. Are there any possibilities to add --sjdbGTFfeatureExon to change the feature of the annotation file??\nBest,\nBenedikt"}, {"role": "system", "text": "Hi @benediktp\nmaybe consider mapping reads without gene annotation and count reads mapped to genes using featureCounts or htseq-count. Do you work with bacterial data? In this case you probably don\u2019t need gapped aligner.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi,\nIs there a tool implemented in Galaxy or elsewhere to convert a 600Mb Fasta alignment file to Nexus?\nThank you in advance,\nAlex"}, {"role": "system", "text": "Hi @aperrat,\nthe seqret tool allows to carry out such conversion."}], [{"role": "user", "text": "Hi everyone,\nI was able to successfully cluster scRNAseq data from this recent paper \" Single-cell analyses define a continuum of cell state and composition changes in the malignant transformation of polyps to colorectal cancer\" in which I was able to identify several different colonic epithelial cell clusters of interest. I considered increasing the resolution of the initial clustering to get more specific epithelial subgroups, but because the annotation of clusters must be done manually, I found it difficult to label the large number of clusters that resulted from the increased resolution. Because I am only interested in further investigating the epithelial clusters and do not wish to look into smaller groups of stromal or immune cells, I was wondering if there was a way to isolate and interrogate just certain clusters from the original dataset and perform downstream subsetting on those specific clusters. Hope that makes sense!"}, {"role": "system", "text": "Hi @jesskope13\nHopefully you have already seen the Galaxy scRNA-seq tutorials, but if not find them here:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Transcriptomics\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThis specific tutorial might be enough\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Clustering 3K PBMCs with Scanpy\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "I have tried uploading several .bam datafiles, but for some reason they cannot be detected as the bam datatype (and therefore not used in my workflow). I\u2019ve tried manually setting the datatype to bam both before and after uploading the files. But it gives me the error \u2018An error occurred setting the metadata for this dataset. Set it manually or retry auto-detection\u2019. Both of these options gave a similar error. The datasets look like the image. I have uploaded .bam files before and it was never an issue. The only difference is that these bamfiles were generated from .cram files. I\u2019m quite novel to working with these datatypes, so hope I\u2019ve given all the information needed to help me with this issue. How can I make sure they are detected as proper bamfiles?\nThanks a lot for the help!\n\nimage574\u00d7730 55.3 KB\n"}, {"role": "system", "text": "Hi @DanielleBianchi\n\n\n\n DanielleBianchi:\n\nI have uploaded .bam files before and it was never an issue. The only difference is that these bamfiles were generated from .cram files.\n\n\nTry this: check the file content before Upload to Galaxy. Try converting bam to sam format with Samtools. You could also try sorting the bam. If it fails for you locally, then it would fail in Galaxy.\nOr, try this: directly Upload the cram data to Galaxy. Most tools do not directly use cram format, but you can convert cram to bam in Galaxy with the tool Samtools view.\nFor either, try letting the Upload tool auto detect the format first. If that doesn\u2019t work, you can assign the datatype instead (during Upload \u2013 not sure if you tried that already for the bam or not).\nI\u2019m also not exactly sure how good we are at auto detecting cram format right now\u2026 but I\u2019ll test that later today or tomorrow since am now curious. If any usage deviates from the above \u2013 I\u2019ll post back.\nIt would be best to do this at the UseGalaxy.org or UseGalaxy.org.au server right now. The UseGalaxy.eu server is undergoing some admin changes this week, and some odd behavior is expected until the banner clears. It is easier to troubleshoot without complicating factors.\nThanks!"}], [{"role": "user", "text": "Dear Admin\nI would like analyse nucleolar localization sequences in my proteome dataset. May I request you to add the  \u201cNucleolar localization sequence Detector (NoD)\u201d tool in the galaxy main webpage https://usegalaxy.org/.\nThanking you\nSurajit"}, {"role": "system", "text": "Hi @microbio,\na PR has been opened in order to install this tool. It will be available within 24 hours.\nRegards"}], [{"role": "user", "text": "Hi, I have 3 iterations of a library, I need to merge them. How to make?"}, {"role": "system", "text": "Hi @Thiely\nIs the goal to process all of the samples together? If yes, using dataset collections would be a good idea.\nGTN Tutorials: Search Tutorials\nIf you are trying to do something else, please provide more details.\nThanks!"}], [{"role": "user", "text": "I can\u2019t currently access usegalaxy.org because it appears that the HSTS certificate has expired. Wanted to make sure the admins know about this.\n\nScreenshot 2022-08-12 1727591920\u00d7923 137 KB\n"}, {"role": "system", "text": "Thanks for reporting this; the error should be resolved now."}], [{"role": "user", "text": "Hi, I would like to share multiple histories with other users on my instance, what is the best way of doing that?\nI have already try Sharing/Publishing the History and moving some of datasets into a common shared data library but it is taking ages. Thanks, Kate"}, {"role": "system", "text": "The best way is to publish them, you do not need to move the datasets to data library, when your users import the history no data is being hard copied. The datasets in the history just must have proper permissions so others can read it. Galaxy 19.01 (should be released in few weeks) has a convenience button to ensure all members of history are published as well."}], [{"role": "user", "text": "is there any api for managing users in galaxy?\ncreating editing and deleting users \u2026"}, {"role": "system", "text": "Yes, you can either use the Galaxy API directly or more easily with BioBlend: https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#module-bioblend.galaxy.users"}], [{"role": "user", "text": "\nmacrogen pipeline704\u00d7564 67 KB\n\nCan anyone assist in recreating this here or designing a pipeline that would work? What did you use instead of the GATK toolkit?"}, {"role": "system", "text": "Hi @Akili_Joseph, try FreeBayes for variant calling. Check GTN tutorials Galaxy Training!\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "I am trying to use featureCounts on an unpaired collection of 9 bam files in my history. For the \u201cGene annotation file\u201d section, I selected \u201cfeatureCounts built-in\u201d. However, I can\u2019t select the genome build I want (mm10).\nHow is this possible? The menu just doesn\u2019t pop down. When I imported these BAM files, I did not specific their build at the time, in the the \u201cGenome (set all):\u201d setting. Is that why?\nThanks"}, {"role": "system", "text": "If your genome is either hg38, hg19, mm10, or mm9 -> use featureCounts built-in option in Gene annotation file. You may need to set genome build for your BAM files (in the image below it is set to hg19. If it is not set you will see ?)\n"}], [{"role": "user", "text": "Hi,\nI\u2019m trying to use the \u2018Relabel list identifiers from contents of a file\u2019 function on a collection of paired-end reads.\nHowever when I try to do this I get the following error message:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: Invalid new colleciton identifier [\u201cErthyrocytic stages (3/3) Yeoh\u201d]\n{\n\u201ctool_id\u201d: \u201cRELABEL_FROM_FILE\u201d,\n\u201ctool_version\u201d: \u201c1.0.0\u201d,\n\u201cinputs\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: [],\n\u201chid\u201d: 149,\n\u201cid\u201d: \u201c3bcdb03089092049\u201d,\n\u201cname\u201d: \u201cYeoh_pairedenddatalist\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201chow|how_select\u201d: \u201ctxt\u201d,\n\u201chow|labels\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cyeoh_relabel\u201d,\n\u201ctags\u201d: [],\n\u201ckeep\u201d: false,\n\u201chid\u201d: 259,\n\u201cid\u201d: \u201cbbd44e69cb8906b5b8d1bfcf6de3b443\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201chow|strict\u201d: \u201cfalse\u201d\n}\n}\nMy relabelling text file is simply:\n\u201cErthyrocytic stages (3/3) Yeoh\u201d\n\u201cErthyrocytic stages (2/3) Yeoh\u201d\n\u201cErthyrocytic stages (1/3) Yeoh\u201d\n\u201cMale gametocytes (3/3) Yeoh\u201d\n\u201cMale gametocytes (2/3) Yeoh\u201d\n\u201cMale gametocytes (1/3) Yeoh\u201d\n\u201cFemale gametocytes (3/3) Yeoh\u201d\n\u201cFemale gametocytes (2/3) Yeoh\u201d\n\u201cFemale gametocytes (1/3) Yeoh\u201d\nand I\u2019m using it on a list of paired data collections (length = 9). I don\u2019t understand from the error message what isn\u2019t working, why is the identifier illegal?\nHope that this is enough information to debug.\nMany thanks,\nTim."}, {"role": "system", "text": "These characters are causing problems: (  )  /\nAlso, avoid whitespace (tabs, spaces).\nTry using only these characters in your new identifier:\n\nAlphanumeric: A-Z a-z 0-9\n\nUnderscores: _\n\nDashes: -\n\n\nIf you are familiar with regular expressions, this is the test that is raising the error:\nFrom lines 2781-2783 in https://github.com/galaxyproject/galaxy/blob/10c2f9cbc286477550e708d65bcd1381857dca15/lib/galaxy/tools/__init__.py\nfor key in new_elements.keys():\n        if not re.match(r\"^[\\w\\-_]+$\", key):\n            raise Exception(\"Invalid new colleciton identifier [%s]\" % key)"}], [{"role": "user", "text": "how do i put a tool in my favorite part which GALAXY adding recently ?"}, {"role": "system", "text": "Every tool should have a button next to versions/options. We will include it in the release notes once they are out.\n\nGalaxy.png916\u00d7238 29.6 KB\n"}], [{"role": "user", "text": "I run a small private Galaxy 18.04 server. Now, I\u2019m trying to install a self-developed tool, which reads a paired collection and produces three single collections. That is, every pair of input datasets will generate three datasets on the output.\nWhat should be an xml definition for this case?\nThanks for helping this newbie Galaxy user! \n======\nThis is my present xml file\n<tool id=\u201curgel_1\u201d name=\u201cURGEL 1 Cutoff\u201d version=\u201c0.1.0\u201d>\n<description>to consider genes as up-regulated, from a list of differential expression values</description>\n<command>\n<![CDATA[perl $__tool_directory__/URGEL1.pl\n#if $unicolista.choice == \u2018Par \u00danico\u2019:\n\u2018$tumoral\u2019 \u2018$controle\u2019\n#else:\n\u2018$tumconlist.forward\u2019 \u2018$tumconlist.reverse\u2019\n#end if\n]]>\n</command>\n<inputs>\n<conditional name=\u201cunicolista\u201d>\n<param name=\u201cchoice\u201d type=\u201cselect\u201d label=\u201cSelect Par \u00danico or Lista de Pares\u201d >\n<option value=\u201csingle_pair\u201d>Par \u00danico\n<option value=\u201cpaired_coll\u201d>Lista de Pares\n</param>\n<when value=\u201csingle_pair\u201d>\n<param name=\u201ctumoral\u201d type=\u201cdata\u201d format=\u201ctxt\u201d label=\u201cTumoral data file\u201d />\n<param name=\u201ccontrole\u201d type=\u201cdata\u201d format=\u201ctxt\u201d label=\u201cControl data file\u201d />\n</when>\n<when value=\u201cpaired_coll\u201d>\n<param name=\u201ctumconlist\u201d type=\u201cdata_collection\u201d collection_type=\u201cpaired\u201d format=\u201ctxt\u201d\nlabel=\u201cTumoral x Control pair list\u201d />\n</when>\n</conditional>\n</inputs>\n<outputs>\n<collection name=\u201cthrsh\u201d type=\u201clist\u201d label=\u201cthr {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"\" directory=\u201cthr\u201d visible=\u201ctrue\u201d />\n</collection>\n<collection name=\u201cdiffx\u201d type=\u201clist\u201d label=\u201cdif {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"\" directory=\u201cdif\u201d visible=\u201ctrue\u201d />\n</collection>\n<collection name=\u201cupreg\u201d type=\u201clist\u201d label=\u201cupr {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"*\" directory=\u201cupr\u201d visible=\u201ctrue\u201d />\n</collection>\n<data format=\u201ctxt\u201d name=\u201carqLog\u201d label=\u201cURGEL1 Log\u201d />\n</outputs>\n<help>\nOriginal: UpRegulatedGeneExtractorList.pl\nThis tool calculates the expression cutoff to consider genes as up-regulated, from a list of genes\nwith corresponding differential expression values.\n</help>\n</tool>\n======\nActually, the tool generates FOUR empty lists (one for the log file, which was intended to be a single file)."}, {"role": "user", "text": "After digging the documentation (Galaxy Tool XML File), I was happy to find the solution by myself. My XML file now looks like this:\n====\n<tool id=\u201ctoolid\u201d name=\u201ctoolname\u201d version=\u201c0.1.0\u201d>\n<description>Description\u2026</description>\n<command>\n<![CDATA[perl $__tool_directory__/tool.pl\n$input.forward\n$input.reverse\n$output.outcoll1\n$output.outcoll2\n$output.outcoll3\n]]>\n</command>\n<inputs>\n<param name=\u201cinput\u201d type=\u201cdata_collection\u201d collection_type=\u201cpaired\u201d format=\u201ctxt\u201d\nlabel=\u201cPaired Collection\u201d\n</inputs>\n<outputs>\n<collection name=\u201coutput\u201d type=\u201clist\u201d label=\"(tool_name) on {on_string}\" >\n<data name=\u201coutcoll1\u201d format=\u201ctxt\u201d />\n<data name=\u201coutcoll2\u201d format=\u201ctxt\u201d />\n<data name=\u201coutcoll3\u201d format=\u201ctxt\u201d />\n</collection>\n</outputs>\n</tool>\n====\nThis way, my output is what Galaxy calls an embedded list, or a list of lists.\nThe tool now works as intended."}], [{"role": "user", "text": "\nScreen Shot 2023-01-11 at 11.45.46 AM700\u00d7998 66.3 KB\n\nBowtie2.0 task is running for a long time. It has been 36 hours and still running. Is that normal? Thanks! I am analyzing ATAC-seq data with paired end reads.\nIt is giving me the following stats although it is still running. Can someone please tell me why? thanks\n\nScreen Shot 2023-01-11 at 11.51.26 AM1202\u00d7538 48.1 KB\n"}, {"role": "system", "text": "Hi @abhay03\nIt looks like the job is still completing the process of writing all the output files. Let this complete to avoid needing to rerun.\nIf you were working at UseGalaxy.org, I can let you know that the clusters were very busy when you wrote in, and are a bit less busy now.\n\n\n\n abhay03:\n\nIt has been 36 hours and still running. Is that normal?\n\n\nYes, sometimes, since those 3 days probably included queue + the actual job execution time. The best advice is to get jobs queued then let them complete. Using collections and workflows will significantly improve total processing time.\n\nHow jobs process: faqs/galaxy/#analysis\n\nCollections and workflows: topics/galaxy-interface\n\n\nIf you think there is still a problem after a few more hours, please either reply and post back a share link to the history and note the dataset involved, or ask for a moderator to start up a direct chat to post that in privately.\n\nHow to share: faqs/galaxy/#sharing-your-history\n\n\nThanks!"}], [{"role": "user", "text": "Where are computeMatrix and plotHeatmap gone? Not showing up in tools today."}, {"role": "system", "text": "Hi @niallghowlett\nWe discussed this over direct email, but for others that may have run into the same problem:\n\n\nBoth tools are available as of yesterday at UseGalaxy.org. Why those appeared to be missing from the tool panel is not known.\n\n\nThe UseGalaxy.eu server had some problems this week. That could have impacted tool panel searches, and still could since the banner on the server is still up. Once the issues are resolved, the banner will be removed. Details here: UseGalaxy.eu -- unexpected server behavior May 2022\n\n\nThanks for reporting the problem!"}], [{"role": "user", "text": "I tried to load data to hicexplorer.galaxy.eu today and it failed. I used FileZilla. usegalaxy.org worked but not hicexplorer.galaxy.eu"}, {"role": "system", "text": "\n\n\n Chen:\n\nhicexplorer.galaxy.eu\n\n\nHi @Chen\nThe server URL is: hicexplorer.usegalaxy.eu\nThis domain-specific public Galaxy server is sub-server of the usegalaxy.eu server. Your account at both is the same.\nUpload data into your account using these FTP credentials in Filezilla. Leave other settings at default.\n\nHost: ftp.usegalaxy.eu\nUsername: your Galaxy account\u2019s email address\nPassword: your Galaxy account\u2019s password\n\nScreenshot with one test file just now loaded (to make sure it is actually working! it is \nhow-to-find-FTP-upload-URL1742\u00d71224 430 KB\nThanks!"}], [{"role": "user", "text": "I am running into this error when I try to do differential seq analysis in galaxy:\nWarning message:\nIn estimateDisp.default(y = y$counts, design = design, group = group,  :\nNo residual df: setting dispersion to NA\nError in eval(ej, envir = levelsenv) :\nobject \u2018ControlvsTreated\u2019 not found\nCalls: makeContrasts \u2192 eval \u2192 eval"}, {"role": "system", "text": "@Deepika_Chandrasekar\nHave you searched the forum?\nhttps://www.biostars.org/p/212200/\nDoes it answer the question?\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi, I generated tabular formatted gene abundance estimate files \u200bwhich have fpkm and tpm using StringTie.\nThen I want to calculate Correlation of variance (CV) using fpkm or tpm from multiple files.\n\uadf8\ub9bc41942\u00d71892 882 KB\n\nWhat module should I use if I want to combine the \u2018fpkm\u2019 or \u2018tpm\u2019 column only from many files?"}, {"role": "system", "text": "Hi @coldi11,\nI guess that the Multi-Join tool is what you need.\nRegards"}], [{"role": "user", "text": "Hello everyone\nI am trying to upload some protein fasta files into Galaxy.eu and while they load fine with no problems, but when I look in my history the box is grey and says \u201cThis job is waiting to run.\u201d and it has been like this for about an 1.5 hrs whereas before it would begin to run and go green within a few minutes.\nI have tried to restart my laptop and clear my chrome cache but with no success therefore, if I could please have some help it would be much appreciated."}, {"role": "system", "text": "Hi @Calvin_Cornell\njobs can stay in queue for some time when Galaxy is busy. Usually servers have limits on concurrent jobs. I tested upload to Galaxy Europe today, and it was nearly instant for a tiny file.\nHope it all worked for you.\nKind regards,\nIgor"}], [{"role": "user", "text": "Normally I add an requirement in my xml file and conda will install it in a environment and the galaxy tool can use that program. Is there a proper or \u201cofficial\u201d way to install packages in the environment of the tool that are only available in pip?\nI can just use pip to install the package on my system and python can find it, but I was wondering if there is a method to install it only in the environment of that specific galaxy tool."}, {"role": "system", "text": "The recommended way is to add your requirement to Bioconda or conda-forge."}], [{"role": "user", "text": "Hello!\nHow can i delete data created with data manager?\nThanks in advance."}, {"role": "system", "text": "Hi @wormball\nThere isn\u2019t an automatic way to \u201cundo\u201d content created by a Data Manager. In most cases, review and edit/remove the files directly.\nOverview Reference Genomes in Galaxy\nEphemeris Welcome to the Ephemeris documentation! \u2014 Ephemeris 0.10.3 documentation\nGalaxy Search \u00b7 data managers \u00b7 GitHub\nRelated Q&A Topics tagged data-manager"}], [{"role": "user", "text": "Hi there!\nI want to make a genome annotation using a reference fasta + 8 fastq libraries. However, the fastq libraries must be converted to fasta in order to be used as EST for Maker, and there is not enough space in the galaxy quota to do this. Does anyone know how I am supposed to do a Maker annotation with fasta files when the quota has such limited space? Thanks!"}, {"role": "system", "text": "Hi @jradfor8,\nit is possible to request additional space in Galaxy.  You can find more information in Account quotas.\nRegards"}], [{"role": "user", "text": "I want to publish our own tool to the Galaxy Tool Shed. Our tool is our own developed C++ program. Our compiled binaries may not necessarily run in Galaxy servers due to different OS platforms. How could I resolve the tool dependency of our own compiled binaries?  Thank you."}, {"role": "system", "text": "Hi !\nBuild up your packages using conda-forge. There is support for compilation using c, c++ and fortran compilers. See https://conda-forge.org/#contribute\nThere are many examples to get started, for example parmed is now on conda-forge and it started as a PR to staged recipes https://github.com/conda-forge/staged-recipes/pull/8736.\nGood luck!"}], [{"role": "user", "text": "Hi all,\nI have a problem with running a Workflow on a local instance of Galaxy (running on a Ubuntu server).\nIn this workflow, a Flye assembly is fed to a Medaka consensus pipeline together with a bunch of fastq files from a MinION run. This workflow runs beautifully on usegalaxy.eu but on my own server I get the follow error:\n{ \"code_desc\": \"\", \"desc\": \"Fatal error: Exit code 126 ()\", \"error_level\": 3, \"exit_code\": 126, \"type\": \"exit_code\" }\n\nand this is shown in the history:\n\nBecause the same workflow runs fine on usegalaxy.eu I guess it has something to do with the installation of medaka because that was not easy. Installing medaka (including the dependencies) from the admin menu always results in an error with Galaxy hanging and need a restart. This seems to be a know issue (see github). So I installed medaka throught the CLI with conda:\nconda create -y --quiet --override-channels --channel conda-forge --channel bioconda --channel defaults --name __medaka@1.7.2 medaka=1.7.2\n\nand then copied everything to the galaxy environment:\ncp -r ./mambaforge/envs/__medaka@1.7.2 ./galaxy/dependencies/_conda/envs/\n\nGalaxy now sees medaka tool with it\u2019s dependencies resolved but running the workflow thus gives an error.\nAny ideas?\nThanks in advance!\nErik"}, {"role": "system", "text": "see also: Medaka consensus pipeline error \u00b7 Issue #5233 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub"}], [{"role": "user", "text": "Hello.\nI have HIC data (.hic and.cool files) and i can\u2019t visualise them in a circular layout.\nI used \u201cCircos visualizes data in a circular layout\u201d form https://usegalaxy.org/ .\nAny suggestions or comments?\nThank you,"}, {"role": "system", "text": "Hi @Oksanak22\nCircos will require data that is structured in specific formats. This tutorial covers example manipulations with sample data in various original formats.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Visualisation with Circos\n\n  Training material for data visualisation in Galaxy\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nFor help with transforming your current datasets, maybe review the tools in the tool group HiCExplorer? These tutorials have more help.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Epigenetics\n\n  DNA methylation is an epigenetic mechanism used by higher...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Data Manipulation Olympics\n\n  Galaxy is a scientific workflow, data integration, and da...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hi,\nI have been trying to access my galaxy page, but it times out and gives the following error:\n\nGalaxy could not be reached\nYou are seeing this message because a request to Galaxy timed out or was refused. This may be a temporary issue which could be resolved by retrying the operation you were performing. If you receive this message repeatedly or for an extended amount of time, please check for additional information on the Galaxy status page or the @galaxyproject Twitter feed. If the issue is not addressed on those sources, you may report it to the support team at galaxy-bugs@galaxyproject.org with details on what you were trying to do and the URL in the address bar."}, {"role": "system", "text": "Hi @Nikita_Jhaveri1\nAll three of the primary usegalaxy.* servers were down for different short time windows, but all are up as of now. https://status.galaxyproject.org/\nNext time, please include this information wherever you are asking the question. Each server is administered independently, and the URL is needed to confirm or help.\n\n\n\n Nikita_Jhaveri1:\n\nwith details on what you were trying to do and the URL in the address bar\n\n"}], [{"role": "user", "text": "At the recent ASMCUE meeting, I discussed with Mo Heydarian how it would be useful to have a tool similar to one used to get FASTQ reads from SRA, but to get FASTA genomes from NCBI Genbank or RefSeq by accession.\nMo suggested that I could add an item to a github tracker, but I wasn\u2019t sure which tracker within galaxyproject would be appropriate.\nWe talked about how this might need to be limited to \u201csmall genomes\u201d but it would be very useful to those of us who work on bacteria and bacteriophage."}, {"role": "system", "text": "Hi Jim,\nI opened an issue in the Galaxyproject/IUC repo here: https://github.com/galaxyproject/tools-iuc/issues/2526\nPlease feel free to provide background and additional information on the feature request.\nThanks for the great suggestion!\nCheers,\nMo"}], [{"role": "user", "text": "I just set up a fresh galaxy installation, following the instructions on: https://galaxyproject.org/admin/get-galaxy/ . But once I try to install a tool I get the following error:\nError cloning repository: [Errno 2] No such file or directory: \u2018hg\u2019: \u2018hg\u2019\nIt seems to always be the same error with every tool. Can someone here help me out?\nEDIT: Found it. Seems I needed to install mercurial. But it seems this is never mentioned in the installation docs?"}, {"role": "system", "text": "I have added a note to our toolshed installation tutorial that should clarify this. https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/#connect-your-galaxy-to-a-tool-shed\nThanks for bringing it up!"}], [{"role": "user", "text": "Dear everyone!\nI am currently analysing the output of a PE sequencing of 96 samples. I have uploaded the reads and included them into a list to use the tools more efficiently. I am running a workflow using snippy, snippy core, Clustal and so. My problem is that 4/96 snippy jobs failed due to memory allocation problems and that interrupted the whole workflow. Even if I delete the failed runs, the output list is marked in red and cant resume the workflow.\nMy question is: Is there a way for me to run the failed snippy jobs and include the output in the list of the successful ones in order to continue with my workflow?\nI have thought of downloading the successful runs output, running the failed ones again, download these outputs and then after re uploading everything again make a new list but my problem is that the snippy output and the bam files are 54Gb each, way too heavy for me to do.\nI really appreciate your assistance on this "}, {"role": "system", "text": "Hi @Alan\n\n\n\n Alan:\n\nMy question is: Is there a way for me to run the failed snippy jobs and include the output in the list of the successful ones in order to continue with my workflow?\n\n\nThere are two primary solutions for this situation:\n\n\nInstead of deleting failed jobs, rerun instead. The bottom of the tool form will have an extra option to resume dependencies in a way that replaces the original failed datasets.\n\n\nRemove failures during the original workflow run. tools-that-manipulate-elements-within-a-collection\n\n\nMore workflow and collection help is here, including examples of programmatically controlled workflow execution.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Really just a minor nuisance, but if you use usegalaxy.org in a post, it gets recognized as a hyperlink target, but the same doesn\u2019t work for usegalaxy.eu or usegalaxy.org.au (presumably because .eu and .au are not recognized as top-level domains?). It would just be convenient if that could be changed."}, {"role": "system", "text": "To my surprise there is actually settings for this in discourse (discourse is great!). I added the two domains there to the allowlist. Test:\nusegalaxy.eu\nusegalaxy.org.au"}], [{"role": "user", "text": "I would like someone to explain to me what is unassigned ambiguity.\nthanks for advance"}, {"role": "system", "text": "Hi @jimkiriakidis22\nconsider checking the manual by clicking at Subread link available in paragraph under Execute button:\nUnassigned Ambiguity: alignments that overlap two or more features (feature-level sum-\nmarization) or meta-features (meta-feature-level summarization).\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi everyone,\nI am running a UMI-tools extract command that works on HPC:\n(example of nextflow command)\numi_tools extract --extract-method=regex \n\u2013bc-pattern=\".+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})$\" \n-I ${sample_name}_trimmed.fastq \n-S ${sample_name}_umi_cleaned.fastq > ${sample_name}_umi_tools.log\nbut it does not run properly on Galaxy\nI need to specify the following regex so UMI-tools detect the UMI at the end:\n.+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})$\nHowever, when I run it on GA, the + and the $ seem to be dropped:\nIn the log, the pattern comes up as: .(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})\nAnd although UMI runs it fails to correctly detect the adapter and UMI at the end of the read. I end up with a few hundred reads 1-2 bp long instead of several million reads with their proper UMI extracted.\nI also tried to use backslashes :.\\+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})\\$\nand an asterisk\n`.*(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})``\nBut both failed\nThanks for your help"}, {"role": "system", "text": "Hi @abracarambar,\nI opened a PR in order to fix it.\nRegards"}], [{"role": "user", "text": "In my galaxy.yml I have:\nuser_activation_on: true\nactivation_grace_period: 0\n\nIf you are not logged in you are still able to see the tool menu. How can I configure galaxy so only logged in users can see the tools?\nEDIT:\nI can also upload when not logged in\nThanks in advanced"}, {"role": "system", "text": "# Force everyone to log in (disable anonymous access).\n#require_login: false\n"}], [{"role": "user", "text": "Hi,\nI\u2019d like to remove all lines for which no gene symbol is known in order to obtain a dataset with only gene symbol annotated RNAseq expression data. Tried several options with the compute package but it either doesn\u2019t work or my syntax is wrong. Any suggestions?\nHenk"}, {"role": "system", "text": "Dear @Henk,\nI assume your question is regarding a solution in Galaxy. Have you tried the tool Filter Tabular, choosing regex replace value in column, selecting the column of the gene names, and as a regex pattern NA.\nKind regards,\nFlorian"}], [{"role": "user", "text": "The history window has frequent glitches when I try to scroll through it\u2013it goes back to the same spot instead of actually scrolling anywhere. Anyone having this issue? Sometimes refreshing Galaxy helps but not always."}, {"role": "system", "text": "Here\u2019s an issue describing what I think you are encountering: History scrolling can stall even in small histories \u00b7 Issue #14767 \u00b7 galaxyproject/galaxy \u00b7 GitHub\nWe\u2019re aware of this and are working on it \u2013 if you have any extra diagnostic feedback you could provide as to what triggers it (particular histories, contents, slow internet, \u2013 anything) that would be useful to add there.  Otherwise please subscribe to that issue to track a resolution to this."}], [{"role": "user", "text": "I want to upload the fastQ data from ArrayExpress database at here\nI know that there are some ways to transfer fastQ data from NCBI database using SRA accession numbers. These ways help the users to transfer data without the need to downloading and uploading the large fastQ files.\nIs there such a way to directly transfer data using the links of the fastQ files from the mentioned link from ArrayExpress? Or we have to download the fastQ files and then upload them to galaxy?"}, {"role": "system", "text": "Welcome, @Maryam_Momeni\nIn short, yes \nClick into the full sample table view at that link, and you\u2019ll see the accession identifiers per sample. Those should be also available at NCBI SRA. I just confirmed this by testing the first one: https://www.ncbi.nlm.nih.gov/search/all/?term=ERS2519373\nThese tutorials cover how to manipulate sample tables for batch data uploads. That includes sample organization (usually worth it, so highly recommended!). But, you can also just isolate the accessions out of the table, and use a tool like Faster Download and Extract Reads in FASTQ format from NCBI SRA.\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Using Galaxy and Managing your Data\n\n  A collection of microtutorials explaining various feature...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello, I\u2019m trying to run stringtie as precursor to analysis with dexseq2. For my annotation file, I\u2019m using WBcel235 downloaded from NCBI.\nWhen I run stringtie, I get the following error:\n\" An error occurred with this dataset: format gtf database ce11 WARNING: no reference transcripts were found for the genomic sequences where reads were mapped! Please make sure the -G annotation file uses the same naming convention for the genome sequences.\"\nI get the gist of this - Galaxy somehow doesn\u2019t like the format of my annotation file. But I am not sure entirely what is wrong with it, or how to fix this. I\u2019ve copied the first few lines of my annotation file below.\nAny help would be greatly appreciated.\n#gtf-version 2.2\n#!genome-build WBcel235\n#!genome-build-accession NCBI_Assembly:GCF_000002985.6\n#!annotation-source WormBase WS283\nNC_003279.8\tRefSeq\tgene\t3747\t3909\t.\t-\t.\tgene_id \u201cCELE_Y74C9A.6\u201d; transcript_id \u201c\u201d; db_xref \u201cGeneID:353377\u201d; gbkey \u201cGene\u201d; gene \u201cY74C9A.6\u201d; gene_biotype \u201csnoRNA\u201d; locus_tag \u201cCELE_Y74C9A.6\u201d;\nNC_003279.8\tRefSeq\ttranscript\t3747\t3909\t.\t-\t.\tgene_id \u201cCELE_Y74C9A.6\u201d; transcript_id \u201cNR_001477.2\u201d; db_x"}, {"role": "system", "text": "Hi @mmchamp\nThe first thing I notice is header lines in the GTF. Many tools will complain if those are not removed.\nNext, check that the sequence IDs between that reference annotation GTF and the reference genome used for mapping all match (exactly).\n\nGalaxy indexed genomes are usually sourced from UCSC and they also host matching reference annotation. Example: Index of /goldenPath/ce11/bigZips/genes.\nIf you used a custom genome (fasta) instead, use an annotation that pairs with that version of the assembly.\n\nFAQs\n\nunderstanding-input-error-messages\nworking-with-gff-gft-gtf2-gff3-reference-annotation\n"}], [{"role": "user", "text": "How often do I have to update Galaxy in a productive environment (just 2-3 users) in order to keep up with new tools?\nDoes updating Galaxy break a lot of dependencies for installed tools in general? Are there some kind of minor and mojor releases?\nIs this documented anywhere?"}, {"role": "system", "text": "Updating galaxy doesn\u2019t usually break tools. The tool wrapper description doesn\u2019t really change much.\nI think Galaxy 16.01 was the last major change for Galaxy tool compatibility.\nBranches in the repo titled \u2018release_XXX\u2019 are the best way to keep up with the releases rather than using the zipped releases as they contain important bug fixes.\nGalaxy unfortunately doesn\u2019t maintain semver patch versions, you have to use the commit hash for the branch."}], [{"role": "user", "text": "Dear Galaxy community,\nI need to add EXAC_NFE info in VCF.\nI am thinking about useing \u201cSnpSIFT annotate\u201d to annotate the SNP ID with EXAC_NFE as INFO.\nHowever, I could not find a way to download the exac03(hg19) file and import it to Galaxy. (Tried already to find information from ANNOVAR, UCSC, and ExAC Browser, but could not find a way to get the file.)\nCan anyone help me with that?\nOr maybe I am wrong, it is not the way to add EXAC_NFE information.\nThanks a lot\nSusan"}, {"role": "system", "text": "Hi Susan,\nthe simplest way to bring in these annotations would be via the GEMINI toolsuite, just that\na) you\u2019ll have to stop working with VCF format at that point because GEMINI converts VCF to an sqlite database during the annotation step and keeps using this format for all further processing and querying\nb) with the current version of GEMINI, you\u2019ll only have ExAC release 0.3 available\nc) you would have to work on usegalaxy.eu for now because GEMINI, for technical reasons, is currently not available on usegalxy.org (though hopefully this will get fixed in the not so distant future)\nIf you want to try to annotate your VCF directly (SnpSift annotate may or may not work for the task - I never tried to use it for anything but dbSNP data) the data you\u2019re looking for is available from:\nftp://ftp.broadinstitute.org/pub/ExAC_release\nThe files you find there have been compressed with bgzip, but if you specify data type \u201cVCF\u201d on import Galaxy should unpack them for you."}], [{"role": "user", "text": "Hello,\nI\u2019m trying to follow the tutorial to install a local galaxy server using ansible and when running the playbook I get the following error:\nTASK [natefoo.postgresql_objects : Create and drop users] ******************************************************************\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: AttributeError: 'NoneType' object has no attribute '__version__'\nfailed: (item={'name': 'galaxy'}) => {\"ansible_loop_var\": \"item\", \"changed\": false, \"item\": {\"name\": \"galaxy\"}, \"module_stderr\": \"Traceback (most recent call last):\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 107, in <module>\\n    _ansiballz_main()\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 99, in _ansiballz_main\\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 47, in invoke_module\\n    runpy.run_module(mod_name='ansible_collections.community.postgresql.plugins.modules.postgresql_user', init_globals=dict(_module_fqn='ansible_collections.community.postgresql.plugins.modules.postgresql_user', _modlib_path=modlib_path),\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 207, in run_module\\n    return _run_module_code(code, init_globals, run_name, mod_spec)\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 97, in _run_module_code\\n    _run_code(code, mod_globals, init_globals,\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/modules/postgresql_user.py\\\", line 1015, in <module>\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/modules/postgresql_user.py\\\", line 935, in main\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/module_utils/postgres.py\\\", line 187, in get_conn_params\\nAttributeError: 'NoneType' object has no attribute '__version__'\\n\", \"module_stdout\": \"\", \"msg\": \"MODULE FAILURE\\nSee stdout/stderr for the exact error\", \"rc\": 1}\n\n\nI have double and triple checked that my files are setup exactly as the tutorial is so I\u2019m not sure what is going wrong on this part.\nThanks!"}, {"role": "system", "text": "Hi! That role does not support check mode. Please run your playbook without it."}], [{"role": "user", "text": "Hello. I am using bowtie2 for my ChIP-seq analysis but not seeing the \u201ctool standard error\u201d for most of my files, though I can see that the job is successfully performed. It is coming up as \u201cempty\u201d. Consequently, I am not being able to see the alignment rate, reads aligned etc. I am new to bioinformatics tools and relying on Galaxy for all my sequencing analysis. Could you please help with this? I will be grateful.\nThank you."}, {"role": "system", "text": "Hi @TIRTHANKAR_BANDYOPAD,\nwhich version of the tool are you using , and on which Galaxy server are you working?\nWith toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.0+galaxy0 on usegalaxy.eu, the stderr is not empty at least.\nAlso, have you seen the option \u201cSave the bowtie2 mapping statistics to the history\u201d? This will redirect the mapping stats you are looking for from stderr to an extra dataset in your history.\nCheers,\nWolfgang"}], [{"role": "user", "text": "Hi.\nI am going through the galaxy training material \u2019 RNA-seq genes to pathways\u2019 with the following link\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: 3: RNA-seq genes to pathways\n\n  Training material for all kinds of transcriptomics analysis.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nCan anyone help me to find the tool \u2019 Compute an expression on every row\u2019 ?\nMany thanks"}, {"role": "system", "text": "Hi @wcq848677\nthe tool was upgraded to v.2 and it is now called Compute on row. If you select an early version during the job setup (three  blocks icons at the top right corner of the middle window), you\u2019ll see the old name.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hello,\nI am using the main galaxy server at https://usegalaxy.org/\nI have a collection of paired-end RNA-seq data with 30 pairs in it. With just this collection and a few other files, my storage usage sits at 5% (although, I suspect this may be incorrect given the size of the collection?). When I try to run cutadapt on this dataset, the storage usage begins to move up, far past what I would expect it to given that it can only be making the files smaller. After only a few pairs are completed, I start getting a \u201cNo space left on device\u201d error. Even stranger, one pair will fail to run due to this error, but the next 10 will complete successfully. Then a couple more will fail due to the same error. When I let it run completely, it finishes at about 80% storage used. When run multiple times, different pairs fail each time.\nI know something must be going on, but I cannot figure it out. Earlier today I had the entire collection run through cutadapt successfully, but I purged it because it wasn\u2019t working with downstream software.\nFinally, I wanted to mention that running the data as \u201cmultiple datasets\u201d rather than \u201cdataset collection\u201d seems to function as intended. However, I lose the organization of the collection in this case, so I\u2019d like to be able to run it as it stands now.\nAny input is appreciated!"}, {"role": "system", "text": "Hello, @Murphytho Thanks for reporting the issue! \nPlease try a rerun now. The problem was due to a server-side issue at usegalaxy.org, now resolved.\nFor others reading and working at this same server, any job that failed with an error message like this over the last few days should be rerun.\n\nOSError: [Errno 28] No space left on device\n\nFind the message at the end of the stderr on the Job Details report.\n"}], [{"role": "user", "text": "I am trying to upload a database of all bacterial nucleotide sequences (gigantic file - 107 GB). I am using FTP via FileZilla, and it got to 80.5 Gb before giving the error \u201c552 Transfer aborted. File too large\u201d. I read something about a 250Gb limit but I think I should be within that - even with files already on Galaxy, it says I am only using 2.7Gb. Not sure what to do (I am definitely no bioinformatician and it took me two weeks to figure out how to download this file -_-). If there\u2019s an alternative to use a database like this on Galaxy that is already uploaded I am all ears. Below is the full set of messages that keep iterating until I stop the queue.\n\n\n\n\nError:\nFile transfer failed after transferring 294,912 bytes in 1 second\n\n\n\n\nStatus:\nStarting upload of /Volumes/MBLab/Bacterial Nucleotide Database - October 2020.fasta\n\n\nStatus:\nRetrieving directory listing of \u201c/\u201d\u2026\n\n\nCommand:\nPASV\n\n\nResponse:\n227 Entering Passive Mode (129,114,60,56,119,69).\n\n\nCommand:\nREST 0\n\n\nResponse:\n350 Restarting at 0. Send STORE or RETRIEVE to initiate transfer\n\n\nCommand:\nMLSD\n\n\nResponse:\n150 Opening BINARY mode data connection for MLSD\n\n\nResponse:\n226 Transfer complete\n\n\nCommand:\nPASV\n\n\nResponse:\n227 Entering Passive Mode (129,114,60,56,120,177).\n\n\nCommand:\nREST 80530636800\n\n\nResponse:\n350 Restarting at 80530636800. Send STORE or RETRIEVE to initiate transfer\n\n\nCommand:\nSTOR Bacterial Nucleotide Database - October 2020.fasta\n\n\nResponse:\n150 Opening BINARY mode data connection for Bacterial Nucleotide Database - October 2020.fasta\n\n\nResponse:\n552 Transfer aborted. File too large\n\n\n\n"}, {"role": "system", "text": "Discussed via email with @aroebuck\nItems that may help others:\n\nThe maximum size of an FTP file is 50 GB at public Galaxy servers. BAMs are a bit different, the maximum size is 25-35 GB.\n\n\nFAQ: https://galaxyproject.org/support/loading-data/\n\n\n\nSetting up a local Galaxy.\n\n\nExample step-by-step for a basic local Galaxy install with an admin account created\n\n\n\nAlternatives, including cloud options. Many can be easier to administrate \u2013 including the GVL of Cloudman.\n\n\nWays to use Galaxy: https://galaxyproject.org >> Use\nThe GVL version of Cloudman is one choice and AWS offers grants. https://aws.amazon.com/grants/\n\n\nMore help and prior Q&A can be found at this site or at the GTN tutorial site. Start by with keywords like \u201cupload\u201d or \u201ccloud\u201d or \u201cftp\u201d. If your issue remains, start a new topic and be as specific as possible for context: How using Galaxy, where using Galaxy (URL if public server), version of local/cloud Galaxy, special configurations\u2026\nBest,"}], [{"role": "user", "text": "Hello,\nI am new to Galaxy, following the tutorial for CLIP-seq data analysis, and RNA-STAR for 2 datasets has been running for 3 days now (it\u2019s colored in orange and the status is \u201cjob is running\u201d). Is this normal?\nThanks!"}, {"role": "system", "text": "Welcome, @Galina_Glousker\nYes. The public Galaxy servers are shared resources. Everyone\u2019s jobs will run as free resources become available. Some tools queue longer than others, and that is very likely for your case since RNA-Star runs at our largest and busiest cluster. The good news is that the job is now executing (yellow/orange).\nNever interrupt a queued or executing job unless you already know it has a problem you want to fix as new jobs are always added back to the end of the queue.\nThese two FAQs explain the how the job scheduling process works, and you can browser search that whole page to find more help.\n\n#understanding-job-statuses\n#will-my-jobs-keep-running\n\nHope that helps "}], [{"role": "user", "text": "Hi, what is causing the error message here:\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/bin/umi_tools\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/umi_tools.py\", line 61, in main\n    module.main(sys.argv)\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/extract.py\", line 477, in main\n    reads = ReadExtractor(read1, read2)\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/extract_methods.py\", line 544, in __call__\n    raise ValueError('Read sequence: %s is shorter than pattern: %s' % (\nValueError: Read sequence: ACGC is shorter than pattern: NNNXXXXNN\n"}, {"role": "system", "text": "\n\n\n biostaring:\n\nValueError: Read sequence: ACGC is shorter than pattern: NNNXXXXNN\n\n\nHi @biostaring,\nit seems that your FASTQ file contains reads whose length is shorter than the barcode pattern; in order to solve it, you need to filter your reads by length >=10, by using for example cutadapt.\nRegards"}], [{"role": "user", "text": "Hello,\nI am looking to get a file of unmapped reads after RNA star. How do I get that"}, {"role": "system", "text": "Hi @NIKITA_JHAVERI You are after any BAM filtering tool. Alternatively, consider HiSAT2 instead of RNA_STAR and in advanced options > outputs activate Write unaligned reads (in fastq format) to separate file(s).\nHope this helps.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi all,\nI\u2019m running galaxy on a compute cluster with Slurm and I\u2019m having trouble running R. We have python running with the .venv, but the only way I\u2019ve been able to run R is by install R on a node, which isn\u2019t sustainable. What do you recommend for setting up R for galaxy with slurm?\nThanks!"}, {"role": "system", "text": "Hi @jdlamstein\nI think the solution is to run a Conda .venv, and to not use a Python .venv. From the start, so that everything else in the local configuration also uses it.\nBut you can decide. The \u201cPython\u201d way will make not just R difficult to manage but also many other dependencies troublesome. Others can correct me (and share back how) if some hybrid works for them.\nReferences:\n\n\nGalaxy Documentation \u2014 Galaxy Project 23.0.2.dev0 documentation\n\n\nSee here first \u2192 Framework Dependencies \u2014 Galaxy Project 23.0.2.dev0 documentation\n\n\n\n\nGalaxy Training \u2014 Admin\n\n\nThen see here \u2192 Connecting Galaxy to a compute cluster\n\n\n\n\nHope that helps! "}], [{"role": "user", "text": "I would either use Ubuntu or Debian. What do you recommend in case I can choose?"}, {"role": "system", "text": "Both are absolutely fine choices."}], [{"role": "user", "text": "I have used the repeatexplorer2 tool to analyze paired-end sequencing data made from a genomic DNA library of a human cell line. The tool appears to have worked successfully and finds many satellite clusters but for rDNA it says \u201cnot found\u201d. What needs to be done differently here to analyze the rDNA? Or is it likely among the \u201cother\u201d category and just not recognized as rDNA? Thanks."}, {"role": "system", "text": "First of all it looks like repeatexplorer is a fairly specific pipeline with its own galaxy server and user registration. On the website https://repeatexplorer-elixir.cerit-sc.cz/ I see the following text:\n\nIf you need help with RepeatExplorer or you want to report a problem and our wiki is not able to give you answers, please contact server administrator.\n\nI think the server administrator can give you a better answer.\nIf you ask your question to the server administrator you may need to clarify what you mean with rDNA and if \u201cnot found\u201d is an error or an output message. I dont know the tool but I would guess that you get more output then just \u201cnot found\u201d (I can be wrong ofcourse).\nDo you mean recombinant DNA? And are it just fastq or fasta files? In addition to that if you do mean recombinant DNA I would guess it is very possible to not find satelites because mostly satelites are non-coding and recombinant DNA (created in a lab) is only coding.\nI hope for now my reply is helpfull and maybe someone with experience with this tool will also see your post."}], [{"role": "user", "text": "Hi,\nI have generated the file using snpsift annotate (snpEff tool). This file is not recognised by Gemini load. Possibly, there is an issued in type of input files.\nWhat should be the input fie in the 2nd option?\n1.Variant input file in VCF format\n2.View dataEdit attributesDelete\nVCF File with ID field annotated (e.g. dbSNP.vcf)\nneed guidance here. Please\u2026\nThank you."}, {"role": "system", "text": "Hi @Shuchi\nThe tool is available at UseGalaxy.eu and UseGalaxy.org.au.\nAs noted on the tool form, any vcf dataset is accepted as an input but it must meet both of these criteria:\n\nBe based on the Human GRCh37 (hg19) genome build assembly\nHave hg19 assigned as the database metadata attribute.\n\n\ngemini-vcf-input808\u00d7296 27.1 KB\n\nFAQs: Galaxy Training!"}], [{"role": "user", "text": "Hi, Today i installed the histogram tool in the graph/Display Data , and when i tried to use i found out it\u2019s written in python2 so i get syntax errors , is a solution out there or should i wait until it\u2019s upgraded by the team."}, {"role": "system", "text": "I can\u2019t speak for this tool, but some legacy/unmaintained tools are unlikely to receive updates.\nYou can deal with such tools using the technique described here."}], [{"role": "user", "text": "I would like to use the Galaxy/hutlab format to write a paper about gut microbiota.\nSo, I would like to ask four questions in order to apply to the IRB.\n\u2460If an unregistered user uploads gut microbiota data, how long does the data take to be deleted? I want to know the exact period.\n\u2461 After the data is completely deleted, will it never be restored?\n\u2462Will the data stored on your site, Galaxy/hutlab be used without the consent of the uploader?\n\u2463Will my uploaded data and analysis results are seen by anyone other than me?\nI have already browsed the following sites\uff1a\n\u30fbManaging Datasets in Galaxy - Galaxy Community Hub\n\u30fbDownloading and Deleting Data in Galaxy"}, {"role": "system", "text": "Hi @m.mihula\nEach public server has its own data policies including terms of service that cover your questions. Some post details on the homepage of the server or under the Help menu.\nYou are working at the huttenhower.sph.harvard.edu/galaxy public Galaxy site, correct? If so, you\u2019ll need to contact them directly for more feedback. Click into this post \u2013 it explains why and how to reach them.\n\n  \n    \n    \n    Problems with Plot Cladogram -- http://huttenhower.sph.harvard.edu/galaxy/ \n  \n  \n    Hi @liuyushu \nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server. \nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers). \nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (belo\u2026\n  \n\n\nShort answer:\n\nAny data that is under restricted privacy requirements would not be appropriate for a public Galaxy server or probably any open public resource even outside of Galaxy.\nConsider a private Galaxy choice. The AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. It was designed to handle restricted data. AnVIL - Galaxy Community Hub\n\nThe Huttenhower lab hosts custom tools that might not work well on the more current releases of Galaxy, and that includes the AnVIL configuration. This is something you could ask them about.\n\nRelated Problem displaying data on UCSC Cell Browser - #3 by jennaj\n&& Default and advanced settings for data privacy - #2 by marten"}], [{"role": "user", "text": "I have used  GROMACS - gmx, 2020.1-Ubuntu-2020.1-1. I runned this command.                                                        \u201cgmx grompp -f nvt.mdp -c em.gro -r em.gro -p topol.top -n index.ndx -o nvt.tpr\u201d\nBut I got this error while running this command.\nWARNING 1 [file topol.top, line 27620]:\nThe GROMOS force fields have been parametrized with a physically\nincorrect multiple-time-stepping scheme for a twin-range cut-off. When\nused with a single-range cut-off (or a correct Trotter\nmultiple-time-stepping scheme), physical properties, such as the density,\nmight differ from the intended values. Since there are researchers\nactively working on validating GROMOS with modern integrators we have not\nyet removed the GROMOS force fields, but you should be aware of these\nissues and check if molecules in your system are affected before\nproceeding. Further information is available at\nhttps://redmine.gromacs.org/issues/2884 , and a longer explanation of our\ndecision to remove physically incorrect algorithms can be found at\nOn The Importance of Accurate Algorithms for Reliable Molecular Dynamics Simulations | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage .\nSetting gen_seed to 237804172\nVelocities were taken from a Maxwell distribution at 300 K\nNOTE 1 [file topol.top, line 27620]:\nThe bond in molecule-type UNK between atoms 20 C4 and 21 C3 has an\nestimated oscillational period of 1.9e-02 ps, which is less than 10 times\nthe time step of 2.0e-03 ps.\nMaybe you forgot to change the constraints mdp option.\n\nProgram:     gmx grompp, version 2020.1-Ubuntu-2020.1-1\nSource file: src/gromacs/gmxpreprocess/readir.cpp (line 2701)\nFatal error:\nGroup Protein_UNK referenced in the .mdp file was not found in the index file.\nGroup names must match either [moleculetype] names or custom index group\nnames, in which case you must supply an index file to the \u2018-n\u2019 option\nof grompp.\nFor more information and tips for troubleshooting, please check the GROMACS\nwebsite at Common Errors \u2014 GROMACS webpage https://www.gromacs.org documentation"}, {"role": "system", "text": "Hi @Anitha\nThis forum is focused on running tools in the Galaxy ecosystem. https://galaxyproject.org\nCommand-line usage would be better asked at general bioinformatics forums or the author\u2019s resources.\n\nAbraham, M. J., Murtola, T., Schulz, R., P\u00e1ll, S., Smith, J. C., Hess, B., & Lindahl, E. (2015). GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX, 1\u20132, 19\u201325. https://doi.org/10.1016/j.softx.2015.06.001\n\n\nIf you want to try in Galaxy, or to just review our example data and curated information, please see:\n\n  \n      \n\n      Galaxy Training Network\n  \n\n  \n    \n\nGalaxy Training: Running molecular dynamics simulations using GROMACS\n\n  Modelling, simulation and analysis of biomolecular systems\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"}], [{"role": "user", "text": "Hello!\nWhen some job ends with error, depending jobs become paused. And when i push \u201crun again\u201d button, there is \u201cResume dependencies from this job\u201d checkmark. However when i set this mark, often no job is resumed. Moreover, if i push \u201crun again\u201d again, this checkmark disappears, so i can not resume these jobs at all. Is there a way to resume such jobs in this case, perhaps with another input? And why jobs are not resumed?\nThanks in advance."}, {"role": "system", "text": "Hi @wormball\nA paused job usually indicates that an upstream job was in an error state. It may not be the job immediately prior but somewhere earlier in the analysis. Or, maybe an input that the tool is expecting is not connected in the workflow? You might want to open the workflow in the editor to check connections between tools (and reset those if needed).\nWorking directly in the history: If an upstream errored job is rerun, the tool form will have an extra option of the form to \u201cresume paused jobs\u201d. If successful, then downstream jobs should process.\nShould you be using collections:\n\nYou can rerun individual elements that failed. Failed elements (red datasets) in a collection will be unhidden by default. Empty elements in a collection may be harder to find/examine/rerun if they didn\u2019t technically fail.\nThere are tools to directly manipulate failed or empty elements out of collections. These tools are found under the `Collection Operations\" section of the tool panel (using default tool sections). Examples include \u201cFilter failed datasets\u201d and \u201cFilter empty datasets\u201d \u2013 those are generally more useful when the collection contains many samples and some do not process well and are to be discarded.\n\nIf that doesn\u2019t help, and it may not, please post back 1) the version of Galaxy you are running and the last update date or the changeset and 2) some screenshots of what is happening. I\u2019m assuming that you are still working in your own Galaxy \u2013 but if the behavior is at a public server please let us know which one."}], [{"role": "user", "text": "I\u2019ve been working on a single cell RNA-seq dataset (SMARTSeq, Nextera libraries) with RNASTAR and StringTie, and have visualised the total dataset using Seurat. I\u2019d now like to look at differential expression between two groups but I\u2019m having trouble working out how to do this with the TPM data generated in StringTie. Could someone please advise which DE tools can use this as inputs? Or if there is a way to do DE analyses within the Galaxy version of Seurat? I\u2019ve tried DESeq2 a few times but it hasn\u2019t worked (and the options for input file didn\u2019t sound quite right as either raw counts or TPM from kallisto/salmon/sailfish, rather than StringTie). I haven\u2019t been able to find any tutorials that discuss DE after StringTie specifically. Any advice much appreciated!"}, {"role": "system", "text": "Hi @carolyn_nielsen,  a detailed explanation of the necessary steps can be found in:\n\n  \n      \n      training.galaxyproject.org\n  \n  \n    \n\nGalaxy Training: De novo transcriptome reconstruction with RNA-Seq\n\nTraining material for all kinds of transcriptomics analysis.\n\n\n  \n  \n    \n    \n  \n  \n\n\nCheers."}], [{"role": "user", "text": "Hello,\nIt has been a couple days that I ran a command (classify.seqs) and it haven\u2019t started yet.\nI reran it twice and same, it is taking so long.\ncould you please advise?\nThanks"}, {"role": "system", "text": "Hi @Funnyme186\nit depends on available resources. If a job uses a lot of CPUs, it might stay in queue for some time if the server is busy. Deleting and resubmitting a job does not help, if not opposite, because resubmitted job might be at the end of the queue.\nKind regards,\nIgor"}], [{"role": "user", "text": "Hi. I\u2019m running rnaSPAdes with the default parameters but searching in the manual, rnaSPAdes supports assemblying with strand-specific RNA-Seq dataset, which is different to FR and RF orientation of paired reads.\nHow can I include the flag --ss when setting the running parameters?\nThanks for your help!"}, {"role": "system", "text": "Hi, the Galaxy wrapper for rnaSPAdes will be updated soon and the --ss flag will be included. Regards, Stephan"}], [{"role": "user", "text": "Hi. I was trying to generate tracks for a microaray data file from mouse, but there is problem with the genome build for Mouse Dec 2011. (GRCm38/mm10). There is error message saying \u201cThe selected genome have not been set up with a Ensembl gene track. If you require this functionality for the selected genome, please contact the HyperBrowser team.\u201d comes up. I can\u2019t figure out a way to contact the Genomic Hyperbrowser team. If anyone knows the solutions to above problem please help."}, {"role": "system", "text": "\n\n\n ahansda:\n\nGenomic Hyperbrowser\n\n\nSometimes public Galaxy servers include contact information on their server\u2019s home page and sometimes they include it in their Galaxy Platform Directory listing. This one has it on server home page \u2013 find it on the \u201cAbout\u201d tab.\n\n\nGalaxy Platform Directory. Search on \u201cPublic Servers\u201d tab to find listings for known public Galaxy servers: https://galaxyproject.org/use/genomic-hyperbrowser/\n\n\nThe Genomic Hyperbrowser listing: https://galaxyproject.org/use/genomic-hyperbrowser/\n\n\nThe Genomic Hyperbrowser server homepage: https://hyperbrowser.uio.no/\n\n\nHope that helps!"}], [{"role": "user", "text": "When I go to usegalaxy.eu I can see that it has Viral (2019) Kraken 2 database installed. But when I go to my kraken 2 database manager, I do only see options to add Standard, Minikraken, Special, and Custom. How could I add Viral database from Kraken 2 to my Galaxy instance like usegalaxy.eu did?"}, {"role": "system", "text": "Hi @kwonhur,\nin your datamanager you can choose custom and than provide your own FASTA file.\n\nHope that helps,\nBjoern"}], [{"role": "user", "text": "I am using galaxy for Trinity assembly data is only 3GB (1sample) 3GB (2 sample). After refreshing also, It is showing \u201cJob is waiting to run\u201d. This is insect data which came late by company. If I could not complete in this week, we will not able to catch insects for validation.\nKindly help to resolve this issue."}, {"role": "system", "text": "Hi @naina_goel\nThe public Galaxy servers execute jobs as the shared computational resources become available.\nLeave queued jobs queued. Avoid deleting and rerunning, as that places new jobs back at the end of the queue.\nFAQs\n\nmy-jobs-arent-running\nunderstanding-job-statuses\n\nUpdate: there is a new server issue. Queued jobs should not be impacted. Server issue at UseGalaxy.org September 2, 2022 -- Status = OPEN ISSUE - #2 by jennaj"}], [{"role": "user", "text": "Hello,\nI have been trying to run the fastq de-interlacer tool for weeks, several times, on different fastq files.\nThe tool takes too long to proceed, staying orange for days.\nIs there any known issue with the de-interlacer tool?\nThank you, have a good day."}, {"role": "system", "text": "Hi @ambrette,\nthe problem is that this tool relies on Python code, which means that it is not very computationally efficient. I suggest you split the file into two (or more) datasets and run each one in parallel. After that, you can merge the corresponding forward and reverse reads.\nRegards"}], [{"role": "user", "text": "Hello,\nI\u2019ve been waiting for >12 hours since uploading files for processing by UCHIME, but they have been stuck in the same status since yesterday. Some files are in gray and some in yellow (job currently running), but none are completed. Is this normal? I was under the impression that UCHIME is not very computationally heavy.\nThanks,\nShu"}, {"role": "system", "text": "Welcome, @lyehui\nYes, that sounds like normal processing. When jobs queue (gray), that means they are waiting for appropriate resources (cluster nodes) to free up. The time spent executing (yellow) is the complete runtime (dispatching, processing, then results written back to Galaxy). The job details view (  per dataset) will report exactly how long the processing itself took to run, and some servers will report even more information like estimated costs, etc.\nExample of prior Q&A here about how jobs execute Topics tagged queued-gray-datasets\nAlso, please see our FAQs\n\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#will-my-jobs-keep-running\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\n"}], [{"role": "user", "text": "Hello\nI follow Classification in Machine Learning, but\nMy history turns red when using the Generalized Linear Model tool?\nWhile I follow all the same steps of training\nI tried to use this tool in other galaxies and I came to the same conclusion\nI sent you the link of this story\nhttps://usegalaxy.eu/u/gh.maaryaam1996/h/unnamed-history"}, {"role": "system", "text": "Hi @maryam-gh99,\ncould you try to run it again by using \u201cClass\u201d instead of \u201cclass\u201d? According to the tabular data, the first letter should be uppercase.\nRegards"}], [{"role": "user", "text": "Dear all, Suppose to have a collection of viral reads from NGS (Illumina) technology in fastq format. After the usual pre-processing step (addressed by fastp), I need to remove the host sequences (contaminants) without having the reference genome (I cannot use bowtie2 and samtools for mapping, of course). I have ready some approaches, but I am still not sure. The goal of my project is to identify the correct taxonomy of the viral reads I have. In detail, we always know the host of our sample even if the reference genome is not available, like bat, rodents, human, or mosquito. Please, can someone suggest an appropriate strategy/starting point/approach? Thanks for your support."}, {"role": "system", "text": "Could you use blast? Without reference it will difficult\u2026 In some cases if you have used specific primers you can filter on that, or on the length of the reads. Did you do some kind of amplicon sequencing?"}], [{"role": "user", "text": "What is the difference between the P-value and the P-adj columns in the DESeq2 data table? Which of these columns is more important for assessing significant differences in gene expression?"}, {"role": "system", "text": "Hi @Hunter,\nas probably you know, the p-value is a measure of how likely you are to get a specific gene if no real difference exists. A p-value threshold of  0.05 states that there is a 5% chance that the result is a false positive (considered differentially expressed when is not).\nWhile 5% is acceptable for one test, if we do a lot of tests on the data, then this 5% can result in a large number of false positives (e.g. a differential expression analysis of 8.000 expressed genes would include 8.000 * 0.05 = 400 false positives!) An adjusted p-value aims to overcome the problems due to multiple testing by reducing the p-value threshold from the common 0.05 to a more reasonable value.\nThen, depending on how strict you want the analysis to be, you should choose one or the other value.\nRegards"}], [{"role": "user", "text": "I have issue with Cutadapt. Sometimes it works normally, but often this tool won\u2019t even start running. Everything is gray, and it can stay like that for hours. When I try something else, like Fastqc, there is no similar problem with running. How can I resolve this problem? Is something wrong with server?\nThank you for answers! "}, {"role": "system", "text": "Welcome, @ive_s\nDifferent tools use different computational resources. Some resources are busier than others.\nIt sounds like your jobs are queued \u2013 and that is normal/expected at public Galaxy servers. These are shared resources, and your jobs will execute when resources become available.\nI added a tag to your post that points to many prior Q&A topics about this. Or you can use this search link: Topics tagged queued-gray-datasets\nFAQs: Galaxy Support\nStart here: Datasets and how jobs execute\nSelected public Galaxy resource/server status page: https://galaxyproject.statuspage.io/\nThanks!"}], [{"role": "user", "text": "Hi everyone,\nI am new to NGS data analysis. I have done various bioinformatics work and I am also comfortable with programming in Python. May I know the smart way of learning NGS data analysis especially the data that I got from Ion Torrent PGM and Nanopore MinION."}, {"role": "system", "text": "Hi @Thotakura_Pragna,\nthere are a few tutorials in the Galaxy Training Network which describe the required steps for analyzing Nanopore data:\nGenome assembly\n\nChloroplast genome assembly\n\nGenome Assembly of MRSA using Oxford Nanopore MinION Data.\n\nMetagenomics\n\n16S Microbial analysis with Nanopore data\nAntibiotic resistance detection\n\nRegards"}], [{"role": "user", "text": "trying to configure the Okta OIDC client. Getting error after authentication on Okta.\nsocial_core.exceptions.AuthTokenError: Token error: Signature verification failed\nshould the discovery URL be configured somewhere? How does it know the jwks_uri to validate tokens?\nThanks"}, {"role": "user", "text": "found my issue. Documentation could use a little clarity. While it states the API URL should be:\nThis has subsequently had the \u2018/v1/authorize\u2019 removed. In productive deployments, this will likely resemble:\nhttps://{company}.okta.com/oauth2/{authServerId}/\nThe source library documentation states:\nhttps://python-social-auth.readthedocs.io/en/latest/backends/okta.html\nPlease note, do not use the  /oauth2/default  endpoint for Okta authentication:\n\u201cdefault\u201d is the API authServerID that comes with Okta. Usually oidc clients have no issues using it. So when configuring galaxy oidc_config.xml and oidc_backend_config.xml, instead of https://{company}.okta.com/oauth2/{authServerId}/ use https://${company}.okta.com/oauth2/\ncheers!"}], [{"role": "user", "text": "Hi Everyone.\nI followed the specific workflow which is fastq\u2013>tophats\u2013>HTseq. I ran tophat and it worked well. But now I am confused in the HTseq. The tool asks for BAM file which is from the tophats while it also asks for GTF which is I think the genome reference GTF file\u2026? Does that mean I have to load the human reference GTF file which I used for tophats (used in build index)?\nScreen Shot 2020-07-31 at 12.06.06 PM1440\u00d7900 365 KB\n"}, {"role": "system", "text": "You need to use a GFF/GTF file (which is a feature list) that corresponds to the reference sequence you used as the reference (a sequence) you used in Tophat. So if you used the human h19 reference genome you should use a GFF/GTF file corresponding to the h19 reference. You can probably find the file you need here: http://useast.ensembl.org/info/data/ftp/index.html"}], [{"role": "user", "text": "Today I can\u2019t even reach the site and the service has not worked properly for me since May 17th. Any clues as to why that is?"}, {"role": "system", "text": "Hi @Biljana,\nThere is a performance issue with our main storage and this is affecting the reliability of the Galaxy service. We are investigating the source of the problem."}], [{"role": "user", "text": "Hi all,\nI am having trouble with the core gene alignment generated by Roary.\nI used Prokka to annotate 69 genomes of bacteria belonging the same genus. Then, I used the gff3 files in Roary to generate a core gene alignment.\nEverything worked out nicely expect that I am not able to use that core gene alignment because the names given to the genomes are very weird (for example \u201cdataset_00b65bd2-68c1-4486-92a6-8c88170ac99d\u201d).\nI am not able to track back those names to find out to which genomes they correspond because they are not the names of the ggf3 files nor the names of the sequence region. I don\u2019t know if I am doing something wrong or if it is a bug.\nHas someone experienced something similar?\nThanks for your help.\nBest,\nArthur"}, {"role": "system", "text": "Hi @Arthur.muller,\nthank you so much for your report. This problem has been fixed, but until January we won\u2019t be able to update the tool version in usegalaxy.eu. I suggest to you use usegalaxy.org, since this instance includes the correct tool.\nRegards"}], [{"role": "user", "text": "I am using a local Galaxy instance and I want to use report to generate reports of results that I have generated. I am using ireport (Galaxy version 1.1) and providing a previously generated webpage which can be used. I tried it with several different results and it keeps giving this error - \u2018base64\u2019 is not a text encoding; use codecs.encode() to handle arbitrary codecs. I saw an earlier post (IReport issue with encode base64?) that says that this is because of the difference in py2/p3 and will be rectified. How can I rectify it? any help or suggestions is highly appreciated. Thank you!"}, {"role": "system", "text": "Hi @priyanka8590\nWorkflow reports are now built-in to Galaxy. These are generated by default every time a workflow is executed. Release notes (see 21.01 for the original, and later releases for updates): https://docs.galaxyproject.org/\nGo to User \u2192 Workflow Invocations and expand the details per invocation. You\u2019ll find a link to an interactive report and a PDF version available for download.\nScreenshot\n\nworkflow-report1434\u00d7582 51.8 KB\n\n\nThe older iReport version was never updated from the original Python 2 environment, and even that may not be enough to make it useable."}], [{"role": "user", "text": "How can I use a config file (yaml, json, etc) as an input to the workflow, to set workflow parameters and tool settings?\nFor example, set reference genome, max insert size for an aligner, min length after trimming for the adapter trimmer, etc - all in 1 file.\nWe currently use multiple workflow parameters such as ${max insert size}, etc. There is also a newer method, using Simple inputs used for workflow logic. Apparently, both of these methods require the user to type in or select 1 parameter at a time. The config file sets multiple parameters per config.\nFor many of our users, this would be more robust and faster than selecting these every time the workflow is run. The config file also provides a nice record of the most frequently changed parameters/settings, for record keeping and for debugging.\nIn most cases, the user changes only a minority of the lines (if any) in a file such as this."}, {"role": "system", "text": "\n\n\n tshtatland:\n\nSimple inputs used for workflow logic\n\n\nThese can have pre-set content. But if the users need to adjust these, then you might want to create copies/variations of the workflow for specific genomes or analysis projects.\nThere isn\u2019t a way to use a config file that I know of through the web interface, but it might be possible when running a workflow through the API. You\u2019ll need to test that out as a potential solution \u2013 IF your users are able/willing to run workflows that way. API help: https://docs.galaxyproject.org/en/master/api_doc.html\nIf you want to make a request to the developers about this, two choices:\n\nVet the request first at Gitter: https://gitter.im/galaxyproject/Lobby\n\nDirectly create an enhancement request issue: https://github.com/galaxyproject/galaxy\n\n\nThanks!"}], [{"role": "user", "text": "For the Maker genome annotation tool in Galaxy, in the Repeat Masking section, there are options for choosing a repeat library source.\nThe listed options are:\nDFam (curated)\nDFam (full version)\nCustom library of repeats\nDisable repeats\nWhen using MAKER on the command line, here are the options for Repeat Masking:\nmodel_org=all #select a model organism for RepBase masking in RepeatMasker\nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker\nrepeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner\nrm_gff= #pre-identified repeat elements from an external GFF3 file\nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no\nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)\nHow would I use a DFam database on the command line version?\nFor example, on Galaxy I use a DFam curated database for arthropoda, which I can type as the repeat source species.\nBut if I repeat this on the command line, do I run RepeatMasker separately with the database I want, and then input this into Maker?"}, {"role": "system", "text": "Hi @ssell\nMaker runs RepeatMasker as part of the processing.\n\nFor using the tool at the command line, please see their documentation, wiki, demos, tutorials: Yandell Lab - Software - MAKER\n\nExample: MAKER Tutorial 2013 - GMOD\n\n\nIf you are curious about how Galaxy wrapped the tool, details are here: Galaxy | Tool Shed\n\nThere is test data available at the development repository and test cases within the tool\u2019s XML file. You could load that data into a Galaxy history, then run simple jobs using the same parameters as the tests do. Galaxy will construct a command line that will be reported under the \u201cJob Details\u201d page for most results (. You wouldn\u2019t use that command line yourself \u2013 it is specific to the server \u2013 but maybe it is informative.\n"}], [{"role": "user", "text": "Hello.\nI\u2019m new in bioinformatics and in galaxy plataform.\nI would like to know about privacy of my sequence data. When I upload my shotgun data, is it available to the public?\nThank you."}, {"role": "system", "text": "By default user data is accessible but unlisted (so theoretically private). You can set permissions in a way that everything including new uploads is always private to you unless you explicitly share it.\nFor details: https://galaxyproject.org/learn/privacy-features/"}], [{"role": "user", "text": "Workflow don\u2019t want to start\u2026"}, {"role": "system", "text": "\n\n\n ipoutcha_thomas:\n\nWorkflow don\u2019t want to start\n\n\nAll of the public servers are very very busy. The best strategy is to leave the workflow queued \u2013 it will run as resources become available. Next time you start a workflow, consider including the option to send yourself notifications (per step or as a whole). Avoid restarting \u2013 that places all of the work back in the end of the queue, further extending wait time.\nThe processing of workflow tools is similar to how tools in the history will process, but with the added advantage that tools will run serially without any more interaction from you. Some steps/tools may run quicker than others \u2013 that\u2019s due to different resource requirements per tool. https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_job_status.html"}], [{"role": "user", "text": "On the upper nav, there is an option to enable/disable Scratchbook. I see no documentation on what Scratchbook is and how to use it. What is it?"}, {"role": "system", "text": "It is an in-page window manager. Take a tour."}], [{"role": "user", "text": "hello,\n.\ni am working with candida genome project.\ni am new with bioinformatics science,i want to use bwa tools to alignment genome strain to  reference genome\nthe refrence stain data easy to download, i need help to download fastaq1 and fastaq2 for the second genome,the SRA number is avilable, i found deposit data at ENA  at FASTQ files (Galaxy).\nso is that right to download the two fastaq file and used directly without fastaqc"}, {"role": "system", "text": "Hi @dina_yamin!\nYes this is correct. You could also use https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fastq_dump/2.9.1.3 to download from SRA.\nHappy research!\nBjoern"}], [{"role": "user", "text": "I have been trying to create a Galaxy InteractoMIX account but have not succeeded. The website says to email interactomix.galaxy@gmail.com to create an account, but I have reached out several times and have not received a response. Is there another way to create an account?"}, {"role": "system", "text": "Hi @christos-efthymiou,\nI suggest you write to Narcis Fernandes-Fuentes or Baldo Oliva, whose emails you can find in the Galaxy InteractoMIX paper.\nRegards"}], [{"role": "user", "text": "I am trying to follow this transcriptomic tutorial, I have a question about the reference files that have been used in the tutorial.\nstar_miRNA_seq.fasta\nmature_miRNA_AT.fasta\nmiRNA_stem-loop_seq.fasta\n\nwhere these files are publically available? Are these files from miRBase database? And what is the star file?"}, {"role": "system", "text": "Hi @Arslan_Tariq,\nthose files are available on the PmiREN website; all of them correspond to Arabidopsis thaliana. The star file includes the sequences that are complementary to the main miRNA sequences (originated from the same hairpin structure); the main sequences are considered the functional ones, but because there are no sufficient experimental evidence to determine if the complementary sequences are not functional, those are also included in the target identification analysis.\nRegards"}], [{"role": "user", "text": "Hello.  Two questions\n\nDoes anyone know whether there is a gemini for hg38?\nIf there is not a gemini for hg38 (only hg19) what is the best alternative?\n\nThank you."}, {"role": "system", "text": "Hello @bionewbie\n\n\n\n bionewbie:\n\nDoes anyone know whether there is a gemini for hg38?\n\n\nThe Galaxy wrapped version of Gemini works with the hg19 genome. There are technical reasons why hg38 is not supported.\nQuote from the tool form, as hosted at https://usegalaxy.eu: Gemini load\n\nOnly build 37 (aka hg19) of the human genome is supported.\n\nGTN Tutorial that covers an example usage: https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/exome-seq/tutorial.html\n\n\n\n bionewbie:\n\nIf there is not a gemini for hg38 (only hg19) what is the best alternative?\n\n\nGemini has unique functionality. Annotation and parsing tools are all one package. But there are many other tools to annotate/parse VCF datasets. Please see this prior Q&A for some tips: GEMINI load alternatives\nBest!"}], [{"role": "user", "text": "Good Morning! I hereby inform you that my assembly has not yet started to run, as you are saying that it is still pendete to run. I would like to know if such an event is normal and how long it takes to start."}, {"role": "system", "text": "@angelo_braga\nIt is normal for jobs (any tool) to queue first, then execute.\nThe timing depends on the tool, how many jobs you have running, and how many other people are using the shared resources at the same public Galaxy server.\nAllow queued jobs to remain queued. If you deleted and rerun, the new job is placed back at the end of the queue again, extending wait time. The exception is if you already know that there is a problem with the tool run and need to start over anyway.\nI added a tag to your post that covers prior Q&A and FAQs about how jobs execute. Click on it to review if things are still not clear.\nThanks!"}]]