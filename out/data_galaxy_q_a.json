[
  [
    {
      "role": "user",
      "text": "Hi, I have a very basic notebook running in our local galaxy portal with interactive tools.\nAs you see, put() and get() functions are not recognized:\n\nScreen Shot 2021-08-18 at 13.30.25859\u00d7789 63 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/573126de29eb324f03006cbeb9e6facf4e656804.png)\nThe tool-wrapper is the following taken from some official galaxy jupyterlab/jupyternotebook tutorial page.\nWhat might I be missing to be able to have get() and put() working?\nTool wrapper:\n<tool id=\"interactive_tool_jupyter_notebook\" tool_type=\"interactive\" name=\"General Jupyter Notebook\" version=\"0.1\">\n  <description>Default empty notebook, reuse a previous one, or upload a new</description>\n  <requirements>\n    <container type=\"docker\">quay.io/bgruening/docker-jupyter-notebook:2021-03-05</container>\n  </requirements>\n    <entry_points>\n        <entry_point name=\"Jupyter Interactive Tool\" requires_domain=\"True\">\n            <port>8888</port>\n            <url>lab</url>\n        </entry_point>\n    </entry_points>\n    <environment_variables>\n        <environment_variable name=\"HISTORY_ID\">$__history_id__</environment_variable>\n        <environment_variable name=\"REMOTE_HOST\">$__galaxy_url__</environment_variable>\n        <environment_variable name=\"GALAXY_WEB_PORT\">8080</environment_variable>\n        <environment_variable name=\"GALAXY_URL\">$__galaxy_url__</environment_variable>\n        <environment_variable name=\"API_KEY\" inject=\"api_key\" />\n    </environment_variables>\n    <command detect_errors=\"aggressive\"><![CDATA[\n        #import re\n        export GALAXY_WORKING_DIR=`pwd` &&\n        mkdir -p ./jupyter/outputs/ &&\n        mkdir -p ./jupyter/data &&\n\n        #set $cleaned_name = re.sub('[^\\w\\-\\.]', '_', str($input.element_identifier))\n        ln -sf '$input' './jupyter/data/${cleaned_name}' &&\n\n        ## change into the directory where the notebooks are located\n        cd ./jupyter/ &&\n        export PATH=/home/jovyan/.local/bin:\\$PATH &&\n\n        #if $mode.mode_select == 'scratch':\n            ## copy default notebook\n            cp '$__tool_directory__/default_notebook.ipynb' ./ipython_galaxy_notebook.ipynb &&\n            jupyter trust ./ipython_galaxy_notebook.ipynb &&\n            jupyter lab --allow-root --no-browser  --ServerApp.token=''  &&\n            cp ./ipython_galaxy_notebook.ipynb '$jupyter_notebook'\n\n        #else:\n            #set $cleaned_name = re.sub('[^\\w\\-\\.]', '_', str($input.element_identifier))\n            cp '$mode.ipynb' ./${cleaned_name}.ipynb &&\n            jupyter trust ./${cleaned_name}.ipynb &&\n\n            #if $mode.run_it:\n                jupyter nbconvert --to notebook --execute --output ./ipython_galaxy_notebook.ipynb --allow-errors  ./*.ipynb &&\n            #else:\n                jupyter lab --allow-root --no-browser  --ServerApp.token='' &&\n            #end if\n            cp ./ipython_galaxy_notebook.ipynb '$jupyter_notebook' \n\n        #end if\n    ]]>\n    </command>\n    <inputs>\n\n        <conditional name=\"mode\">\n            <param name=\"mode_select\" type=\"select\" label=\"Do you already have a notebook?\" help=\"If not, no problem we will provide you with a default one.\">\n                <option value=\"scratch\">Start with a fresh notebook</option>\n                <option value=\"previous\">Load a previous notebook</option>\n            </param>\n            <when value=\"scratch\"/>\n            <when value=\"previous\">\n                <param name=\"ipynb\" type=\"data\" format=\"ipynb\" label=\"IPython Notebook\"/>\n                <param name=\"run_it\" type=\"boolean\" truevalue=\"true\" falsevalue=\"false\" label=\"Execute notebook and return a new one.\"\n                    help=\"This option is useful in workflows when you just want to execute a notebook and not dive into the webfrontend.\"/>\n            </when>\n        </conditional>\n        <param name=\"input\" type=\"data\" optional=\"true\" label=\"Include data into the environment\"/>\n\n    </inputs>\n    <outputs>\n        <data name=\"jupyter_notebook\" format=\"ipynb\" label=\"Executed Notebook\"></data>\n    </outputs>\n    <tests>\n        <test expect_num_outputs=\"1\">\n            <param name=\"mode\" value=\"previous\" />\n            <param name=\"ipynb\" value=\"test.ipynb\" />\n            <param name=\"run_it\" value=\"true\" />\n            <output name=\"jupyter_notebook\" file=\"test.ipynb\" ftype=\"ipynb\"/>\n        </test>\n    </tests>\n    <help>\n    The Jupyter Notebook is an open-source web applicati...\n"
    },
    {
      "role": "user",
      "text": "We have solved the put() error - the problem was due to the lack of certificate cocatination on our galaxy-hepp.uio.no server, so the CA certificate was not available for requests.\nNow in place and put() does not give that error anymore.\n(Another error is being dealt with related to permissions in /storage/galaxy/tmp - but that is another issue)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I met an error when I used the Trinity tool in Galaxy website.\nFirst, I uploaded my paired-end RNA-seq clean data (12 files in total, containing 3 control samples and 3 test samples, such as control1_1.fq.gz, control1-2.fq.gz, control2_1.fq.gz, control2_2.fq.gz, \u2026, which had been quality-controlled and filtered by the fastp software.) from my computer. Then, I found the Trinity software by typing \u201ctrinity\u201d in the tools bar in the upper left corner of the galaxy website homepage and used this tool. All other options are default, except for the option of \u2018paired end\u2019. In the \u201cLeft/Forward strand reads\u201d, I selected all the six X_1.fa.gz files( x represents control1, control2, control3, test1, test2, test3). In the \u201cRight/Reverse strand reads\u201d, I selected all the six X_2.fa.gz files( x represents control1, control2, control3, test1, test2, test3). Afterwards, I clicked on the \u201crun tools\u201d button to start the analysis. However, the results turns red. One of them show that \u201cTrinity on data 47, data 45, and others: Gene to transcripts map\u201d, An error occurred with this dataset: \u683c\u5f0f tabular \u6570\u636e\u5e93 [?]., while the other show that \u201cTrinity on data 47, data 45, and others: Assembled Transcripts\u201d, An error occurred with this dataset:\u683c\u5f0f fasta \u6570\u636e\u5e93 [?]\nCould you help me to solve this problem? Thank you in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nduring job setup click at three blocks (versions) icon at the top right corner of the middle window and select any available version from the pull-down menu.\nYou can use assembled transcripts and transcript to genes map from history I shared.\nYou can copy datasets from one history to another using See histories side by side (in history menu)\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m attempting to run HISAT2 on paired RNAseq data. I have run it successfully previously on the main server using the mm10 built-in reference genome, however, I am now using a local server and the built-in reference genomes have apparently not been included in the set-up. I\u2019m hoping to get some assistance on how to obtain the right reference genome file for mm10 installed, or even better how to update the local server so the same built-in reference genome will be available as in the main server?\nThank you,\nAsta"
    },
    {
      "role": "system",
      "text": "Hi,\nThe mm10 reference genome/build is sourced from UCSC.\nYou\u2019ll need to index the genome on your server with Data Manager tools.\nInstall the genome with Data Managers (sourced from the ToolShed (@link http://usegalaxy.org/toolshed), all are in a separate category there). Install DMs like any other tool using the Admin functions.\nYou\u2019ll need these DMs at a minimum. Execute them in this order first:\n\n\nFasta fetcher (@link https://toolshed.g2.bx.psu.edu/repository/browse_repositories_in_category?sort=name&operation=view_or_manage_repository&id=cdf172d633d603c2) \u2013 has an option to pick UCSC as the data source.\nSAM indexer\nPicard indexer\n2bit (twoBit) indexer\n\nThen get the DMs that create indexes for the tools you want to use. Run these after the others have above have completed for the best results.\nThe above was copied over and slightly updated from this prior Galaxy Biostars Q&A. It has more details (worth reviewing):\n@link https://biostar.usegalaxy.org/p/19371/\nA search here with the keyword string \u201cbiostars data manager fetch sam picard\u201d will find more prior Q&A that covers many different use cases: @link https://galaxyproject.org/search/\n\n\nMore details\n\nBe very careful about not running an indexing tool twice \u2013 duplicated lines will cause problems and are a hassle to correct.\nIf you use a \u201cdbkey\u201d that is already available, you must make sure that your genome is an exact match for that content. Genomes that are already indexed on one of the public Galaxy servers (and associated with an existing dbkey) can be found here:  @link http://datacache.galaxyproject.org/\n\nData Libraries are great for storing datasets commonly used (copies do not consume any disk space), and you  could  put in a fasta for a genome in one to use it with tools as a Custom Genome (no precomputed tool  indexes  can be used from there or a history, so don\u2019t bother to load them).\nCustom genome help is in this FAQ and a related FAQ. If you are indexing a genome from an uploaded fasta file for some reason (not available directly from a known source), the same fasta formatting rules apply:\n\nPreparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/)\nMismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\n\n\n\nBut IF you are admin of a server  it would be better to install the genome directly and index it for tools on the server. Jobs will use fewer resources and run faster this way, plus be much less likely to fail for memory reasons.\nData Manager tools are installed/used under the \u201cAdmin\u201d masthead, top section. The \u201cloc\u201d files/table content is there as well.\nThe jobs that are run by Data Manager tools will be sent to your active history. It is strongly recommended to set up a distinct history for each genome, or batch of genomes, and date it in the name, to better keep track of what was done over time.\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "submitting a job to a SGE 8.1.9 Cluster  from an application called galaxy logged the following error:\ngalaxy.jobs.runners.drmaa WARNING 2019-02-13 13:20:43,111 (427) drmaa.Session.runJob() failed, will retry: code 17: MUNGE authentication failed: Invalid credential format\nI have verified UIDs and GIDs across host and cluster are alike as well as verified perms for munge dirs and files match install docs.  Also verfied munge.key matched across cluster and host.\nUsed this python script outside of galaxy to submit job to cluster with success:\nimport drmaa\nfrom multiprocessing.pool import ThreadPool\nimport tempfile\nimport os\nimport stat\n\n\nsession = drmaa.Session()\nsession.initialize()\ndef main():\n    smt = \"ls . > test.out\"\n    script_file = tempfile.NamedTemporaryFile(mode=\"w\", dir=os.getcwd(), delete=False)\n    script_file.write(smt)\n    script_file.close()\n    print \"Job is in file %s\" % script_file.name\n    os.chmod(script_file.name, stat.S_IRWXG | stat.S_IRWXU)\n    jt = session.createJobTemplate()\n    print \"jt created\"\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    print \"environment set\"\n    jt.remoteCommand = os.path.join(os.getcwd(),script_file.name)\n    print \"remote command set\"\n    jobid = session.runJob(jt)\n    print \"Job submitted with id: %s, waiting ...\" % jobid\n    retval = session.wait(jobid, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n\nif __name__=='__main__':\n    main()\n\nWHEN I try this same script with Python Multithreading, I get error Script and error are:\nimport drmaa\nfrom multiprocessing.pool import ThreadPool\nimport tempfile\nimport os\nimport stat\n\npool = ThreadPool(1)\n\nsession = drmaa.Session()\nsession.initialize()\n\ndef pTask(n):\n    smt = \"ls . > test.out\"\n    script_file = tempfile.NamedTemporaryFile(mode=\"w\", dir=os.getcwd(), delete=False)\n    script_file.write(smt)\n    script_file.close()\n    print \"Job is in file %s\" % script_file.name\n    os.chmod(script_file.name, stat.S_IRWXG | stat.S_IRWXU)\n    jt = session.createJobTemplate()\n    print \"jt created\"\n    jt.jobEnvironment = {'BASH_ENV': '~/.bashrc'}\n    print \"environment set\"\n    jt.remoteCommand = os.path.join(os.getcwd(),script_file.name)\n    print \"remote command set\"\n    jobid = session.runJob(jt)\n    print \"Job submitted with id: %s, waiting ...\" % jobid\n    retval = session.wait(jobid, drmaa.Session.TIMEOUT_WAIT_FOREVER)\n\npool.map(pTask, (1,))\nResult is:\nJob is in file /home/svc-clingalprod/tmpu3A6Rk\njt created\nenvironment set\nerror: getting configuration: MUNGE authentication failed: Invalid credential format\nremote command set\nTraceback (most recent call last):\n  File \"remote_mthread.py\", line 29, in <module>\n    pool.map(pTask, (1,))\n  File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 250, in map\n    return self.map_async(func, iterable, chunksize).get()\n  File \"/usr/lib64/python2.7/multiprocessing/pool.py\", line 554, in get\n    raise self._value\ndrmaa.errors.DeniedByDrmException: code 17: MUNGE authentication failed: Invalid credential format\n\nWhere do I go from here in isolating the cause of the Invalid Credential format error?"
    },
    {
      "role": "system",
      "text": "Yes, but that\u2019s more complicated than necessary. If you can use qsub on the host where Galaxy runs your best bet is probably to use the command line interface runner with the PBS backend. I\u2019m using this in production (though ssh), this should be a viable option."
    }
  ],
  [
    {
      "role": "user",
      "text": "I need a tool which can change the transcript id to ensemble or entraz ID ? i need it for goseq analysis, i think the problem in that tool is from the IDs.\nthanks"
    },
    {
      "role": "system",
      "text": "The mapping may not be 1-1.\nI would suggest using the Entrez IDs directly from Featurecounts instead to ensure distinct geneIDs. Then, at the end, you can convert those IDs to Ensembl for other purposes you may have.\nFollow the tutorial, it will help to avoid problems like this."
    }
  ],
  [
    {
      "role": "user",
      "text": "hi\ni am working with galaxy for analysis of metagenomics data. unfortunately i could not use the finch visualization.could you help me ,why pinch dont work for me\nthank you"
    },
    {
      "role": "system",
      "text": "Thanks for sharing your history. I see that you are using your own datasets here, not the tutorial data. This means you will also have to create your own metadata file (e.g. if you have groups). You can also create the biom file without such a metadata file as a start. I was able to create this biom file and view it in Phinch.\nAlso to answer your earlier questions about the \"16 more\u2026 \" in the Krona plot: this means there are a lot more taxons at that level that could not be displayed in this view, but you can click on taxons to \u201czoom in\u201d in Krona to see what these additional entries are. For example:\n\nimage1060\u00d7906 148 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2a4a20c9e6b83a724df4568151a7380f24a4842a.jpeg)"
    }
  ],
  [
    {
      "role": "user",
      "text": "hi all, I\u2019m trying to align some fasta using bowtie2 but I get the error:\ntool error\nAn error occurred with this dataset:\nUnable to finish job\nany help is appreciated!"
    },
    {
      "role": "system",
      "text": "Update: Fixed\n@user @system @system @system and any others that ran into this issue, with any mapping tool over the last few weeks\n@quote\nNote to all end users running mapping:Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.Ticket:https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to do Augustus training, but the rat model is not available on the list. Is there any other ways to do it?\nAnd when I look at the Maker training set, I see that I have to use the ESTs and protein evidence databases. Where to find those databases? I have downloaded the rat (rn7.2) assembly dataset. Is it correct that I use the rna.fna file from the rn7.2 assembly as the ESTs or assembled cDNA and use the protein.fna from the rn7.2 assembly as the protein evidence? It seemed not working in this way, but what else I should use then?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nregarding your first question, you can generate an Augustus model by using  the Augustus training (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/augustus_training/augustus_training/3.3.3) tool.\nRespecting the following questions, if you pretend to re-annotate the rn7.2 assembly, then it is not a good idea to use the protein.fna and rna.fna datafiles from the rn7.2 assembly because that information has already been used for performing the annotation, then it won\u2019t provide any additional information.\nYou can get an updated EST dataset from Genbank (@link https://www.ncbi.nlm.nih.gov/nuccore). I recorded a short video about how to get the sequences.\n\n\n\n (@link https://www.youtube.com/watch?v=yq37HP_-MC0)\n\nOn the other hand, the protein sequences can be obtained from UniProt (@link https://www.uniprot.org/).\n\n\n\n (@link https://www.youtube.com/watch?v=Y7DL_8E4suI)\n\nLet me know if you need additional information.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Good day,\nAre there ARCTIC v4 amplicon info and v4 bed files to use for SARS-CoV-2 analysis. We recently started using the ARCTIC V4 primers. I had to try running the sequence data using the V3 primer set and bed on the SARS-CoV-2 workflow but it doesn\u2019t generate any info."
    },
    {
      "role": "system",
      "text": "Hi @user,\nwe\u2019ve updated our shared covid-19 resources history on @link http://usegalaxy.eu a while ago to include more primer schemes. Galaxy | Europe | Published History | COVID-19 resources (@link https://usegalaxy.eu/u/wolfgang-maier/h/covid-19-resources) has ARTIC v4 primers (and amplicon info) and others since then.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am a Galaxy user and I wish to download the FASTQC report generated as output on galaxy server, but I am unable to do so. I have run \u201cFASTQC read quality report\u201d to check the quality of my fastq reads, and the program ran successfully giving a HTML file as output. But I am not able to download/view the HTML file.\nIs this a bug of galaxy or is there anything I can do to download the file?\nKindly advise\n\u2013Lakshmi"
    },
    {
      "role": "system",
      "text": "Update: maintenance completed.\nThis notice applies to everyone using the server.\nThe @link http://UseGalaxy.org server is undergoing scheduled maintenance. Follow here for current status and updates: @link https://status.galaxyproject.org/\nThe estimated time window for the maintenance is:\nTuesday October 13 8:00 AM to 5:00 PM CDT\nUpdate: maintenance has been extended until October 14, 10 AM CDT\nUpdate: maintenance has been extended through October 14. No set end time.\nOnce that is completed and you can access the server again, please try any problematic actions again.\nThanking everyone for their patience, Galaxy team"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have started my own galaxy. I have registered my email id and also changed in galaxy.yml file. But I am not getting admin menu in the menu bar. Kindly help me to fix this. Thanks in advance."
    },
    {
      "role": "system",
      "text": "@quote\nadmin_users: \u2018<redacted>@gmail.com\u2019\nRemove the single quotes and restart \u2013 that should solve the problem.\nExample: @link https://galaxyproject.org/admin/get-galaxy/#become-an-admin"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All\nI am getting an unexpected during pear end read merger fastq error showing:\nAn error occurred with this dataset:\nformat fastqsanger database.\nHowever two samples are merged successfully.\nKindly suggest what to do.\nRegards"
    },
    {
      "role": "system",
      "text": "Also, try this. Reloading the data was just one guess about why the reads are not all paired.\n@quote\nThe solution would be to give fastq files as input with the same amount of (matching) reads.\nThere are a few ways to filter out any forward or reverse reads that are missing a mate. After you are done, the Fastq info tool should report the same number of reads in the forward and reverse files. You won\u2019t be able to do more until that is resolved.\n\n\nRun the data through a QA tool that sorts the output into intact pairs and singletons. Example: Trimmomatic. See the tool form for how to use it, and our QA tutorials for usage examples.\n\n\nUse tools that compare the read identifiers between files, and only retain output where both files contain the base read name. You might need to convert fastq \u2192 tabular, do tabular manipulations, then covert tabular \u2192 fastq at the end. See the tutorial below for the choices with examples.\n\n\nUse tools from the Seqtk tool group. These do all sorts of logical data manipulations. Converting to an interleaved format then back to individual forward + reverse format can get rid of stray unpaired singleton reads. If you do not understand what these terms mean, see the first QA tutorial I linked before. You can compare data visually to the examples to learn the concepts.\n\n\nReferences:\n\nText manipulation tools: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n\nQA tools \u2013 some basic checks to make sure data has valid format and content are always recommend, even if you don\u2019t plan on doing any trimming.\n\n@quote\nThese tutorials can help with understanding fastq data (format/content/variants) along with example QA steps/tools. Also see domain/topic tutorials at that same site for protocol-specific QA steps.NGS data logisticsQuality ControlMapping"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am using the RNA-seq dataset (triplicates) to analyze the differentially expressed genes in bacterial systems . I could analyze until featurecounts stage without any problem. But when I used Deseq2 tool to upload the featurecounts results, I couldn\u2019t find the featurecounts on the collection file for uploading. It is just showing the single replicate counts and not the collection file that has all three replicates. It would be great if soemone could help me with this.\nThank you\nJawahar"
    },
    {
      "role": "system",
      "text": "Hi Jawahar,\nthe standard error file has the following:\n/usr/local/tools/R/3.2.1/iuc/package_r_3_2_1/e46a7803f17b/lib/R/bin/exec/R: error while loading shared libraries: libreadline.so.6: cannot open shared object file: No such file or directory\nIt is an old version, and it might be non-functional. Try other versions, for example 2.11.39. Some of these tools fails when a header line present in count tables. If it fails with error pointing to non-numeric value, remove the first line from count files.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I wanted to move/copy my dataset collection (fastqsanger.gz files, >2GB each) from usegalaxyorg to usegalaxyeu as on the latter server there is a tool I am going to use. I tried to do as it was written in Question 2, Help 2 (@link https://help.galaxyproject.org/t/help-needed-regarding-usegalaxy-eu-and-usegalaxy-org/3598/2). But as I copy-paste the URL to the dataset (from the \u2018Copy link\u2019 - chains icon) to the upload data tool it shows very small size of the data (few bytes) and the transfer ends with an error ('The uploaded file contains invalid HTML content\").\nThe history is accessible (Share and Publish).\nI have a collection (a list of pairs with 21 items) of total 42 fastqsanger.gz files.\nCan you help me what am I doing wrong?\nAccording to old posts it should work.\nBest regards,\nMarcin"
    },
    {
      "role": "system",
      "text": "The fact you have your account in the \u201caccess\u201d field makes it inaccessible for everyone else. Remove it with the X and save. Then the link for the import to the other Galaxy will work."
    }
  ],
  [
    {
      "role": "user",
      "text": "Could anyone advise on average runtime for the MiGMAP tool to map BCR sequences from single cell data? I started a MiGMAP run with a collection of 37 paired reads from a single cell sequencing experiment (SmartSeq, Nextera, HiSeq4000) 3.5 weeks ago and it is still showing in orange and as \u2018currently running\u2019. Does this tool just take a long time or is it likely there is an error?\nAnd are there any alternative tools in Galaxy to extract BCR variable region sequences from single cell data?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis tool is already in Galaxy: MiXCR (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/mixcr_analyze/mixcr_analyze/3.0.5.0).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone, I am still having a problem creating a QIIME2 artefact in galaxy. I have my paired end sequences and i have my metadata loaded up. All i need to do is combine them in QIIME2. The tutorial on this page Importing demultiplexed sequence data \u2014 QIIME 2 Cancer Microbiome Intervention Tutorial (@link https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/020-tutorial-upstream/030-importing.html) is pretty straight forward but it only teaches you to copy and paste the data from the website and not how to really create your own. Can anyone point me in the right direction or know who I can ask?\nimage1637\u00d7665 47 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/ee7a9a33087a2751bbff85c2052c18638073d8bc.png)\nKind regards"
    },
    {
      "role": "user",
      "text": "Hi Sargentl, I managed to sort it. It was such a simple thing i had not thought about and that is the names of my files had to have been in Casava format and they wasn\u2019t.\nThank you for spending time to help me with this.\nKind regards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nHope I can find help here. I have a Galaxy server that was purchased a number of years ago from Bioteam (Slipstream Appliance). They administered it up to version 16.07 but then discontinued their support. I need to update to the latest Galaxy version using their instructions supplemented with instructions here - Galaxy Community Hub (@link https://galaxyproject.org/admin/maintenance/) . I am at a beginner level of dealing with Linux servers (why I purchased a supported setup) so forgive my ignorance.\nSetup:\nBioteam Appliance (Galaxy preloaded Linux server)\nUbuntu 14.04.6 LTS (Trusty Tahr)\nPostgreSQL 9.3 database server\nGalaxy 16.07\nCurrent approach to upgrade:\n\nLogin, become root, and take a ZFS snapshot to enable rollback if problem.\n\nsudo \u2013I\ncd /slipstream\nsupervisorctl stop all && service postgresql stop\nzfs snapshot @link mailto:slipstream@pre_upgrade_from_galaxy16.07_2021-11-13\nservice postgresql start && supervisorctl start all\n\nFetch new version using git, update virtualenv, and migrate DB\n\ncd /slipstream/galaxy/production/galaxy-dist\ngit fetch origin && git checkout release_17.01 && git pull --ff-only origin release_17.01\n./scripts/common_startup.sh\nreboot\nsh manage_db.sh upgrade\ngit status\nsh ./scripts/migrate_tools/0011_tools.sh install_dependencies\nservice postgresql start && supervisorctl start all\n\nThe screen output I get with the last command is:\n\n\nStarting PostgreSQL 9.3 database server [ OK ]\n\nhandler1: ERROR (abnormal termination)\nweb0: ERROR (abnormal termination)\nweb1: ERROR (abnormal termination)\nweb5: ERROR (abnormal termination)\nweb4: ERROR (abnormal termination)\nhandler0: ERROR (abnormal termination)\nweb2: ERROR (abnormal termination)\nweb3: ERROR (abnormal termination)\nThere are other screen outputs from the prior commands that I could also share. Or can look for specific issues.\nI looked at the log files \u2013 comparing log files from startup before upgrade and after upgrade. One thing I noticed is that before upgrade it loads up to database version 135 (and indicates that the version is 135). But after the upgrade it only loads up to version 133 \u2013 and then checks for version and says that the database version is 135. Could this by related to why it won\u2019t start??\nAny suggestions would be greatly appreciated.\nThanks\nMichael"
    },
    {
      "role": "user",
      "text": "Just in case anyone reads this and is trying to upgrade Galaxy from pre-Galaxy 20 versions (in my case 16) with older pre-Ubuntu 20 versions (in my case 14) )\u2014 it is not possible since Galaxy and Ubuntu are co-dependent through this time period.  Ubuntu is very difficult/impossible to upgrade successfully.  Similarly Galaxy\u2019s requirements (mainly Python) change greatly through this period. I ended up copying raw data off my Galaxy installation; wiping the server; installing Ubuntu 20; and then the Ansible version of Galaxy 21 \u2013 the latter based on these tutorials.\nNew install of Galaxy\n\n\n@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html\n\n\nGalaxy Training: Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\nAdd on Ansible \u2013 install Proftpd\n\n\n@link https://usegalaxy.be/training-material/topics/admin/tutorials/ftp/tutorial.html\n\n\nGalaxy Training: Enable upload via FTP (@link https://usegalaxy.be/training-material/topics/admin/tutorials/ftp/tutorial.html)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\nAnsible does make it easy - although there are a couple of errors and poorly explained sections in the tutorials that required some headscratching and tweaking.\n\nCross-posted topic: toolshed not working anymore with old galaxy? (@link https://help.galaxyproject.org/t/toolshed-not-working-anymore-with-old-galaxy/5302/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I submitted three datasets to the MethylDackel tool and it is running for the past two days (other users said it took them 2-3 hours to get results). does anyone know what\u2019s the problem?\nThanks"
    },
    {
      "role": "system",
      "text": "Any reason you can not use \u2026\ngrafik"
    }
  ],
  [
    {
      "role": "user",
      "text": "Yesterday I had a project in my account in perfect condition. Today I log in and all I find is: \u201cThis history is empty. You can load your own data or get data from an external source\u201d.\nWhat can I do to recover my history?"
    },
    {
      "role": "system",
      "text": "This issue has been dealt with. Sorry again for the inconvenience."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI launched a job on Friday for alignment on SPADES on @link http://usegalaxy.org\nI used FASTQ to check the quality of my uploaded sequences and Trimmomatic to trim my data beforehand. I used the \u201cpaired\u201d output from Trimmomatic to launch SPADES.\nI launched 24 alignments at the same time (all from a same pair-end Illumina run; 24 samples) and with the same settings (run only assembly; careful correction; k-mers to use 33,55,99; coverage cutoff auto).\nTwo jobs are still ongoing, 12 succeeded and 10 failed.\nFor the failed jobs, the error reported is:\n\ntool error\nAn error occurred with this dataset:\nRemote job server indicated a problem running or monitoring this job.\n\nI\u2019m not too sure what that means.\nI don\u2019t understand why 10 jobs failed as from the quality analysis (FASTQ) it did not seem to me that there was any difference between the 12 that worked and the 10 that failed.\nCan this be a server issue? Is it worth it to re-launch the jobs that failed?\nThanks for your help\nVanina"
    },
    {
      "role": "system",
      "text": "@quote\nrun Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nsince some months we are having an issue with the toolshed. It\u2019s not installing any tools anymore printing the following error:\n\u201cRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser.\u201d\nI tried the enabling of third party cookies but it does not change anything.\nOur instance is quite old (16.07) but a the moment we cannnot update.\nIs there anything I can do to get access to the toolshed again?\nI found a number of related topics, which are not helping with my problem:@quote\nHi \nI\u2019m trying to install a tool from the Toolshed but I got this error: \n\nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser. \n\nThe cookies are OK (tested on Firefox and Chrome). \nI\u2019ve found this similar topic:https://biostar.usegalaxy.org/p/8548/index.htmlSo I checked the admin_users parameter in galaxy.yml file. It was empty. \n-> First of all it\u2019s weird that I could connect as admin mode (withadmin@galaxy\u2026\n@quote\nHi@systemBoth the Main and Test ToolSheds were recently undergoing updates. Issues around that are resolved as of earlier today. \nRelated Q&A: \n\nThanks!\n@quote\nI\u2019ve tried installing blast2go in the browser version of Galaxy. Yet, I always get the same error (mentioned below). \nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser. \nI have allready checked my cookie settings in both google chrome as in mozilla firefox and both browsers allow the third-party cookies. However, the error still occurs\u2026 Any advice\nCheers\nJochen"
    },
    {
      "role": "system",
      "text": "To add, if you\u2019re running into problems updating your Galaxy instances, let us know.\nFor such old releases I\u2019d recommend doing year-by-year updates.\nSo if you start on release_16.01, I\u2019d recommend updating to release_17.01, doing the database migrations (./manage_db.sh -c config/galaxy.ini), start up Galaxy and verify the interface works, and then going to release_18.01 and so on. Once you reach release_20.01 you have to migrate to Python 3, instructions for updating this are here: Supported Python versions \u2014 Galaxy Project 21.01 documentation (@link https://docs.galaxyproject.org/en/master/admin/python.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone!\nI\u2019m trying to use the tool Stacks2 : process_radtags to demultiplex paired-end data. I\u2019ve got 2 huge files, R1 and R2, each one at about 45 Gb, in fastqsanger.gz format. The problem I encounter is that when I select \u201cpaired-end files\u201d in the first line of the tool, it can\u2019t find my data files. On the other end, if I leave it at \u201cSingle-end files\u201d, it finds both of the files with no problem. Should I interlace my two files for this tool to work?\nCan anyone help me with this small issue?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi again :wink:\nIn fact, it appears that to work properly with the 2.4 tool version, you need to use the \u201cBuild list of Dataset Pairs\u201d function, not \u201cBuild a Dataset Pair\u201d, even if you only have one pair\u2026 I think this will be ok!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m using Bowtie2 via Galaxy to align my RNAseq reads against a published transcriptome. If I want more stringent conditions for the alignment, what parameters I should adjust from the default?\nThank you very much"
    },
    {
      "role": "system",
      "text": "The paired reads must be a match for the R1(forward) and R2(reverse) input to use Bowtie2 or Trinity.\nDuring your processing, if one read from a pair was filtered out with BLASTN (determined to be contamination) you will also need to filter out the other read from the pair before continuing.\nOther tools (besides Trimmomatic) that can create paired fastq output.\n\n\nFastQ Interlacer followed by FastQ Deinterlacer\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI\u2019m trying to run the Blast2GO tool in @link https://proteomics.usegalaxy.eu but keeps on giving me this error:\nAn error occurred with this dataset:\nformat tabular database @link https://proteomics.usegalaxy.eu/workflows/run?id=33f28aeab5cd1b8d#\nBlast2GO JAR file not found: /opt/b2g4pipe_v2.5/blast2go.jar\nPlease advise."
    },
    {
      "role": "system",
      "text": "@quote\nWhat more can I do?\nConsider using alternative methods?\n\nReview tool choices in the Annotation section of the tool panel.\nOr, do some direct data manipulations with tabular formatted data for the query and target. This is how everyone originally did Bioinformatics  :slight_smile: When you come up with a custom process you like, consider creating a reusable workflow. You can \u201cfavorite\u201d workflows and they will show up in the tool panel just like any other tool. Under the hood this is what all tools really are: groupings of manipulations.\n\nReferences\n\nData Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n\nSearch Tutorials (@link https://training.galaxyproject.org/training-material/search?query=workflow) (query=workflow)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there, I have seen several posts pertaining to annotation of the DESEQ2 output GeneID with gene name (ENSRNOG00000046834 etc) and for mice and humans it works easily with the AnnotateMyIDs tool.  I work with rat tissue.  I have installed this AnnotateMyIDs on our server, but it does not support rats. I have tried to fiddle with the code to get it to accept rats too, but have failed so far and am worried about stuffing the server!\nIs there a way to add rat support to this, as admin?  IF not is it possible to make a feature request somewhere for this? \u2026It was developed by IUC. I don\u2019t know who that is. I ran  Uniprot mapping and this gives a list of the converted gene names, but not in the same file (unless I did it wrong). Is there an easier way? Thanks.\nI have written a program to do this in Python outside of Galaxy. Be nice to all be inside though!"
    },
    {
      "role": "system",
      "text": "I\u2019ve added rat to annotatemyids and submitted a PR to the IUC repo here, let\u2019s see if it\u2019s accepted @link https://github.com/galaxyproject/tools-iuc/pull/2462\nBy the way there is also this deg_annotate tool that can annotate deseq2 output with a GTF so maybe that\u2019s another option? @link https://toolshed.g2.bx.psu.edu/repository?repository_id=e33c4ee81be2b1b4"
    }
  ],
  [
    {
      "role": "user",
      "text": "Need an AI ML based variant calling tools for Illumina and Oxford data, please. Is there any Galaxy instance I could not find?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI have opened a PR for including the DeepVariant tool DeepVariant PR (@link https://github.com/galaxyproject/tools-iuc/pull/3919).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to assemble a plant genome using ONT data with Flye. Prior to launch the assembly with all my data (about 37 Gb in fastqsanger.gz) I recently tried using half of them and the job was successful after 2 days running with a descent genome size obtained. I then launched with all the available sequences (37 Gb in fastqsanger.gz) and it\u2019s now been running for 5 days. I am just wondering if I am using more ressources than allowed for this tool or if it\u2019s still running. What makes me worrying is that now my quota usage now indicates 0% suggesting there might be a bug somewhere ?\nThank you very much.\nBen"
    },
    {
      "role": "system",
      "text": "We updated the funannotate databases and provided more cores to this tool.\nYou can see how many cores and memory every tools gets here: infrastructure-playbook/tool_destinations.yaml at master \u00b7 usegalaxy-eu/infrastructure-playbook \u00b7 GitHub (@link https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml#L196)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am developing galaxy tools, and constructed a workflow to chain them together. The problem I am encountering comes from one of the tools executing before a previous tool finishes to run.\nAttached are two images. One with the workflow where circled in red (RP Selenzyme) is the tool that executes too early (before RP Thermodynamics finishes). The second image is the side panel showing that the tool RP Selenzyme returns an error due to an empty input since it executes earlier than it should.\n\nScreenshot from 2020-02-24 12-54-391908\u00d7907 133 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/44108cc385bbaa96eca635a8894187be9e7b18cd.png) \nScreenshot from 2020-02-24 12-47-371339\u00d7538 112 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/351fc181ea5cfdc0ce242699ff4a54faa61f61d5.png)\nAny help would be very welcomed. Do not hesitate to ask me for any logs. I tried this on the Galaxy 19.05 and 20.01 releases and the same error occurs.\nThank you,"
    },
    {
      "role": "system",
      "text": "So connecting a text parameter to a select is a new feature in 20.01, this won\u2019t work on 19.09. Then there is the issue that I think there are 2 parameters called input in RpSelenzyme. Can you try changing the one Taxonomy JSON input to a different name and rebuild and re-run the workflow ?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to do the minimap2 with my own rat genome assembly against the reference rat genome. It went through well when I mapped against rn6 which is on the reference list. But when I tried to map agaisnt rn7 which is not on the list and had to upload as a seperate dataset, it failed constantly whatever parameters I have changed. The error message showed out of memory. Is there any solution for this circumstance? Or may I ask if the rat rn7 could be added on the reference list on the server?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe rn72 indexed genome will be available on @link http://useGalaxy.eu in a few hours.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\n\nScreen Shot 2022-05-05 at 2.42.17 PM1438\u00d7860 87.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0118fd899e6bb8bd3dbf1d5b6ba8f5d763e68cfe.png)\n\nScreen Shot 2022-05-05 at 2.42.38 PM1550\u00d71448 212 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4a0da03c59617ce1661d5b911cb62d929b5f54ed.png)\nI am trying to align my RNA-seq reads (Bacteria), using HISAT2 to a reference genome (assembly.fna). Then,to measure the expression in the resulted bam.file, I used (featurecounts) tool, using the (gtf) annotation file. However, although both fna/gtf files belong to the same strain tested, I got this empty report;\nYour help much appreciated"
    },
    {
      "role": "system",
      "text": "Hi @user,\noutput of featureCounts #14 has counts for 157 genes, so it does count reads against some genes. The small number of genes is an unintended consequence of the gene annotation. Tools such as featureCounts and htseq-count count reads against a feature in 3d column of GTF file and aggregate the results using an attribute from the last column. For example, feature in the 3d column can be \u2018exon\u2019, and attribute is \u2018gene_id\u2019. The tool counts reads against axons and returns count for a value (gene ID) in \u2018gene_id\u2019. Your GTF file does not have \u2018exon\u2019 feature for some genes. Filter the GTF file on column 3 using Filter data on any column using simple expressions with c3==\u2018exon\u2019. You\u2019ll see 157 records. As bacterial genes are not spliced, the featureCounts estimated read counts for genes with available \u2018exon\u2019 feature. You can select a feature used by featurCounts for counting in Advanced options. If you  set it to CDS, featureCounts returns 5092 lines. With CDS you\u2019ll miss some ncRNAs. Try different features, examine the annotation, and select a feature most appropriate for purpose of your analysis.\nYou can unshare the history if you don\u2019t have any other questions: History menu > Share or Publish > Unshare it.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Starting server in PID 30796.\nserving on @link http://127.0.0.1:8080\ngalaxy.queue_worker INFO 2022-01-06 13:49:48,449 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Instance \u2018main.web.1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,449 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel default.\ngalaxy.model.database_heartbeat DEBUG 2022-01-06 13:49:48,560 [pN:main.web.1,p:30796,w:1,m:0,tN:database_heartbeart_main.web.1.thread] main.web.1 is config watcher\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,992 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel default finished (542.171 ms)\ngalaxy.tools.search DEBUG 2022-01-06 13:49:48,992 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel ontology:edam_operations.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,175 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel ontology:edam_operations finished (182.193 ms)\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,175 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Starting to build toolbox index of panel ontology:edam_topics.\ngalaxy.tools.search DEBUG 2022-01-06 13:49:49,538 [pN:main.web.1,p:30796,w:1,m:0,tN:Thread-1] Toolbox index of panel ontology:edam_topics finished (362.471 ms)"
    },
    {
      "role": "system",
      "text": "Is galaxy running on the same computer as where you are trying to access it? Or are you running galaxy on a (remote) server and you are trying to access it with another laptop/computer?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have two assembled fasta files, one for control and second for disease condition. I need to fine differentially expressed genes in both of them. Plz guide me how to perform this type of analysis.\nRegards"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can use Salmon Quant for performing the differential expression analysis. I recommend you to have a  look at this tutorial: Quantification of gene expression: Salmon (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/mirna-target-finder/tutorial.html#quantification-of-gene-expression-salmon).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m a relatively savvy Galaxy user, however just today, we are experiencing issues in downloading any file from Galaxy Main larger than 1Gb. Irrespective of file size, the resulting downloaded archive is 1.08Gb, and fails to expand on opening.\nI am downloading using a good internet connection (University-based) and am doing nothing different than usual!\nPlease help!\nBest wishes,\nD"
    },
    {
      "role": "system",
      "text": "Downloading larger datasets in a history archive (potentially subsetted to only include important data) is the current workaround. More details in this post: Files will not download completely (@link https://help.galaxyproject.org/t/files-will-not-download-completely/4282/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "The QualiMap Multi-Sample BamQC wrapper in @link http://usegalaxy.eu does not work, showing a pink-colored flagged error message: \u201cUncaught exception in exposed API method:\u201d"
    },
    {
      "role": "system",
      "text": "@user You have fallen into a bug.\n\n@link https://github.com/galaxyproject/galaxy/issues/12068\n\n\n\n\n\n\n\n\nnull / -1 HID?  (@link https://github.com/galaxyproject/galaxy/issues/12068)\n\n\n\n        opened 01:15PM - 28 May 21 UTC\n\n\n\n\n          hexylena\n         (@link https://github.com/hexylena)\n\n\n\n\n\n\n\nSpotted today during a class.\n\n![image](https://user-images.githubusercontent.\u2026 (@link )com/458683/119989389-68a56900-bfc7-11eb-99fd-5f58f1348fea.png)\n\n![2021-05-28_15-12](https://user-images.githubusercontent.com/458683/119989403-6e02b380-bfc7-11eb-9344-a985f7dbbe24.png)\n\nhistory id b798433517f2aa48 for the european admins.\n\n\n\n\n\n\nAs you can verify, the first steps of your history don\u2019t have a number, but null.\nHow did you produce those steps? Will be really useful to fix the bug if you can provide some details (in the issue page better).\nDid you get there by clicking on re-run on an element of a mapped over collection ? This is an action we know to create this issue.\nAs a temporary workaround, I can suggest creating a new history, move the needed datasets there and run the tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, this is my first time reporting an issue, so I apologize if I overlook info, can\u2019t format code correctly in this flavor of markdown, etc.  I\u2019ve submitted a bug report via the Galaxy server, but, at the risk of redundancy, wanted to ask for help here to try to get the issue resolved sooner in case someone knows what might be going wrong.  I can\u2019t find a previous mention of this particular problem in GalaxyHelp or via Google.\nI\u2019m trying to assemble a genome on the Galaxy main server (\u201cversion_major\u201d: \u201c18.09\u201d), and have trimmed my reads (SRR5906897, Trimmomatic headcrop 12, slidingwindow 4, 25, minlen 100).  When I attempt a de novo assembly via Unicycler, however, it consistently fails with the message:\nError: SPAdes crashed! Please view spades.log for more information.\n\nAfter issuing the bug report, I received an email with the additional stdout code that appears to have the main issues in the following lines:\n\nlibgomp: Thread creation failed: Resource temporarily unavailable\n\n== Error == system call for: &quot;['/pylon5/mc48nsp/xcgalaxy/conda/envs/__unicycler@0.4.6/share/spades-3.12.0-1/bin/spades-hammer', '/pylon5/mc48nsp/xcgalaxy/main/staging/21785278/working/spades_assembly/read_correction/corrected/configs/config.info']&quot; finished abnormally, err code: 1\n\nI\u2019m uncertain whether the crucial issue is the \u201cResource temporarily unavailable\u201d line or whether spades-hammer can\u2019t access the config.info file indicated.  I\u2019ve tried re-running the job with \" Skip SPAdes error correction step\" set to \u201cYes\u201d, but I appear to receive the same error.  I\u2019m assuming this is a legitimate bug that will be hard to resolve on my end, but would appreciate any received wisdom as to the best course of action.  Can anyone advise?\nThanks!"
    },
    {
      "role": "system",
      "text": "@quote\nrun Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I have been using LEfSe in the past and was successfully able to run the full pipeline with the Galaxy LEfSe tool. Now when I am trying to run the pipeline using new tabular files (I believe to be formatted correctly) and even old ones that successfully worked in the past I run into two errors:\nStep A) Info\nERROR:galaxy.datatypes.data:Unable to count lines in file /export/galaxy-central/database/files/004/dataset_4265.dat\nStep B) Error message:\nNumber of significantly discriminative features: 36 ( 36 ) before internal wilcoxon\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, 'lefse_ru\nI have tried to resolve these problems many different ways, but find myself very stuck. If anyone has any reconditions. It is greatly appreciated."
    },
    {
      "role": "system",
      "text": "Update: Looks like they are working on it, and suggest alternative tools too. Leaving this link here for anyone who happens to run into the same issue. LEfSe issues and errors - #5 by Kelsey_Thompson - LEfSe - The bioBakery help forum (@link https://forum.biobakery.org/t/lefse-issues-and-errors/5397/5)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Everyone,\nVery new to galaxy. I\u2019m using my university\u2019s big computer called OSCAR to run galaxy, and I\u2019m having a few issues. I was able to install and run galaxy so I had my own instance. However, some sources say I need to become a root user of the computer to be able to make myself admin and restart galaxy so that I can add tools, but I\u2019m not allowed to have root access. (for the record, galaxy is in its own directory within home) I was able to add myself as an admin in the galaxy.yml file and restart without using \u201csudo\u201d but I\u2019m not sure if that\u2019s good enough. there are no options for admin on my galaxy instance. Is it possible to make myself a galaxy admin without root? I can\u2019t use docker on OSCAR for the same reason. Additionally, partially because I can\u2019t become admin, I can\u2019t import the toolshed I need, mentioned in this page: @link https://snvphyl.readthedocs.io/en/latest/install/galaxy/#overview I really need these SNVPhyl tools for my project. Can anyone help me with these problems?\nBest,\nCS"
    },
    {
      "role": "system",
      "text": "Hello,\nIt is definitely possible to run Galaxy without being root. And having root access has nothing to do with becoming \u201can admin\u201d in Galaxy.\nI added at few tags to your post, including \u201cbecome-an-admin\u201d. Review the prior Q&A with same links to find out how to get set up with a basic local install with admin configured. You\u2019ll also find more links to our FAQs, tutorials, and help for advanced configuration.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nDoes anyone have any pointers for indexing a reference sequence for use with GATK tools? I have indexed reference sequences that I use for BWA and minimap2, but I can\u2019t get the reference sequences to appear with GATK. I receive an error when I use the \u201cGATK-sorted picard indexes builder\u201d.\nThanks!"
    },
    {
      "role": "system",
      "text": "Thanks @system for the extra feedback.\nThe version installed at ORG has problems. We are working to fix that now.\nMeanwhile, you can run the job at @link http://UseGalaxy.eu with this version of the tool. It was updated with some important changes finalized and published on Oct 6, 2021.\n\nGATK4 Mutect2 - Call somatic SNVs and indels via local assembly of haplotypes (Galaxy Version 4.1.7.0+galaxy1)\n\nAny other version of the tool could have problems. This was complicated to fix by the tool-dev group (since January) but all the changes are now incorporated.\nTracking ticket: Update GATK4 Mutect2 \u00b7 Issue #345 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/345)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy folks,\nI have been using galaxy for a while to analyse RNAseq and I think it\u2019s incredible. Now I was wondering if there would be any workflow to work with single cell RNAseq data from FASTQ files.\nI have been trying to follow-up these tutorials\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/scrna-preprocessing/tutorial.html\n\n\n\nGalaxy Training: Pre-processing of Single-Cell RNA Data (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/scrna-preprocessing/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\nBut so far I couldn\u2019t finish it so I was wondering if any of you know about a complete workflow.\nI liked this one from the EMBL-EBI @link https://www.youtube.com/watch?v=1w4sA-qyO3g\nbut they use data from GXA as the source, so the preprocessing of the data it\u2019s not done and I am struggling with it.\nHope you can help me.\nBest,"
    },
    {
      "role": "system",
      "text": "Table of Contents\n_________________\n\n1. Determining the 10x inputs\n.. 1. Order of inputs\n.. 2. Barcode Size and Chemistry\n2. Obtaining annotation data\n.. 1. GTF and Whitelist\n3. Running the tool\n.. 1. Setting Basic Parameters\n..... 1. Setting the input pairs\n..... 2. (Optional) Multiple input pairs\n..... 3. GTF and Whitelist\n.. 2. Setting Advanced Parameters\n\n\nHi Javi, thanks for sharing your history with me.\n\n\n1 Determining the 10x inputs\n============================\n\n1.1 Order of inputs\n~~~~~~~~~~~~~~~~~~~\n\n  The first thing you need to do is to determine which of your paired\n  reads is the cDNA read, and which is the barcode read. Typically\n  barcode is Read1 and cDNA is Read2, but we should confirm this\n  manually:\n\n  * By picking two _read1.fastq's at random, we can see that these are\n    the barcoded reads since most of sequences end with poly-T tails.\n  * By picking two _read2.fastq's at random, we can infer that these are\n    cDNA reads because the sequence is more varied.\n\n\n1.2 Barcode Size and Chemistry\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Next, we need to find out what 10x chemistry we are using, and we can\n  determine that by looking at the size of our barcodes, as given in the\n  [tutorial].\n\n  * By picking a few random reads in a _read1.fastq file, we can count\n    from the start of a sequence to the first 'TTTT' (allowing for a few\n    small mismatches), and we see that the barcode size is 26bp, meaning\n    this is v2 10x chemistry.\n\n\n[tutorial]\n<https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-preprocessing-tenx/tutorial.html#10x-chemistries>\n\n\n2 Obtaining annotation data\n===========================\n\n  To run the input FASTQ data on STARsolo, we need the whitelist of cell\n  barcodes and the GTF file with the gene annotations.\n\n\n2.1 GTF and Whitelist\n~~~~~~~~~~~~~~~~~~~~~\n\n  From the [Zenodo link]:\n  * Copy the urls of the *3M-february-2018.txt.gz* and\n    *Homo_sapiens.GRCh37.75.gtf*\n  * Paste them into Galaxy using the upload data dialog\n  * Wait for them to be imported into your history\n\n\n[Zenodo link] <https://zenodo.org/record/3457880>\n\n\n3 Running the tool\n==================\n\n  Finally we can run the tool using all the inputs and additional files\n  we have obtained.\n\n\n3.1 Setting Basic Parameters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n3.1.1 Setting the input pairs\n-----------------------------\n\n  Since your FASTQ files are individual files and not collections, we\n  set the *Input Type* to \"Single files\". Then, for each Input Pairs we\n  set\n  * *Barcode reads* to a <prefix>_read1.fastq.gz file\n  * *cDNA reads* to a <prefix>_read2.fastq.gz file\n\n  where the <prefix> should be identical for each pair of inputs.\n\n\n3.1.2 (Optional) Multiple input pairs\n-------------------------------------\n\n  If you have multiple input pairs you can insert them all at once by\n  clicking on the *+ Insert Input Pairs* button and repeating the above\n  for a different <prefix>. This approach comes with some\n  advantages/disadvantages:\n\n  * Pro - Multiple input pairs at once has the benefit of having one\n    large all-inclusive count matrix at the end of the run.\n  * Con - If just *one* of your inputs is malformed, then the entire\n    tool fails and it will be hard to know which dataset was\n    responsible.\n\n\n  Normally one should first try setting all inputs at once, and if that\n  run fails, the sequential runs approach.\n\n\n3.1.3 GTF and Whitelist\n-----------------------\n\n  Now we need to use our additional files. For *RNA-Seq Cell Barcode\n  Whitelist* set this to your \"3M-february-2018\" dataset.\n\n  Set the *Custom or built-in reference genome* to \"Use a built-in\n  index\", and set the *Reference genome with or without an annotation*\n  to \"use genome reference without builtin gene-model\".\n\n  For *Select reference genome* set this \"Human Dec. .... hg19\", since\n  this is the version of our GTF file.\n\n  Then for *Gene model (gff3,gtf) file for splice junctions* set this to\n  our GTF file.\n\n\n3.2 Setting Advanced Parameters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n  Now, under the *Configure Chemistry Options* set this \"Cell Ranger v2\"\n  as we determined from our barcode size before.\n\n  Finally we need to tell RNA STARsolo that the barcode size is variable\n  due to all the poly-T bases in the barcode sequence. If you scroll\n  down in the RNA STARsolo tool there is an option *Barcode Size is same\n  size of the Read*, which is set to Yes by default. Set this to \"No\".\n\n  After that hit execute, and your jobs should commence!\n\n  Best, Mehmet"
    }
  ],
  [
    {
      "role": "user",
      "text": "Md simulation stopped suddenly and xtc file have not been produced. I restarted several times but got same error. Please suggest me to overcome this error."
    },
    {
      "role": "system",
      "text": "I tried rerunning your job, just changing the number of steps to 500, and it completed without any error. So the problem doesn\u2019t appear to be on the GROMACS side.\nI don\u2019t have any helpful suggestion, but I recommend you use your own resources for conducting these kind of simulations.\nAll the best,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!\nI\u2019m trying to do QC on my RNAseq samples, and after mapping with STAR (no gene counts, 2-pass, hg38 with GFF file for spli junctions) and passing the BAM file to Qualimap RNA-seq QC, I get a error.\nThe problem is that the program doesn\u2019t even spit the error, as seen in the image.\n\nimage1581\u00d7716 41.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/48a16d45e624e8c5742b8fc9645b9b551562c07b.png)\n\"\nJava memory size is set to 1200M\nLaunching application\u2026\ndetected environment java options -Djava.awt.headless=true -Xmx7680m\nQualiMap v.2.2.2-dev\nBuilt on 2019-11-11 14:05\nSelected tool: rnaseq\nInitializing regions from features.gtf\u2026\nInitialized 1\n\"\nUpon further inspection, this is the error:\n\"\n\nDataset Error Report\nAn error occurred while running the tool  toolshed.g2.bx.psu.edu/repos/iuc/qualimap_rnaseq/qualimap_rnaseq/2.2.2d+galaxy1 .\n\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 255 ()\nTool generated the following standard error:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/031/100/31100658/_job_tmp -Xmx7g  -Xms256m\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/031/100/31100658/_job_tmp -Xmx7g  -Xms256m\nFailed to run rnaseq\njava.lang.IllegalArgumentException: Comparison method violates its general contract!\nat java.base/java.util.TimSort.mergeLo(TimSort.java:781)\nat java.base/java.util.TimSort.mergeAt(TimSort.java:518)\nat java.base/java.util.TimSort.mergeForceCollapse(TimSort.java:461)\nat java.base/java.util.TimSort.sort(TimSort.java:254)\nat java.base/java.util.Arrays.sort(Arrays.java:1441)\nat org.bioinfo.ngs.qc.qualimap.common.TranscriptDataHandler.createHelperMaps(TranscriptDataHandler.java:652)\nat org.bioinfo.ngs.qc.qualimap.common.TranscriptDataHandler.constructTranscriptsMap(TranscriptDataHandler.java:175)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.loadRegionsFromGTF(ComputeCountsTask.java:702)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.initRegions(ComputeCountsTask.java:608)\nat org.bioinfo.ngs.qc.qualimap.process.ComputeCountsTask.run(ComputeCountsTask.java:479)\nat org.bioinfo.ngs.qc.qualimap.process.RNASeqQCAnalysis.run(RNASeqQCAnalysis.java:68)\nat org.bioinfo.ngs.qc.qualimap.main.RnaSeqQcTool.execute(RnaSeqQcTool.java:221)\nat org.bioinfo.ngs.qc.qualimap.main.NgsSmartTool.run(NgsSmartTool.java:190)\nat org.bioinfo.ngs.qc.qualimap.main.NgsSmartMain.main(NgsSmartMain.java:113)\n\"\nI\u2019ve used the BAM file that STAR generates and the same GFF reference I used to align with STAR.\nAny ideas?"
    },
    {
      "role": "system",
      "text": "@system\nUPDATE: The smaller datasets completed running ok in the .org instance, although it took much more time to start/complete. The bigger one is still running on .org.\nAlso there are interesting differences about the org/eu runs:\n\nEu\n\nhas Command Line and Tool Standard Output text; Tool Standard Error empty:\n\nimage789\u00d7209 10.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/74beb08c333dfb618af7bcfd0d359db2681568a2.png)\n\nOrg\n\nhas Command Line empty, but has Tool Standard Output and Tool Standard Error text:\n\nimage716\u00d7216 9.46 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7cf7846a893f50bd38f8fd36d8e02a4eeb28d21c.png)\nOutput values are the same:\n\nEu\n\n\n\n\n\nReference size\n12,567,654\n\n\n\n\nNumber of reads\n267,167\n\n\nMapped reads\n267,167 / 100%\n\n\nUnmapped reads\n0 / 0%\n\n\nMapped paired reads\n0 / 0%\n\n\nSecondary alignments\n0\n\n\nSupplementary alignments\n1,737 / 0.65%\n\n\nRead min/max/mean length\n30 / 151 / 94.83\n\n\nOverlapping read pairs\n0 / 0%\n\n\nDuplicated reads (flagged)\n0 / 0%\n\n\nDuplicated reads (estimated)\n48,903 / 18.3%\n\n\nDuplication rate\n16.42%\n\n\nClipped reads\n143,727 / 53.8%\n\n\n\n\nOrg\n\n\n\n\n\nReference size\n12,567,654\n\n\n\n\nNumber of reads\n267,167\n\n\nMapped reads\n267,167 / 100%\n\n\nUnmapped reads\n0 / 0%\n\n\nMapped paired reads\n0 / 0%\n\n\nSecondary alignments\n0\n\n\nSupplementary alignments\n1,737 / 0.65%\n\n\nRead min/max/mean length\n30 / 151 / 94.83\n\n\nOverlapping read pairs\n0 / 0%\n\n\nDuplicated reads (flagged)\n0 / 0%\n\n\nDuplicated reads (estimated)\n48,903 / 18.3%\n\n\nDuplication rate\n16.42%\n\n\nClipped reads\n143,727 / 53.8%\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good morning,\nI run roary for 59 genomes, I got 600 core genes, but the alignment file is empty.\ncan someone please help me understand why, I ran roary two times and I got the same result.\nthank you"
    },
    {
      "role": "system",
      "text": "Update 2023-03-07\nFile naming issue resolved. Use version 3.13.0+galaxy2 or later.\n\nHi @system @system @user\nRoary is one of the few tools that interprets dataset names by necessity. Most other tools don\u2019t \u2013 metadata like the datatype, or the actual dataset contents are used instead.\nWorkarounds that tend to solve most issues with weird results from this tool are listed in the issue ticket here: Roary 3.13.0 fails at usegalaxy.org -- likely installation issue \u00b7 Issue #293 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/293#issuecomment-605089574)\nIf those don\u2019t work for some reason, we would definitely like an example in a shared history link (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history). A small public example is best as we might want clone it to include in that ticket\u2019s workaround or reassess whether any part of the issue can be addressed. And if the tool is actually silently failing for suspected resource reasons with larger data/stricter params, an example would be helpful. Post the shared link here as a reply (public), or ask @system or me to start up a private message to share it in (if not shared already!).\nEvery example of odd failures or odd outputs I\u2019ve reviewed was not dependent on the number/size of inputs \u2013 all were dataset naming issues. But that could be a biased set. While it is certainly possible to create a really massive computationally expensive job that fails to produce results at a public site (could happen at one or multiple \u2013 each uses different resources), those usually result in \u201cred\u201d error results. Very few produce empty (putatively successful) \u201cgreen\u201d results, and I don\u2019t think that is a known yet about this particular tool.\nThanks in advance for the feedback!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy team,\nI would like to use HiSat or Star for aligning Zebrafish RNAseq data. Under Reference Genome there is a note \u201cIf your genome of interest is not listed, contact the Galaxy team (\u2013genomeDir)\u201d. If this is not the right contact, kindly redirect me.\nThank you."
    },
    {
      "role": "system",
      "text": "@user on @link https://usegalaxy.eu we have installed the Zebrafish Genome. We try to harmonize all reference genomes in the near future.\nHope that helps!\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am currently in the midst of performing DGE analysis through a HISAT2, StringTie and DeSeq2 approach. I am at the stage where I need to use feature counts for my HISAT2 and StringTie merge, but everytime I try to run the program I get an error with this messagw \u201cUnable to run this job due to a cluster error, please retry it later\u201d. I then ran stringtiemerge again and the same message appeared. Prior to today when I used StringtieMerge with the same dataset it worked. I\u2019m just wondering is this a server issue with galaxy?"
    },
    {
      "role": "system",
      "text": "Thanks, we confirmed there was a problem impacting some jobs that is now resolved.\nFor anyone that ran into this problem, please rerun any jobs that failed for this reason.\nJobs that are currently queued and seem \u201chung\u201d should process on their own as cluster resources are available. The cluster is quite busy, so some queue wait-time is expected. Jobs will process faster if allowed to stay queued.\nHowever, if any job stays queued longer than 24 hrs, a rerun is probably Ok for this specific situation. Warning: this is normally not recommended \u2013 if a job is queued, any delete/rerun just puts the new rerun job back at the end of the queue, increasing wait time. But a rerun might be needed for some of these (due to the nature of the original problem)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m getting the following error.\n\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid\n\nThe analysis on a dataset collection results in some completing fine, the rest with this error. Thanks for any help!"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe fixed this a bit earlier today at @link http://UseGalaxy.org. Please try a rerun for any jobs that failed this way.\nThanks for reporting the issue!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have a gene expression matrix that is taken from the logarithm of gene expression values.\nNow I want to return the logarithm of values to the normal state, how to convert the value from logarithm to normal?\nAnd what tool should I use?"
    },
    {
      "role": "system",
      "text": "Hi @user\nit seems Datamesh does not have power transformation option but you can try awk. Assuming you have log2 values, calculate raw values with 2^$column_number_with_log_values and print columns you need.\nHere is an example:\n@link https://usegalaxy.org.au/u/igor/h/powertransformation\nLog values are in column 2, and I printed the whole line plus additional column with raw/non-transformed values.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI\u2019m following the tutorial for ATAC-seq analysis for the platform. They use Genrich for peak calling but it doesn\u2019t show up when I look for it in \u201ctools\u201d. Isn\u2019t it available yet?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi,\nI am very happy to see that this tutorial is used.\nWhen you go to the tutorial webpage, in the overview frame, you have an icon written Galaxies. If you click on it, you will see where you can run the full pipeline. As you will see, you can run it on @link http://usegalaxy.eu.\nI hope it helps,\nBest"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI have mounted the singularity containers from the galaxyproject with cvmfs and am wondering what the expected speed and responsiveness should be. Currently listing the files takes a while the first time, but I think that is expected. But using a container is also a bit slow. I am in the EU and I thought it would use the Freiburg mirror (cvmfs1-ufr0), also because I enabled the GEOAPI function. But looking at the network traffic it seems to connect the penn state / psu server, which is off course far away. Is this expected? Or should I change something in the setup.\nPart of the config\nCVMFS_SERVER_URL='http://cvmfs1-psu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-iu0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-tacc0.galaxyproject.org/cvmfs/singularity.galaxyproject.org;http://cvmfs1-ufr0.galaxyproject.eu/cvmfs/singularity.galaxyproject.org;http://cvmfs1-mel0.gvl.org.au/cvmfs/singularity.galaxyproject.org'    # from /cvmfs/cvmfs-config.galaxyproject.org/etc/cvmfs/domain.d/galaxyproject.org.conf\nCVMFS_SHARED_CACHE=yes    # from /etc/cvmfs/default.conf\nCVMFS_STRICT_MOUNT=no    # from /etc/cvmfs/default.conf\nCVMFS_SYSLOG_FACILITY=\nCVMFS_SYSLOG_LEVEL=\nCVMFS_SYSTEMD_NOKILL=\nCVMFS_TIMEOUT=5    # from /etc/cvmfs/default.conf\nCVMFS_TIMEOUT_DIRECT=10    # from /etc/cvmfs/default.conf\nCVMFS_TRACEFILE=\nCVMFS_TRUSTED_CERTS=\nCVMFS_USER=cvmfs    # from /etc/cvmfs/default.conf\nCVMFS_USE_CDN=yes    # from /etc/cvmfs/default.local\nCVMFS_USE_GEOAPI=yes    # from /etc/cvmfs/default.local\n\nOutput from iftop and nethogs:\n# iftop\nserver     => cvmfs1-psu0.galaxyproject.org             23.1Kb  32.0Kb  39.1Kb\n      <=                                                      1.74Mb  2.56Mb  3.31Mb\n# nethogs\n  13093 cvmfs    /usr/bin/cvmfs2      ens2f0      5.861     456.206 KB/sec\n\nThanks for sharing experience and insights!"
    },
    {
      "role": "system",
      "text": "Hi!\nThe public key that you pasted is correct. There was a problem with our server. At the moment, I expect data.galaxyproject.org, main.galaxyproject.org, sandbox.galaxyproject.org, singularity.galaxyproject.org, refgenomes-databio.galaxyproject.org and usegalaxy.galaxyproject.org to work on the Freiburg server. Can you try again?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI am running Roary for a pangenome analysis on Prokka output. The jobs are automatically being paused. I have tried several time to \u201cresume jobs\u201d via the history options with no success.\n\nScreenshot 2023-03-08 172551371\u00d7935 62.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c19ad0ea6890a0e3be8e93551a68a63226bf29ad.png)"
    },
    {
      "role": "system",
      "text": "So, this turned out to be a non-trivial case, but:\nthe key to finding out what\u2019s wrong was to expand the view of one of your paused jobs. When you do that it says: \u201cInput dataset \u2018IHIT32037\u2019 was deleted before the job started. To resume this job fix the input dataset(s).\u201d\nThat\u2019s rather clear, but the challenge is to locate that dataset inside your large input collections. How I did this was via the advanced search interface (downwards-facing arrows at the top of the history panel) and configure it like shown in the screenshot (filter by name: IHIT32037; deleted: Yes).\nThis will reveal two deleted datasets from two different collections (the two datasets are actually at index number 1494 inside their collections, but unfortunately the current UI doesn\u2019t have a way to show the deleted state from inside the collection - that\u2019s why @system\u2019s and my own previous advice wasn\u2019t really helpful). If you undelete these, you should be able to resume the paused jobs or to run the tool again without getting into the paused step.\n\nScreenshot from 2023-03-15 10-50-33284\u00d7646 27.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/830b34d77485f13ef5465325e18349c69218d0f6.png)\nNo idea how you arrived at this situation, but that should allow you to proceed finally."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run DESeq2 with featureCounts files from a single cell sequencing project. I have 8 different samples, so 8 collections of tabular files. Each file has a column with Geneid and a column with the count. When I try to run DEseq (4 samples from one group vs 4 samples in another group) I get an error message saying:\nError in data.frame(\u2026, check.names = FALSE) :\narguments imply differing number of rows: 135280, 235928, 226602, 203217, 210838, 214950, 279139, 133575\nCalls: get_deseq_dataset \u2026 eval \u2192 eval \u2192 eval \u2192 cbind \u2192 cbind \u2192 data.frame\nThe report also states that:\nThe tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\nHow can I resolve this?\nMany thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nTrimmomatic \u2192 RNAStar \u2192 Filter BAM datasets on a variety of attributes \u2192 StringTie \u2192 StringTie Merge \u2192 GffCompare \u2192 FeatureCounts\nTry this order instead, and omit GffCompare. The original protocol is creating a unified GTF to base counts on after the counts have been generated and is likely the source of the problem.\n\nTrimmomatic\n\nRNAStar (or HISAT2) If using HISAT2 be sure to use the output setting to create a BAM dataset compatible with Stringtie. This option is located under Advanced Options > Spliced alignment options > Transcriptome assembly reporting > select the radio button for \u201cReport alignments tailored for transcript assemblers including StringTie\u201d)\n\nFilter BAM datasets on a variety of attributes \u2013 this is optional, and usually used to restrict the BAM contents. I\u2019m assuming you are doing this on purpose and with a purpose specific to your analysis goals.\n\nStringTie Merge \u2013 enter just your reference annotation GTF under the input Reference annotation to include in the merging. This \u201cgrooms\u201d the GTF to a standardized format Stringtie can interpret.\n\nStringtie \u2013 using the groomed GTF from step 4\n\nStringTie Merge \u2013 enter the groomed GTF from step 4 plus all of the GTFs produced by Stringtie in step 5 (one per sample). This step is optional and depends on how you want to do the counting.\n\nIf you are only interested in known transcripts/genes, skip step 6.\nIf you are interested in both known transcripts/genes + novel transcripts/genes within your reads, run step 6 to create a unified GTF. Be aware that any novel transcripts/genes that do not merge with known transcripts/genes will be named by Stringtie.\n\n\n\nFeatureCounts \u2013 run this with the GTF output of either step 4 (known only) or step 6 (known + novel), along with all BAM mapped read results.\nProceed to DEseq2 using the same exact GTF used in step 7 and the current error about count inputs having a different number of lines (genes) will resolve.\nNote: The message about the same input count file being input twice is a common usage problem, so that warning is presented to help with troubleshooting whenever the tool fails. From your screenshot and description of the inputs to DEseq2, this doesn\u2019t seem to be what is going on for your case.\nWhy are are removing extra header lines from GTFs created by Stringtie is unclear. The reference GTF for known transcript/genes may contain extra comment lines, and those should be removed, but Stringtie GTFs should contain one header line (contains sample IDs). It looks like the sample ID header line is in your data \u2013 so maybe I misinterpreted what you are doing.\n\nCreating a unified GTF (known only, or known + novel) and using that to generate counts will then be based on the same set of transcripts/genes, and the count files will have the same number of rows (one per gene included in the unified GTF). GffCompare is a useful tool but not for DEseq2 (or edger or limma).\nPrior Q&A: Searching the Galaxy (@link https://galaxyproject.org/search/?q=%22stringtie+merge%22#gsc.tab=0&gsc.q=%22stringtie%20merge%22&gsc.page=1)\nThe first few results from that search points to Q&A that describes the solution (on our prior forum):\n\nStringTie and StringTie merge - when to apply the Guide gff (reference annotation file)? (@link https://biostar.usegalaxy.org/p/29938/)\nStringtie Merge (@link https://help.galaxyproject.org/t/stringtie-merge/319)\n\nOne issue ticket that describes the problem with the usage protocol above included (in part). This is very technical and isn\u2019t really needed \u2013 the same information is in this post and in the prior Q&A linked above: Stringtie 1.3.3 errors when the option to output Deseq2/EdgeR is used \u00b7 Issue #1322 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/1322)\nFAQ:Extended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\nFor others reading (@user does have this data): If you do not have a known transcript/gene GTF, that can be omitted. But you should still Stringtie merge all of the Stringtie GTFs before running Featurecounts to generate counts. For this type of usage, all transcript/gene names will be assigned by Stringtie (random but unique identifiers \u2013 the MSTRG.N content in your data are those types of identifiers, and many would resolve/merge into Known Genes if the protocol above is followed).\nTutorials: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy community,\nI\u2019am new to the Galaxy project. I tried using the Volcano plot tool. Everytime I got th efollowing error message: [1] \u201cHeader row detected\u201d\nError in log10(pvalue) : non-numeric argument to mathematical function\nCalls: print \u2026  \u2192 f \u2192 sca\nDoes anybody knows what to do? I\u2019am not familiar with bioinformatics or programming and really do not know what to do.\nThanks in advance.\nCS"
    },
    {
      "role": "system",
      "text": "Hi @system\nDid you apply the adjustments that @system suggested? The screenshot shows those are not done yet.\n\nRemove the header line\nReplace commas with dots in numerical values\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n\n\nGalaxy Training: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am having following goseq error while using two inputs shown below:\n\nimage295\u00d7697 5.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d25b2f657c22a24636c6b216bd3d4e479b8fd78d.png)\nimage\nAs a  genome file I used both gtf and fasta files of the potato genome but the error message has not changed (the file format was not indicated in the tutorial).\nHow should I proceed?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin that case the problem is that the gene annotation file should have this format: annotation file (@link https://raw.githubusercontent.com/galaxyproject/tools-iuc/master/tools/goseq/test-data/category.tab).\nThis paper (@link https://www.frontiersin.org/articles/10.3389/fpls.2020.01193/full) provides some information about how to obtain the GAF (Go Annotation File) corresponding to S. tuberosum.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI download some data from EBI website. So these datas are small rna seq data and the types are Fastq.gz. Is there any way to converting them to fastqsanger, i mean some how remove the gun zip(gz). ? I try that with changing their data set manually but in mapping step it make some errors.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi amir - I know this is a rather belated response but bowtie2 requires that it\u2019s input files be in the .fastqsanger/.fastqsanger.gz format which is effectively the same as fastq but provides a guarantee that the PHRED quality encoding is the more recent version. You can designate your file as fastqsanger at upload, edit their format from the history or feed them through the fastq groomer tool to convert them to fastqsanger"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having an issue mapping paired-end data when the data resides in a collection. I can upload 6 pairs of fastq files, make the collection, perform FastQC and Trimming. However, when I go to run minimap2, only 1 of the 6 pairs is successfully mapped; the other 5 pairs fail.  They all have the same the error:\nCould not display BAM file, error was:file has no sequences defined (mode=\u2018rb\u2019) - is it SAM/BAM format? Consider opening with check_sq=False\nI can map the 6 pairs individually, but I would much rather use collections. Why is this error specific to collected datasets?"
    },
    {
      "role": "user",
      "text": "I think it is a memory issue. I dropped the allocated memory to my virtualbox from 20 to 10 GB and, instead of 1 alignment working, both failed. If anyone knows a way to decrease minimap memory usage, please let me know."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m having trouble running the GeneFamilyClassifier tool on the main galaxy server. I have no problem running the command line version of this tool.\nHere is what I see. I can\u2019t even find what the problem is\u2026\n\nScreen Shot 2022-10-27 at 2.23.19 PM580\u00d7692 40.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9742677dd560deaa61432235ef543727ee1a4344.png)\nScreen Shot 2022-10-27 at 2.23.24 PM\nAny help is appreciated.\nBest"
    },
    {
      "role": "system",
      "text": "Ok, thanks. I\u2019ve updated an issue ticket here with the information from this tool and the error messages: Fix dependency for plant_tribes_kaks_analysis 1.0.3.0 \u00b7 Issue #535 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-tools/issues/535#issuecomment-1463020217)\nThe other tool in that ticket appears to be working Ok, but theoretically has an update pending.\nThis fix won\u2019t be immediate. Please follow that ticket for updates. Even if we split it off, all will be linked together. You can unshare your history now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am trying to locally install an instance of Galaxy on a MacOSX (High Sierra), and am getting the following error when I try the following command. Please help!\nsh run.sh\nInstalling node into /usr/local/Cellar/galaxy/.venv with nodeenv.\n * Install prebuilt node (10.13.0) .Traceback (most recent call last):\n  File \"/usr/local/Cellar/galaxy/.venv/bin/nodeenv\", line 10, in <module>\n    sys.exit(main())\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 1076, in main\n    create_environment(env_dir, opt)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 905, in create_environment\n    install_node(env_dir, src_dir, opt)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 692, in install_node\n    download_node_src(node_url, src_dir, opt, prefix)\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 542, in download_node_src\n    dl_contents = io.BytesIO(urlopen(node_url).read())\n  File \"/usr/local/Cellar/galaxy/.venv/lib/python2.7/site-packages/nodeenv.py\", line 564, in urlopen\n    return urllib2.urlopen(req)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 154, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 429, in open\n    response = self._open(req, data)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 447, in _open\n    '_open', req)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 407, in _call_chain\n    result = func(*args)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1241, in https_open\n    context=self._context)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1198, in do_open\n    raise URLError(err)\nurllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:727)>\n"
    },
    {
      "role": "user",
      "text": "Update on this, my issue was that I have urllib3 installed, but it was looking for urllib2, hence the errors. I have updated the galaxy path to use urllib3 and it is now installed successfully"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am having a weird issue trying to push my tool (a wrapper for alevinQC) to the main toolshed. I used planemo to prepare my submission to the toolshed.\nI have a working version on the test toolshed: @link https://testtoolshed.g2.bx.psu.edu/view/florian.wuennemann/alevinqc/49fe36f5ad92\nI tried pushing to the main toolshed using planemo and it created an entry but somehow that entry is not findable by others in the toolshed and I can\u2019t install it in my galaxy from the main toolshed:\n@link https://toolshed.g2.bx.psu.edu/view/fwuennemann/alevinqc/000000000000\nWhen I try to interact with the main toolshed entry via planemo, it tells me it doesn\u2019t exist, but when I try to create a new toolshed entry, it tells me I own a tool with that name\u2026 (see code below)\nThanks for the help!\nplanemo shed_create --shed_target toolshed\nUnexpected HTTP status code: 400: {\"err_msg\": \"You already own a repository named <b>alevinqc<\\/b>, please choose a different name.\", \"err_code\": 400008}\n\nplanemo shed_diff --shed_target toolshed\nshed_diff: Repository [alevinqc] does not exist in the targeted Tool Shed.\n\n planemo shed_update --check_diff --shed_target toolshed\nRepository [alevinqc] does not exist in the targeted Tool Shed.\nFailed to update repository 'alevinqc' as it does not exist on the main Tool Shed.\n"
    },
    {
      "role": "user",
      "text": "I was finally able to fix this issue, by logging into the toolshed and uploading a tarball directly on the defective repo page for alevinqc. Now the entry seems to be correct and installable via toolshed. Any confirmation of correct install from a second source would be appreciated!\n@link https://toolshed.g2.bx.psu.edu/view/fwuennemann/alevinqc/5f0da20666fa"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am following the workflow on RNA-seq data analyses in galaxy but I have been having some challenges. I ran trimmomatic on my fastq files but it always return an error, even when the files did not give any error when I ran fastqc on them. I went further to align them to the human genome using hisat2 and it came out well, but when I ran the differential analyses with DESeq2, I encountered same error, please what could be the problem and how can I resolve this? Thank you."
    },
    {
      "role": "system",
      "text": "Not sure if you can do anything about it now. I came across this one Trimmomatic Error: Unable to detect quality encoding (@link https://www.biostars.org/p/329735/) and I don\u2019t see the option to choose the phred encoding. There is already an github isseu Trimmomatic error: unable identify Phred quality encoding \u00b7 Issue #79 \u00b7 fls-bioinformatics-core/galaxy-tools \u00b7 GitHub (@link https://github.com/fls-bioinformatics-core/galaxy-tools/issues/79). I think for now you could maybe try to use another tool like cutadapt or Trim Galore. (Trim galore uses cutadapt in the background but it has an \u201cautomatic detection feature\u201d)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI have been running a Trinity assembly to create a reference transcriptome and a gene-to-transcript map. It has been running for almost two weeks and I am wondering how I can see the progress or an estimated time for completion?\nRecently, it switched from \u201cthis job is currently running\u201d to \u201cthis job is waiting to run\u201d, and I would also like to know why this happened. I have been on a time crunch for a project and would really really love to have some sort of time estimate for when this tool can finish.\nThank you for your help!\nJulie"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nIs there any possible way that you can unpause this job?\nA \u201cgray\u201d colored job is waiting for resources to become available. Meaning, it is in the job queue already.\n@quote\nIt is stuck in the \u201cwaiting to run\u201d phase, even though previous to being stopped it was running nonstop for two weeks (in the yellow phase).\n@quote\nI am on theusegalaxy.orgserver\nA \u201cyellow\u201d job is actively executing. It will end in either a \u201cgreen\u201d (successful) or \u201cred\u201d (error) state at @link http://UseGalaxy.org. Some other public Galaxy servers do this a bit differently, and will automatically rerun a job one or more times if it originally failed. An example public Galaxy server that automatically reruns all failed jobs one time is @link http://UseGalaxy.eu.\nThat said, the cluster that runs Trinity jobs at @link http://UseGalaxy.org did undergo some configuration changes over the last month. Some jobs were impacted and administratively rerun. My guess is that is what happened in your case as you explain it, and from what I can see in your account.\nThe Trinity job was started on Feb 12. It is batched with 8 total paired-end inputs, pooled = yes, and no QA/QC was done on the reads, or rather, no QA was included in the history where this work is included. The data appears to have been downloaded from SRA and directly input to Trinity (?).\n\u201cPooled\u201d can be set to either yes or no.\n\nFor \u201cpooled = yes\u201d, that means all inputs will be combined into one assembly. The datasets are quite large to be pooled, between ~10-20 GB for each end of each pair. An assembly at a public Galaxy server (any) is unlikely to be successful with that much data run as \u201cpooled = yes\u201d. It will run out of resources and fail, for either runtime or memory reasons.\nFor \u201cpooled = no\u201d, there will be one assembly job/result per paired-end read input. Some of your pairs would probably assemble, but it is unlikely that all would, especially the first pair in the collection as it is now (the largest, with no QA).\n\nWe granted you some extra quota space already, so sent you links about the ways to use Galaxy, but I also added them below again for others reading. Public servers usually not a good choice for any single job that involves a very large amount of data. Many smaller individual jobs are usually fine. Quota space is storage space, and unrelated to the amount of memory/resources a tool uses when executing. Public Galaxy servers have significant computational resource allocations but not as much as what you can set up yourself in your own Galaxy (for practical reasons).\nWhat to do from here:\nIf you didn\u2019t intend to pool the data, then you should rerun it as \u201cpooled = no\u201d. You can still use a collection input. Definitely do some QA first. If any of the jobs fail, then you\u2019ll either need to adjust the inputs (downsample the reads with a tool like Seqtk) or run the job at a Galaxy server with more resources allocated (your own).\nIf you did intend to pool the data, expect this job to fail for resource reasons. QA won\u2019t make a difference by itself at @link http://UseGalaxy.org, but you should do some QA if/when you decide to run the job at your own Galaxy server. Downsampling + QA might help at @link http://UseGalaxy.org, but you would need to test that to see how much is needed to get a successful job \u2013  and decide if the assembly using that much downsampling is still of value to you. The maximum total size of all inputs sent to a Trinity job is between 20-35 GB when working at public servers. This is not exact since the size of the data is just one factor \u2013 the read content is another (and is why a QA step is strongly recommended).\nWays to use Galaxy:\n\n\nWebinar: Webinar: Use Galaxy on the web, the cloud, and your laptop too (@link https://galaxyproject.org/events/2020-12-webinar-where/)\n\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources (@link https://galaxyproject.org/use/)\n\n\nDecision matrix: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources (@link https://galaxyproject.org/use/#which-platform-platform-type-to-choose)\n\n\nThe GVL version of Cloudman is one choice and AWS offers grants. AWS Programs for Research and Education (@link https://aws.amazon.com/grants/)\n\n\nHope that helps you to make some decisions about how to proceed.\nJen"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\ngenome.ucsc.edu is down due to a disk array failure last night. It\u2019ll come back soon, but in the meantime, can Galaxy users open their files on genome-euro.ucsc.edu ? Is there any workaround for them to get the URL to the file on Galaxy and paste it on the genome-euro.ucsc.edu ?\nThanks!\nMax"
    },
    {
      "role": "system",
      "text": "the HEAD request for metadata files has been implemented in Allow HEAD request for requesting metadata files by martenson \u00b7 Pull Request #16113 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/pull/16113) and will be available in the next release"
    }
  ],
  [
    {
      "role": "user",
      "text": "Silly, simple question,  but I can\u2019t find a good solution.\nI have 2 VCF files being produced in a workflow, and I\u2019d like to combine these files as a single VCF.  The bcf_concat tool will do it, but how do I feed both files to the tool in a workflow?\nCheers,\nMat"
    },
    {
      "role": "system",
      "text": "I actually do this a bit differently than what @system suggested.\nUse the Merge Collections tool to merge all of your collections into one big one, but set the\n\u2018How should conflicts (or potential conflicts) be handled?\u2019 advanced option to \u2018Append suffix to every element identifer\u2019 and the default suffix \u2018_#\u2019. Then send that into the Apply Rules tool with the following rule:\n{\n    \"rules\": [\n        {\n            \"type\": \"add_column_metadata\",\n            \"value\": \"identifier0\"\n        },\n        {\n            \"type\": \"add_column_regex\",\n            \"target_column\": 0,\n            \"expression\": \"\\\\d+$\"\n        }\n    ],\n    \"mapping\": [\n        {\n            \"type\": \"list_identifiers\",\n            \"columns\": [\n                1,\n                0\n            ],\n            \"editing\": false\n        }\n    ]\n}\n\nThe benefit of this method is it works with any amount of input collections. You can then use the Relabel Collection tool to change the identifiers from numbers if need be."
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried Create a collection from a list of datasets, then the error shows:\n\u00d7The following selections could not be included due to problems:\n\nPaired-end data (fastq-dump) is a collection, this is not allowed\nPaired-end data (fastq-dump) is a collection, this is not allowed\n\nAt least one element is needed for the collection. You may need to [cancel ](javascript:void(0))and reselect new elements.\nCould anybody let me know how to do that in Galaxy?\nThank you,"
    },
    {
      "role": "system",
      "text": "@quote\nBut if we have same situation which we just have individual paired files, how we can add them together as one connection file.\nIf data are ever in a collection, organized in a way that you don\u2019t want to use, and the collection manipulation tools are not working for some reason, you can try this:\n\nGo into the Hidden tab of your history\nUnhide the datasets you want to restructure\nCreate new collections\n\nCollections are like folders. The files inside them are datasets. It is fine to delete collection folders that you don\u2019t need \u2013 just be sure to not deleted the dataset files inside of collections or you will need to upload the data again."
    }
  ],
  [
    {
      "role": "user",
      "text": "hi there, I uploaded to @link http://usegalaxy.org some fasta files with mozilla, but when I click to start the import of those file in the \u2018choose FTP file\u2019 I get a warning: bad request (400) error. tried to reupload those files with filezilla but still the same error. can someone help me?"
    },
    {
      "role": "system",
      "text": "Update 4/21/2021\nFTP Upload has been corrected at @link http://UseGalaxy.org\nThanks again for reporting the problem!\n\nHi @system \u2013 Ok, I can confirm the problem. A few files will load, others fail for the same reason as you report.\nThis is likely related to the reconfiguration going on right now but I\u2019ll confirm that with the dev team.\nAn immediate solution would be to use a different public Galaxy server right now if your work is urgent."
    }
  ],
  [
    {
      "role": "user",
      "text": "As the Galaxy tools use the CPU, but some of the tools (GROMACS) and similar are able to use the GPU for better performance as well.\nIs it possible to run the galaxy or specific tools on GPU?\nKindly suggest some tutorials or help to run GROMACS on GPU through Galaxy."
    },
    {
      "role": "system",
      "text": "Yes, this is a general Galaxy feature.\nPlease have a look at this file: galaxy/job_resource_params_conf.xml.sample at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/job_resource_params_conf.xml.sample)\nAnd here the part for the job_conf.xml file. galaxy/job_conf.xml.sample_advanced at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/job_conf.xml.sample_advanced#L1090)\nPlease also note that we have the entire @link http://usegalaxy.eu confuration online on GitHub. See here for example: infrastructure-playbook/job_resource_params_conf.xml at master \u00b7 usegalaxy-eu/infrastructure-playbook \u00b7 GitHub (@link https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/config/job_resource_params_conf.xml)\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "run Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d\nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler?\nThanks."
    },
    {
      "role": "system",
      "text": "Thanks @system\nWe did some more adjustments this morning at @link http://UseGalaxy.org. All tests were successful :partying_face:\nAnyone that had a recent failure with this tool should please try again now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have been running into issues when using Unicycler on @link https://usegalaxy.eu/ (job fails in seconds).\nSo I have been wanting to try to repeat the job on @link https://usegalaxy.org/ as sometimes something won\u2019t work on a server but will on another one.\nTo do so I needed my long reads so I used the \u201ccopy link\u201d to import my data to the US server.\nBut I get an error message after a few seconds:\n\nerror\nAn error occurred with this dataset:\nThe uploaded file contains invalid HTML content\n\nWhat I tried to transfer is a fastq file originating from filtlong (ran on the European server with no error).\nCan anybody help?\nThanks\nVanina"
    },
    {
      "role": "system",
      "text": "@quote\nUsing default server-settings atusegalaxy.eurequired that the history and the datasets inside it were set to a \u201cshared by link\u201d state in order to transfer a dataset fromusegalaxy.eutousegalaxy.org.@systemmay have comments about whether that is expected or not.\nThat\u2019s a very good observation @system (learnt something about our server config from it ;))\nWhether or not accessing (and thus uploading to a different server) datasets via known links works or not by default is controlled by your settings in User -> Preferences -> Set Dataset Permissions for New Histories. In there you have an access category, and users that you have listed under it will have link-based access to any of your datasets. Now if you leave this list empty, it means any  user has access.\nApparently, the default setting for newly created accounts has changed on @link http://usegalaxy.eu (maybe also on @link http://usegalaxy.org?) at some point (about 2 years ago it seems). Old accounts (like mine) have an empty list, while newer accounts start out with just the individual user\u2019s email.\nIf you have an account with restricted access, you will have to Share via Link to make data tarnsfers work as @jennai pointed out. If you have the unrestricted setting, data transfers will work out of the box (at the cost of your datasets being potentially discoverable by anyone).\nAlso note that the setting is called \u201cSet Dataset Permissions for New Histories\u201d. Whatever changes you make to it will, thus, not change properties of datasets in existing histories!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy team!\nI started training on Label-free data analysis using MaxQuant.\nI have a problem with \u201cVisualize this data\u201d on the cut tool result.\nWhen I selected box plot(jqPlot) it did not visualize data.\nInstead a message \u201cPlease confirm the settings before rendering the results\u201d appeared.\nWith best regards,\nArman"
    },
    {
      "role": "system",
      "text": "Ok, thanks for clarifying. I\u2019m looking now\u2026\nHi @system\nThe how-to for the job seems to loop, and never produces a graphic. It cannot even be downloaded. I tried modifying the chart metadata to make the naming \u201cunique\u201d but that was not enough.\nThis seemed specific to certain chart types, and I found an issue ticket. The correction will be available at the UseGalaxy.* servers after the next release. No \u2026 sorry, that was a different chart type. I opened up a new issue, and used your example from the tutorial (copied over to the @link http://usegalaxy.org server to make sure not server-specific problem).\n\n\n@link https://github.com/galaxyproject/galaxy/issues/16191\n\n\n\n\n\n\n\n\nVisualizations/Charts: Boxplot fails to render (@link https://github.com/galaxyproject/galaxy/issues/16191)\n\n\n\n        opened 07:03PM - 05 Jun 23 UTC\n\n\n\n\n          jennaj\n         (@link https://github.com/jennaj)\n\n\n\n\n\n\n\n**Describe the bug**\nBoxplot fails to render. Tabular output is generated, but \u2026 (@link )no graphic. Either in Galaxy or when downloaded (\"blank png\").\n \nSeems related to https://github.com/galaxyproject/galaxy/issues/13437\n\n**Galaxy Version and/or server at which you observed the bug**\nGalaxy Version: (check <galaxy_url>/api/version if you don't know)\nCommit: (run `git rev-parse HEAD` if you run this Galaxy server)\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create a boxplot graphic\n2. Wait for that to process\n3. No graphic\n4. Seems to loop back into creating the graphic if that is access from User > Visualizations\n\n**Expected behavior**\nCreate a graphic. Other charts do plot on the same input, including the formatted output from this chart type.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Additional context**\n\nHistory with output from the boxplot tool. Never renders.\nhttps://usegalaxy.org/u/jen-galaxyproject/h/test-ghelp-8152\n\nThis is the blank graphic produced...\n![New Chart](https://github.com/galaxyproject/galaxy/assets/8999735/b2b7da34-bc35-4d87-88d9-37e2c7e3eff9)\n... in between here.\n\n\n\n\n\n\nI\u2019m not sure of an exact workaround \u2026 but the tabular file is an appropriate input. There is likely a way to use the application directly as an alternative for the graphing portion. @link http://www.jqplot.com/\nApologies for missing this earlier!!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having trouble viewing the 512 successfully aligned TCR clonotypes from MiXCR. See 1) report of successful MiXCR run with 513 clonotypes and 2) empty clonotype file\n\n\n\n\nAnalysis date: Thu Oct 28 16:38:14 CEST 2021\nInput file(s): mixcr_analysis.clna\nOutput file(s): mixcr_analysis.contigs.clns\nVersion: 3.0.5; built=Mon Feb 25 00:04:20 CET 2019; rev=af538f7; lib=repseqio.v1.5\nCommand line arguments: --report mixcr_analysis.report mixcr_analysis.clna mixcr_analysis.contigs.clns\nAnalysis time: 59.16s\nInitial clonotype count: 513\nFinal clonotype count: 513 (100%)\nLongest contig length: 553\nClustered variants: 0 (0%)\nReads in clustered variants: 0.0 (0%)\nReads in divided (newly created) clones: 0.0 (0%)\n\n\n\n\nimage"
    },
    {
      "role": "system",
      "text": "Hi, I am one of the developers of MiXCR. If you are still having that issue please try the latest version (v.4.2) of mixcr and let me know if there are still any issue. Also, since the interface changed a bit I can help you with writing the command if needed.\nAll the best"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, my input is 9M reads. I ran it through samtools mpileup following BWA alignment. I set the max per-file depth to 1M and I got an output of about 250,000 reads. I know I didn\u2019t lose that much to quality, so why is my output number higher? I also noticed that if I set the max per file depth to anything between 300,000-1,000,000, I still get 250,000 as my output. Does samtools have a read limit?"
    },
    {
      "role": "system",
      "text": "So you are expecting more then 250,000 reads per position. You could double check this with Samtools depth. Also in samtools mpileup you can disable the max depth by using a value of 0.\nEDIT:\nYou also may need to rephrase your question.\n\nI also noticed that if I set the max per file depth to anything between 300,000-1,000,000, I still get 250,000 as my output.\n\nIf the max is 300,000 and the depth is lower then that (250,000) why would it change? I would expect a change if you set the max-depth to a lower then 250,000 value. Like 8000 or something"
    }
  ],
  [
    {
      "role": "user",
      "text": "I ran into this error when running Trinotate on Galaxy Europe:\n\nThe job creating this dataset has been resubmitted\nAn error occurred with this dataset:\nThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource. memory memory memory memory * Running CMD: wget \"@link ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/unipr\n\nDoes this mean the job is running again, or do I need to resubmit?"
    },
    {
      "role": "system",
      "text": "Update\nIssue ticket: Bug: Trinotate (Galaxy Version 3.2.1) \u00b7 Issue #3875 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/3875)\n\nHI @system\nThis looks like an actual bug. Specifically, the environment created for the job is failing to retrieve one of the required inputs (from a 3rd party\u2019s site).\nYou are working at @link http://UseGalaxy.eu, correct? If so, please submit your error as a bug report and include a link to this topic at Galaxy Help in the comments. Both provide technical context. Step 4 here explains how: Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.  Twice in the month of July I\u2019ve been unable to successfully run RNA STAR 2.7.8a on the @link http://usegalaxy.org server.  During the first week of July I had this issue and deleted my job after a week of waiting.  I\u2019m currently attempting another job, which has remained queued for the last 6 days.  I\u2019m curious if this is expected behavior for this application.  Thank you and I apologize for the bother."
    },
    {
      "role": "system",
      "text": "Dear @user,\nUnfortunately, it is. Depending on the organism, your input data, some settings, and the day, STAR can take very long.\nFor example:\n\nMapping to human in comparison to cyanobacteria takes longer.\nThe deeper you sequence the longer it will take to map your data.\n\nUse 2-pass mapping for more sensitive novel splice junction discovery activating this function also takes longer.\nIf you use also a non-standard genome/transcriptome, which is not supported by Galaxy, then STAR takes especially longer, because it has to first generate an index file and then runs the mapping.\nIt could be that you have used Galaxy on days, when a workshop, conference or training was given. During those peak times, jobs can take a bit longer.\n\nI hope I could clarify, why your job might have taken so long. Maybe to speed up your process here are few things you can do:\n\nRe-run your job, like you have done already.\nCheck a few settings for STAR to speed up the process (like the 2-pass mode, if you have selected it).\nIf you have used a non-standard genome/transcriptome, then maybe it is better to first run RNA STAR locally once to generate the index file, which can then be provided for Galaxy.\n\nI hope I could help.\nHave a good week and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI would like to do a metagenomics analysis with the \" MetaPhlAn2 to profile the composition of microbial communities \" tool from Galaxy and using as input file, one that is the output of \" FASTQ joiner on paired end reads \" tool. Because I would like to use the forward and reverse fastq sequences from the same sample at the same time.\nHowever, I always get an error and failure when I try this. It seems to work only with the forward or the reverse fastq sequence, but no when they\u00b4re together.\nSomeone could help me? Is there specific parameters that I should have in mind?\nThanks!"
    },
    {
      "role": "system",
      "text": "If your method by joining the paired reads and execute methaplhan2 is the right method I don\u2019t know. But Illumina data should just work. As far as I know fastq and fastqsanger is basically the same format. But the datatype \u201cfastqsanger\u201d is a galaxy way to make sure it got the ASCII_BASE 33 quality scores. Only it is possible that the galaxy tool or in other words the wrapper is created to give the output the galaxy datatype fastqsanger. But it is just a FASTQ file.\nSo now you will think, why does it not work then if fastq and fastqsanger is the same format. This is because metaphlan2 has a parameter --input_type and will get the value of the galaxy datatype of the input file.\nIf you are interested you can see it here: @link https://github.com/galaxyproject/tools-iuc/blob/master/tools/metaphlan2/metaphlan2.xml\nIn summary Illumina data just works but after FASTQ joiner you need to change the galaxy datatype to fastq because the metaphlan2 galaxy wrapper is created that way. It has nothing to do with your data itself but with galaxy.\nIf, but I highly doubt it you got ASCII_BASE 64 quality score you could use something like @link https://toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer\nHere a page with the scores:\n@link https://drive5.com/usearch/manual/quality_score.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Keep getting \u201cThis is a new dataset and not all of its data are available yet\u201d for all plots I used (histogram, Spectra plot) and pretreatments. how can i fix this?"
    },
    {
      "role": "system",
      "text": "Hi @user, you see this when the input dataset is not yet available - for instance, when it hasn\u2019t uploaded yet, or is the output of another job which is still running. The input will turn green when it is ready."
    }
  ],
  [
    {
      "role": "user",
      "text": "I aligned RNA-seq reads against a custom-build reference genome using Bowtie2. I use the produced BAM with a \u201cchromosom\u201d txt file to generate a WIG file using \u201cBAM to Wiggle\u201d but I get a message error\u2026\nExecution resulted in the following messages:\n\u201cFatal error: Exit code 1 (An error occured during execution, see stderr and stdout for more information)\u201d\n\u201cFatal error: An error occured during execution, see stderr and stdout for more information\u201d\nTool generated the following standard error:\n\u201cProcessing CP004075.1 \u2026\nProcessing CP001179.1 \u2026\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/bin/bam2wig.py\u201d, line 105, in \nmain()\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/bin/bam2wig.py\u201d, line 102, in main\nobj.bamTowig(outfile = options.output_prefix, chrom_sizes = chromSizes, chrom_file = options.chromSize, q_cut = options.map_qual, skip_multi=options.skip_multi,strand_rule = options.strand_rule, WigSumFactor=norm_factor)\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__rseqc@2.6.4/lib/python2.7/site-packages/qcmodule/SAM.py\u201d, line 2597, in bamTowig\nif strandRule[key] == \u2018+\u2019:Fwig[pos] +=1.0\nKeyError: \u2018+\u2019\u201d\nCould anyone help me please?\nThanks,\nAlicia"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI repeated the analysis in the previous version and it ran fine, then the problem seems to be related to the last update. I\u2019ll review it.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi !\nI try to access the UseGalaxy (.eu and .org) webpage on my old 2013 iMac, and I can\u2019t reach it\u2026 It\u2019s leading me to a blank page. I cleared my cache and it didn\u2019t help.\nCould it be due to the version of my iMac? Is there anything I can do?"
    },
    {
      "role": "user",
      "text": "Hi ! Sorry, it took me a while to answer.\nI updated Firefox, like you suggested, and it worked ! :smiley:\nThank you very much !!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Fastp jobs that were queued over 16 hrs ago have still not started running."
    },
    {
      "role": "system",
      "text": "Update 09/26/2022\nThe @link http://UseGalaxy.org server is still very busy.\nLeave jobs queued for the fastest processing.\n\nUpdate 09/23/2022\nAt @link http://usegalaxy.org we were able to confirm the delays and fixed a small technical issue.\nPlease leave queued jobs queued. Avoid deleting and rerunning as that will lose the job\u2019s original place in the queue.\nThanks to everyone who reported the problem!\n\nHi @user\nThe public servers might just be busy. @link https://status.galaxyproject.org/\nAre you working at @link http://usegalaxy.org? There aren\u2019t any known issues as of now, so this prior Q&A would apply. If you think more is going on, please post screenshots or share your history. If we find something on our own, we will post an update.\n@quote\nHi@systemAre these jobs queued (gray) or executing (yellow)?understanding-job-statusesA single trimmomatic job that has been queued for five days is probably Ok.\nA single trimmomatic job that has been executing for five days would be unusual.\n\nJobs run after any upstream jobs complete (inputs for the tool are ready to use) and as public shared resources appropriate for the tool become available. Some of your jobs run, then some of other people\u2019s jobs run, then more of yours, repe\u2026\nAnd, someone else reported longer than usual delays at @link http://usegalaxy.eu a few hours ago \u2013 no update yet: FastQC not running (@link https://help.galaxyproject.org/t/fastqc-not-running/8716)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to align my fastqsanger.gz files with HISAT2 but it is not working. The problem arose after I had successfully ran an initial set of samples for DEGs, using HISAT2 as part of it. Now that I uploaded the rest of my files, the concatenated fastqsanger.gz files will not go through HISAT2. I even try running my previously successful files with no avail, which makes me think it is in fact a HISAT2 or galaxy issue.  Can anyone help? Thanks in advance!"
    },
    {
      "role": "system",
      "text": "Update: Fixed\n@user\n@quote\nNote to all end users running mapping:Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.Ticket:https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was not able to upload fastq.gz files from my PC (size around 1.5 Gb) to the @link http://usegalaxy.eu. I tried it many times with different files; the message was: Warning: Upload request failed. (0)\nIs it possible to fix it somehow?"
    },
    {
      "role": "system",
      "text": "Hi tsa.s,\nthe instruction for using our FTP service are on Galaxy Europe | FTP service instructions (@link https://galaxyproject.eu/ftp)\nPlease take a look at the manual and make sure you are following all the directions."
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried running a FASTA sequence on AlphaFold2 and it showed me this error:\nE0902 16:02:23.065268 22797118093120 hhsearch.py:56] Could not find HHsearch database /data/pdb70/pdb70\nTraceback (most recent call last):\nFile \u201c/app/alphafold/run_alphafold.py\u201d, line 445, in \napp.run(main)\nFile \u201c/opt/conda/lib/python3.7/site-packages/absl/app.py\u201d, line 312, in run\n_run_main(main, args)\nFile \u201c/opt/conda/lib/python3.7/site-packages/absl/app.py\u201d, line 258, in _run_main\nsys.exit(main(argv))\nFile \u201c/app/alphafold/run_alphafold.py\u201d, line 353, in main\ndatabases=[FLAGS.pdb70_database_path])\nFile \u201c/app/alphafold/alphafold/data/tools/hhsearch.py\u201d, line 57, in init\nraise ValueError(f\u2019Could not find HHsearch database {database_path}')\nValueError: Could not find HHsearch database /data/pdb70/pdb70\nThis is the sequence I tried to run:\n\n33\nMPAPAPLLIILRRRIRKQAHAHSKGGGGSMR\nTSYLLLFTLCLLLSEMASGGNFLTGLGHRSD\nHYNCVSSGGQCLYSACPIFTKIQGTCYRGKA\nKCCKEAAAKFPGVGFPGVGFPGVGKPKPKPL\nGIYVYVRRKKSLSGCYWCLKKFSWWDPGFFI\nKKGLPIRASRLVFKKKRRCAVLLYKKVIFSN\nISNWVYKKRLGNGTFYKKLQVQFAVFKKNSP\nYIPYLLKKSFQAWLTNQPYKKCAQLRYLVFK\nKFIAVIPAANFKKYAIALVAELLKKMNITGY\nTDAYKKLLLAITSNIVKKLIKGNFLEYVKKS\nLSGCYWCLIKKIYVYVRRCVLKKFAVFGILP\nCVKKDSQFTLIKPYKKPLGLWARVLAEALII\nGFSWWDPGFFIIAAYSFQAWLTNQPYASFMV\nAAYFIAVIPAANFPGVLLTQAAYSVIFSNIS\nNWVYAAYTVYFLPNSIITNVFFIAAYMVSLI\nYLASFLFKSSSTAAAYRLISTALKARALSLD\nSAAYRRRLANSLRLATVIYGKAAYEIRWSFT\nLNCSASDSEAAYEYVRILMRKLLLQITRHHH\nHHH\n\nPlease let me know if there is an error with my input or within the tool\nThank you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe problem is caused by the tool itself. I suggest you to try to run your analysis in the Australian Galaxy server; you can apply in the following link:  AlphaFold 2.0 (@link https://site.usegalaxy.org.au/request/alphafold).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "HI\nMy trimmomatic analysis shows this error, Please what can I do?\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/jetstream2/scratch/main/jobs/47313312/_job_tmp -Xmx28g -Xms256m\nTrimmomaticSE: Started with arguments:\n-threads 8 fastq_in.fastqsanger fastq_out.fastqsanger SLIDINGWINDOW:4:20\nError: Unable to detect quality"
    },
    {
      "role": "system",
      "text": "hi @user\nfastp is available on public Galaxy servers, hence no need for command line use.\nI don\u2019t see any issue with your FASTQ file, but it is not compatible with Trimmomatic in Galaxy because of extended range of Phred quality score. I reduced the range of Phred quality score on a PacBio read, to match \u2018illumina\u2019 range, and Trimmomatic has no issue with that data.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am very naive to bioinformatics. I am using this tutorial (@link https://galaxyproject.org/tutorials/ngs/) to map a genome, but when I use multiQC tool to analyze my BAM file (MarkDuplicate Metrics) I get an error. The error is:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nModule 'picard: \u2018picard.analysis.AlignmentSummaryMetrics\u2019 not found in the file 'chlamyDNA_4_S7__001_fastq\u2019\nCould anyone help me please?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThere is an open issue with the tool that appears to be what is going wrong with your example. MultiQC fails when input datasets that have the same dataset name/label -- tool issue \u00b7 Issue #315 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/315)\nIn short, when inputs share the same \u201cname\u201d the tool cannot accurately define/label those inputs as distinct samples when creating the report. This creates a duplicated label conflict and fails the tool.\nSince your data is in a collection, try the tool  Collection Operations > **Relabel List Identifiers** from contents of a file to give the inputs unique names.\nOr, perhaps @system @system have a better suggestion? This wasn\u2019t an easy problem to address during the prior tool update.\nApologies for the confusion!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI would like to cite galaxy EU correctly. Regarding the information on this website Citing Galaxy (@link https://galaxyproject.org/citing-galaxy/) one is supposed to additionally use the first publication of a public instance used. Which publication (additionally to the general primary publication) should be used to especially cite Galaxy EU with the public server in Freiburg?\nThanks a lot!\nRose"
    },
    {
      "role": "system",
      "text": "Hi,\nwe have a small section about Acknowledgments here: Galaxy Europe (@link https://galaxyproject.eu/about)\nIf you could cite the main Galaxy paper (@link https://pubmed.ncbi.nlm.nih.gov/29790989/) and acknowledge the EU server we would be super happy :slight_smile:\nThanks,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m following along the de novo transcriptome assembly Galaxy tutorial (De novo transcriptome reconstruction with RNA-Seq (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#transcriptome-assembly)) with my own data.  I\u2019m stuck at the stringtie-merge step; I\u2019m getting the following error message:\nError: could not any valid reference transcripts in /corral4/main/objects/6/5/e/dataset_65e2a2c5-762d-4f88-bec9-002deee1ae23.dat (invalid GTF/GFF file?)\nIs this saying that none of the annotated sequences in my reference GTF file overlap or correspond with those in my Stringtie assembled transcripts?\nSo far, I\u2019ve checked that the chromosome names are the same in the FASTA and gtf file and I\u2019ve tried using both gtf and gff3 file formats for the reference annotation file."
    },
    {
      "role": "system",
      "text": "Ok, the second reply helps me to understand.\n\n\nStringtie \u2013 the inputs are mapped reads. Whatever ever was mapped against is where the coordinates are derived from. All other inputs that are coordinate-based need to be based on that exact same reference genome.\n\n\nYou mapped against the hg38 reference genome at this step in the tutorial (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#mapping).\n\n\nYou can incorporate reference annotation during mapping (HISAT2) and/or transcript assembly (Stringtie) and/or transcript merging (Stringtie merge), then will also include the reference annotation for later steps (differential expression: Featurecounts and DeSeq2 if following the tutorial).\n\n\nThe reference annotation that is first used represents the known genes/transcripts for your genome. You will still capture novel transcripts if you use one. The different tool forms will state the choices: create transcripts that are known (only \u2013 those included in the annotation) OR create known plus novel. You can run it both ways in different tools and compare.\n\n\nAs you move through the analysis, the reference annotation will be updated and reformatted at certain steps. The updated version at an earlier step is what you want to use in the next downstream steps.\n\n\nThe hg38 reference annotation GTF is now best sourced from UCSC. Not from the Table Browser but from the Downloads area here (@link https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/). There are a few Gene tracks to choose from. You can review what each of those represents then upload the selected GTF to Galaxy with the Upload tool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\n\n\n@quote\nFor the sake of consistency, should I just use the gtf file in my history for HISAT2 and move forward from there?\nI\u2019m not sure what that annotation represents or where you sourced it, but some data providers create GTF data that are not quite in the correct format. It is also possible you have annotation with chromosome identifers that are a mismatch for hg38. I would stick with the UCSC GTF if this analysis is new to you \u2013 nothing special needs to be done to prepare it for use with tools. But if you really want to use another source, Gencode is probably the best alternative. You\u2019ll need to remove the header lines and then update the datatype first. Instructions are in this prior Q&A (@link https://help.galaxyproject.org/t/reference-genome-and-associated-gene-model-gtf-gff3-file-for-splice-junctions-in-rna-star/4365).\nNote: you may find prior Q&A that specifically states to avoid the UCSC GTF. That was about using the version from the Table Browser before UCSC started generating properly formatted GTF data available in the Downloads area for all of their genomes that had a Gene annotation track.\nIn my opinion, the version of hg38 reference annotation in the UCSC Downloads area is now absolutely the best choice. Especially if you choose the RefSeq Gene annotation (hg38.ncbiRefSeq.gtf.gz) \u2013 as that one is updated regularly, about monthly. Other gene tracks are fixed at prior releases. Other data sources may need reformatting and some are missing important attributes. But you can review and decide. Or maybe run distinct analysis paths with the different GTF choices and compare results. However you do this \u2013 start with a specific annotation GTF then keep using that throughout the same analysis. Meaning: don\u2019t switch from RefSeq to Ensembl in the middle of an analysis or expect problems.\n@quote\nThere\u2019s another problem in that case because if I try to use a reference from my history, I can\u2019t seem to select gtf files, only ones in a FASTA format\u2026\nFor this, my guess is that you are mixing up where to input a reference annotation (GTF) versus a reference genome or reference transcriptome (fasta) on the tool forms. Your analysis will include all three.\n\nA reference annotation will be selected from the history for your use case. Featurecounts does have annotation available for a few genomes, including hg38, but that is for when you don\u2019t need annotation for any other steps \u2013 and it won\u2019t match external sources \u2013 so don\u2019t mix that in for now.\nA reference transcriptome will be selected from the history (always).\nA reference genome for your case is the already indexed built-in version of hg38. Technically you can input a reference genome from the history, too, but that only works for very small genomes and there is no need if Galaxy has already indexed it.\n\nThat should cover all the questions/problems you were having. Please try this out :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Original title: Server Indexed File (Human (homo sapiens): hg38) does not appear to be functional\nUsing bedtools GetFastaBed to get sequences in Server Indexed Files: Human (homo sapiens): hg38 underlying ChIP-seq binding sites in a collection of BED files, all returned files were 0 bytes with warnings saying \u201cWARNING. chromosome (chr1) was not found in the FASTA file. Skipping.\u201d (and many more similar warning lines)\nI\u2019m assuming this has something to do with the server maintenance and reloading of Server Indexed Files. When do you think this will be resolved?\nThank you,\nJustin"
    },
    {
      "role": "system",
      "text": "Update 10/21/20 1pm EST: Resolved. Please rerun prior jobs that failed.\n\nUpdate 10/20/20 7pm EST:\nThe correction is still pending. Ticket with details @link https://github.com/galaxyproject/usegalaxy-playbook/issues/311\nOnce confirmed to be fixed, that ticket will close out. It should be very soon\n\nCorrection in progress.\nTools with known issues (are likely more):\nBamLeftAlign\nGetFastaBed\nBWA-MEM and most other mapping tools\nFreebayes\nDetails:\nWe can confirm the problem and are correcting it right now. It might be Monday before everything is done. Write back if still a problem on Tuesday for an update.\nMeanwhile, you can do the same operation at @link http://usegalaxy.eu or @link http://usegalaxy.org.au.\nCopy the link from a dataset\u2019s \u201cdisk\u201d icon and paste it into the other server\u2019s Upload tool to move data around. Some servers require that the history/datasets are in a shared state first. Do this under User > Histories > menu (gear) > Share or Publish. Choose the \u201cshare by URL\u201d option making sure the box is checked to also share the objects inside of the history (eg datasets). Or, reset all permissions to share objects by default in user-defined ways under User > Preferences.\nEach public server has the default data permissions set a bit differently, but these FAQs cover all of the sharing/permissions in case you get stuck: @link https://galaxyproject.org/support/#data-options\nThanks again for the followup, appreciated"
    }
  ],
  [
    {
      "role": "user",
      "text": "i was running wheat genome reference to create BAM file using BWA_MEM . it was running smoothly\nbut out file was 34.3 MB and the finaly error message was surfaced reading as:\nAn error occurred setting the metadata for this dataset Set it manually or retry auto-detection"
    },
    {
      "role": "system",
      "text": "I assume this is similar to a problem we\u2019ve recently had: pysam is not able to handle the large chromosomes of wheat. To index the BAM file, it can only handle contigs (chromosomes) of max 512MB. The solution provided by the wheat consortium is that they split the large chromosomes."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been trying to annotate vcf.gz files that I have using the SnpEff tool on Galaxy. They\u2019re human samples so I\u2019m using the hg19 genome as the Snpff Genome Version Name.\nI keep getting the following error message - I\u2019m guessing from the error message that it has to do with the fact that the reference and alternate bases are not single bases and rather 2 bases (TG and CA, respectively, in the below example) - is that the case?\nHow can I resolve this error? GATK was used for variant calling.\n\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/966/40966912/_job_tmp -Xmx7g -Xms256m\nError: Error while processing VCF entry (line 176) :\nchr1 16890671 . TG CA 125.238 PASS AF=0.53012;AO=44;DP=84;FAO=44;FDP=83;FDVR=10;FR=.;FRO=39;FSAF=21;FSAR=23;FSRF=20;FSRR=19;FWDB=-0.0269551;FXX=0.0119048;HRUN=1;HS_ONLY=0;LEN=2;MLLD=170.568;OALT=CA;OID=.;OMAPALT=CA;OPOS=16890671;OREF=TG;PB=0.5;PBP=1;PPD=0;QD=6.03554;RBI=0.0269856;REFB=0.0133951;REVB=-0.00128241;RO=39;SAF=21;SAR=23;SPD=0;SRF=20;SRR=19;SSEN=0;SSEP=0;SSSB=-0.0302714;STB=0.516705;STBP=0.775;TYPE=mnp;VARB=-0.00753567 GT:GQ:DP:FDP:RO:FRO:AO:FAO:AF:SAR:SAF:SRF:SRR:FSAR:FSAF:FSRF:FSRR 0/1:91:84:83:39:39:44:44:0.53012:23:21:20:19:23:21:20:19\njava.lang.StringIndexOutOfBoundsException: String index out of range: 3\njava.lang.StringIndexOutOfBoundsException: String index out of range: 3\nat java.lang.String.substring(String.java:1963)\nat org.snpeff.snpEffect.HgvsProtein.simplifyAminoAcidsLeft(HgvsProtein.java:395)\nat org.snpeff.snpEffect.HgvsProtein.simplifyAminoAcids(HgvsProtein.java:384)\nat org.snpeff.snpEffect.HgvsProtein.toString(HgvsProtein.java:491)\nat org.snpeff.snpEffect.VariantEffect.getHgvsProt(VariantEffect.java:633)\nat org.snpeff.vcf.VcfEffect.set(VcfEffect.java:1031)\nat org.snpeff.vcf.VcfEffect.(VcfEffect.java:147)\nat org.snpeff.outputFormatter.VcfOutputFormatter.addInfo(VcfOutputFormatter.java:98)\nat org.snpeff.outputFormatter.VcfOutputFormatter.toString(VcfOutputFormatter.java:286)\nat org.snpeff.outputFormatter.OutputFormatter.endSection(OutputFormatter.java:112)\nat org.snpeff.outputFormatter.VcfOutputFormatter.endSection(VcfOutputFormatter.java:230)\nat org.snpeff.outputFormatter.OutputFormatter.printSection(OutputFormatter.java:145)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotate(SnpEffCmdEff.java:292)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotateVcf(SnpEffCmdEff.java:468)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.annotate(SnpEffCmdEff.java:142)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:1029)\nat org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:984)\nat org.snpeff.SnpEff.run(SnpEff.java:1183)\nat org.snpeff.SnpEff.main(SnpEff.java:162)\n\nThank you in advance for your help."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe error is related to the fact that the vcf line does not encode a simple snp but an mnp, yes, but is not directly caused by it.\nIt rather seems that SnpEff, after having completed the annotation of the line internally, tries to simplfy the amino acid change it\u2019s going to report by stripping off leading and trailing identical amino acids.\nLets assume for example: in your TG \u2192 CA change, the T->C is silent (leaving say an Ala unaltered), but G->A changes a Leu->Ser (completely making this up of course). Then the original AA change annotation would be AlaLeu \u2192 AlaSer, but obviously that could be simplified to just Leu \u2192 Ser.\nNow the problem is that SnpEff assumes in that simplifying code that amino acids are encoded in 3-letter code at this point. For some reason that doesn\u2019t seem to be true in your case, but you seem to arrive at that piece of code with single-letter code (like AL \u2192 AS in the example). Now when the code tries to compare the first three letters of the left string to the first three letters of the right one, it fails with that String index out of range: 3 from the error message.\nLooking for an explanation of why you arrive there with one letter codes, I can only think of the setting:\nUse one letter Amino acid codes in HGVS notation. E.g. p.R47G instead of p.Arg47Gly in the SnpEff annotation options. Maybe that is what\u2019s incompatible with MNPs?\nJust guessing here, but if you have checked that option, just try rerunning without it.\nGood luck,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Attempting to run @link http://usegalaxy.org/u/aun1/h/covid-19-assembly and both Unicycler and Spades are failing with\ncode 17: slurm_submit_batch_job error: Invalid account or account/partition combination specified\n"
    },
    {
      "role": "system",
      "text": "Hi @system\nThe cluster that runs Unicycler from @link http://usegalaxy.org is very busy, with a longer queue than usual. There are not any known technical issues \u2013 just more queued jobs than expected.\nPlease leave your queued jobs queued and those will process as resources become available. If you delete and rerun, the new jobs will end up at the back of the queue again, further extending the wait time.\nOur administrator is double-checking why that particular cluster is so busy. If there is some problem uncovered that requires action on your part (rare!), we\u2019ll write back \u2013 but most likely the queue will simply start to move again and your existing job(s) will process.\nThanks for reporting the issue and we appreciate your patience. This is a very popular workflow under heavy usage.\nBest!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi team,\nI try many times, but I don\u2019t know how to use online reference genome.\nWhen I use bwa-mem.\nIf use own upload fasta.\nbwa-mem will index fasta every time.\nCould you told me how to fix it.\nVery thank you,\nJacky"
    },
    {
      "role": "system",
      "text": "Ah, I think I figured it out =) I got it working by installing data_manager_all_fasta_path first:\n@link https://toolshed.g2.bx.psu.edu/repository?repository_id=0a5e20872a520157\nAfter installing you have a new data manager and trough this one you can add your fasta file. I only filled in the path field for the entry field.\nEDIT:\nCan a developer now tell me how to remove the files again? =)\n@link https://biostar.galaxyproject.org/p/13591/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m new to RNA seq and I\u2019m trying to use blastx on galaxy to blast a fasta file of possibly novel lncRNAs using the ncbi nr database. I want to remove transcripts with significant homology to known proteins with: e-value < 1e-10, target coverage > 80%, and identity > 90%, so I can preserve transcripts that are most likely lncRNAs. I tried to change the e-value/expectation value to 0.0000000001 and query coverage to 80% but I cannot change the pident setting (percentage of identical matches). How do I do this? Also, when I run the tool on default settings it is running for more than a day."
    },
    {
      "role": "user",
      "text": "@system I tried it with choose remote file option and it worked perfectly"
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to use Wig/BedGraph-to-bigWig tool for a file generated with mapping to CHM13 T2T but the bedgraph file I am trying to convert does not allow me to designate the reference genome as CHM13 T2T (it does not come up on the list)."
    },
    {
      "role": "system",
      "text": "@quote\nRuntimeError: Invalid interval bounds!\nThis part of the error indicates a mismatch between the chromosomes + coordinates between the two inputs. Double check that all are based on the exact same reference genome assembly/build.\nYou have mentioned both hg38 and CHM13_T2T_v2.0 \u2013 so I\u2019m not sure which this was based on, but using data anchored to one assembly for all inputs will work best. Those two are close enough that you might not realize there is a mismatch problem at first. Content issues do not always trigger a tool failure, instead, tools can produce odd \u201cputatively successful\u201d results that have a scientific problem."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was looking into the web hooks available in Galaxy and from what I can tell, in terms of job scheduling, the only hook available is once the job is submitted by the user. There doesn\u2019t seem to be a hook which can be triggered when a job begins running on a cluster (using Slurm for scheduling on our HPC), or when it finishes (either success or fail).\nWould it be possible for me to add something like this to my Galaxy install, or perhaps there\u2019s an easier way to do so. Ideally this would be for all tools, so it\u2019s not quite feasible to edit all the tool XMLs and add custom execution code.\nEssentially what I would want is to POST to a URL (external REST API) with a certain message (BEGIN or END FAIL or END SUCCESS, and the submitting user\u2019s username) when jobs begin or finish.\nAny suggestions are appreciated!"
    },
    {
      "role": "system",
      "text": "You should definitely look at an alternative to modifying Galaxy.\nMost users don\u2019t own their copy of Galaxy and you will struggle to convince sys-admins to maintain custom deployments of Galaxy.\nI also can\u2019t think of a reason someone would need immediate notification of a job completing. If the job takes longer than the person is willing to wait, they usually go off and do other things and come back when they are ready.\nGalaxy does offer the option to send an email upon job completion. You could look into making a contribution to Galaxies code base, extending this existing functionality."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI need to upload the FASTA file of my reference genome to galaxy as a dataset and change the settings in history to allow me to select that reference genome when I import gene data of the organism that I performed RNAseq analysis on. I really need help with this please, the species I need is not in the standard database that galaxy allows you to choose from when importing data. Any help will be greatly appreciated.\nKind regards\nJ"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis didn\u2019t work for two reasons:\n@quote\nThe URL I am using ishttps://workflow4metabolomics.usegalaxy.fr/(I basically just want to use the Workflow4Metabolomics steps to analyse my data by PLS-DA analysis).I enter this into the FileZilla host fieldftp://usegalaxy.org\n\n\nThe FTP URL is always specific to the server you are working on, and your account there. I don\u2019t see the current FTP URL on their site, but you could ask them. Usually, it is the last part of the server URL (for this case that would be \u201c@link http://usegalaxy.fr\u201d) \u2013 but NOT always. They have a forum specific for their community where the information may be posted, see: Workflow4Metabolomics - Galaxy Community Hub (@link https://galaxyproject.org/use/workflow4metabolomics/)\n\n\nIf you are ever working at @link http://UseGalaxy.org, the FTP URL is \u201c@link http://usegalaxy.org\u201d \u2013 without the \u201cftp://\u201d at the start.\n\n\nThis help applies to most public Galaxy servers that support FTP. The FTP URL will be the last part of the server URL, or the server, or help on their server or directory page will specify what to use or how to ask about details. All known public Galaxy servers are in the directory here (some do not use this forum, or have a supplemental forum or contact): Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\n\n\n@quote\nHowever, my datasets are in csv file format.\nWhat happens if you convert csv to tabular format and input that instead? Click on the pencil icon for the dataset to reach the edit attributes forms, click into the second tab named \u201cConvert\u201d, and find the option in the drop-down menu. Only a few tools interpret csv (comma-separated values) and most expect tabular (tab-separated values).\nThe tool form will usually note what is expected/supported in the help section with examples. Another quick way to find out for any tool: create a new empty history, then load up a tool form. The options on the form where input datasets are selected will list out the expected input datatype(s).\nThat server is set up a bit differently than other public servers, with special tools/workflows etc. Their forum is probably the best place to ask questions if you run into problems. There could be a known problem or you might be the first to report something they could fix.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to use BBMap on an uploaded FASTQ dataset but it shows as unavailable. Also does this if I upload FASTQ.GZ and unzip in Galaxy. Wondering how to get around this. Data is originally pacbio.\nThanks."
    },
    {
      "role": "system",
      "text": "Hi @user\nMost tools in Galaxy expect reads in fastqsanger or fastqsanger.gz format and with one of those datatypes assigned. Those datatypes represent a specific type of quality score scaling: Sanger Phred +33.\nThis is produced by Illumina 1.8+ pipelines. PacBio used Phred+33 scaling to start with and now the scaling is neutral. You can double check with a tool like FastQC or FASTQ info to see what those report.\nThere are some tools that expect just fastq or fastq.gz (scaling unspecified). But if you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:\n\nCreate a new empty history\nLoad up the tool form\nReview the input datatypes listed in the tool form\u2019s input area\nNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\nExample\n\nwhat-datatype-is-accepted-input2346\u00d7554 87.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e526ccad7216bc30f1408215478e172bbae0cda4.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "What is going on?! I had 75% of my space quota full just yesterday and now, when I returned today everything is GONE! The history has been purged. Clean. Zero. I had not received emails, no warnings\u2026 I have just only one account with Galaxy. Admins, what is going on?"
    },
    {
      "role": "system",
      "text": "@user\nYou had this account restored back in January 2021 after it was administratively suspended then purged as a duplicate account in October 2020.\nOnce enabled again, you created more work in a history that was technically purged and already had the old datasets in it purged. Old or new data in a purged history will continue to be screened for and removed every 14 days.\nThere is a technical corner-case problem on our side but it will only present when an account has been administratively managed. Specifically, one of your histories was not labeled as purged in the GUI until this last cycle of administrative cleanup. The other histories were labeled as purged and couldn\u2019t be worked in, and now this one is, too.\nUntil we correct that, anyone with an enabled administratively managed account should not create new work in any history that has a creation date earlier than the date the account was re-enabled. Or better, simply avoid using multiple accounts to start with. Terms of use are one account per person at each distinct public Galaxy server, including @link http://UseGalaxy.org.\nGetting an account at Galaxy Main (http://usegalaxy.org) (@link https://galaxyproject.org/support/account/)\nWe are sorry that you lost your new work in that single history with the wrong state assigned when the account was re-enabled. But the good news is that there was not much new work, and there is one new history you created/worked in after the account was re-enabled and it was not impacted. In our email communications in January 2021, you asked for the account to be enabled with or without prior data. I re-explained the terms of usage and how you can proactively resolve multiple accounts yourself before those are administratively managed for breaking the terms of service. The processes that detect and remove multiple accounts that are in use by the same person do not always capture all of the multiple accounts an individual may have. But everyone can fix this if the terms were unclear: consolidate data into one account and delete any extras under User > Preferences. Follow the terms of service and there are never problems.\n@quote\nMy account were active last night and running data, however this morning I tried logging in to find a message saying the account had been marked as deleted. How do I recover the data that was in the account?\nThanks for helping to keep @link http://UseGalaxy.org a free and fair public resource for researchers worldwide."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\njust new to galaxy and I am designing a workflow whenever I insert a Trimmomatic into the workflow I can not update the numbers of inputs as when I changed from single ends to paired ends(two separate inputs) nothing changes. Am I missing a step here?"
    },
    {
      "role": "system",
      "text": "Alright it\u2019s all up to date and @user example works fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am a new user on @link http://Galaxy.eu. I have set up a run for RepeatMasker of my (large, 2.5G) de novo genome assembly. After a few days, it\u2019s still running. Is it normal, or should I have turned on some parameters such as only rodent specific repeat, or skip bacteria insert, etc to make process much quicker? I had no idea what the results would look like, so I chose the default setting. Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nwe have increased the assigned cores for RepeatMasker; please repeat the analysis in a few hours.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,  Yesterday I generated a .bam file using a local galaxy instance.  It was viewable from history.  This morning I can only see the file (.dat file) from /home/ubuntu/galaxy/database/files/000/dataset_1.dat.\nHow do I \u201crestore\u201d this file to my Galaxy UI history ?  Should I download the .dat file and re-upload into History?  Since the filesize is about 18 GB I really want to use the most efficient method for its restore.\nPlease let me know whether I should include any additional information.  There were no error messages or messages of any kind.\nThank you!"
    },
    {
      "role": "system",
      "text": "Sure, that will work though it\u2019s not fixing the underlying issue, and you should definitely be able to log out of your account without losing access to data."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI\u2019m currently in the process of performing DEGS idenitification with DESEQ2 in the galaxy pipeline.\nI performed all my steps with Galaxy.\nI performed whole RNA transcriptome sequencing. The obtained FASTq files I imported into galaxy and then I performed the following steps:\n\nFASTP\nFASTQC\nRNA-STAR\nfeature counts\n\nAS next step I wanted to perform DESEQ2 analysis to identify DEGS between the two groups of samples.\nI input the feature counts file into the DESEq2 pipeline and I started to perform the analysis.\nAfter 10s I get the following error: \u201cError in data.frame(sample = basename(filenames_in), filename = filenames_in,  :   duplicate row.names: /data/dnb06/galaxy_db/files/f/b/0/dataset_fb0b5307-b5a9-4a57-ba8f-a3077494c305.dat\u201d.\nHowever, I don\u2019t have any duplicates. I have 58051 rows of individual genes without any duplicates.\nHas anybody encountered the same issue?\nThank you very much for your help!"
    },
    {
      "role": "system",
      "text": "Hi @user\nsample #20 is specified in both conditions.\nNote two black angles at the bottom right corner of sample selection window. Move the mouse of these two angles and drag it down to increase the window size. Do it for the both conditions and check samples used for the job.\nHope that helps.\nIf the issue is resolved, unshare/unpublish the history.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been working on metagenome data. I am using the vserach dereplication tool. And it has been running for two days and still it is showing the run status.\nHow much time will it usually take to complete? and is it a normal run time.\nPlease help and guide"
    },
    {
      "role": "system",
      "text": "According to the error report, it seems to be a bug in this script tools-iuc/export2graphlan.xml at master \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/export2graphlan/export2graphlan.xml). I\u2019ll review it.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Do you know the meaning of the following error messages from HISAT2 after mapping?\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of \u2018int\u2019\n(ERR): hisat2-align died with signal 6 (ABRT)\n[bam_sort_core] merging from 6 files and 1 in-memory blocks\u2026\nEspecially the \u201c(ERR)\u201d one, I\u2019m afraid my alignment might have not been completed at all."
    },
    {
      "role": "system",
      "text": "Hi Mario\nThis is paired end data, isn\u2019t-t? And you have provided two fastq files\u2026, well, the alignment process stop (i.e.: died) because of the different number of reads in the two fastq files. Maybe one file is corrupt (e.g.: incomplete download) or something went wrong during the pre-processing?\nCheck the integrity of the files with FASTQC\nRegards, Hans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have the metagenome data as fastq.gz files i have uploaded them and unable to form contigs (make.contigs). what would be the possible reason? what type of data is required for the contigs?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tool expects reads in fastqsanger format. Try Upload without specifying the datatype. Galaxy will detect and assign the datatype that matches the best. If the upload fails or the datatype is not what you expect, this indicates a format or content problem. You may need to reload the data or possibly correct the data if the source is your own computer. QA or data manipulation tools can be used to validate most uploaded data.\n\nGTN tutorials that cover Mothur and related Metagenomics tools/workflows are here:\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/metagenomics/\n\n\nGalaxy Training: Metagenomics (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\n\nIf you are not sure how to Upload read data in a way that tools can interpret, please review the NGS data logistics tutorial here\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/introduction/\n\n\nGalaxy Training: Introduction to Galaxy Analyses (@link https://training.galaxyproject.org/training-material/topics/introduction/)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\nand the Quality Control tutorial here\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/\n\n\nGalaxy Training: Sequence analysis (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/)\nAnalyses of sequences\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to do single-cell data analysis, but when import data with \" Import Anndata and loom from different format\" and then do \u201cthe full data matrix\u201d inspect with \" Inspect AnnData object\", I always get the ERROR, I help you can help me to settle my problem, you can view and import my History by visiting the following URL: Galaxy | Europe | Published History | wfq (@link https://usegalaxy.eu/u/yangmingjie/h/wfq)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tests are done now.\n\n\nCreating an object with all of the raw data is too large to process. This might be something that can be adjusted. Send in a bug report from your error (a failed run in the same history with the exact inputs actually used) and the EU admins can investigate options. In short, the tool is being sent to a cluster node that isn\u2019t large enough to process the fully expanded data array.\n\n\nPre-filtering the raw data first does create data that works with this tool, and others, and is what you will likely want to do anyway for practical analysis reasons (quality). I filtered using defaults \u2013 but you can tune that.\n\n\nThe original history has an example of that now and the sc-RNA tutorials here have many more details.\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI have installed a local instance of Galaxy (release_16.07) on my CentOS (7.3.1611) server and added a custom tool for taxonomic analysis using qiime2.\nWhen I run the tool I get the error:\nUnicodeEncodeError: 'latin-1' codec can't encode character u'\\u2018' in position 408: ordinal not in range(256)\nbut the job command line produced by the tool actually works when typed in the terminal.\nI had noticed this type of behaviour in other different tools as well and noticed that when they produced an output like a warning or any additional output, Galaxy mistakes it as an error and produces the same error.\nNone of the tools have anything to do with MySQL, I think it\u2019s some kind of internal error from Galaxy\u2019s database or something.\nSo far I managed to get around that by redirecting stderr ( 2>/dev/null ) but in this case nothing works.\nCould you help me out?\nThank you in advance,\nThe full error is the following:\nTraceback (most recent call last):\n  File \"/home/user/galaxy/lib/galaxy/jobs/runners/local.py\", line 128, in queue_job\n    job_wrapper.finish( stdout, stderr, exit_code )\n  File \"/home/user/galaxy/lib/galaxy/jobs/__init__.py\", line 1332, in finish\n    self.sa_session.flush()\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py\", line 150, in do\n    return getattr(self.registry(), name)(*args, **kwargs)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2004, in flush\n    self._flush(objects)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2122, in _flush\n    transaction.rollback(_capture_exception=True)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\", line 60, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2086, in _flush\n    flush_context.execute()\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 373, in execute\n    rec.execute(self)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 532, in execute\n    uow\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 170, in save_obj\n    mapper, table, update)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 692, in _emit_update_statements\n    execute(statement, multiparams)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 914, in execute\n    return meth(self, multiparams, params)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 323, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1010, in _execute_clauseelement\n    compiled_sql, distilled_params\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1146, in _execute_context\n    context)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1344, in _handle_dbapi_exception\n    util.reraise(*exc_info)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1139, in _execute_context\n    context)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 450, in do_execute\n    cursor.execute(statement, parameters)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/cursors.py\", line 187, in execute\n    query = query % tuple([db.literal(item) for item in args])\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/connections.py\", line 278, in literal\n    return self.escape(o, self.encoders)\n  File \"/home/user/galaxy/.venv/lib/python2.7/site-packages/MySQLdb/connections.py\", line 208, in unicode_literal\n    return db.literal(u.encode(unicode_literal.charset))\nUnicodeEncodeError: 'latin-1' codec can't encode character u'\\u2018' in position 408: ordinal not in range(256)"
    },
    {
      "role": "system",
      "text": "You can always see what version your database is at by checking the migrate_version table in the database.\nfor the unicode error this could be a lead: @link https://github.com/galaxyproject/galaxy/issues/7421"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, everyone\nI am familiar with Galaxy, but only now have I started setting it up on our local computer (no server, just a well-built computer running in UNIX). I have succeeded to install it, set admin accounts, download reference files (i.e. reference genomes) and install new packages.\nMy problem now is, how can I set directories to store data (i.e. reference files, output files, and temp job files, if possible)? Moreover, how to set them to a location on another disk volume, other than the one galaxy was installed?\nI have tried troubleshooting from this official link (@link https://galaxyproject.org/admin/tools/data-managers/how-to/define/) and this post (@link https://biostar.galaxyproject.org/p/11020/), but I cannot make it work.\nFor one thing, I believe the galaxy.ini file has been replaced by galaxy.yml file. Is that correct?\nIf so, I found some lines similar to what is referenced on the official link, and did the following changes:\nOn line 178, it reads:\n# on any cluster nodes that will run Galaxy jobs, unless using Pulsar.\nfile_path: /media/mol/hdmol/galaxy/database/files\n\n# Where temporary files are stored. It must accessible at the same\n# path on any cluster nodes that will run Galaxy jobs, unless using\n# Pulsar.\nnew_file_path: /media/mol/hdmol/galaxy/database/tmp\n\nNote: /media/mol/hdmol is the path to the other disk volume. And all sub-directories from galaxy were copied to that location. Hence, I as using the exact same sub-directories, and changing only the volume (to /media/mol/hdmol/).\nAny help is very much appreciated.\nThank you!"
    },
    {
      "role": "system",
      "text": "First, that is not a valid XML (it misses two closing tags).\nSecond, I think you do not want to use hierarchical or distributed OS, so something like this should suffice:\n <?xml version=\"1.0\"?>\n <object_store type=\"disk\">\n    <files_dir path=\"/media/mol/hdmol/galaxy/database/files1\"/>\n    <extra_dir type=\"temp\" path=\"/media/mol/hdmol/galaxy/database/tmp1\"/>\n    <extra_dir type=\"job_work\" path=\"/media/mol/hdmol/galaxy/database/job_working_directory1\"/>\n </object_store>\n\nThird, make sure you tell Galaxy about this store configuration by putting it in the galaxy.yml\nobject_store_config_file: config/object_store_conf.xml\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI created a bash script that runs a tool and then it takes some of the output files put them in  a directory that I\u2019ve created for this tool and copy them into some bash variables:\ncp ${output_dir}/file1.txt $2\ncp ${output_dir}/file2.txt $3\nTherefore in the XML tool definition file that runs this bash script I specify this outputs:\nsh -e $__tool_directory__/tool.sh\n      $input \n      $file1\n      $file2\n\n  <outputs>\n            <data name=\"file1\" format=\"txt\" label=\"file 1\"/>\n            <data name=\"file2\" format=\"txt\" label=\"file 2\"/>\n    </outputs>\n\nSo I force Galaxy to put this scripts in a directory created by me. Is this wrong? Or should I let Galaxy put the output files in the default directory?"
    },
    {
      "role": "system",
      "text": "Yes, but you remove the folder after you moved or copied it to galaxy. So it will be the very last line of the bash script. $2 and $3 are basically paths."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi i am using paired end data and downloaded the sra file from ncbi  and uploaded it on galaxy server. and can i use this data directly for assembly or first i have to split the files then use it"
    },
    {
      "role": "system",
      "text": "For your Trinity questions:\n\nTrinity requires that paired-end inputs are \u201cmatched pairs\u201d. Meaning, both ends of the same read are input.\nIf one of the ends fails QA/QC (Trimmomatic, Fastp, others), then the other associated end cannot be used, even if it happens to passes QA.\nWhen extracting from SRR, the original data will be paired.\nWhen running through QA tools, the data can become un-paired. That said, the  output from QA tools, for example Trimmomatic, the data will be sorted into four datasets.\n\nPaired forward + Paired reverse = use these for assembly inputs\nSingle forward + Single reverse = do not use these for assembly inputs. One end of the original pair did not pass QA, and the assembly will fail if input.\n\n\n\nPlease be aware of a few current factors that can impact assembly success/failures when using the public Galaxy Main @link https://usegalaxy.org server right now. There is a banner on the server explaining. More details:\n\n\nTrinity and Unicycler are running with reduced memory allocation at this time.\nMake sure to use the most current version of all tools, or unexpected problems can occur. The most current version of any tool\u2019s form will load from the Tool Panel.\nIf your job fails, confirm that you are using the most current tool version.\n\n\nIf not, rerun using the updated version.\nIf yes, then the failure may be due to the reduced memory resources. Try one rerun. If that fails again, there may be some other problem with your inputs. How to check for common input problems is discussed in the topic below.\nMy inputs are Ok \u2013 How to work-around the reduced memory allocation? a) Consider using an alternative public Galaxy server (@link https://galaxyproject.org/use/) b) Decide if down/sub-sampling your inputs will meet your goals (see Seqtk tools).\n\n@quote\nCheck to see if there is a known usegalaxy.* server issue. \n\nServer Status:https://status.galaxyproject.org/If the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nDoes any one else encountered a \u201cInternal Server Error (500)\u201d when trying to upload files? Does anyone know how to resolve this?\nBest"
    },
    {
      "role": "system",
      "text": "Hello,\nthanks for your replies @user and @system !\nTo avoid being stuck, I did a local installation of galaxy in the end (Get Galaxy - Galaxy Community Hub (@link https://galaxyproject.org/admin/get-galaxy/)), and installed the ribogalaxy packages and tools from the Tool Shed as admin.\nBest,\nPaul"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nIn my galaxy there are 3 users with 474, 312 and 0 gigabytes of used space as galaxy says. However the database actually uses 1.2 TB of disk space. I ran cleanup scripts Cleaning up Dataset Objects (@link https://galaxyproject.org/admin/config/performance/purge-histories-and-datasets/) but they freed exactly 0 bytes.\nWhat else my database contains that is not shown and can not be deleted by scripts? Can i delete it in some other way?\nThanks in advance."
    },
    {
      "role": "user",
      "text": "I reviewed our workflow, and it turned out that it were poorly written custom tool xmls that left tons of garbage behind them. Now i had fixed them, and galaxy is innocent."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to import a history from another Galaxy server.  When I download the history from the other server, I receive a URL, which I can click to download the tar.gz file.  I have tried to copy that URL into the Users > Import interface on @link http://usegalaxy.org, but it ends in an error, screen capture below.  I also tried to upload the local file (both as tar.gz and the extracted file).  No action happens when I click on \u201cImport History\u201d.  (I read a post dated approximately 10 months ago with a similar issue, and I have made the history accessible by link to try the export again, but I am still getting the error.) Please let me know what I am doing wrong.  Thank you!\nimage746\u00d7754 29.2 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/526118bed6a06f5861db72d4ba9ec2a836e10cec.png)\nn"
    },
    {
      "role": "system",
      "text": "Hi @user\nhere is URL for a tiny history on Galaxy Australia, if you want to try history import:\n@link https://usegalaxy.org.au/api/histories/8d9830910a17f854/exports/dd411d68aa684fae\nI imported it to Europe and ORG.\nHard to say. Some file systems do not support big files, at least it was the case in the past. In this situation big files might be truncated, creating a broken archive. Personally, I found history export very slow, at least on Galaxy Australia, and rarely use it for big histories. I transfer big files individually. New version of Galaxy provides link to files (chain icon). Copy and paste it into target Galaxy upload menu, Get/Fetch data section. It works like a charm between usegalaxy.* servers (with small number of files).\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Local Galaxy instance.\nI upgraded my local galaxy instance to 19.09 from 19.04.  No initial issues were observed. No errors.\nWhen I restart Galaxy, all installed \u201cshed_tools\u201d disappear from my gui tools menu.   Interesting \u2026\nIf I delete the intergrated_tool_panel.xml file, the shed_tools reappear in the gui menu when I restart.\nRestarting again, and the tools disappear.\nIt would appear that while the shed_tools are added to the GUI menu correctly, they do not appear to be added to the integrated_tool_panel.xml file ( The contents of tools_config.xml is added to the file, but the shed_tools are never added to this file).  If you restart, galaxy reads from the integrated_tools_panel.xml file and therefore no shed_tools.  Delete the file and it works until restart.\nAny ideas,  this one is beyond me\u2026\nRegards,\nMat"
    },
    {
      "role": "user",
      "text": "@quote\nHi Marius, Marten,\nYes, uncommenting the shed_tool_config line in galaxy.yml resolved the issue.\n#tool_config_file: config/tool_conf.xml\nshed_tool_config_file: config/shed_tool_conf.xml\nOnce I removed the \u201c#\u201d, it worked.   Interestingly, I did not need to uncomment the line  tool_config.xml in galaxy.yml.\nThank you all!!\nRegards,\nMat"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am following along with a tutorial given on the coursera genomic data science course. It involved first joining two datasets on a column column to produce another data set. Then it involves grouping the resultant dataset on a column. In the video it shows that there is a drop down list of column names from the dataset being grouped to pick from. No such drop down appears. When I tried typing the column name in from the dataset it seems to have ignored it when executing the task with an error indicating that no column was specified. Any suggestions?"
    },
    {
      "role": "system",
      "text": "Update\nResolved, along with similar tools that had the same dependency issue."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody,\nfinally I managed to load my custom tool into Galaxy. What I need is give as input the ID of a gene and retrieve some data to be stored/displayed in a table.\nNow my python code is storing this table in a tab file but I\u2019m not able to display it in Galaxy.\nThat\u2019s my xml file for the tool:\n<tool id=\"get_pheno\" name=\"Get phenotypes\" version=\"0.1.0\">\n  <description>for a gene from IMPC</description>\n  <requirements>\n    <requirement type=\"package\" version=\"3.8\">python</requirement>\n\t<requirement type=\"package\" version=\"2.18.4\">requests</requirement>\n\t<requirement type=\"package\" version=\"0.8.9\">tabulate</requirement>\n\t<requirement type=\"package\" version='8.0.1'>IPython</requirement>\n  </requirements>\n  <command><![CDATA[python3 '${__tool_directory__}/get_pheno.py' '$input' '$output']]></command>\n  <inputs>\n\t  <param name=\"input\" type='text' label='Input gene:'/>\n  </inputs>\t  \n  <outputs>\n    <data format=\"tabular\" name=\"output\" label='${tool.name} on ${input}' />\n  </outputs>\n\n  <help>\nThis tool retrieves a list of phenotypes related to a gene entered by the user. The input must be an MGI id.\n  </help>\n\n</tool>\n\nI tried substituting \u201cdata\u201d with \u201coutput\u201d but it\u2019s not workning.\nHow can I get the output as a table?\nI already tried to read some documentation but I didn\u2019t find anything useful for me.\nThank you for the help"
    },
    {
      "role": "system",
      "text": "@quote\ntextfile = open(\"output.tab\", \"w+\")\nYou are not giving galaxy the output file, it should be:\ntextfile = open(sys.argv[2], \"w+\")\n\nOr! you do this:\n<data format=\"tabular\" name=\"output\" from_work_dir=\"output.tab\" label='${tool.name} on ${input}' />\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Team,\nI would like to perform paired-end RNA seq analysis for 21 samples with all of them having a biological replicate. I would like to know how should I build a dataset list for such a cohort. should one data list have all R1 and the other datalist have all R2 to be able to run commands on all sample at once? And how should I account the biological replicates?\nThank you very much in advance for your help!\nBest,\nShweta"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthere are different options; since you have technical replicates, I suggest you build a list of dataset pairs for each sample and its technical replicate. Assuming that you are going to follow this pipeline:\nCutadapt \u2192 RNASTAR \u2192 featureCounts \u2192 DESeq2\nYou can work with that collections until the DESeq2 stage. Then you need to merge the technical replicates together in order to use them in DESeq2, which can be done by using two tools:\n\n\nColumn join on multiple datasets (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/collection_column_join/collection_column_join/0.0.3):  it allows you to collapse the the technical replicates collections into a single count file with three columns (GeneID, counts replicate 1 and counts replicate 2).\n\nTable Compute computes operations on table data (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy0): it will allow you to sum up the count of each replicate.\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. This is my first time using galaxy and I got stuck straight away so it might be a basic error that I\u2019ve done but I have been following step by step tutorials and can\u2019t figure out what is wrong. I could not find any help from previous messages.\nI have 2 kinds of errors when trying to run an assembly with Unicycler.\nFirst I have an issue when I try to select the reads I want to use. When I scroll down, I do not see every reads, even though they are loaded properly. When I use the \u201csearch\u201d instead and choose the right read, it turns from green to white and \u201cunavailable\u201d appears before the name of the read like in the screen shot below:\n\nScreen Shot 2019-12-05 at 2.58.51 PM2790\u00d71008 379 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/01f154e935238e24d7ee87e8c4b0c948222dc77a.png)\nI tried to use the very few reads that appear in the scroll down menu and it\u2019s working! But I can\u2019t access all the reads (anything before entry \u201c52\u201d is unavailable, which is most of my data).\nWhen I try to run an assembly anyway with the reads saying \u201cunavailable\u201d, I get the error message:\ntool error\nAn error occurred with this dataset:\nFailed to communicate with remote job server.\nCan anybody help?\nShould I reload all my reads data?\nThanks\nVanina"
    },
    {
      "role": "system",
      "text": "Hi @system\nNanopore reads can be in either fasta or fastqsanger format. When uploading, allowing Galaxy to assign the datatype is usually best but you can change this after upload if Galaxy guessed incorrectly.\nWhy multiple files failed upload \u2013 this could be due to a few reasons, including a slower internet connection. Most residential wifi is optimized for data downloads, not uploads, and many are still working from home right now. Maybe try using FTP to load the data instead of browsing local files? If any transfer fails, it can be resumed.\nI added a few tags to this post that will point to details about using FTP for upload.\nAlso, if you are ever not sure about which datatypes a tool is expecting for input, try this: Create a new empty history, then load up the tool form. Be aware that if data is compressed (example: fastqsanger.gz), Galaxy will uncompress the data at runtime (as additional hidden datasets) for tools that require uncompressed input. Sometimes it is better to uncompress yourself first (pencil icon > Convert), then purge the original compressed, to avoid too much data duplication/quota usage. A side effect is that whatever tool you are running might run better since it doesn\u2019t have to do the \u201cuncompress\u201d step as a pre-processing step for the primary tool. For larger work, that can often make a difference between a successful or failed job (if fails due to resource reasons).\nExample:\n\nunicycler2140\u00d7762 77.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c53458e5a7af2aade5cdbdc2c4ce77d2f6e2c579.png)\nHope that gives some options that will work for you, if you haven\u2019t found a solution yet."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve initialized a Galaxy instance on aws and am an admin on there.\nI am wondering how to parallelize the use of some tools on there. For example tophat2 has an option -p where on the command line it allows for parallel processing but that option is not one of the options that can be configured in the User Interface tool. Is there a way around this?\nThere is a notification on the homepage of Cloudman UI that says this version of Galaxy has configured some tools to run on 4 cores in the file \u201cjob_conf.xml.cloud\u201d, but is it possible to edit this file to use more processors?\nThanks"
    },
    {
      "role": "system",
      "text": "Yes, you can edit the file. You will need to create a new destination with the number of cores you want set. Something like:\n        <destination id=\"slurm_cluster_cpu16\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=16</param>\n        </destination>\n\nThe default can be seen here:\n\n\n@link https://github.com/galaxyproject/ansible-cloudman-galaxy-setup/blob/master/files/galaxy/job_conf.xml.cloud\n\n\n@link https://github.com/galaxyproject/ansible-cloudman-galaxy-setup/blob/master/files/galaxy/job_conf.xml.cloud\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n            <param id=\"drmaa_library_path\">/mnt/galaxy/slurm-drmaa/lib/libdrmaa.so</param>\n        </plugin>\n        <plugin id=\"pulsar_rest\" type=\"runner\" load=\"galaxy.jobs.runners.pulsar:PulsarRESTJobRunner\" />\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\" />\n    </plugins>\n\n    <handlers default=\"main\">\n        <handler id=\"main\" tags=\"handlers\">\n            <plugin id=\"slurm\" />\n        </handler>\n    </handlers>\n\n    <destinations default=\"slurm_cluster\">\n        <destination id=\"slurm_cluster\" runner=\"slurm\"/>\n        <destination id=\"slurm_cluster_cpu4\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=4</param>\n\n\n  This file has been truncated. show original (@link https://github.com/galaxyproject/ansible-cloudman-galaxy-setup/blob/master/files/galaxy/job_conf.xml.cloud)\n\n\n\n\n\n@system, if this file is edited, how should it be persisted?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have paired end fastq files from   illumina Novaseq using whole transcriptome mRNA-seq profiling. My RNA STAR result looks OK (using hg38 gtf file from ucsc table browser).\n                      Number of input reads |\t38164847\n                  Average input read length |\t201\n                                UNIQUE READS:\n               Uniquely mapped reads number |\t30007228\n                    Uniquely mapped reads % |\t78.63%\n                      Average mapped length |\t200.97\n                   Number of splices: Total |\t18124401\n        Number of splices: Annotated (sjdb) |\t17921546\n                   Number of splices: GT/AG |\t17970447\n                   Number of splices: GC/AG |\t117674\n                   Number of splices: AT/AC |\t16850\n           Number of splices: Non-canonical |\t19430\n                  Mismatch rate per base, % |\t0.19%\n                     Deletion rate per base |\t0.01%\n                    Deletion average length |\t1.73\n                    Insertion rate per base |\t0.01%\n                   Insertion average length |\t1.47\n                         MULTI-MAPPING READS:\n    Number of reads mapped to multiple loci |\t7150744\n         % of reads mapped to multiple loci |\t18.74%\n    Number of reads mapped to too many loci |\t62501\n         % of reads mapped to too many loci |\t0.16%\n                              UNMAPPED READS:\n   % of reads unmapped: too many mismatches |\t0.00%\n             % of reads unmapped: too short |\t2.44%\n                 % of reads unmapped: other |\t0.03%\n                              CHIMERIC READS:\n                   Number of chimeric reads |\t0\n                        % of chimeric reads |\t0.00%\n\nHowever, when I use featureCounts I get very few assigned reads, most of the reads are in Unassigned Multimapped category. (I use reverse stranded option as indicated by \u2018infer experiment\u2019)\n\n\n\n\nAssigned\n7712538\n\n\n\n\nUnassigned_Unmapped\n0\n\n\nUnassigned_MappingQuality\n0\n\n\nUnassigned_Chimera\n0\n\n\nUnassigned_FragmentLength\n0\n\n\nUnassigned_Duplicate\n0\n\n\nUnassigned_MultiMapping\n37553849\n\n\nUnassigned_Secondary\n0\n\n\nUnassigned_NonSplit\n0\n\n\nUnassigned_NoFeatures\n4138912\n\n\nUnassigned_Overlapping_Length\n0\n\n\nUnassigned_Ambiguity\n18155778\n\n\n\nWhat could be the reason behind it? Is there a way to improve on that? I am losing a lot of reads due to multimapping.\nI also checked the read distribution. It looks OK to me too.\n\nrseqc_read_distribution_plot1200\u00d7800 37 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b1d3e603023d5d32d3208ba04d797c72fae138c2.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nFeatureCounts only reports unique matches with default settings.\nYour reads are likely hitting more than one \u201cexon\u201d, which leads to \u201cmultimapping\u201d counts when summarized at the Gene level.\nReview the \u201cAdvanced Options\u201d. In particular, pay attention to these parameters, but also review others and see what results. There isn\u2019t a single right answer for everyone. It depends on how you want these counted up, if at all.\n\n\u201cAllow read to contribute to multiple features\u201d (default=no)\n\u201cLargest overlap\u201d (default=no)\n\u201cCount multi-mapping reads/fragments\u201d (default=disabled) and the sub-option (when enabled) \u201cAssign fractions to multimapping reads\u201d\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am comparing an assembled transcript file (GTF) with a reference annotation file (GFF) using the GFFCompare tool in Galaxy. In parameters, the only criteria am selecting is \u201cdiscard \u2018duplicate\u2019 query transfrags within a single sample (-D)\u201d. The transcript accuracy report is looking like this-\ngffcompare v0.11.2 | Command line was:\n#gffcompare -r ref_annotation -D -e 100 -d 100 -p TCONS gffread_on_data_171__gtf\n\n#= Summary for dataset: gffread_on_data_171__gtf\nQuery mRNAs :   74051 in   18336 loci  (60300 multi-exon transcripts)\n(9152 multi-transcript loci, ~4.0 transcripts per locus)\nReference mRNAs :   46683 in   36960 loci  (37011 multi-exon)\nSuper-loci w/ reference transcripts:        0\n#-----------------| Sensitivity | Precision  |\nBase level:     0.0     |     0.0    |\nExon level:     0.0     |     0.0    |\nIntron level:     0.0     |     0.0    |\nIntron chain level:     0.0     |     0.0    |\nTranscript level:     0.0     |     0.0    |\nLocus level:     0.0     |     0.0    |\n Matching intron chains:       0\n   Matching transcripts:       0\n          Matching loci:       0\n\n      Missed exons:  206513/206513\t(100.0%)\n       Novel exons:  204613/204613\t(100.0%)\n    Missed introns:  168155/168155\t(100.0%)\n     Novel introns:  115436/115436\t(100.0%)\n       Missed loci:   36960/36960\t(100.0%)\n        Novel loci:   18336/18336\t(100.0%)\n\nTotal union super-loci across all input datasets: 18336\n74051 out of 74051 consensus transcripts written in gffcmp.combined.gtf (0 discarded as redundant)\nThis is not making any sense to me. Why the precision and sensitivity are of 0 value. Thank you so much for your input."
    },
    {
      "role": "system",
      "text": "Sure, I will notify you when the tool is available, probably in a couple of weeks.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am currently trying to analyse some methylation sequencing data using Bismark. When I try to extract the methylation status using the Bismark Meth. Extractor, I am expecting to also get a .cov coverage file. However, this does not appear to be being generated.\nThe tool output gives the following:\nWriting bedGraph to file: dataset_e734b08e-b355-49c5-94cc-61ef7b831d1e.datbedGraph.gz.\nAlso writing out a coverage file including counts methylated and unmethylated residues to file: dataset_e734b08e-b355-49c5-94cc-61ef7b831d1e.datbismark.cov.gz\nHowever, I do not see the cov.gz in the resulting result archive. Am I using an incorrect setting (I had asked it to give me the genome wide cytosine report) or is this something that it currently does not output?\nAny help would be greatly appreicated!\nMany thanks,\nLewis"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI have updated  Bismark extractor in order to provide the coverage file: Bismark methylation extractor: include coverage output by gallardoalba \u00b7 Pull Request #1131 \u00b7 bgruening/galaxytools \u00b7 GitHub (@link https://github.com/bgruening/galaxytools/pull/1131). It will be available soon.\nHowever, I recommend you to use Methyldakel (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/pileometh/pileometh/0.5.2+galaxy0) instead of Bismark.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nBriefly, I would like to download a species genome (CBVK0000000000.1 Picea abies :: NCBI (@link https://www.ncbi.nlm.nih.gov/Traces/wgs/CBVK01?display=contigs)) into Galaxy.\nHowever, the fasta of the full genome of the species in many pieces (32). I have tried Getdata/Collection/Paste-Fetch data. There I pasted all the 32 URLs(links) to the files.\nAfter, there was an option to build a list. I created the list. Now I have no idea what to do. How I can reconstruct the full file or one file with all sequences\u2026I mean how I will get the full genome of the species?\nI\u2019m new to Galaxy and I\u2019m not a programmer.\nPlease advice.\nBest, Thend"
    },
    {
      "role": "system",
      "text": "@quote\nCBVK0000000000.1 Picea abies :: NCBI\nThis is where the genome fasta and reference annotation is located: @link ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/067/695/GCA_900067695.1_Pabies01"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have run out of space in the @link http://usegalaxy.eu server and am trying to download files to delete them from my history to make space on my account. However, I am experiencing an issue when trying to download files from the @link http://usegalaxy.eu/ history, it fails exactly at 1.08Gb. I have tried\nwget -O pool5.tgz https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX  (@link https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX)\nand\ncurl -o pool5 https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX  (@link https://usegalaxy.eu/api/dataset_collections/XXXX/download?key=XXXX)\nand each time I try to check the files of the content with tar -zvft , it starts reading the file names until it gets to the halfway and then I get the following error\ntar: Truncated input file (needed 101140480 bytes, only 0 available)\ntar: Error exit delayed from previous errors.\nI have tried in a Mac and a Windows laptop, and different files and with one month of difference. I\u2019d appreciate some help with this.\nMany thanks,\nRamiro"
    },
    {
      "role": "user",
      "text": "Dear @system,\nAn update on the subject. The workaround you propose of making a history archive and downloading it has worked. And there does not appear to be any error on the downloaded data.\nThanks for your help."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI deployed Galaxy locally using sh run.sh on Ubuntu 16.04 LTS. My tool was dockerized and set to run locally through Galaxy interface. The tool ran ok as I saw files generated. But Galaxy interface couldn\u2019t list files in the webpage. Here is my Galaxy logs copied from my terminal (at  galaxy logs (@link https://drive.google.com/open?id=1UPh4GWXMcDP5ZvbtuzNAJPWXttfoacdE))\nMy job_config.xml is like\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"1\"/>\n    </plugins>\n    <destinations default=\"local\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"docker_local\" runner=\"local\">\n          <param id=\"docker_enabled\">true</param>\n        </destination>\n\n        <destination id=\"metaboflow-container\" runner=\"local\">\n           <param id=\"docker_enabled\">true</param>\n           <param id=\"docker_volumes\">$defaults,/opt/galaxy_data:rw</param> -->\n           <param id=\"docker_sudo\">false</param>\n           <param id=\"docker_auto_rm\">false</param>\n        </destination>\n\n    </destinations>\n    <tools>\n\n     <tool id=\"metaboflow\" destination=\"metaboflow-container\"/>\n    </tools>\n</job_conf>\n\n\nmy tool xml is\n<tool id=\"metaboflow\" name=\"Metabo Flow (in docker)\">\n    <description>Metabo Flow</description>\n    <requirements>\n      <container type=\"docker\">jianlianggao/metaboflow_pre:20203</container>\n    </requirements>\n    <command><![CDATA[\n      /usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o \\$PWD\n     ]]>\n    </command>\n    <inputs>\n       <param name=\"testdata_input\"  type=\"data\" label=\"Test Dataset\" help=\"Demo data set (Dataset_Metaboflow_Ionomic_Workflow.csv) can be downloaded from the link below.\" />\n        <param name=\"pre_define_sd\"  type=\"data\"  label=\"Pre-defined standard deviation matrix\" />\n    </inputs>\n    <outputs>\n      <!--  <collection name=\"metaboflow_output_plots\" type=\"list\" label=\"MetaboFlow output plots from ${on_string}\">\n\n             <discover_datasets pattern=\"(?P&lt;name&gt;)\\.pdf$\" ext=\"pdf\" />\n\n                 </collection> -->\n         <data name=\"data_long\" format=\"csv\" /> <!--  from_work_dir=\"data_long.csv\" label=\"data_long.csv\"/>\n          <data name=\"output2\" file=\"/opt/galaxy_data/data_wide.csv\" label=\"data.wide.csv\"/>\n         <data name=\"output3\" file=\"/opt/galaxy_data/data_wide_Symb.csv\" label=\"data.wide_Symb.csv\"/>\n         <data name=\"data4\" file=\"/opt/galaxy_data/stats.median_batch_corrected_data.txt\" label=\"stats.median_batch_corrected_data.txt\"/>\n         <data name=\"data5\" file=\"/opt/galaxy_data/stats.outliers.txt\" label=\"stats.outliers.txt\"/>\n         <data name=\"data6\" file=\"/opt/galaxy_data/stats.raw_data.txt\" label=\"stats.raw_data.txt\"/>\n         <data name=\"data7\" file=\"/opt/galaxy_data/stats.standardised_data.txt\" label=\"stats.standardised_data.txt\"/>\n         <data name=\"data8\" file=\"/opt/galaxy_data/plot.logConcentration_by_batch.pdf\" label=\"plot.logConcentration_by_batch.pdf\"/>\n         <data name=\"data9\" file=\"/opt/galaxy_data/plot.logConcentration_z_scores.pdf\" label=\"plot.logConcentration_z_scores.pdf\"/> -->\n    </outputs>\n    <tests>\n      <test>\n         <param name=\"testdata_input\" value=\"Dataset_Metaboflow_Ionomic_Workflow.csv\"/>\n         <param name=\"pre_define_sd\" value=\"pre_defined_sd.txt\"/>\n         <output name=\"data_long\" file=\"data_long.csv\"/>\n      </test>\n    </tests>\n    <help>\n    </help>\n</tool>\n\nLook forward to getting your comments to help me sort out my issue. Thank you very much.\nBest regards,\nJianliang"
    },
    {
      "role": "system",
      "text": "My first question would be, did you tried it without that extra output folder. So doing something like -o \"\" when you execute the script.\nIf you need or want that folder I think in your case you could do.\n<![CDATA[\n      mkdir output && /usr/local/bin/runPreProc.R -d ${testdata_input} -p ${pre_define_sd} -o output\n     ]]>\n\nSo in your tool xml or in the R scripts itself"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m following the Scaling and Load Balancing \u2014 Galaxy Project 22.09.dev0 documentation (@link https://docs.galaxyproject.org/en/latest/admin/scaling.html) and cannot get gunicorn to bind to the socket file.\n\u2013 gunicorn-galaxy.socket \u2013\n[Unit]\n\nDescription=gunicorn socket for galaxy\n\n[Socket]\n\nListenStream=/run/gunicorn-galaxy.sock\n\n[Install]\n\nWantedBy=sockets.target\n\n\u2013 galaxy.yml gunicorn section \u2013\n  gunicorn:\n    enable: true\n    bind: 'unix:/run/gunicorn-galaxy.sock'\n    workers: 2\n    timeout: 300\n    extra_args: '--forwarded-allow-ips=\"*\"'\n    preload: true\n\n\u2013 gunicorn-galaxy.service \u2013\n[Unit]\n\nDescription=gunicorn daemon for galaxy\n\nAfter=network.target\n\nAfter=time-sync.target\n\n[Service]\n\nPermissionsStartOnly=true\n\nType=simple\n\nUser=galaxy\n\nGroup=www-data\n\nRestart=on-abort\n\nWorkingDirectory=/home/galaxy/galaxy\n\nTimeoutStartSec=10\n\nExecStart=/home/galaxy/galaxy/.venv/bin/galaxy \\\n\n--state-dir /home/galaxy/galaxy/database/gravity\n\nEnvironment=VIRTUAL_ENV=/home/galaxy/galaxy/.venv PATH=/home/galaxy/galaxy/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n[Install]\n\nWantedBy=multi-user.target\n\nThis is the serviced error:\n$ sudo systemctl status gunicorn-galaxy.service\n\u25cf gunicorn-galaxy.service - gunicorn daemon for galaxy\nLoaded: loaded (/etc/systemd/system/gunicorn-galaxy.service; enabled; vendor preset: enabled)\nActive: active (running) since Mon 2022-10-17 21:20:23 UTC; 14min ago\nTriggeredBy: \u25cf gunicorn-galaxy.socket\nMain PID: 5053 (galaxy)\nTasks: 9 (limit: 18962)\nMemory: 577.3M\nCPU: 7min 57.313s\nCGroup: /system.slice/gunicorn-galaxy.service\n\u251c\u25005053 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/galaxy --state-dir /home/galaxy/galaxy/database/gravity\n\u251c\u25005055 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/supervisord -c /home/galaxy/galaxy/database/gravity/supervisor/supervisord.conf --nodaemon\n\u251c\u25005058 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery worker --concurrency 2 --loglevel DEBUG --pool threads --queues celery,galaxy.internal,galaxy.external\n\u251c\u25005059 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/celery --app galaxy.celery beat --loglevel DEBUG --schedule /home/galaxy/galaxy/database/gravity/celery-beat-schedule\n\u251c\u25005063 /usr/bin/tail -f /home/galaxy/galaxy/database/gravity/log/gunicorn.log /home/galaxy/galaxy/database/gravity/log/celery.log /home/galaxy/galaxy/database/gravity/log/celery-beat.log\n\u251c\u25005083 /home/galaxy/galaxy/.venv/bin/python -c \u201cfrom multiprocessing.semaphore_tracker import main;main(7)\u201d\n\u2514\u25005323 /home/galaxy/galaxy/.venv/bin/python /home/galaxy/galaxy/.venv/bin/gunicorn \u201cgalaxy.webapps.galaxy.fast_factory:factory()\u201d --timeout 300 --pythonpath lib -k galaxy.webapps.galaxy.workers.Worker -b unix:/run/gunicorn-galaxy.sock --workers=2 --config python:galaxy.web_stack.gunicorn_config --preload \u201c\u2013forwarded-allow-ips=*\u201d\nOct 17 21:34:07 ip-10-0-1-180 galaxy[5063]: [2022-10-17 21:34:07 +0000] [5310] [ERROR] Can\u2019t connect to /run/gunicorn-galaxy.sock\nThe file exists and is rw to all:\n$ ls -l /run/gunicorn-galaxy.sock\nsrw-rw-rw- 1 root root 0 Oct 17 20:55 /run/gunicorn-galaxy.sock"
    },
    {
      "role": "user",
      "text": "Hello @system ,\nThank you for the instructions. If I had the help of a unix sysadmin, I would certainly ask them to look into ansible.\nWe finally got galaxy to run with a working file upload on our system:\n\n\ntusd settings for external tusd server: we could not figure it out, due to the \u201cMixed Content\u201d error that masked tusd configuration issues (see below). We are now using the tusd that comes already shipped with galaxy. I was very confused by the fact that the tusd config has to be disabled in galaxy.yml in order for tusd to work by default ?!\n\n\nScaling and Load Balancing \u2014 Galaxy Project 22.05.1 documentation (@link https://docs.galaxyproject.org/en/master/admin/scaling.html?#systemd) : on our system, the group has to be set to www-data and not galaxy, even though the galaxy user is in the galaxy group.\n\n\nthe NGINX config statement in the docs: \u201cproxy_set_header X-Forwarded-Proto $scheme;\u201d causes uploads to fail on Chrome and Firefox with error: \u201cMixed Content: The page was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint\u201d. Replacing $scheme with https fixed this.\n\n\nI searched the project tickets in GitHub and since nobody reported the same issues, I assume the problems are due to our setup, where an external load balancer translates http requests to https -an institutional constraint over which we have no control."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI run into problems with mummerplot, could you please help me?\nI have submitted a reference genome and an assembly (both in fasta format), for comparison, and I got the following message:\nReading delta file /data/dnb02/galaxy_db/files/008/660/dataset_8660337.dat\nWriting plot files out.fplot, out.rplot, out.hplot\nWriting gnuplot script out.gp\nRendering plot out.png\nWARNING: Unable to run \u2018false out.gp\u2019, Inappropriate ioctl for device\nDid I miss something or is there a pb with the tool?\nthanks"
    },
    {
      "role": "system",
      "text": "Hi @system and @user,\nsorry for my late reply. This should be fixed now.\nThanks,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m using a @link http://usegalaxy.org instance to analyze illumina data (paired end, trimmomatic trimmed) to a reference fungal genome (one I downloaded from Ensembl). I am using BWA-MEM in paired-end mode, but I think that it is failing in the indexing.\nEvery time I run it, I get this error:\n[E::idx_find_and_load] Could not retrieve index file for \u2018/galaxy-repl/main/files/042/990/dataset_42990690.dat\u2019\n[E::idx_find_and_load] Could not retrieve index file for \u2018/galaxy-repl/main/files/042/990/dataset_42990690.dat\u2019\nAnyone have any suggestions?"
    },
    {
      "role": "system",
      "text": "This was fixed (@link https://github.com/galaxyproject/usegalaxy-playbook/commit/47b9ee1d690f3f077cd2e93975c15af789f554a9) as of September 22, sorry for not updating here. Please let us know if you\u2019re still encountering issues."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have tried running some RNAseq datasets on Trimmomatic but they keep failing. The error I get when clicking on the bug icon is the following:\nFatal error:\nWhen checking the information section, I get \u201cempty\u201d for \u201cTool standard error\u201d, and \u201cTool standard output\u201d is as below:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/046/864/46864652/_job_tmp -Xmx28g -Xms256m\nTrimmomaticSE: Started with arguments:\n-threads 6 fastq_in.fastqsanger.gz fastq_out.fastqsanger.gz ILLUMINACLIP:/corral4/main/jobs/046/864/46864652/configs/tmptcibpyu4:2:30:10 LEADING:30 TRAILING:30\nUsing Long Clipping Sequence: \u2018ATCGGAAGAGCACACGTCTGAACTCCAGTCACGGTGAACCATCTCGTATG\u2019\nUsing Long Clipping Sequence: \u2018GATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTTCCAACGCGTGTAGATCT\u2019\nUsing Long Clipping Sequence: \u2018ATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTTCCAACGCGTGTAGATCTC\u2019\nUsing Long Clipping Sequence: \u2018GATCGGAAGAGCACACGTCTGAACTCCAGTCACGGTGAACCATCTCGTAT\u2019\nILLUMINACLIP: Using 0 prefix pairs, 4 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nQuality encoding detected as phred33\njava.io.EOFException: Unexpected end of ZLIB input stream\nat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\nat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\nat java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)\nat org.usadellab.trimmomatic.util.ConcatGZIPInputStream.read(ConcatGZIPInputStream.java:73)\nat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\nat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\nat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\nat java.io.InputStreamReader.read(InputStreamReader.java:184)\nat java.io.BufferedReader.fill(BufferedReader.java:161)\nat java.io.BufferedReader.readLine(BufferedReader.java:324)\nat java.io.BufferedReader.readLine(BufferedReader.java:389)\nat org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:71)\nat org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)\nat java.lang.Thread.run(Thread.java:745)\nException in thread \u201cThread-0\u201d java.lang.RuntimeException: java.io.EOFException: Unexpected end of ZLIB input stream\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:56)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\nat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\nat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\nat java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)\nat org.usadellab.trimmomatic.util.ConcatGZIPInputStream.read(ConcatGZIPInputStream.java:73)\nat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\nat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\nat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\nat java.io.InputStreamReader.read(InputStreamReader.java:184)\nat java.io.BufferedReader.fill(BufferedReader.java:161)\nat java.io.BufferedReader.readLine(BufferedReader.java:324)\nat java.io.BufferedReader.readLine(BufferedReader.java:389)\nat org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:71)\nat org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)\nat org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)\n\u2026 1 more\nInput Reads: 17849000 Surviving: 17595136 (98.58%) Dropped: 253864 (1.42%)\nTrimmomaticSE: Completed successfully\nWhat I was trying to do, was to run a fastq.gz file and trim the beginning and end bases below a certain quality score (using LEAD and TRAIL commands). I was also trying to trim the adapter sequences shown as overrpresented in FastQC report, entering the specific FASTA sequences shown in the report.\nTried other trimming tools (Trim Galore! and Cutadapt) but they also result in errors (\u201cFatal error: Exit code 1 ()\u201d and \u201cFatal error: Exit code 2 ()\u201d respectively). FastQC works fine.\nIf you could help discovering what went wrong, would really appreciate it - I\u2019m a total newbie and this is something I have been struggling with for the last couple of days."
    },
    {
      "role": "system",
      "text": "Thanks for following up! I reviewed carefully and think everything is working how it is expected to.\n\nFastQC\n\n\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: ID line didn\u2019t start with \u2018@\u2019\n\nInput dataset 8 \u2013 datatype/format sra_manifest.tabular\nOutput dataset 9 \u2013 \u201cred\u201d errored result\nThe tool is expecting fastq reads and filters for those in the input drop down menu of potential datasets. Dataset 8 is not in the last listing. I\u2019m guessing that you dragged-and-dropped the dataset instead (?) That can lead to errors since it is an \u201coverride\u201d function that bypasses initial format checks.\nIf a dataset is not in the listing for a tool\u2019s input area (any) \u2013 either the datatype/format needs to be modified or the content isn\u2019t a match for what that tool can manipulate. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#tool-doesnt-recognize-input-datasets\nSeveral of the early FastQC runs in your history are similar. The input wasn\u2019t a content match, and at least one input was in an error state.\n\nFastQC\n\n\njava.io.EOFException: Unexpected end of ZLIB input stream\n\nInput dataset 45 \u2013 datatype/format sra_manifest.tabular\nOutput dataset 46 \u2013 \u201cgreen\u201d (putatively) successful result\nThe output was purged so I can\u2019t see all of it, just a bit from the \u201craw\u201d output (none of the HTML). The input does appear to be in a fastq format. The datatype was compressed \u201cfastqsanger.gz\u201d. I can\u2019t check the actual contents except for the first few lines. Those look to be a hybrid between \u201csubmitted\u201d and \u201cprocessed\u201d SRA reads that NCBI hosts.\nIf you really want to investigate, this older FAQ @link https://training.galaxyproject.org/training-material/faqs/galaxy/#ncbi-sra-sourced-fastq-data has a list of all the manipulations we knew about to standardize the various read formats that NCBI hosts. Why so much variation from NCBI in the past? Changes over time in sequencing protocols and the SRA submission process itself and in how NCBI creates/standardizes the \u201cprocessed\u201d read data then versus now, and probably some other historical things I don\u2019t know or can\u2019t remember :slight_smile:\nConfusing but shouldn\u2019t matter now, as format issues will be avoided with the Faster tool, and you are using that now.\n\nFastq info\n\nThis is different. It is the output from the tool, and it happens to also be in a job log.\n\nfastq_utils 0.25.1\nDEFAULT_HASHSIZE=39000001\nScanning and indexing all reads from /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat\nRead name provided with no suffix\n> 10000020000030000040000050000060000070000080000090000010000001100000120000013000001400000150000016000001700000180000019000002000000210000022000002300000240000025000002600000270000028000002900000300000031000003200000330000034000003500000360000037000003800000390000040000004100000420000043000004400000450000046000004700000480000049000005000000510000052000005300000540000055000005600000570000058000005900000600000061000006200000630000064000006500000660000067000006800000690000070000007100000720000073000007400000750000076000007700000780000079000008000000810000082000008300000840000085000008600000870000088000008900000900000091000009200000930000094000009500000960000097000009800000990000010000000101000001020000010300000104000001050000010600000107000001080000010900000110000001110000011200000113000001140000011500000116000001170000011800000119000001200000012100000122000001230000012400000125000001260000012700000128000001290000013000000131000001320000013300000134000001350000013600000137000001380000013900000140000001410000014200000143000001440000014500000146000001470000014800000149000001500000015100000152000001530000015400000155000001560000015700000158000001590000016000000161000001620000016300000164000001650000016600000167000001680000016900000170000001710000017200000173000001740000017500000176000001770000017800000\nERROR: Error in file /corral4/main/objects/9/4/8/dataset_94835bb6-102b-4cdf-9484-a19ea90a0d6c.dat: line 71398384: file truncated\n\nInput dataset 32 \u2013 datatype/format fastqsanger.gz\nOutput dataset 253 \u2013 \u201cgreen\u201d successful result\nHow to interpret this: the Fastq info tool ran successfully (didn\u2019t fail itself) and it reported back a problem it found in the input. This test history I created a few months ago has an example that matches your use case (artificially created). I\u2019ll leave this shared as a reference. Galaxy | Accessible History | test fastq_info (@link https://usegalaxy.org/u/jen-galaxyproject/h/test-fastqinfo)\nHow to catch these: These are text files so are simple to parse/filter/combine. Even if all the data is in a collection, you\u2019ll be able to tell which datasets any errors are from. I just did this two different ways in that shared history as examples and there are likely 100+ ways to do this. Anything \u201ctext manipulation\u201d you can do on the command line can usually be done in Galaxy, with the bonus of being able to add that to workflows, see @link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet\nThanks again and hope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m trying to use an RNA-seq pipeline to analyze my own data for C. elegans.\nEvery time I try to upload the gtf file download from UCSF, wormbase, or ensemble, I get the following error message \u201cAn error occurred setting the metadata for this dataset\u201d.\nAfter one day trying different options on solving the problem, I have tried to upload the same dataset on the @link http://galaxy.eu in which I did not receive any error. This made me to think that there is no problem with the file itself.\nMay I ask if there is a bug on the @link http://galaxy.org my local Galaxy server and if it\u2019s possible to fix it?\nThank you so much!\nIni"
    },
    {
      "role": "system",
      "text": "Hi @user\nData upload problems are usually the first indicator of some configuration problem after a new Galaxy is first started up.\nThat said, sometimes odd errors, including this one, are produced when a file is not completely uploaded (new server or not). There could be many reasons for that with a local Galaxy server. A local Galaxy needs some admin configuration to be used without problems coming up. Some of this we can help with and some of this will be specific to your computer/network that we can\u2019t help with (since it is private) but we do have documentation that covers the basic setup and use cases.\n@quote\nGalaxy resourcesQuick start:Get Galaxy - Galaxy Community HubTutorials:Galaxy Training!Docs:Galaxy Documentation \u2014 Galaxy Project 21.09.1.dev0 documentation\nThere will be a training event in March that includes administrative topics. Many of the tutorials linked above will be included. Anyone can join. The event is global, asynchronous, and many more people from the community will be available to help. GTN Sm\u00f6rg\u00e5sbord 2: 14-18 March | Gallantries (@link https://gallantries.github.io/posts/2021/12/14/smorgasbord2-tapas/)\nYou can also go through those tutorials on your own now and try to uncover the problem, or at least be able to provide more context about your configuration so the community can assist here. Examples of information that will help: Galaxy version, computer OS/version, dependency management choice(s), what admin configurations you have done, plus how some tests worked out. Have you installed tools and run tool tests? The goal is to find out things like: Do other datasets load properly? This same file type fails (always/sometimes? from different sources or just one particular source?). Do other data upload successfully \u2013 or do all uploads fail? Has any upload ever worked? Have any tools ever worked? If the analysis was successful before, have you made any changes since things worked?\n\nWhat may be best if you do not want to go through all the administrative setup/tests, and this is a new server that you haven\u2019t heavily invested in already:\n:information_source: Try an alternative deployment choice. Docker Galaxy comes with more of the administrative details pre-configured. Details here: Galaxy Docker - Galaxy Community Hub (@link https://galaxyproject.org/use/galaxy-docker/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have four R1 and four R2 fasq.gz files of a bacterial genome sequence. Do I need to concatenate each of four R1 or each of four R2 files directly?"
    },
    {
      "role": "system",
      "text": "Hello, the titel of your post and the text contain a different question.\n\nDo I need to concatenate each of four R1 or each of four R2 files directly?\n\nMostly not, what do you want to do with the files? What kind of analyses do you want to perform?\nThis can also be a good place to start:\n\n\n@link https://training.galaxyproject.org/training-material/\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have a fresh new installation of Galaxy (version 19.09 on Ubuntu) hooked into PostgreSQL.  After users install a version of bwa in Galaxy (revision: 01ac0a5fedc3) from the \u201cInstall new tools (Beta)\u201d link under the \u201cAdmin\u201d section, the new bwa shows missing dependencies.  Conversely, when I install an older version  (revision: c71dd035971e) using the \u201cInstall New Tools\u201d link (NOT Beta), the dependencies are installed along with bwa.  I do not know how to resolve/debug these missing dependencies given my 3 weeks experience with Galaxy.  Both bwa and samtools are installed in Ubuntu and included in the PATH variable for operating system user Galaxy.  This seems like it should be an obvious thing so I\u2019m seeking guidance on a good approach to debug this.  Should I expect all tools under \u201cInstall New Tools (Beta)\u201d to work?   I wonder whether the PATH is not being properly passed to Galaxy so it can\u2019t locate the operating system installations of bwa and samtools.  In the Galaxy app under \u201cManage Dependencies\u201d I tried selecting \u201cInstall checked dependencies using Conda\u201d but it generates errors in the galaxy.log.  Sorry, not sure how to include the galaxy.log without going over the 30K limit.\nBackground Info:\nI verified that bwa and samtools are both in the operating system PATH for user galaxy (see below).\nI used \u201capt-get\u201d instead of conda to install bwa on Ubuntu:  \u201c(sudo apt-get update -y;   sudo apt-get install -y bwa)\u201d.\nI\u2019m launching galaxy as \u201csh /apps/galaxy/galaxy_app/run.sh start > /apps/galaxy/galaxy_app/galaxy_start.log 2>&1\u201d.\nBWA operating system verification\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ which bwa\n/usr/bin/bwa\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ bwa\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li @link mailto:lh3@sanger.ac.uk\nSAMTOOLS operating system verification\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ which samtools\n/usr/bin/samtools\ngalaxy@PB-DEV-01:/apps/galaxy/galaxy_app/config$ samtools\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.6 (using htslib 1.6)\nBWA tool information in Galaxy after installation\nTool shed:    toolshed.g2.bx.psu.edu\nName:          bwa\nDescription:\nRevision:       01ac0a5fedc3\nOwner:          devteam\nLocation:       /apps/galaxy/galaxy_app/database/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/bwa/01ac0a5fedc3/bwa\nDeleted:        False\nDependencies:\nbwa       version:0.7.17  Resolver: none  Exact Version: True  Installation Status:  not installed\nsamtools  version:1.6     Resolver: none  Exact Version: True  Installation Status:  not installed"
    },
    {
      "role": "system",
      "text": "Ok, please also verify that /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda is present, and check the output of /apps/galaxy/galaxy_app/database/dependencies/_conda/bin/conda info."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI\u2019m trying to cut barcodes out of my reads and append them to the read name. Cutadapt clearly shows how to do this using the -u --rename function and then indicating barcode = {cut_prefix} (suffix, in my case). The cutadapt documentation is here Cutadapt Documentation (@link https://cutadapt.readthedocs.io/en/latest/guide.html#specifying-adapter-sequences).\nUnfortunately, this functionality seems to be lost in the Galaxy wrapper. You are allowed to add a prefix or suffix to the name, but it only seems to put 1 text value in as the prefix. Does anyone know of a workaround or maybe I\u2019m doing something wrong?\nBarcode splitter seems to separate every barcode into different files. I have a random 9bp sequence set, so there would be 10\u2019s thousands of files with 1 sequence in them each.\nThanks,\nRick"
    },
    {
      "role": "system",
      "text": "Hi @user, we have updated cutadapt recently. The last version will be available in @link http://usegalaxy.eu in a few days.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am trying to provide a list of input coordinates so that I can use this tool to stitch together existing MAF blocks, but it is surprisingly complicated. I don\u2019t understand why galaxy is insisting that the \u201cchoosing intervals\u201d input is a file (shouldn\u2019t it just be intervals, such as (1,6)?\nI\u2019m also not sure what to put for MAF type. Any help would be immensely appreciated, thank you!\n\nimage1658\u00d71144 83.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1cfdc5998c447dde9168d5c44ce74619ded60216.png)"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nThis publication I helped to author from 2012 covers the usage of this tool suite in great detail. It is open source. You\u2019ll notice that the UI has been updated but the underlying logic and data formats are still the same, and the original example data is still available in a shared history. Please give that a review, as I think it will address all of your questions, but you can ask if anything is not clear.\n\n\n\nPubMed Central (PMC) (@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4282168/)\n\n\n\nUsing Galaxy to Perform Large-Scale Interactive Data Analyses (@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4282168/)\nInnovations in biomedical research technologies continue to provide experimental biologists with novel and increasingly large genomic and high-throughput data resources to be analyzed. As creating and obtaining data has become easier, the key...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone!\nI started a unicycler run two days ago (07/08/2023) on a set of 87 paired reads. Everything went fine (80 ok, 4 with errors) until yesterday morning when I noticed two jobs were running and 1 was waiting for quite a long time. Past issues solved themselves with time and the problem was me being impatient so I decided to wait, today these two jobs are still running and the other one is still waiting, so I suspect something\u2019s off. Should I keep on waiting?\nThanks a lot!"
    },
    {
      "role": "system",
      "text": "@quote\nBy the way, things are still unchanged\nThe jobs look failed now, so you could try rerunning.\n@quote\nI didn\u2019t try it yet but would it be possible to download the assembled reads even if the collection hasn\u2019t been processed as a whole? If so, I might do this and retry the assembly with the affected reads.\nYes, you can filter a collection to exclude empty or failed datasets: see tools in the group Collection Operations.\n@quote\nIf so, I might do this and retry the assembly with the affected reads.\nDo you mean create a new collection with just those datasets that didn\u2019t process with the batch? If yes, you can go into the hidden datasets tab in the history, unhide the inputs, create a new collection out of them, then run those. Later on if that works, you could combined the two collections together again (original results + rerun results).\nThat said, the usual way is to rerun those elements again while in the collection making sure to check the box to replace back into the original output collection.\nAnd, you could get fancy with filtering by element identifiers but with so few datasets and no workflow considerations, just doing it directly might be faster.\nSo that\u2019s a few ways to go forward. Hope one works out for you!"
    }
  ],
  [
    {
      "role": "user",
      "text": "ampvis2 is a great tool, and I see it is available in the toolshed. Will it make it to Galaxy EU?"
    },
    {
      "role": "system",
      "text": "Its called ampvis2 you can search for it. This is one tool out of many: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/ampvis2_venn/ampvis2_venn/2.7.22+galaxy0"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to use Generate gene to transcript map for Trinity assembly (Galaxy Version 2.8.4) tool. I end up with this warning.\nimage\nCan anyone say what is the problem and how to rectify.\nThank you"
    },
    {
      "role": "system",
      "text": "@quote\nGenerate gene to transcript map\nThe tool is specifically having trouble parsing the \u201c>\u201d fasta assembly title lines.\nThis is an example of the type of \u201c>\u201d title lines this particular tool is designed to parse: tools-iuc/Trinity.fasta at master \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/trinity/test-data/raw/Trinity.fasta)\nIf you ran Trinity successfully in Galaxy the format will be Ok by default. If you ran Trinity someplace else and uploaded the result, compare your title lines (transcript IDs) with the example.\nAssemblies generated by other methods would not be appropriate inputs for this tool. That said, it could still be possible to parse out transcript identifiers and genes using other tools (general Text Manipulation tools) \u2013 it depends on the content of your data.\nNote: This tool wasn\u2019t always available in Galaxy. This is prior Q&A from about a year ago when this specific tool\u2019s functionality was recreated with a simple workflow. It could be used now as an example for custom parsing (IF your title lines have transcript nomenclature that encodes some type of transcript-to-gene relationship/grouping):\n@quote\nDear all, \nAfter I executed Trinity, I obtained 2 files as assembly transcripts.fasta and log.txt files. \nI would like to know that is it possible to download \u2018Trinity.fasta.gene_trans_map\u2019 from Galaxy Trinity. It is important for my downstream analysis. \nThank you so much, \nKamoltip\nIf you still need help after reviewing:\nPost back 10-20 of your assembly \u201c>\u201d title lines for some help in figuring out how to parse the data or some help in determining if this type of parsing is even possible with your given data\u2019s content. Please don\u2019t include the sequences, just the \u201c>\u201d title lines \u2013 and enough lines that the data is representative of the whole.\nThe Select tool can be used to isolate title lines from large fasta datasets. Use the option \u201cMatching\u201d with the regular expression: ^>\nThe regular expression means:\n\n\n^ the start of a line\n\n> a \u201cgreater than\u201d symbol (how fasta title lines are designed)\n\nFasta format FAQ: Datatypes - Galaxy Community Hub (@link https://galaxyproject.org/learn/datatypes/#fasta)\nSmall disclaimer: How Trinity formats title lines changed a bit between releases, so that may be where your problem is. But the help here should help even in that case \u2013 the workflow could definitely be tuned for any Trinity-based format."
    }
  ],
  [
    {
      "role": "user",
      "text": "Can I request Genome scaffolder on galaxy?"
    },
    {
      "role": "system",
      "text": "Hello @user\nNew wrapped tool requests can be made to the IUC (@link https://galaxy-iuc-standards.readthedocs.io/) by creating a ticket here: @link https://github.com/galaxyproject/tools-iuc/\nOr, you can vet the request first at their Gitter channel here. Someone may already be working on wrapping this tool, or a related tool: @link https://gitter.im/galaxy-iuc/iuc\nExisting wrapped tools can be searched for at the ToolShed @link https://usegalaxy.org. You can check to see which servers have any of these tools installed already by reviewing/searching the tool panel of that server (keyword: \u201cassembly\u201d will almost always work best).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI was using galaxy earlier (was able to log on and install tools) but when I was uploading data from my computer to the local galaxy (link copy) I got an error message and it logged me out of galaxy. When I tried to log back into galaxy, I am unable to, the web log page does not respond and then times out. I have tried clearing my history and deleted stored passwords to galaxy but I am still unable to log in.\nThe last line in my terminal is:\ngalaxy.webapps.galaxy.controllers.user DEBUG 2020-04-21 15:04:28,746 [p:8320,w:1,m:0] [uWSGIWorker1Core3] trans.app.config.auth_config_file: /Volumes/My_Book/20.01/galaxy/config/auth_conf.xml\nI have no idea how to resolve this. Any help would be hugely appreciated.\nadditional when run sh run.sh, I am not given a http link to galaxy but was able to access the web browser page because I had the address copy and available to paste from earlier.\nadditional details:\nUsing Mac OS Catalina, galaxy 20.01(new clone re-installed yesterday)"
    },
    {
      "role": "system",
      "text": "Are you using python 3.7?  This is a bug I have a fix for in @link https://github.com/galaxyproject/galaxy/pull/9642.  You can pull that manually if you\u2019d like, or, in the interim downgrade to python 3.6, and you should be good to go.  Technically, we only support 3.5 and 3.6, but this code will fix login issues with 3.7."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nOur lab uses Trinity for most of our assembly work, but lately, I\u2019ve been unable to get the tool to run correctly.\nWhat I am attempting to assemble is a set of paired-end Illumina reads, with a file size of about 360MB for each strand. This created a near-instantaneous error report when the job transitioned from \u2018pending\u2019 to \u2018running\u2019. The job API ID is bbd44e69cb8906b567f21c7489016a08, and the error code is \u201c[Errno 122] Disk quota exceeded\u201d. My understanding of that error code is that it is a memory issue, but we\u2019ve assembled larger datasets than that in the last without issue. Any clarification you could give would be appreciated."
    },
    {
      "role": "system",
      "text": "@system\nFor your run sent in, the job did hit a cluster problem that should now be resolved (was present for a short time today).\n@user\nAlso please try a rerun (with intact fastq).\nAbout 20 GB per end (compressed) is the largest I\u2019ve seen assemble at a public Galaxy. Larger work can be moved to a custom Galaxy (@link https://help.galaxyproject.org/t/job-failure-due-to-exceeding-resources-at-public-galaxy-servers-solution-modify-the-job-or-consider-a-custom-galaxy-server/6614/2).\nThanks to you both for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m going through the following tutorial on @link http://usegalaxy.org. I got stuck at the point where I should click D. melanogaster link to see mapped reads on IGV after Mapping with TopHat2.\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rb-rnaseq/tutorial.html\n\n\nGalaxy Training: Reference-based RNAseq data analysis (long) (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rb-rnaseq/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nWhen I try it on Chrome, it suddenly closes the window that opens in milliseconds.\nWhen I try it on Firefox, it asks me to \u201csave/open with\u201d a file (igv.jnlp) 2.0KB.\nI cannot get the view of the reads that are mapped on the genome as shown in the tutorial. What am I supposed to do?\nThank you"
    },
    {
      "role": "system",
      "text": "Welcome, @user!\nHave you used IGV before in your current machine?\nCheck Chrome downloads; you may have the *jnlp file automatically downloaded in chrome, that\u2019s why it closes quickly; but Firefox first asks what to do with it;\nThe *.jnlp file is opened by IGV (JWS, Java Web Start); e.g. IGV 2.3 | Integrative Genomics Viewer (@link http://software.broadinstitute.org/software/igv/igv2.3) :\n\n(1) Java Web Start (JWS). Use the buttons below to launch IGV directly from our web site.\n\n\nClicking on one of the buttons will download a .jnlp file and then execute the file using Java Web Start (JWS).\n\n\nSome web browsers will only download the .jnlp file and not automatically launch IGV. For these browsers, locate the file in your folder designated for browser downloads, and double-click on the file to launch IGV.\n\n\n\nYou\u2019ll find more info in the IGV User Guide | Integrative Genomics Viewer (@link http://software.broadinstitute.org/software/igv/UserGuide)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am getting this error when trying to run RNA-STAR, how can I resolve it? Thank you.\nFatal error: Matched on FATAL ERROR\nTranscriptome.cpp:48:Transcriptome: exiting because of INPUT FILE error: could not open input file /cvmfs/data.galaxyproject.org/managed/rnastar_index2/hg38/dataset_950901_files/exonGeTrInfo.tab\nSolution: check that the file exists and you have read permission for this file\nSOLUTION: utilize --sjdbGTFfile /path/to/annotantions.gtf option at the genome generation step or mapping step\nMar 06 00:51:59 \u2026 FATAL ERROR, exiting"
    },
    {
      "role": "system",
      "text": "Hello,\nYou need to supply a reference annotation GFT dataset from the history at runtime.\nThe GTF should be based on the UCSC \u201chg38\u201d genome build. Some choices:\n\nFor  Gencode, copy the link to the GTF and paste it into the  Upload  tool. Hg38 data is here @link https://www.gencodegenes.org/. After it is loaded, remove the headers (lines that start with a \u201c#\u201d) with the  Select  tool using the options \u201cNOT Matching\u201d with the regular expression  ^# . Once the formatting is fixed, change the datatype to be  gft  under Edit Attributes (pencil icon). The data will be given the datatype  gff  by default, which works fine with some tools and but not with others. Avoid the  gff3  version of this particular data (contains duplicated IDs and several RNA-seq tools do not work with annotation in that format anyway).\nFor  iGenomes, the archive corresponding to the target genome/build needs to be locally downloaded, the tar archive unpacked, and then just the  genes.gtf  data uploaded to Galaxy (browse the local file, or use FTP). Find all available genome/builds here: @link https://support.illumina.com/sequencing/sequencing_software/igenome.html\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am developing a new galaxy tool using R as main programming language.\nWhen I upload a number of txt files with the Upload tool in Get Data, I noticed that it stores the files in galaxy/database/files/000  directory and changes the file names from the originalname.txt  into dataset_number .dat. How can I set up Upload tool in order to store the file with the originalname.txt format.\nThank you very much in advance for your help."
    },
    {
      "role": "system",
      "text": "This isn\u2019t possible, Galaxy may store files in object stores that don\u2019t resemble classical file systems.\nThe correct way is to reference files in your tool using $file.element_identifier."
    }
  ],
  [
    {
      "role": "user",
      "text": "Heyya,\nI am trying to adapt the HISAT2 options to my samples and I encountered this problem:\n\nCapture2.PNG941\u00d7181 5.71 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/fa28c84e769e41457b67b3238c58e8aaeba7ae95.png)\nI know what I want: I don\u2019t want HISAT2 to try and align individual mates.\nSo I should add the command --no mixed behaviour.\n\nCapture.PNG1325\u00d792 6.56 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/886fd5df1c2607d5398d1c4474eb3df45f0988ad.png)\nBut what this means in Galaxy? Yes or No?\nIt basically says: HISAT2 does this. By default is False (no-mixed). Do you want for No-mixed behaviour to be disabled? Y/N.\nIt sounds like I am disabling the button that disables? Is Galaxy using the default settings for HISAT2 (and other programs)? Or it comes with its own recommendations?\nI am not sure what Yes and No mean in this context. Am I reading too much into this?"
    },
    {
      "role": "system",
      "text": "@quote\nI know what I want: I don\u2019t want HISAT2 to try and align individual mates.\nThen leave the option as the tool form default (set to \u201cNo\u201d).\nI agree this is a bit confusing \u2026 underlying tool defaults versus tool form defaults \u2026 but hope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to follow this tutorial here: @link https://galaxyproject.github.io/training-material/topics/visualisation/tutorials/jbrowse/tutorial.html\nI am using a local installation of Galaxy v.19.01 and JBrowse 1.16.5+galaxy5. I whitelisted JBrowse and imported the datasets via the links from the tutorial to my history.\nJBrows itself seems to run fine, but when I want to look at the results, as soon as I open a track I get the following error messages:\n\nAn error was encountered when displaying this track.\nError: HTTP 200 when fetching @link http://XXXgalaxy/datasets/f0e0bb0fb5(...)/display/data/raw/6aa7a84(...)_0.gff.gz bytes 0-262143\n\ngalaxy_errors1913\u00d7395 106 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/29067cf2f1d908c1284dfcc59e2d96e5a8d32320.png)\nI\u2019d be happy for any tips why this happens and how to solve this.\nthanks,"
    },
    {
      "role": "system",
      "text": "Hi @user, @system, @system!\nI\u2019m sorry for the delayed response here, I didn\u2019t notice this issue previously.  I\u2019m one of the two devs and maintainers of that tool.\nJBrowse can generate this error when range requests do not work properly. JBrowse makes a request to your galaxy for the datasets, but it only requests the portion that you\u2019re viewing with a header like Range: bytes 0-135. Given that it is making a range request, JBrowse expects to see a 206 Partial Content http return code, but instead it sees 200 OK, as your webserver does not support range requests, and thus returns the whole file.\nIf you\u2019re using Nginx or Apache, please consider turning on the sendfile mechanism:\n\n@link https://docs.galaxyproject.org/en/master/admin/nginx.html?highlight=jbrowse#sending-files-with-nginx\n@link https://docs.galaxyproject.org/en/latest/admin/apache.html?highlight=jbrowse#sending-files-with-apache\n\nUnfortunately this means JBrowse does not always work nicely with development servers, and works best under a proper webserver.\nI\u2019ve tested the honour-range option from uwsgi (@link https://uwsgi-docs.readthedocs.io/en/latest/Options.html#honour-range) but it seems to have some issues (@link https://github.com/unbit/uwsgi/issues/1638) so I have to recommend nginx or apache with sendfile.\nSorry for not seeing this sooner!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have tried to assemble paired reads (Illumina) with nanopore reads but I always obtain the same error:\nAn error occurred with this dataset:\nformat fasta database [?]\ncannot find \u2018lr\u2019\nCould anyone help me with this?\nThanks in advance"
    },
    {
      "role": "system",
      "text": "Updates, please see run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210 (@link https://help.galaxyproject.org/t/run-unicycler-always-got-unicycler-command-not-found-resolved-at-usegalaxy-org-20230210/9297)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, I have created paired dataset for my samples. Trying to download, but it keeps failing. Is it because of the size of the dataset? How can I work around this?"
    },
    {
      "role": "system",
      "text": "Update\nTo learn which Galaxy servers are appropriate per tutorial, see the pull-down menu in the top information box labeled \u201cAvailable on these Galaxies\u201d.\nThe two tutorials involved are not support at @link http://UseGalaxy.org for now. If that changes, the menu will automatically update.\nPlease retry at a Galaxy server that supports the tools/data included in the tutorial."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy4) and it is giving fatal error since the 15th, after your maintenance.\nI am using @link http://usegalaxy.org\nUse of uninitialized value in subroutine entry at /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-dd38f3b35796eaa94d0b5d1d9c1428ec987dcd7f39e81a08df8df138203a8004/bin/\u2026/lib/Circos/Configuration.pm line 781.\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /cvmfs/main.galaxyproject.org/deps/_conda/envs/mulled-v1-dd38f3b35796eaa94d0b5d1d9c1428ec987dcd7f39e81a08df8df138203a8004/bin/\u2026/lib/Circos/Colors.pm line 348,  line 6."
    },
    {
      "role": "system",
      "text": "Hi @user, I\u2019m one of the tool authors.\nThe earlier versions of the wrapper had some significant bugs in the color functions, those should all be fixed in the newer version. Please try 0.69.8+galaxy7 if it is available on .org.\nIt should be available on @link http://UseGalaxy.eu."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI want to get gene ids from the results of SAlmon. Which GTF/CSV should be used? Do I need to use tximport. Please help"
    },
    {
      "role": "system",
      "text": "Hi @user\nI see a few problems with these fasta title lines (the \u201c>\u201d lines). These need to be just transcript identifiers in the same exact format as included in your transcript-to-gene mapping file. Item 1 below definitely needs to be addressed. Items 2 and/or 3 might need to be changed.\nThis formatting problem will definitely cause a mismatch and needs to be fixed before that fasta file is used with any tool. It is the minimal change required and the tool NormalizeFasta can be used (explained in the prior post).\n\nextra content after the first whitespace \u2013 \u201crange\u201d and everything after should be removed as it is not part of the transcript identifier portion of the fasta title lines\n\nThese two might cause problems, it depends on how those same identifiers are formatted in your mapping file.\n\n\nextra content before the first whitespace, where the \u201cidentifier\u201d is located \u2014 the \u201chg38_knownGene_\u201d portion might need to be removed\n\n\nthe transcript identifiers include the version (the extra .N where N is a number). If your mapping file includes the version, then it is a match. If not, then it won\u2019t match.\n\n\nThe mapping file wasn\u2019t posted, but you can compare yourself to the fasta identifiers.\n\nThis ENST00000456328\n\nIs different than ENST00000456328.2 or hg38_knownGene_ENST00000456328.2\n\nAnd none of those will match up with the full title \u201c>\u201d line as shown in your screenshot.\n\nIdentifiers cannot include any whitespace (spaces, tabs). And as long as the identifier used matches in all your files, and doesn\u2019t include any whitespace, this tool will work.\nHope you solved the problem already, but if not yet, the above should help you to find and resolve the mismatches. :slight_smile:\nFAQ for fasta: Datatypes - Galaxy Community Hub (@link https://galaxyproject.org/learn/datatypes/#fasta)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI\u2019ve been working and exploring Galaxy quite intensely this last year, and many histories are of no added value because they are mainly small attempts to find out the possibilities of the Galaxy platform . They do, however, obscure those histories that contain interesting workflows and that I like to have a look at every now and then. Is it possible to completely remove these obsolete histories (they are purged and do not take up any memory) from my account/the server?"
    },
    {
      "role": "system",
      "text": "I see. We can not remove these histories. We keep all of them for reproducibility reasons. Can you maybe tag your histories and search/sort for tags to get rid of your junk histories?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nThe download links for bam output files (13G) from MarkDuplicate and AddOrReplaceReadGroups are downloading only few Mbs of html file rather bam. I tried using \u201cwget\u201d, \u201ccurl\u201d both and also tried to transfer these files to @link http://useglaxy.org server but it failed.\nPlease tell me how to download appropriate files or what is the problem with copy link option.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user and @system\nGood detective work. The how-to is a bit different at EU. Maybe someone already discovered how this works, but in case not:\n@link http://UseGalaxy.eu is set up to have all data private and not publically accessible by URL. If you are trying to move data between servers, the accounts are distinct, so the sharing state needs to be set at EU to be more permissive to transfer data by URL to anywhere else (other Galaxy servers, external cloud storage, etc).\nTry this:\n\nGo to the history that contains the dataset\nPull down the history :gear: menu and click into Share or Publish.\nSet the first option to share the history by link, and make sure that the dataset you want to share is included. You don\u2019t need to publish.\nGo back to your history with the dataset\nThen copy/paste the dataset link other places (other Galaxy servers Upload tool).\nUnshare the history or leave it in a shared state.\n\nAdjusting privacy settings for your account:\nAccounts at any of the UseGalaxy.* servers have the options to adjust the default sharing status for what is most convenient for you. It would be very unlikely for anyone to guess what a random link is, let alone understand what that data represents, but any access at all has to be an explicit opt-in at the EU server for GDPR reasons.\nTo see how it works \u2013 try doing the reverse between the ORG server to the EU server without any changes. The data will transfer unless you set up the account preferences to be different from the default.\nAnother way to see how it works is to toggle the first of these two global settings at EU in User > Preferences, create a new history, add a new dataset, and what happens when transferring that dataset by URL. Data created before the toggle will not be impacted, data created after will be.\nThe second setting makes more permanent changes to all current data (everything has the state changed to private) but know that this cannot be undone with one change. Instead, you\u2019d need to directly reshare any histories (and the datasets contained) / workflows / etc that were previously shared if that matters to you.\nSet Dataset Permissions for New Histories\n\nGrant others default access to newly created histories. Changes made here will only affect histories created after these settings have been stored.\n\nand\nMake All Data Private\n\nClick here to make all data private.\n\nDownloading to a local desktop from the :floppy_disk: works at EU with any sharing state since when you are logged into your own account, your account is assumed to be private and only directly accessible by you, and only while logged in. I\u2019m not sure if the API key is needed for wget  at the EU server. The EU team can advise, or someone can test that out and post back what works and doesn\u2019t at the different servers. We could make an FAQ to capture the information in one place, and attribute your account here, or better, your github account.\nThe March 24th webinar here has more details. We went over the account preferences options then ran through a collaborate-by-sharing demonstration: Galaxy-ELIXIR webinars series: Advanced Features | ELIXIR (@link https://elixir-europe.org/events/galaxy-elixir-webinars-series-advanced-features).\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nIm currently in the middle of my uni project and I\u2019m trying to follow an RNA-Seq HiSat pipeline to analyse reads obtained from NCBI Gene Expression Omnibus (GEO). I\u2019ve got the data, and tried to put it through HiSat. Didn\u2019t work and came back with error messages, realised that I probably should have put it through FastQC/Trimmomatic first.\nTried however, it didn\u2019t accept or recognise any fastq files, couldn\u2019t understand why til I looked at the data files. It seems that I had \u201cfastq.gzReadsPerGene.out.tab.gz\u201d instead of normal fastq files. Apparently from what i\u2019ve been told, I\u2019ve got from GEO are mixed messages. The first part \u201cfastq.gz\u201d would represent raw NGS reads, which should either be in fastqsanger format or can be converted to fastqsanger.\nIf FastQC doesn\u2019t run and won\u2019t accept the files then it is unlikely they are in a fastq format. The second part  \u201cReadsPerGene.out.tab.gz\u201d suggests these are aligned sequences with gene counts.\nSo i\u2019ve now been trying to determine whether these files are in fastq format or not. Tried to convert the files using FastQ Groomer, didn\u2019t accept the files. FastQC didn\u2019t accept them so I\u2019m assuming they\u2019re not in fastq format. Tried running through HiSat again and I just encounter the same errors as before. I\u2019ve looked almost everywhere to see if I can fix this problem and I\u2019ve emailed again to ask what to do as I\u2019m relatively new to using this tool. Does anyone know how I resolve this issue?\nThis was all done on my uni\u2019s galaxy server."
    },
    {
      "role": "system",
      "text": "The filename suggest it is something else then a fastq file to me. How did you got the files? Would it be possible for you to open the files on your local computer? So download them first, unzip (gunzip) them and just open it with a text editor?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was using QIIME 2 on the Galaxy server and I got an error message saying;\n[Errno 28] No space left on device. I tried deleting histories but it didn\u2019t work. How can I fix this? I\u2019ve used only 3.5GB of storage.\nThank you in Advance.\nBrigitta"
    },
    {
      "role": "system",
      "text": "Heya, thanks for the report, and sorry it was cumbersome to file it.\nRe: your main issue: We\u2019ve adjusted resource allocations to ease the pressure on the file system and memory when running that particular tool. I was able to replicate the error, and confirm that the changes made resolved it, so you should be good to go! It\u2019s possible, however, that other, more complex/large datasets might exceed this new allocation, so I\u2019ll be keeping an eye on future executions more closely to see if further adjustments are needed.\nRe: your issue issuing issues: we are working on making it easier to report errors directly to us instead of driving you to go through intermediaries (thanks @system and @system !); those changes should be live soon.\nThanks again for the report, very valuable and actionable info!"
    }
  ],
  [
    {
      "role": "user",
      "text": "SPADes does not seem to be running at all. No error, just highlighted grey to show that the job is queued.\nPlease advice on what to do, or enlighten us on whether this is normal.\nThanks."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe SPAdes job is queued and will run. The inputs look appropriate. Our clusters are all very busy \u2013 so please give it some time to complete.\nThanks for clearing up the other account. :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "in processing some RAT RNA-seq data I recieved the following error:\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nEXITING because of FATAL ERROR: could not open genome file /cvmfs/data.galaxyproject.org/managed/rnastar/2.7.4a/rn6/rn6/dataset_f3c6453e-d0d8-4981-acbc-d8ed812ad69e_files//genomeParameters.txt\nSOLUTION: check that the path to genome files, specified in --genomeDir is correct and the files are present, and have user read permsissions\nSep 11 20:53:44 \u2026 FATAL ERROR, exiting\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038405.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038405.dat\u201d : No such file or directory\n[E::hts_open_format] Failed to open file \u201c/jetstream/scratch0/main/jobs/30550923/outputs/dataset_45038406.dat\u201d : No such file or directory\nIs this a problem with the GTF file and if so can someone point me towards the correctly formatted file type?"
    },
    {
      "role": "system",
      "text": "Update:\n\n\nrn6 is now indexed for RNA-Star\nmore other genomes are also indexed\nas new genomes are completed, these will become available in the tool form\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, I have uploaded my WGS fq.gz files yesterday but they are still waiting to run\u2026\nAnyone else with similar problems or with any suggestion? Thanks for any help."
    },
    {
      "role": "system",
      "text": "I tested upload (Upload menu > paste/fetch and drag-and-drop into upload window)  on Galaxy Europe with small  text files. Waiting time was under one minute, and uploads have been completed within one minute. The files were in Australia. I don\u2019t see any issue with upload in Europe.\nHow do you upload the data? Do you use specialized tools, such as download and extract reads? Many servers limit number of concurrent jobs, but I don\u2019t know settings for Galaxy Europe.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have seen this issue indicated several times here but I couldn\u2019t find any clear solution.  Hope anyone can help me.\nI\u2019m following the ITS DADA2 Pipeline Workflow to analyze my data: 36 paired fastqs files obtained using Illumina MiSeq. The input samples were fungal ITS amplicons. I followed the tutorial first in Galaxy and then in Rstudio and I run into the same problem. When assigning taxonomy this happens:\n#Assign Taxonomy\npath \u2190 file.path(\u201cUNITE_public_10.05.2021.fasta.gz\u201d, \u201cUNITE_public_10.05.2021.fasta\u201d)\ntaxa \u2190 assignTaxonomy(seqtab.nochim, path, multithread = TRUE, tryRC = TRUE,\ntaxLevels = c(\u201cKingdom\u201d, \u201cPhylum\u201d, \u201cClass\u201d, \u201cOrder\u201d, \u201cFamily\u201d, \u201cGenus\u201d, \u201cSpecies\u201d),\nverbose = FALSE)\nError in C_assign_taxonomy2(seqs, rc(seqs), refs, ref.to.genus, tax.mat.int,  :\nMemory allocation failed.\nIn addition: Warning message:\nIn .Call2(\u201cfasta_index\u201d, filexp_list, nrec, skip, seek.first.rec,  :\nreading FASTA file UNITE_public_10.05.2021.fasta.gz/UNITE_public_10.05.2021.fasta: ignored 8 invalid one-letter sequence codes\nI tried to increase the memory limit and this is what I have\n\nmemory.size()\n[1] 11921.24\nmemory.limit()\n[1] 31975\n\nThese are the characteristics of my laptop:\nProcessor\tAMD Ryzen 7 PRO 4750U with Radeon Graphics   1.70 GHz\nRAM \t32.0 GB (31.2 GB utilizable)\nSystem\tSistema operativo de 64 bits, procesador x64\nIf necessary this is the step before the taxonomy assignment:\n\n#Track reads through the pipeline\ngetN \u2190 function(x) sum(getUniques(x))\ntrack \u2190 cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\ncolnames(track) \u2190 c(\u201cinput\u201d, \u201cfiltered\u201d, \u201cdenoisedF\u201d, \u201cdenoisedR\u201d, \u201cmerged\u201d, \u201cnonchim\u201d)\nrownames(track) \u2190 sample.names\n\n\nhead(track)\ninput filtered denoisedF\n1-01A_S63_L001_R1_001.fastq 39949    39663     37575\n1-01B_S64_L001_R1_001.fastq 51059    50727     47766\n1-01C_S65_L001_R1_001.fastq 71229    70897     68033\n1-04A_S78_L001_R1_001.fastq 48126    47701     44850\n1-04B_S79_L001_R1_001.fastq 78663    78350     75736\n1-04C_S80_L001_R1_001.fastq 67420    67102     64241\ndenoisedR merged nonchim\n1-01A_S63_L001_R1_001.fastq     34082   3900    3831\n1-01B_S64_L001_R1_001.fastq     41556   4241    4139\n1-01C_S65_L001_R1_001.fastq     54961  16499   16157\n1-04A_S78_L001_R1_001.fastq     41881  13130   12954\n1-04B_S79_L001_R1_001.fastq     69691  11161   11070\n1-04C_S80_L001_R1_001.fastq     59966  25060   24773\n\nThank you very much in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for sending in the report. The reference fasta has title lines (\">\" lines) that Dada2 doesn\u2019t know how to interpret. Help for the expected format is near the bottom of the tool form in Galaxy.\nTry this:\n:one: Reformat the fasta title lines with the tool Text transformation with sed using the following two lines of script for the \u201cSed Program\u201d. Note that using Sed is just an example \u2013 you can use any tool/method you want for this, as long as the end result is the same.\ns/(^>)(.+\\|)(.+)(\\|.+)/\\1\\3/\ns/[a-z]{1}__//g\n\nThat will convert fasta title lines that originally are formatted like this:\n\n>UDB016649|k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Thelephorales;f__Thelephoraceae;g__Thelephora;s__Thelephora_albomarginata|SH1502188.08FU\n\nTo this format:\n\n>Fungi;Basidiomycota;Agaricomycetes;Thelephorales;Thelephoraceae;Thelephora;Thelephora_albomarginata\n\n:two: Rerun using that reformatted reference fasta input. Be sure to enter the taxonomy levels \u2013 it looks like you included that content in your Rscript but it wasn\u2019t included with the job sent in for the bug report.\nThe option is right under where the reference fasta is input on the tool form, labeled as \u201cNames of the taxonomic levels in the data set\u201d [comma separated list (taxLevels)].\n\nKingdom,Phylum,Class,Order,Family,Genus,Species\n\n:slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "after installing galaxy locally using git when i execute  the script run.sh it\u2019s hanged after showing the server address localhost:8080 when i try to call server address localhost:8080 through browser it\u2019s showing a blank page. I have galaxy 18.0 in the same system its working fine.please help"
    },
    {
      "role": "system",
      "text": "@quote\n127.0.0.1 - - [26/Mar/2019:14:29:43 +0600] \u201cGET / HTTP/1.1\u201d 200 - \u201c-\u201d \u201cMozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0\u201d\nOk, one more attempt. Is Firefox 31 really what you are using? If so, that is really quite outdated. Have you verified that things aren\u2019t working with a different browser?\nEverything else you\u2019ve pasted looks rather innocuous to me."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am having an issue running STAR on some paired end data. I am running galaxy through AWS and cloudman. My data is one set of paired end illumina fastq data. around 2.3 MB each. I also have uploaded the GR38 .gtf file from ensemble. WHen I try to run star with the options\npaired-end (as individual datasets,)\nuse a built-in index\nuse genome reference without builtin gene-model\nhg38 as the reference genome\nGR38.gtf (that I uploaded) as the gene model\n49 as the length of genomic sequence around annotated junctions (read length was 50 from my QC)\nThe error I get is\n\u201cEXITING: fatal error trying to allocate genome arrays, exception thrown: std::bad_alloc\nPossible cause 1: not enough RAM. Check if you have enough RAM 32002674832 bytes\nPossible cause 2: not enough virtual memory allowed with ulimit. SOLUTION: run ulimit -v 32002674832\u201d\nI was wondering how I could go about fixing this issue? Since I am on AWS using a large cluster I don\u2019t see how I could not have enough RAM.\nThanks"
    },
    {
      "role": "system",
      "text": "Thanks for the extra info.\nYes, the Gencode GTF looks to be not the problem.\nRNA-STAR is a compute-intensive tool. 16 GB is not sufficient for human mapping. The memory needed to run the tool line-command is the same as when running it within Galaxy. See: @link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4631051/\nQuote from that publication:\n\nNecessary Resources\nHardware\n\nA computer with Unix, Linux or Mac OS X operating systems.\nRAM requirements: at least 10 x GenomeSize bytes. For instance, human genome of ~3 GigaBases will require ~30 GigaBytes of RAM. 32GB is recommended for human genome alignments.\n\n\nSufficient free disk space (>100 GigaBytes) for storing output files.\n\n\nAlternative ways to run your own Galaxy server, including Cloud choices. Since you are already using Cloudman, you probably just need to bump up the VM choice to one with more resources: @link https://galaxyproject.org/use/.\nPlease also see this recent Galaxy Blog post. This is an alternative for using a local Galaxy with on-demand cloud resources incorporated: @link https://galaxyproject.org/blog/ > Enabling cloud bursting for Galaxy (@link https://galaxyproject.org/blog/2019-05-gcr)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy users,\nI am trying to analyse some 16S rRNA data following the Galaxy tutorial. When i run the unique.seqs call i get the following message:\nAn error occured while running the tool toolshed.g2.bx.psu.edu/repos/iuc/mothur_unique_seqs/mothur_unique_seqs/1.39.5.0.\nTool execution generated the following messages:\nFatal error: Exit code 137 ()\n/home/nick/galaxy/database/jobs_directory/002/2067/tool_script.sh: line 25: 31810 Done                    echo \u2018unique.seqs( fasta=fasta.dat, format=name )\u2019\n31811                       | sed \u2018s/ //g\u2019\n31812 Killed                  | mothur\n31813                       | tee mothur.out.log\nDo you know why this keeps happening? The previous call was the screen.seqs() and was successful.\nKind regards\nNick"
    },
    {
      "role": "system",
      "text": "Ok, that does help to confirm that memory is the root problem.\nPlease see the Galaxy administrator help for how to increase memory allocation for tools. This tuning is usually most appropriate for those that are running jobs on a cluster, which might not be your case.\nBy default, when running Galaxy locally, job memory is limited by whatever hardware you have natively available on your computer. 16 GB is considered the minimum, but many tools and certainly large datasets can need more.\nYou might want to consider moving to a cloud version of Galaxy if you plan to process large data. See:\n\n@link https://galaxyproject.org/choices/\n@link https://galaxyproject.org/use/\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to use a genome build for sheep (OviAri4 UCSC gold fasta) for mapping single end RNAseq data with bowtie2. No matter what I try, downstream use of the mapped BAM output with featurecounts fails give give any counts. I have tried all the options/advice described under the help section, but to no avail.\nAny possibility that the sheep genome (and if possible the goat genome) could be added as built-ins for bowtie2?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe genomes are now available.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody.\nI\u2019m developing a tool that allows to choose between different options for a query.\nNow my code is inside a python file where for each type of query I have a different function.\nFollowing this guide (@link https://training.galaxyproject.org/training-material/topics/dev/tutorials/tool-integration/slides.html#22) I created the XML file and everything is running correctly without errors when I start Galaxy.\nThe problem is that when I run the tool (with every option) I get this error:\nJob submission failed.\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\nIn the terminal I get this:\ngalaxy.tools ERROR 2022-02-28 11:43:36,932 [pN:main.web.1,p:13472,w:1,m:0,tN:uWSGIWorker1Core0] Exception caught while attempting to execute tool with id 'impc_test':\nTraceback (most recent call last):\n  File \"lib/galaxy/util/template.py\", line 80, in fill_template\n    return unicodify(t, log_exception=False)\n  File \"lib/galaxy/util/__init__.py\", line 1060, in unicodify\n    value = str(value)\n  File \"/Users/andrea/Desktop/galaxy/.venv/lib/python3.6/site-packages/Cheetah/Template.py\", line 1053, in __unicode__\n    return getattr(self, mainMethName)()\n  File \"cheetah_DynamicallyCompiledCheetahTemplate_1646045016_93172_79883.py\", line 91, in respond\nEOFError: EOF when reading a line\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/tools/__init__.py\", line 1770, in handle_single_execution\n    flush_job=flush_job,\n  File \"lib/galaxy/tools/__init__.py\", line 1858, in execute\n    return self.tool_action.execute(self, trans, incoming=incoming, set_output_hid=set_output_hid, history=history, **kwargs)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 529, in execute\n    handle_output(name, output)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 455, in handle_output\n    data.name = self.get_output_name(output, data, tool, on_text, trans, incoming, history, wrapped_params.params, job_params)\n  File \"lib/galaxy/tools/actions/__init__.py\", line 784, in get_output_name\n    return fill_template(output.label, context=params, python_template_version=tool.python_template_version)\n  File \"lib/galaxy/util/template.py\", line 124, in fill_template\n    python_template_version=python_template_version,\n  File \"lib/galaxy/util/template.py\", line 126, in fill_template\n    raise first_exception or e\n  File \"lib/galaxy/util/template.py\", line 80, in fill_template\n    return unicodify(t, log_exception=False)\n  File \"lib/galaxy/util/__init__.py\", line 1060, in unicodify\n    value = str(value)\n  File \"/Users/andrea/Desktop/galaxy/.venv/lib/python3.6/site-packages/Cheetah/Template.py\", line 1053, in __unicode__\n    return getattr(self, mainMethName)()\n  File \"DynamicallyCompiledCheetahTemplate.py\", line 89, in respond\nEOFError: EOF when reading a line\ngalaxy.tools.execute WARNING 2022-02-28 11:43:36,932 [pN:main.web.1,p:13472,w:1,m:0,tN:uWSGIWorker1Core0] There was a failure executing a job for tool [impc_test] - Error executing tool with id 'impc_test': EOF when reading a line\n\nHere\u2019s the .py code that I\u2019m using as test and its XML file:\ntest.py (here I don\u2019t have functions since I\u2019m making tests on the input which is the part that, in my opinion, is raising the error)\nimport sys\nimport argparse\nimport requests\nimport pandas as pd\nimport numpy as np\n\ndef main():\n    \n    # define options\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"-k\", \"--option\", type=str, help=\"Option selected\")\n\n    # parse\n    args = parser.parse_args()\n    option = str(args.option)\n    \n    if option == 'q1':\n        print(\"Hello 1! Input: \" + sys.argv[1])\n        textfile = open(sys.argv[2], \"w+\")\n        textfile.write( \"Here's the input of first query: \" + sys.argv[1] + \"\\n\")\n        textfile.close()\n    elif option == 'q2':\n        print(\"Hello 2! Input: \" + sys.argv[1])\n        textfile = open(sys.argv[2], \"w+\")\n        textfile.write( \"Here's the input of second query: \" + sys.argv[1] + \"\\n\")\n        textfile.close()\n    else:\n        sys.exit(\"Error!\")    \n    \nif __name__ == \"__main__\":\n    main()        \n\ntest.xml\n<tool id=\"impc_test\" name=\"Test \" version=\"0.1.0\">\n\t<description>for impc query selection</description>\n    <requirements>\n      <requirement type=\"package\">requests</requirement>\n      <requirement type=\"package\">pandas</requirement>\n      <requirement type=\"package\">numpy</requirement>\n    </requirements>\n    <command>\n            python3 \"{$__tool_directory__/test.py}\" '$query_type.input' '$output' #set $option = str($selector) --option $option\n\t</command>\n\t<inputs>\n\t\t\t<conditional name=\"query_type\">\n\t\t\t\t<param name=\"selector\" type=\"select\" labels=\"Select a query:\">\n\t\t\t\t\t<option value=\"q1\">First</option>\n\t\t\t\t\t<option value=\"q2\">Second</option>\n\t\t\t\t</param>\n\t\t\t\t<when value=\"q1\">\n\t\t\t\t\t<param name=\"input\" type=\"text\" label=\"Input 1:\"/>\n\t\t\t\t</when>\n\t\t\t\t<when value=\"q2\">\n\t\t\t\t\t<param name=\"input\" type=\"text\" label=\"Input 2:\"/>\n\t\t\t\t</when>\n\t\t\t</conditional>\n\t</inputs>\n    <outputs>\n            <data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n    </outputs>\n\t<help>\n\t\tJust a test for IMPC queries.\n\t</help>\n</tool>\n\nEssentially, I\u2019m trying to set \u2018option\u2019 on different values so inside the code I can use it to call different functions.\nThank you all for the help."
    },
    {
      "role": "system",
      "text": "You are also missing ${input}:\n<data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input}\"/>\n\nYou use it in your output but to what is it pointing? Try to just use some text first and check if it works. Something like:\n<data format=\"tabular\" name=\"output\" label=\"testing output\"/>\n\nAnd on top of my head, once you do have added $input also try this:\n<data format=\"tabular\" name=\"output\" label=\"${tool.name} on ${input.display_name}\"/>"
    }
  ],
  [
    {
      "role": "user",
      "text": "HI,\nWhen I try to run galaxy local, I get the message \u201cconda installation requested and failed\u201d.\nError: did not recognise option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\u2019, please try -h\nI have gone into the galaxy.yml file and amended it as so:\nconda_prefix: <tool_dependency_dir>/_conda\ntool_dependency_dir: dependencies\nconda_auto_init: true\nI cannot understand what the issue is. Despite this, it allows me to open galaxy on the web and download tools but the tools do not work.\nany help would be much appreciated!!"
    },
    {
      "role": "system",
      "text": "Ah! I think you have problems because of the space in your path.\nLook at the error:\ndid not recognize option \u2018Book/Galaxy/galaxy/database/dependencies/_conda\nAnd look at the path that it should be:\n/Volumes/My Book/Galaxy/galaxy/database/dependencies/_conda\nTry to change /Volumes/My Book to something like  /Volumes/My_Book. Besides your problem now you should not have spaces in folder or file names. Good luck and let us know if this solved it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI updated galaxy from 20.09 to 21.05 and tried to invoke my workflow. But i got a message like this:\nAn error occurred\nAn error occurred while updating information with the server. Please contact a Galaxy administrator if the problem persists.\n\ufffcOk \ufffcDetails\nThe following information can assist the developers in finding the source of the error:\n\n{\n  \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"21.05\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"{\\\"err_msg\\\": \\\"Uncaught exception in exposed API method:\\\", \\\"err_code\\\": 0}\",\n    \"responseJSON\": {\n      \"err_msg\": \"Uncaught exception in exposed API method:\",\n      \"err_code\": 0\n    },\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"1970-01-01T00:00:00.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n.....\n.......\n......\n\nand was logged out of the server, and some tasks were not scheduled. When i tried to save the workflow in the workflow editor, i got similar error and also was logged out.\nGalaxy.log says something like this:\ngalaxy.jobs.runners.local DEBUG 2021-10-04 15:23:37,549 [pN:main.web.1,p:514441,w:1,m:0,tN:LocalRunner.work_thread-8] (149) executing job script: /media/transgeneprep/DataStore/galaxy/database/jobs_directory/000/149/galaxy_149.sh\ngalaxy.web_stack.handlers INFO 2021-10-04 15:23:38,179 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] (Job[id=153,tool_id=star_fusion]) Handler '_default_' assigned using 'HANDLER_ASSIGNMENT_METHODS.DB_SKIP_LOCKED' assignment method\ngalaxy.tools.execute DEBUG 2021-10-04 15:23:38,432 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Executed 1 job(s) for tool star_fusion request (1138.948 ms)\ngalaxy.workflow.run DEBUG 2021-10-04 15:23:39,276 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Workflow step 106 of invocation 5 invoked (7473.255 ms)\ngalaxy.web.framework.decorators ERROR 2021-10-04 15:23:51,970 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core3] Uncaught exception in exposed API method:\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n  File \"lib/galaxy/web/framework/decorators.py\", line 184, in decorator\n    rval = func(self, trans, *args, **kwargs)\n  File \"lib/galaxy/webapps/galaxy/api/datasets.py\", line 157, in show\n    return self.hda_serializer.serialize_to_view(dataset,\n  File \"lib/galaxy/managers/base.py\", line 690, in serialize_to_view\n    return self.serialize(item, all_keys, **context)\n  File \"lib/galaxy/managers/hdas.py\", line 436, in serialize\n    return super().serialize(hda, keys, user=user, **context)\n  File \"lib/galaxy/managers/datasets.py\", line 606, in serialize\n    serialized = super().serialize(dataset_assoc, keys, **context)\n  File \"lib/galaxy/managers/base.py\", line 603, in serialize\n    returned[key] = self.serializers[key](item, key, **context)\n  File \"lib/galaxy/managers/hdas.py\", line 403, in <lambda>\n    'resubmitted': lambda i, k, **c: self.hda_manager.has_been_resubmitted(i),\n  File \"lib/galaxy/managers/hdas.py\", line 174, in has_been_resubmitted\n    return self.app.model.context.query(query.exists()).scalar()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3523, in scalar\n    ret = self.one()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3490, in one\n    ret = self.one_or_none()\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3459, in one_or_none\n    ret = list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3535, in __iter__\n    return self._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT EXISTS (SELECT 1 \nFROM job_to_output_dataset, job_state_history \nWHERE job_to_output_dataset.dataset_id = ? AND job_state_history.job_id = job_to_output_dataset.job_id AND job_state_history.state = ?) AS anon_1]\n[parameters: (182, <states.RESUBMITTED: 'resubmitted'>)]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.workflow.run ERROR 2021-10-04 15:23:52,919 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Failed to schedule Workflow[id=3,name=main.pipeline.PE.updated.24.08.2021 (imported from uploaded file)], problem occurred on WorkflowStep[index=21,type=tool].\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/workflow/run.py\", line 193, in invoke\n    workflow_invocation.steps.append(workflow_invocation_step)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/attributes.py\", line 294, in __get__\n    return self.impl.get(instance_state(instance), dict_)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/attributes.py\", line 730, in get\n    value = self.callable_(state, passive)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/strategies.py\", line 759, in _load_for_state\n    return self._emit_lazyload(\n  File \"<string>\", line 1, in <lambda>\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/strategies.py\", line 900, in _emit_lazyload\n    q(session)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/ext/baked.py\", line 544, in all\n    return list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/ext/baked.py\", line 444, in __iter__\n    return q._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT (SELECT workflow_invocation_to_subworkflow_invocation_association.subworkflow_invocation_id \nFROM workflow_invocation_to_subworkflow_invocation_association \nWHERE workflow_invocation_to_subworkflow_invocation_association.workflow_invocation_id = workflow_invocation_step.workflow_invocation_id AND workflow_invocation_to_subworkflow_invocation_association.workflow_step_id = workflow_invocation_step.workflow_step_id) AS anon_1, workflow_invocation_step.id AS workflow_invocation_step_id, workflow_invocation_step.create_time AS workflow_invocation_step_create_time, workflow_invocation_step.update_time AS workflow_invocation_step_update_time, workflow_invocation_step.workflow_invocation_id AS workflow_invocation_step_workflow_invocation_id, workflow_invocation_step.workflow_step_id AS workflow_invocation_step_workflow_step_id, workflow_invocation_step.state AS workflow_invocation_step_state, workflow_invocation_step.job_id AS workflow_invocation_step_job_id, workflow_invocation_step.implicit_collection_jobs_id AS workflow_invocation_step_implicit_collection_jobs_id, workflow_invocation_step.action AS workflow_invocation_step_action \nFROM workflow_invocation_step \nWHERE ? = workflow_invocation_step.workflow_invocation_id]\n[parameters: (5,)]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.workflow.run ERROR 2021-10-04 15:23:54,931 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Failed to execute scheduled workflow.\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/workflow/run.py\", line 82, in __invoke\n    outputs = invoker.invoke()\n......\n.....\n.......\n\nor this:\ngalaxy.jobs.handler ERROR 2021-10-04 15:41:35,174 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerStopQueue.monitor_thread] Exception in monitor_step\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/handler.py\", line 927, in monitor\n    self.monitor_step()\n  File \"lib/galaxy/jobs/handler.py\", line 958, in monitor_step\n    newly_deleted_jobs = self.sa_session.query(model.Job).enable_eagerloads(False) \\\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3373, in all\n    return list(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3535, in __iter__\n    return self._execute_and_instances(context)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py\", line 3560, in _execute_and_instances\n    result = conn.execute(querycontext.statement, self._params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1011, in execute\n    return meth(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 298, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1124, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1316, in _execute_context\n    self._handle_dbapi_exception(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1510, in _handle_dbapi_exception\n    util.raise_(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 182, in raise_\n    raise exception\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n[SQL: SELECT job.id AS job_id, job.create_time AS job_create_time, job.update_time AS job_update_time, job.history_id AS job_history_id, job.library_folder_id AS job_library_folder_id, job.tool_id AS job_tool_id, job.tool_version AS job_tool_version, job.galaxy_version AS job_galaxy_version, job.dynamic_tool_id AS job_dynamic_tool_id, job.state AS job_state, job.info AS job_info, job.copied_from_job_id AS job_copied_from_job_id, job.command_line AS job_command_line, job.dependencies AS job_dependencies, job.job_messages AS job_job_messages, job.param_filename AS job_param_filename, job.runner_name AS job_runner_name_1, job.job_stdout AS job_job_stdout, job.job_stderr AS job_job_stderr, job.tool_stdout AS job_tool_stdout, job.tool_stderr AS job_tool_stderr, job.exit_code AS job_exit_code, job.traceback AS job_traceback, job.session_id AS job_session_id, job.user_id AS job_user_id, job.job_runner_name AS job_job_runner_name, job.job_runner_external_id AS job_job_runner_external_id, job.destination_id AS job_destination_id, job.destination_params AS job_destination_params, job.object_store_id AS job_object_store_id, job.imported AS job_imported, job.params AS job_params, job.handler AS job_handler \nFROM job \nWHERE job.state IN (?, ?, ?) AND job.handler = ?]\n[parameters: (<states.DELETED_NEW: 'deleted_new'>, <states.DELETING: 'deleting'>, <states.STOPPING: 'stop'>, 'main.web.1')]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\ngalaxy.jobs.handler DEBUG 2021-10-04 15:41:35,268 [pN:main.web.1,p:514441,w:1,m:0,tN:WorkflowRequestMonitor.monitor_thread] Grabbing WorkflowInvocation failed (serialization failures are ok): (sqlite3.OperationalError) database is locked\n[SQL: UPDATE workflow_invocation SET update_time=?, handler=? WHERE workflow_invocation.id IN (SELECT workflow_invocation.id \nFROM workflow_invocation \nWHERE workflow_invocation.handler IN (?, NULL) AND workflow_invocation.state = ? ORDER BY workflow_invocation.id)]\n[parameters: ('2021-10-04 12:41:30.263975', 'main.web.1', '_default_', 'new')]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:45,073 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Exception during reset or similar\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:45,073 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Exception closing connection <sqlite3.Connection object at 0x7f6c64150300>\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n    self._dialect.do_close(connection)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n    dbapi_connection.close()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102957258496 and this is thread id 140102974043904.\ngalaxy.webapps.galaxy.controllers.user DEBUG 2021-10-04 15:41:45,433 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core2] trans.app.config.auth_config_file: /media/transgeneprep/DataStore/galaxy/config/auth_conf.xml\ngalaxy.auth.providers.localdb DEBUG 2021-10-04 15:41:45,466 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core2] User: wormball@gmail.com, LOCALDB: True\nsqlalchemy.pool.impl.NullPool ERROR 2021-10-04 15:41:56,525 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] Exception during reset or similar\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n    dbapi_connection.rollback()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102974043904 and this is thread id 140102437172992.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n    self._dialect.do_close(connection)\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n    dbapi_connection.close()\nsqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140102974043904 and this is thread id 140102437172992.\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.tools.parameters.dynamic_options WARNING 2021-10-04 15:41:56,729 [pN:main.web.1,p:514441,w:1,m:0,tN:uWSGIWorker1Core1] could not filter by metadata: snpeff_db unknown\ngalaxy.jobs.handler ERROR 2021-10-04 15:42:08,088 [pN:main.web.1,p:514441,w:1,m:0,tN:JobHandlerStopQueue.monitor_thread] Exception in monitor_step\nTraceback (most recent call last):\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1276, in _execute_context\n    self.dialect.do_execute(\n  File \"/media/transgeneprep/DataStore/galaxy/.venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 608, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n.....\n.........\n........\n\nI tried several times but got about the same result (however number of scheduled steps was different every time). However scheduled steps are performed as charm (at least now that i corrected paths and invoked data managers). Similar behavior is also observed when i purge histories (but not when i delete history steps).\nIs there a way to overcome these errors?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "It is hard to debug. So in the basics a sqlite database can only be used by one process at the time. (Only one process can write to that database at the time and during writing it will be locked). There is a situation where it can be locked if the database file is located on a NFS-mounted or remote drive but if it worked before this is probably not the case. Here I read that it can also happen if you have unsaved actions Python SQLite: database is locked - Stack Overflow (@link https://stackoverflow.com/questions/2740806/python-sqlite-database-is-locked) but I would not know how to solve that. Maybe if you dont have anything important in that database you could create a new one."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am getting error with the STR DETECTION. This file cannot be used for further analysis.\nerror-1\n/corral4/main/jobs/040/701/40701606/galaxy_40701606.sh: line 123: galaxy-set-metadata: command not found"
    },
    {
      "role": "system",
      "text": "Hi @user\nIt looks like you are using a custom reference fasta for hg38 in some steps, and in other steps, you are assigning the built-in index for hg38.\nMixing up which reference genome is used in the same analysis almost never works out.\nTry using the same exact reference genome for all steps. If you really need to use a custom genome, promote it to a custom build (with a custom \u201cdatabase aka dbkey\u201d that isn\u2019t already in use), then assign that custom database to your data.\nPlease note that using a very large custom genome fasta will cause some tools to run out of memory, including the \u201ccustom build\u201d functionality. \u201cVery large\u201d includes the human genome, most other mammalian genomes, some plant genomes. For these cases, using the built-in index is a better choice.\nI\u2019ve added a tag to your post that links to other Q&A that describes how to format a custom genome plus how to create a custom build from one. You should start with the fasta formatting first, then try to create a build from it (will likely fail but you can try). Or, you can go directly to using the built-in indexed version of that same genome instead."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI just finished deep learning tutorials and still some points are not clarified for me. for example : How do i should determine the number of neurons, batch size and epochs?\nRegards\nAmir"
    },
    {
      "role": "system",
      "text": "Hi Amir,\nThanks for using the deep learning tutorials. I agree these \u201cjobs\u201d shown in grey did not start because they require GPUs to process and none of them is free at the moment. Once, at least one or more become free, these jobs would start. We are currently working on finding an ideal job scheduling scheme for such tools to maximize the usage of CPUs and GPUs that we currently have."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi guys! I had a Galaxy installed in a local server, which I had used for 2 years. However, the TI guy updated the Linux, as well as change the folders, because he made an HDs upgrade too. After that, I can\u2019t open anymore the old Galaxy version, but I have the folder with all datasets and a lot of data analyzed. We tried to re-install the old version, but we can\u2019t, cause an incompatibility of systems versions as well as the Python version. We tried unsuccessfully a lot of times and in different ways.\nHow can I recover these data in an installed new version of Galaxy?\nthanks for any help.\nLeandro"
    },
    {
      "role": "system",
      "text": "Make sure you backup your database!\nYou can try setting up a fresh copy of the database, and a fresh copy of Galaxy. It will detect the database, and then if everything goes well, it starts migrating up to the latest version of the database schema. Generally database migrations just work, but always backup first in case!\nThat might work. You\u2019ll need to configure your galaxy with the same tools and same path to data as the old galaxy was configured, and that can take some time.\nUsually we recommend going release by release as @system mentions, but sometimes that isn\u2019t an option."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have about 5000 variants on one chromosome that I would like to phase in a few families. I have the data from NGS in a tab delimited format. Can this be done with Galaxy tools?\nthanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\na PR has been opened in order to include this tool: Pull Request #3760 (@link https://github.com/galaxyproject/tools-iuc/pull/3760).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to assemble a bacterial genome sequenced with long reads (miNION), with Minimap2/Miniasm. The assembly works fine (it produces 1 contig; the same as with Flye and Raven+Racon) but then I need to polish the assembly, and Minipolish is not in the Galaxy toolshed.\nDoes anyone have any suggestions to overcome the absence of Minipolish? I would like to feed the three assemblies to Trycycler/Medaka to get a more robust asembly, but it seems that polishing after Miniasm assembly is essential\u2026"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas I mentioned to you in the email, the problem is that, in order to generate the overlap file, the reads should be mapped against the reference genome.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to transfer a history from @link http://usegalaxy.org to @link http://usegalaxy.eu, however, the import job fails with a python error. The history has been exported to a URL and published on the .org side.\nHistory:\nGalaxy | Published History | Unnamed history (@link https://usegalaxy.org/u/mdondrup/h/unnamed-history) (~6GB)\nEdit: Further testing with a very small history (70 bytes) yields the same error.\nFurther testing to import the same exported history from @link http://usegalaxy.org into usegalaxy.no yields the same error. Further testing exporting a history from @link http://usegalaxy.eu into usegalaxy.no results in the same error.\nI conclude that this has to be a general problem with these Galaxy instances, or the function has been disabled, in which case it should be removed from the UI, or the documentation is misleading and the functionality is not to be used this way.\nTraceback (most recent call last):\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 92, in <module>\n    main(options, args)\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 73, in main\n    check_archive(archive_file, dest_dir)\n  File \"/opt/galaxy/server/lib/galaxy/tools/imp_exp/unpack_tar_gz_archive.py\", line 38, in check_archive\n    with tarfile.open(archive_file, mode=\"r\") as archive_fp:\n  File \"/usr/lib64/python3.8/tarfile.py\", line 1608, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n"
    },
    {
      "role": "system",
      "text": "This is working fine for me, can you share the export link ?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI want to specify an input parameter that allows selection of one and only one output from the history. I read the XML specifications, but still don\u2019t know how to fulfill this goal.\nThanks for reading."
    },
    {
      "role": "system",
      "text": "We need to distinguish between different layers here. On the tool level - what a tool developer can control in the XML file - you have done a good job. You made sure that only one file can be processed at a time. its not possible to put multiple files into one job. Now you want to avoid that a single user can submit many jobs, aka run your tool on 1000 individual datasets, producing 1000 of different jobs, right? This is to be controlled on the Galaxy Admin level. So the Galaxy admin, that worries about computational load can set this. You can say a user is only allowed to submit 3 concurrent jobs etc \u2026 or you can say this tool get a lower priority on your cluster. Which means a user can still submit 1000 of jobs but they will be processed with a lower priority. There are multiple of these setting to keep the load not to high on our cluster. The important thing here is, that these are different level: 1) tool level - defining an interface, and 2) Admin level keeping the cluster load reasonable.\nPlease let me know if this is unclear, I know my explanations can be bad :frowning:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI want to convert SRA files from GEO into fastq files in order to map them on the genome. I have uploaded SRA files directly on Galaxy but it does not seem to be the right thing to do.\nNow, I\u2019m going through the instructions on the support page of Galaxy: NCBI SRA Fastq (@link https://galaxyproject.org/support/ncbi-sra-fastq/#:~:text=Prior%20to%20importing,to%20each%20condition)\nIt asks to manipulate the file before importing to Galaxy but I cannot see mentioned tools on the NCBI SRA Run Selector page (highlighted above). Is there a more detailed explanation or tutorial to follow because the link I shared seems to be the thing I need but it is not very clear to me how to proceed.\nThank you."
    },
    {
      "role": "user",
      "text": "Dear all, @system @system\nThank you for your responses. I somehow did not see these messages. Sorry for the late reply.\nI figured out that, in Galaxy, it\u2019s easier to merge multiple files into a data collection, than splitting a collection into multiple files.\nSo, I uploaded multiple Acc_List.txt in groups depending on the condition. Each contained only three (replicates) SRA Run accessions for each sample."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear support team at GALAXY,\nI have recently performed a ChIP-seq experiment and I have just got my raw seq files from the seq service. I would like to perform my analysis but each of my samples was sequenced twice; meaning that each sample has two seq files. This is because each sample was sequenced twice to get a workable number of total reads (because of very low concentration of my ChIPed DNA). Thus, I am looking for a tool in GALAXY that will allow me to merge the two seq files of the same samples in to just one, with whom I will then proceed to the mapping with Bowtie2, the Peak calling with MACS2 etc. Could you please propose me a tool that I can properly perform such a task of merging?\nMany thanks in advance,\nGreetings,\nManolis"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis tool tends to work best with uncompressed inputs.\nPlease try uncompressing the inputs, then run the tool. Galaxy does some intermediate steps to transform the data during processing (in this case: fastq > plain text > fastq). When compressed fastq is input, sometimes a compression version isn\u2019t exactly what is expected at a technical level. Uncompressing first will usually avoid problems with any text manipulation tool.\nTo uncompress a fastqsanger.gz dataset:\n\nClick on the pencil icon to reach the Edit Attributes functions\nClick into the Convert tab\nThe drop-down menu will include a choice to produce an uncompressed version of the data (these will be new datasets)\nRun the Concatinate tool on the uncompressed fastqsanger datasets\nAfter everything is completely done and confirmed to be correct, go back and permanently delete (purge) the original compressed data to remove it from your quota usage.\n\nI tested this out last week for another person (same exact tool/problem) and it worked, but please let us know if that doesn\u2019t work for your case and we can troubleshoot more. :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello all,\nI\u2019ve been struggling here for days now on attempting to get multiple factors to work in Galaxy for either EdgeR OR limma. I\u2019ve searched every post about this and each says it\u2019s a header issue. It is NOT a header issue. If I have only one factor and input the 5 separate groups within that one factor, each with it\u2019s own count file it works fine and I can compare between each via contrasts. The count files have headers as GeneID for column1 and the unique sample name in column 2. However, when I attempt to add a separate Factor and include use the same files that worked in the previous step\u2026I always get a duplicate row name error. I\u2019ve tried even doing this via a count matrix file with all of the samples and counts\u2026as well as a Factor info file; for both I followed the EXACT format as described on the package help section displayed if you scroll down. This did not work either.\nCurrently, I have absolutely no way to utilize the platform to peform DE analysis with the appropriate factor levels. Again, this is not a header issue with the counts files. I\u2019ve tried this with both htseq counts (which I manually entered the header) AND featurecounts. These work fine if I only have one Factor level\u2026as soon as I add an additional factor level it fails, despite being the exact same files that worked previously. I REALLY need help. I\u2019d be greatly appreciative as my advisor needs this analysis for a grant application."
    },
    {
      "role": "system",
      "text": "I just glanced over it as it is late here, but your files in both factors needs to be the same. Please consider the example of treated vs. un-treated and single vs. paired in this tutorial as example: @link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#analysis-of-the-differential-gene-expression for a 2 factor study.\nI hope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nPlease I have a question regarding ChIP-Seq data analysis using Galaxy platform! During the trimming step (using Trim Galore) for ChIP-Seq data, I got error message:\nFatal error: Exit code 1 ()\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\n\n\nAUTO-DETECTING ADAPTER TYPE\n===========================\nAttempting to auto-detect adapter type from the first 1 million sequences of the first file (>> input_1.fastq.gz <<)\n\nFound perfect matches for the following adapter sequences:\nAdapter type\tCount\tSequence\tSequences analysed\tPercentage\nIllumina\t207\tAGATCGGAAGAGC\t1000000\t0.02\nsmallRNA\t3\tTGGAATTCTCGG\t1000000\t0.00\nNextera\t2\tCTGTCTCTTATA\t1000000\t0.00\nUsing Illumina adapter for trimming (count: 207). Second best hit was smallRNA (count: 3)\n\n\ngzip: stdout: Broken pipe\nWriting report to './input_1.fastq.gz_trimming_report.txt'\n\nSUMMARISING RUN PARAMETERS\n==========================\nInput filename: input_1.fastq.gz\nTrimming mode: single-end\nTrim Galore version: 0.4.3\nCutadapt version: 1.13\nQuality Phred score cutoff: 15\nQuality encoding type selected: ASCII+33\nAdapter sequence: 'AGATCGGAAGAGC' (Illumina TruSeq, Sanger iPCR; auto-detected)\nMaximum trimming error rate: 0.1 (default)\nMinimum required adapter overlap (stringency): 3 bp\nMinimum required sequence length before a sequence gets removed: 20 bp\nOutput file(s) will be GZIP compressed\n\nWriting final adapter and quality trimmed output to input_1_trimmed.fq.gz\n\n\n  >>> Now performing quality (cutoff 15) and adapter trimming in a single pass for the adapter sequence: 'AGATCGGAAGAGC' from file input_1.fastq.gz <<< \n10000000 sequences processed\n20000000 sequences processed\n30000000 sequences processed\n40000000 sequences processed\n50000000 sequences processed\n60000000 sequences processed\n70000000 sequences processed\nThis is cutadapt 1.13 with Python 3.6.1\nCommand line parameters: -f fastq -e 0.1 -q 15 -O 3 -a AGATCGGAAGAGC input_1.fastq.gz\nTrimming 1 adapter with at most 10.0% errors in single-end mode ...\ncutadapt: error: Line 1 in FASTQ file is expected to start with '@', but found '\\n'\n\n\nCutadapt terminated with exit signal: '256'.\nTerminating Trim Galore run, please check error message(s) to get an idea what went wrong...\n\n\ngzip: stdout: Broken pipe\n\nplease if you can help me solve it!\nThank you very much!\nSham"
    },
    {
      "role": "system",
      "text": "HI @user \u2013 The problematic formatting may be internal, not at the start. The error is a bit misleading.\n\nLine 1 in FASTQ file\n\nCould be interpreted as:\n\nLine 1 in FASTQ record\n\nThis can be due to:\n\n\nTruncated dataset \u2013 usually occurs because Upload was not complete, or the file was truncated from an earlier data transfer upstream from Galaxy.\n\n\nManipulated dataset \u2013 most often seen when multiple fastq datasets were concatenated together, and one or more contained an empty blank line at the end which ends up somewhere in the middle of the final dataset. This could happen in Galaxy or upstream from Galaxy.\n\n\nTools that can help find the problem:\n\n\nFastq Groomer \u2013 this tool has enhanced data checks.\n\nUse all default settings.\nData that is already in fastqsanger format (and assigned that datatype) will remain unchanged in the new output if there are no problems and could be permanently deleted once the QA is done/passes to reduce quota usage/duplicate data.\nIf there are some formatting problems, the first malformed fastq record will be reported in the error message. Be aware that there could be more problems, but this tool can give some clues about where/what an example problem is, so you can find all occurrences like it.\n\n\n\nSelect last lines of a dataset (tail) \u2013 this is how to review the ends of files to see if they are truncated or contain empty blank trailing lines.\n\nThe last 10 lines (the default) should be sufficient.\n\n\n\nAdvanced methods \u2013 to examine (any) data in more depth, try using tools in these groups:\n\nText Manipulation\nFilter and Sort\nJoin, Subtract and Group\n\n\n\nReference: FAQ for Fastq formatting\n\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nSee >> @link https://galaxyproject.org/learn/datatypes/#fastq\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am tryng to run a very small tblastn, with only three query sequences.  Both a conventional FASTA file and a BlastDB for the subject are sitting \u201cThis Job is Waiting to Run\u201d\nIs there a problem with the server?"
    },
    {
      "role": "system",
      "text": "Hi all,\nThe @link http://usegalaxy.org server is very very busy (and has been for about a week).\nThe best advice is to leave the queued jobs queued and to avoid rerunning, otherwise, the new jobs will end up back at the end of the queue again.\nI\u2019ve also checked in with our administrator @system, and there is a bit of a backlog. This should clear up \u2013 how long to completely clear up depends on when you originally started the jobs and how many jobs you have queued.\nThe above backlog mostly applies to Trinity, Unicycler, and SPAdes but not BLAST (that last tool runs on a different cluster, also busy, but not as busy.\nFAQ: How Jobs Execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nHope that helps & thank you @system for helping first!!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am trying to install galaxy in my own compurter and I got this error when I run common \u201csh run.sh\u201d.\n\u201cFile \u201c/anaconda3/envs/galaxy/lib/python3.6/site-packages/filelock/init.py\u201d, line 8\nfrom future import annotations\n^\nSyntaxError: future feature annotations is not defined\u201d\nI understand the error means that I need a newer version of python, however, I do have a python version 3.9, but it still runs in the python version 3.6. So I am not sure where went wrong.\nPlease help, thank you."
    },
    {
      "role": "system",
      "text": "@quote\nSyntaxError: future feature annotations is not defined\u201d\nHi @user,\nthat problem has been recently fixed, but is not included in the 21.09 release. If you get the dev branch, it should work fine:\ngit clone https://github.com/galaxyproject/galaxy.git\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi I\u2019m studying on a plant that has  no reference genome  and only has  one scaffold assembly  and  one gff3  annotation file. If I want to do  de novo assembly  using the  Galaxy  Can I import the links of 20 fastq files from the RNA-seq of the samples together to construct the Reference Genome with  Trinity ? If this is possible, please help me import the links of these files together into Galaxy and make the reference genome. Maximum number of samples or data can be used for Deno Assembly in Galaxy?\nThanks a lot"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nIn short, yes you can assemble within Galaxy. Trinity is designed to assemble RNA-seq reads into a Transcriptome Assembly (not Genome). Genome assembly from WGS reads works best with smaller genomes (procaryotic) when working at public Galaxy servers due to resources (Unicyler is one tool choice for that purpose).\nThe reads need to be prepared and input properly. The upper limit of what can be assembled is usually based more on the read content and genome size/characteristics, rather than the volume/size of the read data. You won\u2019t know until you try, to see how your data will assemble at any particular server (your own or a public site). The help below covers QA, input format/structures, and advice about downsampling as needed.\nAlso, keep in mind that some plant genomes can be quite complicated to assemble for biological reasons, and can require more resources (especially memory). Some will require employing advanced assembly methods. Much discussion about plant assembly and strategies can be found online with a few searches. Issues around large genomes in general, and sub-features like high repetitive content (example: Lettuce), high ploidy content (example: Strawberry), large chromosomes (example: Wheat) can all be factors to consider. But try the simple approach described below first, certainly can\u2019t hurt.\nRequirements/Best practices for Reference Transcriptome assembly using RNA-seq reads:\n\n\nReads must be in fastqsanger format (Sanger Phred+33 quality scores, what results from Illumina 1.8+ sequencing) with that datatype assigned. If the data is in compressed fastqsanger.gz format, uncompressed versions of the inputs will be created as hidden datasets. This can unexpectedly increase quota usage. Sometimes uploading uncompressed fastq can be a better choice \u2013 it depends on the tools you plan to use \u2013 some work with compressed fastq and some do not (but Galaxy will \u201cmanage\u201d that for you). Just something to be aware of.\n\n\nLoad reads by URL or with FTP. If \u201cautodetect\u201d for datatype assignment is used, fastqsanger will be assigned automatically if the reads are actually in that format. If you get a different autodetected datatype, there is an input problem that needs to be addressed first (examples: fastqillumia== needs quality score rescaling OR just fastq == probably a format issue).\n\n\nNOTE: If you decide to upload and directly assign fastqsanger.gz to preserve compression, you should definitely run the QA steps to verify that datatype is assigned correctly, or expect problems (assembly jobs failing due to unexpected quality score scaling \u2013 will look like any other memory failure, and require that you have to back up to the start, run QA, and fix the reads if that is the actual issue). If the data is actually uncompressed and assigned that datatype, any tool can also fail, and most but not all will report what the problem is. FastQC will guess the quality score scaling. That can be compared with the specific datatypes. See the Support FAQs to learn how to interpret the report and how to convert to fastqsanger if needed (tool: \"Fastq Groomer`).\n\n\nPunch line: It is more satisfying/less frustrating to get the inputs correct at the start, so you don\u2019t need to hunt for basic issues later on, possibly after much other work is already done. No one likes to \u201cstart over\u201d.\n\n\n\n\nRun some QA on your reads. FastQC followed by MultiQC (to summarize the raw FastQC reports). Running FastQC on the individual original datasets, in pairs (R1+R2) will be more informative. This can be run in batch or by using Dataset Collection(s), if wanted.\n\n\nNext, run Trimmomatic if adaptor or low-quality ends are reported by FastQC. You may want to run this anyway to get your reads paired up and catch content issues not found by FastQC (only checks a subset of the data, first 200k reads or so, and can be biased). There will be four outputs per paired-end input. Two will be the reads that are still paired after QA. This is important \u2013 Trinity requires paired-end fastq inputs to be in intact pairs & tends to run better (not fail for resource reasons) with cleaned-up reads.\n\n\nUse Concatenate to merge all R1 (forward) reads into one dataset and all R2 (reverse) reads into another. two distinct runs.\n\n\nIf the Trinity run fails for exceeding resources, then subsampling your reads can help. You may decide to do this per-pair or after concatenating (after QA is done) \u2013 your choice. See the Seqtk tools: seqtk_sample or seqtk_seq are good choices. The first is a specific function, the latter more comprehensive with that function included. In most cases, you\u2019ll need to run the tool twice (once to output R1 forward reads, once to output R2 reverse reads). Be conservative first, to preserve as much of the original data as possible, then move to more restrictive as needed.\n\n\nIf the data will not assemble at all then one of these is going on: a) some input problem is present, b) not enough QA was done, c) the data exceeds processing needs that a public Galaxy server, or your own, has allocated. Public resources are fixed for most tools, including Trinity. A cloud Galaxy may be more appropriate.\n\n\nBe aware that if you combined the data into a Dataset Collection (R1 forward collection + R2 reverse collection, or in an R1+R2 paired-end collection) at the start, you\u2019ll need to use Collection Operations tools to manipulate the read data to fit the tools (example: Collapse Collection will perform the same manipulation as Concatenate). See the Dataset Collection \u201cCollection Operations\u201d tutorials linked below to understand how to work with collections. Help is also on the tool forms. With so many inputs, Collections will be very useful, but also a bit trickier to use if you haven\u2019t tried them yet. That said, these are definitely worth learning how to use, now or later. Often a mix of paired-end and single-end collections, used at different steps, is needed to work through an analysis.\nTutorials & Support FAQs & Related:\n\nGTN tutorials: @link https://training.galaxyproject.org/ (all)\n\nStart with those in the groups \u201cAssembly\u201d, \u201cTranscriptomics\u201d, and \u201cData Manipulation\u201d (collection manipulations are covered).\n\n\nSupport FAQs: @link https://galaxyproject.org/support/#troubleshooting\n\nStart with the first few in \u201cUnexpected Results\u201d and \u201cGetting Inputs Right\u201d to understand format, datatypes, error messages (and what to do about each!), etc. See \u201cLoading Data\u201d if you are not sure how to use FTP (usually only needed with slower internet connections, but is fast/simple for batch uploads too).\n\n\n\nFastQC is covered in many tutorials, but reading the tool\u2019s FAQs about specific statistics reported is informative, see: @link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ (Docs & Example reports)\n\nI also added some tags to your post. Clicking on any may provide more clues about what may be going wrong if you run into trouble.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Currently, i am trying to prepare the workflow for the Qiime2. But unfortunately, the file type for the Qiime2 (QZA,QZV and others) are not present in the list provided with the current version of the Galaxy.\nKindly help me how to add another File type as input for the same problem."
    },
    {
      "role": "system",
      "text": "Hi,\nqza is a binary format so you do not have to define a new datatype. The only thing you have to do is to include in the datatypes_conf.xml :\n<datatype extension=\"qza\" type=\"galaxy.datatypes.binary:CompressedZipArchive\" display_in_upload=\"true\"/>"
    }
  ],
  [
    {
      "role": "user",
      "text": "After upgrading to the latest version of FASTQC (0.11.7 to 0.11.8) the HTML files cannot be viewed in browser. Is there a fix available ?"
    },
    {
      "role": "system",
      "text": "Hi - You will need to set some admin permissions for viewing the HTML output. This is set by tool and version.\nIn the Galaxy UI, go to Admin > Tool Management > Manage whitelist and check off the updated FastQC tool version. I can\u2019t remember if this requires a restart or not, so if it doesn\u2019t seem to work after, restart and all should work.\nUpdate: Does NOT require a server restart"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI have a sequnce logo of an miRNA, that gives me no output? All the steps are done correctly in Galaxy sequnce logo tool. What can be evaluated on such an output?\nNo conservation?\nThanks!"
    },
    {
      "role": "system",
      "text": "Thanks for the extra info.\nThe Sequence Logo tool requires a fasta input that is in an alignment format, not just a regular fasta file. ClustalW can be used to generate the proper fasta input \u2013 use the output option \u201cOutput alignment format\u201d as \u201cfasta\u201d.\nPlease be aware that ClustalW does have some input limitations on the public server, so do not submit too many fasta sequences at once or that tool will fail. How many exactly depends somewhat on the length of the input fasta sequences, so you\u2019ll need to test what works for your data if > 100 sequences or so.\nI\u2019ll also go in and run a test with some sample data to make sure there is not more going on and will write back with that result. I am also going to test MAFFT (potential alternative tool for ClustalW step) \u2013 cannot recall if the aligned fasta produced by that tool works with Sequence Logo or not.\nPlease try ClustalW in the meantime and see if that works for you."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone.\nA few months ago I created for a colleague of mine a Galaxy instance using Docker on a virtual machine on our cloud.\nAfter she was done with her analyses and I explained her how to download her results, I deleted the docker container and the virtual machine. I saved the mapping directory \u201cjust in case\u201d.\nShe recently asked me if I could restart the Galaxy instance since she had a problem when saving her datasets.\nNow I have troubles starting a new container using the same mapping directory.\nI had to go inside the Docker container and manually launch the startup script to get more information.\nI had the following error:\n\nException: Your database has version \u2018134\u2019 but this code expects version \u2018141\u2019. Please backup your database and then migrate the database schema by running \u2018sh manage_db.sh upgrade\u2019.\n\nSo I backuped my database, upgraded postgresql, reimported the backuped database and launched the startup script again.\nNow I can access the Galaxy instance but I cannot log to the admin account (which was used by my colleague to do her analyses. She didn\u2019t bother creating a new account). I have no error message but the \u201cadmin\u201d tag doesn\u2019t appear and when I click on \u201cuser\u201d it says \u201cconnected as\u201d with nothing following.\nI even tried to import the database backup into a \u201cvirgin\u201d Docker container created from the same Galaxy Docker image but there were no datasets in this new instance.\nI know the datasets are there. There are a lot of files in mapping/galaxy-central/database/files/000/ but named dataset_XXX.dat. I don\u2019t know where to find the information about the real datasets names, the histories names\u2026\nIs there a way to get back the datasets with their metadata?\nThank you for your help."
    },
    {
      "role": "user",
      "text": "I don\u2019t know Paste or uwsgi. I didn\u2019t change anything to Galaxy\u2019s inner architecture or config.\nMy instance is based on Docker image bgruening/galaxy-stable:17.05.\nBut I just found a way to solve my problems. Here is what I did:\n\nCreate a Docker container based on the same Docker image I used previously\nand mapping to the same mapping directory I used previously and which contains\nmy datasets.\n\n\nsudo docker run -d -p XXXX:80 -v /my/old/mapping/dir/:/export/ --name galaxy myGalaxyImage\nsudo docker exec -it galaxy /bin/bash\n\n\nBack-up Postgresql database (inside Docker container).\n\n\nsu - galaxy\npg_dump -U galaxy -W -F t galaxy > dump_galaxy.tar\nexit    # exit from galaxy user\ncp /home/galaxy/dump_galaxy.tar /export/galaxy-central/\n\n\nCreate a new Docker container still based on the same Docker image\nbut mapping to a new, empty mapping directory.\n\n\nsudo docker run -d -p XXXX:80 -v /my/new/mapping/dir/:/export/ --name galaxy_clean myGalaxyImage\n\n\nCopy back-uped database from the \u201cold\u201d mapping directory to the \u201cnew\u201d mapping directory\n\n\nsudo cp /my/old/mapping/dir/galaxy-central/dump_galaxy.tar /my/new/mapping/dir/galaxy-central/\n\n\nConnect to \u201cnew\u201d Docker container\n\n\nsudo docker exec -it galaxy_clean /bin/bash\n\n\nRestore the back-uped Postgresql database (inside \u201cnew\u201d Docker container).\n\n\ncp /export/galaxy-central/dump_galaxy.tar /home/galaxy\nsu - galaxy\npg_restore -d galaxy dump_galaxy.tar -c -U galaxy\nexit    # exit from galaxy user\nexit    # exit from Docker container\n\n\nCopy datasets files from \u201cold\u201d container to \u201cnew\u201d one\n(which are stored in /export/galaxy-central/database/)\n\n\nsudo tar czf /my/old/mapping/dir/galaxy-central/database.tar.gz /my/old/mapping/dir/galaxy-central/database\nsudo cp /my/old/mapping/dir/galaxy-central/database.tar.gz /my/new/mapping/dir/galaxy-central/database.tar.gz\nsudo mv /my/new/mapping/dir/galaxy-central/database /my/new/mapping/dir/galaxy-central/database_bak\nsudo tar xzf /my/new/mapping/dir/galaxy-central/database.tar.gz\n\n\nRestart \u201cnew\u201d Docker container\n\n\nsudo docker restart galaxy_clean\n\nI don\u2019t know if what I did is really clean but at least it allowed me to get back my datasets simply from the mapping directory.\nMaybe this can help other people as well."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,  I\u2019m trying to get TPM data from FastQ file(mouse, illumina, pair-end, 150bp, 20-30M), I tried Stringtie, Salmon, Sailfish, Killisto quant, and I did get the results, but I have a couple of questions about the data:\n\n\nStringtie data showed one gene with multiple TPM values, my understanding is these different reads mapped to different sites of the same gene(there is information about the ch site), should I just plus all the TPM together as the final TPM for each gene?\n|Rb1cc1| cov 143.169418| FPKM 8.402312| TPM 17.528645|\n|Rb1cc1| cov 0.007920| FPKM 0.000465| TPM 0.000970|\n|Rb1cc1| cov 0.353770| FPKM 0.020762| TPM 0.043313|\n|Rb1cc1| cov 0.006247| FPKM 0.000367| TPM 0.000765|\n|Rb1cc1| cov 0.608617| FPKM 0.035718| TPM 0.074515|\n|Rb1cc1| cov 1.899041| FPKM 0.111451| TPM 0.232505|\n|Rb1cc1| cov 0.131482| FPKM 0.007716| TPM 0.016098|\n|Rb1cc1| cov 0.025351| FPKM 0.001488| TPM 0.003104|\n|Rb1cc1| cov 0.081178| FPKM 0.004764| TPM 0.009939|\n|Rb1cc1| cov 0.661685| FPKM 0.038833| TPM 0.081012|\n\n\nthe data from Salmon, Sailfish, and Killisto quant are more confusing, the target ids are like what showed as following, which one I should use to count TPM? or it\u2019s the issue of the reference genome I use(Grcm38)?\nENSMUST00000130201.7|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072660.1|Mrpl15-203|Mrpl15|1894|protein_coding|\nENSMUST00000156816.6|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072659.1|Mrpl15-206|Mrpl15|4203|protein_coding|\nENSMUST00000045689.13|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072661.1|Mrpl15-201|Mrpl15|497|nonsense_mediated_decay|\nENSMUST00000115538.4|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072664.1|Mrpl15-202|Mrpl15|910|processed_transcript|\nENSMUST00000192286.1|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000127355.1|Mrpl15-207|Mrpl15|4600|retained_intron|\nENSMUST00000146665.2|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072662.2|Mrpl15-205|Mrpl15|1569|protein_coding|\nENSMUST00000132625.1|ENSMUSG00000033845.13|OTTMUSG00000029329.3|OTTMUST00000072663.1|Mrpl15-204|Mrpl15|654|retained_intron|\n\n\nhonestly, it\u2019s the first time I work with RNAseq data, so for all the programs, I always use the default setting, based on the sequencing parameter (mouse, illumina, pair-end, 150bp, 20-30M), does anyone can give me some advice about those parameters?\n\n\nthank you so much, everyone!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin the case of Salmon, if you provide an annotation file, it generates two outputs, a quantification file (which provides information about the transcripts) and a gene quantification file (which provides information at gene level). Probably the last file is the one you are interested in.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "hello, I followed the tutorials: Production (@link https://docs.galaxyproject.org/en/latest/admin/production.html), Scaling and Load Balancing (@link https://docs.galaxyproject.org/en/latest/admin/scaling.html) and Proxy (@link https://docs.galaxyproject.org/en/latest/admin/nginx.html) to create a Production Galaxy Instance.\nI am trying to bind everything to the Gunicorn socket while using a Proxy with Nginx. But I do not know if I am making a mistake in the configuration files. I tried to Google the issue and consulted the Gunicorn official documentation.\nWhat is the path to the Gunicorn supposed to be? I searched in: /etc/, /run/, /var/, /tmp/ and so on. My Galaxy instance is on a different \u201cgalaxy\u201d user. Binding by IP on port 8080 works very well, but I am trying to bind by socket to access the server by a subdomain, like the tutorial \u201c@link http://galaxy.example.org\u201d while also securing with lets encrypt.\nThank you!"
    },
    {
      "role": "system",
      "text": "Ah! Yes, you just set a path, gunicorn creates the socket on startup. This will need to match the path you specify in the nginx config, but gunicorn is what will create it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am trying to upload files to Galaxy for analysis. However, the following error appears: Failed: Internal Server Error (500)"
    },
    {
      "role": "system",
      "text": "Update 3/15/2022\nThe Biobakery Lab that hosts this Galaxy server wrote back that the Upload functionality at the server is now functional. Please try now. Thanks!\n\nHi all,\nIt sounds like most of you are working at this public Galaxy server: http://huttenhower.sph.harvard.edu/galaxy/\nA different version of Lefse is available as an alternative at: @link http://UseGalaxy.eu, @link http://UseGalaxy.org, and @link http://UseGalaxy.org.au.\nIf you need to work at the Huttenhower public Galaxy server, please know that server\u2019s administrators do not track this forum. We can sometimes help with tool errors and related issues, but it does seem that the Upload tool is non-functional from my testing, and we cannot correct it or give any status here.\n\n2022-03-10-huttenhower.sph.harvard.edu-upload-failure2510\u00d71018 276 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2a204a31b97081c089351b8b988a3f1e8f09491e.png)\nThe administrators of that server will need to be contacted directly about the problems with the Upload tool at that server. Contact details are below, and if anyone gets an update, please feel free to post it back to this thread. I didn\u2019t find any notifications on their Galaxy server or at their forum.\nGeneral info:\n@quote\nThe tool you are using is specific to thehttp://huttenhower.sph.harvard.edu/galaxy/domain-specific public Galaxy server.The tools and protocols hosted there differ significantly from theLefsetools hosted at most other public Galaxy servers (including usegalaxy.* servers).The administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).Contact:Huttenhower LabHelp/tutorials: (also linked from the server\u2019s home page, with example data available):lefse \u2013 The Huttenhower Lablefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHubPrior Q&A and other related resources including other versions of this tool and different tools that may be helpful when working with this type of data:Lefse Cladogram prior Q&A at this forum:Search results for \u2018Cladogram\u2019 - Galaxy Community HelpAll Galaxy resources:Searching the Galaxy"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nCould you recommend any alternatives to \u201cGemini load\u201d to work with dog genome (canFam3), since Galaxy \u201cGEMINI load\u201d supports only human genome.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Ah I see. Well, for this it sounds as if GEMINI and a database would have been overkill anyway.\nInstead, things like that are not too hard to do with the original VCF file:\n\nYou can use SnpEff to annotate the variants in your VCF with the genes and transcripts affected by them\nuse the SnpSift Filter tool to filter for variants that affect your gene of interest, etc.\ncould have a look at the MiModD Report Variants tool if you find VCF output tedious to parse\n\nBeyond that, there are lots of other tools to manipulate VCF data. On @link https://usegalaxy.org you can find them combined nicely in the NGS: VCF Manipulation section of the tools, on @link https://usegalaxy.eu they are part of the section Variant Calling."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m doing gene enrichment analysis using goseq. The two input files for this were generated by following the instructions from 3: RNA-seq genes to pathways (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-genes-to-pathways/tutorial.html#frequently-asked-questions).\nI keep getting the following error and I\u2019m not sure what is the matter:\n\n{ \u201ccode_desc\u201d: \u201cAn undefined error occured, please check your input carefully and contact your administrator.\u201d, \u201cdesc\u201d: \u201cFatal error: An undefined error occured, please check your input carefully and contact your administrator.\u201d, \u201cerror_level\u201d: 3, \u201cmatch\u201d: \u201cError in\u201d, \u201cstream\u201d: \u201cstderr\u201d, \u201ctype\u201d: \u201cregex\u201d }\n\nCould it be that I picked the wrong GeneID format? I picked Entrez ID, as the tutorial suggests, but Ensembl ID was the default option\u2026\nAssistance would be appreciated :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas I indicated to you in the email, you need to remove the version suffixes (so, you should use ENSG00000168209 instead of ENSG00000168209.6) and the headers.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "When I display Galaxy   @link https://usegalaxy.eu/ when processing MS mzml files there is a blank area on right side of tools sidebar that cuts off that side. I can send a snapshoot of the display blank area.\nDoes the problem lay with Galaxy or Firefox?  And how do I fix it. I cannot select what I cannot see.\nkleps@uic.edu"
    },
    {
      "role": "user",
      "text": "Chrome does not have this problem.\nFF*^ with incognito has no effect."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I\u2019m new to Galaxy and I noticed that the Cut tool seems to be broken relative to its usual functionality in unix as well as what is described in the galaxy 101 tutorial. Namely, I can keep and discard columns, but they cannot be rearranged.\nFor example, if I try to cut (keep) columns 1, 2, 10, and 3 in that order, they are automatically sorted back to 1, 2, 3, and 10.\nIs there another tool for rearranging? Thanks!"
    },
    {
      "role": "system",
      "text": "@quote\nCut tool seems to be broken relative to its usual functionality in unix\nI think you\u2019re misremembering that, the output of cut -f1,2 is the same as cut -f2,1, cut does not reorder columns."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nToday, the link to UCSC genome browser in the visualization window (bigwig file) has disappeared.\nHow can I visualise my file again in this browser that I prefer above IGB or IGV?\nI tried:\n\n\u201cBuild custom track for UCSC genome browser (Galaxy Version 1.0.1)\u201d but this resulted in an error.\n\u201cbigWigToBedGraph Convert from bigWig to bedGraph format (Galaxy Version 0.1.0)\u201d but the visualisation window just addad the Ensembl link\n\nThanks for suggestions."
    },
    {
      "role": "system",
      "text": "Hi @system\nNot sure if this was resolved already or not \u2026 but all of the link outs are showing up for me in that same example history as before. This is the history import link if you want to check it out in more detail, or maybe the screenshot helps by itself. Galaxy | Europe (@link https://usegalaxy.eu/u/jenj/h/test-bam-upload-to-ucsc)\nucsc_linkout2806\u00d71182 339 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8281fdf2fece868eb90b78056addd065e0bd5c60.jpeg)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good Morning,\nI have been trying to get Trinity running for the last week  I have two fastq files that I have been trying to run.  One is a raw fastq (no trimming) about 17.5 GB (pre and post Fastq Groomer), one is a trimmed fastq about 15.8 GB (pre and post Fastq Groomer).   I am trying to run single strand. I have tried F, R, and Not Set for direction.   I have dumped all the data sets that were up previously to free up space. I have tried changing the data type from fastqsanger to fastq.  I currently have one post Fastq Groomer file up (15.86 GB) and it is still hanging. Nothing I have tried seems to work.\nRNA STAR worked fine, even on the raw data.  However, I am trying to do a de novo assembly, so Trinity looks like the only option available.  Any insights welcome.\nThanks"
    },
    {
      "role": "system",
      "text": "Thanks for clarifying.\nMany jobs are queued up for the particular cluster used to run assemblies (Trinity, Unicycler) and it is different from other jobs and their target clusters. All will process in the order originally submitted.\nAvoid deleting/rerunning or the current job will lose the queue placement \u2013 unless you already know there is a problem (inputs or assembly settings) that needs to be changed."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was wondering if anyone has more detailed instructions on installing a local instance of galaxy server (release 23.0) on an Ubuntu server LTS 22.04.\nI have tried the straightforward approach outlined in the Galaxy documentation here (@link https://galaxyproject.org/admin/get-galaxy/#for-production-or-single-user), but it has failed several times.  The folder downloads from github without issue, but when I use  sh run.sh the installation takes about 10 - 20 minutes before resulting in an error message.  I\u2019ve received:\n\nerror code exit 1,\nerror code exit 134, and\na repeating attempt to synchronize and wake up \u201ccelery-beat\u201d that never resolves.\n\n==> ../log/celery-beat.log <==\n[2023-06-21 09:49:02,380: DEBUG/MainProcess] beat: Synchronizing schedule\u2026\n[2023-06-21 09:49:02,387: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.\n\nPerhaps it\u2019s a dependency issue, and it would be very helpful to know which packages for Ubuntu should be pre-installed before attempting the Galaxy build.  For example, I know python is required, but what version? What other packages would be required for a successful build?\nIf someone has built a galaxy server on Ubuntu and wouldn\u2019t mind sharing their steps, I\u2019d be very thankful!\nForgive me if this is a naive question; I\u2019m very new to this.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hello,\nSorry, the link did work.  Didn\u2019t realize it would require a Box account. I set it so anyone with the URL should be able to download it. Annoying this forum doesn\u2019t allow pdf files to upload.  Anyway, try this link.\n\n\n@link https://sites.science.oregonstate.edu/~keistc/Servers_galaxy.pdf\n\n\n@link https://sites.science.oregonstate.edu/~keistc/Servers_galaxy.pdf\n@link https://sites.science.oregonstate.edu/~keistc/Servers_galaxy.pdf\n393.92 KB\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have my datasets from bulk RNA Sequencing. I tried to process my datasets, it\u2019s mouse, using the workflow from Galaxy tutorial. But after the trimming and going in to the alignment with RNA STAR, it gives me lots of error. Like:\nAug 17 10:25:31 \u2026 started STAR run\nAug 17 10:25:31 \u2026 loading genome\nAug 17 10:33:47 \u2026 started 1st pass mapping\nEXITING because of FATAL ERROR in reads input: quality string length is not equal to sequence length\n@A00643:482:HYYN7DSXY:1:1101:2\nand error on gtf file\nAug 17 11:17:05 \u2026 started STAR run\nAug 17 11:17:05 \u2026 loading genome\nAug 17 11:21:48 \u2026 processing annotations GTF\nFatal INPUT FILE error, no valid exon lines in the GTF file: /data/dnb06/galaxy_db/files/9/3/e/dataset_93e3f860-b7ec-4bec-8cb2-6d\nI\u2019m a noobs in this sequencing analysis stuff. How can I fix the error? I will highly appreciate your help :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user\nOk \u2013 that screenshot definitely helps.\nYour annotation GTF has Ensembl chromosome identifiers like 1. The mouse reference genome index you are mapping against in Galaxy (mm10) was sourced from UCSC, and has UCSC chromosome identifiers like chr1.\nQuote from @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n\n\nInput mismatch tips.\n\nDo the chromosome/sequence identifiers exactly match between all inputs?\n\u201cChr1\u201d and \u201cchr1\u201d and \u201c1\u201d do not mean the same thing to a tool.\n\n\n\n\n\nYou have two options. The first is much easier and strongly recommended unless you have a specific reason to use the annotation you already have.\n\n\nGet a GTF reference annotation file from a data provider that uses UCSC identifiers instead.\n\nThis includes the UCSC sources I posted in the prior reply.\nFor example, use this GTF instead @link https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/genes/mm10.ensGene.gtf.gz\n\n\n\n\nTechnically, you can also try to convert the chromosome identifiers using this tool \u2192 Replace column by values which are defined in a convert file.\n\nThis will involve multiple steps, and might not work well.\nInstructions are on the tool form \u2013 be sure to scroll all the way to the bottom to find where to source a mapping file.\nOther related methods are included in this FAQ. All involve complex manipulations without exact help, and none are recommended except in extreme use-cases. I posting this mostly for reference, and not recommending that you try it. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#mismatched-chromosome-identifiers-and-how-to-avoid-them\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "When using RNA STAR in the past, I\u2019ve been able to select from many built-in genomes as a reference. However, under the \u201cselect reference genome\u201d I only see \u201cNo options available.\u201d This occurs if I select with or without a built-in gene model. If anyone can give me any tips on how to find a way to select a reference genome (I only need mm10 and hg38) it would be much appreciated!"
    },
    {
      "role": "system",
      "text": "The issues with RNA-Star should be resolved now. Please try a rerun if you have not already.\n\n\n@link https://github.com/galaxyproject/usegalaxy-playbook/issues/306\n\n\n\n\n\n\n\n\nJetstream: Fails RNA-STAR 2.7.5b (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/306)\n\n\n\n        opened 12:31AM - 15 Sep 20 UTC\n\n\n          closed 11:02PM - 30 Sep 20 UTC\n\n\n\n\n          jennaj\n         (@link https://github.com/jennaj)\n\n\n\n\n\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.5b\nTest history. Includes both mouse and human genome targets. hg19 failed -- and it was thought to have...\n\n\nfunctionality on Main\n\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\n\nI\u2019m trying to run the LEfSe galaxy, and I made 2 different input files.\nAll the data are exactly same, except for taxonomy name format.\n\n\nimage.png1157\u00d7389 9.99 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/87bfeac0c838988ffd310acab402494b5404f1c7.png) .\n\nimage.png1395\u00d7412 20.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/5d3b42042aa72b36ab46fbf1224dd08eb7369e31.png)\nHowever, the results of LEfSe from 2 different input files are totally different.\n\nimage.png827\u00d7444 41.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/8390fb438a5adf67761241b0ffe0a0e795086a3f.png)\nand\n\nimage.png434\u00d7685 50.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/8f1ff6cb13d8163cef42bd8f858016c8c49ffdae.png)\nI don\u2019t know which make input files make totally different results. Please explain it.\n\nI have one more question about the LEfSe input file.\nWhen I see the input format from the LEfSe website, it is not perfectly matched with relative abundance files\nIt looks like the upper taxonomy category includes all the relative abundance in the lower taxonomy class.\nI want to make the same format input files, so is there anyone to teach me some command line to make LEfSe input file?\n"
    },
    {
      "role": "system",
      "text": "Confirmed issue with the original tool authors. Might be fixed already. See the way microorganisms are named affect the final results of lefse - #6 by jennaj (@link https://help.galaxyproject.org/t/the-way-microorganisms-are-named-affect-the-final-results-of-lefse/8337/6)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nAfter using RNA STAR on 6 rat RNA-seq samples only about 50-60% of reads were mapped uniquely with 30-40% of reads mapped to multiple loci. Is there a way to improve mapping quality or determine what sort of contamination there might be?\nIs there a possibility that the issue is from the gene annotation file? My first attempt at mapping was using a .gtf annotation file from ensembl but it didn\u2019t work. Instead the one I got from UCSC main worked. My hunch is that it was just a formatting issue.\nEnsembl source - @link https://useast.ensembl.org/Rattus_norvegicus/Info/Index\nUCSC source -  UCSC Genome Browser Downloads (@link https://hgdownload.soe.ucsc.edu/downloads.html#rat)\nHere is the RNA STAR log for one of the samples.\n                             Started job on |\tJan 29 08:14:51\n                         Started mapping on |\tJan 29 08:36:14\n                                Finished on |\tJan 29 12:26:14\n   Mapping speed, Million of reads per hour |\t6.18\n\n                      Number of input reads |\t23700301\n                  Average input read length |\t74\n                                UNIQUE READS:\n               Uniquely mapped reads number |\t13214262\n                    Uniquely mapped reads % |\t55.76%\n                      Average mapped length |\t73.63\n                   Number of splices: Total |\t2475399\n        Number of splices: Annotated (sjdb) |\t2413443\n                   Number of splices: GT/AG |\t2408845\n                   Number of splices: GC/AG |\t22608\n                   Number of splices: AT/AC |\t3324\n           Number of splices: Non-canonical |\t40622\n                  Mismatch rate per base, % |\t0.71%\n                     Deletion rate per base |\t0.02%\n                    Deletion average length |\t1.29\n                    Insertion rate per base |\t0.02%\n                   Insertion average length |\t1.43\n                         MULTI-MAPPING READS:\n    Number of reads mapped to multiple loci |\t9233338\n         % of reads mapped to multiple loci |\t38.96%\n    Number of reads mapped to too many loci |\t146819\n         % of reads mapped to too many loci |\t0.62%\n                              UNMAPPED READS:\n\nNumber of reads unmapped: too many mismatches |\t0\n% of reads unmapped: too many mismatches |\t0.00%\nNumber of reads unmapped: too short |\t1075213\n% of reads unmapped: too short |\t4.54%\nNumber of reads unmapped: other |\t30669\n% of reads unmapped: other |\t0.13%\nCHIMERIC READS:\nNumber of chimeric reads |\t0\n% of chimeric reads |\t0.00%"
    },
    {
      "role": "system",
      "text": "Dear @user,\nCheck with FASTQC if there is any adapter contamination still in your data. Furthermore, it might be worth to extract your multi-mapped reads and use featurecounts or blast to see if these come from rRNA (using a gff/gtf for featurecounts or a rRNA sequence file for blast). High ratio of muli-mapped reads in RNA-seq is often a sign of an incomplete depletion of rRNA.\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am new to using Galaxy and tried to follow the Galaxy 101 tutorial. Unfortuantely. I can\u2019t extract the workflow from history because although my history panel has the cog icon, when I click it, no such option shows up. This is contrary to what is shown in this tutorial: @link https://galaxyproject.org/tutorials/g101/#creating-and-editing-a-workflow\nPlease help me :slight_smile:\nBest,\n4symmetry"
    },
    {
      "role": "system",
      "text": "Hi @user and welcome,\nare you signed in when you\u2019re trying to do this? In general, workflow functionality on Galaxy servers is only available to registered users (cause the workflow has to be associated with you somehow).\nBest,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "How to use annotateMyIDs or UniProt ID mapping and retrieval (Galaxy Version 0.2) to covert the refSeq gene ID to gene Name use Galaxy?\nsuch as following  refSeq gene ID:\nNM_001005508.2\nNM_001005510.2\nNM_001005916.2\nNM_001007575.2\nNM_001008501.2\nNM_001008700.3"
    },
    {
      "role": "user",
      "text": "Still nobody reply my question, I have settled this problem by my self, I find the gene name of NM_001005508.2 and NM_001005508 are same, so I deleted \u201c.\u201d and the number following it,  than I can use galaxy tool \u201cannotateMyIDs\u201d to get the other information (including gene name) of the gene IDs."
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there a command(s) to analyze miseq illumina 16S community profile data for the presence of antibiotic resistance or antibiotic producing genes?\nWe have already completed the 16S Microbial Analysis with mothur (extended) (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html) tutorial, but we would like to identify the presence of antibiotic resistance or antibiotic producing genes. If there is a way to run a few more command lines to determine this, we would greatly appreciate the help.\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nFirst, someone with experience in metagenomics might provide a better answer. Second, I have a semantic issue: 16S data does not contain ARGs, hence can be used only for prediction of ARGs, while detection of \u2018ARG presence\u2019 requires whole metagenomic data. I might be wrong here, and people consider prediction or extrapolation as a presence. Maybe this approach provides a good approximation, but it seems there is no validation or confirmation of this in the paper.\nMetagenome cannot be assembled from 16S.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I ended up with this error message after using Maker in the Main Galaxy instance: \u201cPossible unintended interpolation of @2 in string \u2026\u201d. My attempts to autodetect the metadata to correct the problem did not work.\nI saw that others had this error with other tools (e.g. Trinity) within the last couple years. In one case, @system said the error meant insufficient memory was allotted to the job or that the inputs were wrong. Hoping this was the case, I checked the input and ran the job again, but got the same result.\nIn another case, Matthias Bernt said (@link https://github.com/bioconda/bioconda-recipes/issues/14466) it was a miscommunication between Galaxy, which uses the \u201c@\u201d symbol, and the perl interpreter that doesn\u2019t know what to do with it. He suggested adding a \" \\ \" in front of the \u201c@\u201d, although I don\u2019t know how to access the script to do this.\nAny other ideas on how to handle this type of error? I still can\u2019t tell whether it\u2019s possible to salvage my results, which were apparently successfully run aside from the metadata problem that couldn\u2019t be resolved. Or should I simply keep rerunning the job until it works?\nI got this error while doing the genome annotation tutorial (@link https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/annotation-with-maker/tutorial.html#first-maker-annotation-round), but I want to know how to avoid this issue when I\u2019m using my data.\nThanks!"
    },
    {
      "role": "user",
      "text": "It appears to be working now, although I\u2019m still checking the output.\nI still get the @2 interpolation error, so I\u2019m concerned that could cause problems downstream. But now the data sets are usable instead of empty. The difference seems to be that I didn\u2019t select a species for the Dfam database this time.\nTo future users battling this error, check the above links and the responses I got from @system, @system and @system here (@link https://github.com/galaxyproject/galaxy/issues/12133). Thanks, guys."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m working on adding a couple custom tools to our Galaxy install, and one problem I\u2019m running across is with the names of datasets.\nMore specifically, one tool we use requires all input files to be in a certain \u201cinput\u201d directory. So, first thing my custom tool XML does is copy (using cp) the datasets from the list a user chooses as input (see below) to this folder. However, the datasets all have names such as dataset_9964.dat, dataset_9967.dat, etc. and not their names in the history/original names such as bin1.fasta, bin2.fasta, etc.\nAs such, when running this tool, the output (see second image below) in tabular TSV format things that all my user_genomes which I submitted to the tool are in fact called dataset_9964, dataset_9963, etc. as it uses the input file names to determine row names in output.\nI\u2019m wondering if there\u2019s any way I can retrieve the original files\u2019 names upon job submission.\nOur input section for the tool XML is as follows:\n<inputs>\n    <param name=\"input_files\" type=\"data\" multiple=\"true\" format=\"fasta\" label=\"Input FASTA files\" />\n</inputs>\n\nInput list/collection of FASTA files:\ncollection-input228\u00d7630 15 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/af3bb245066b7555c2abeb24051e17a0512de96d.png)\nFirst column of output TSV, column name is \u201cuser_genome\u201d, and each row should represent a different input file (dataset_9963 = bin1.fasta, etc.):\noutput\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "The ParSNP tool has a similar requirement.\nSee @link https://github.com/brinkmanlab/galaxy-tools/blob/00868930fc05f48a702fe8357b58f004cf899238/parsnp/ParSNP.xml#L20-L27\nBascally, you don\u2019t need to copy the files, just create symlinks in a working folder.\nThe name of the dataset is accessible as $input_file.element_identifier"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am currently doing my project work on Exome sequencing and is using the following tutorial (Exome sequencing data analysis for diagnosing a genetic disease). I have an error in GEMINI load tool. It is showing that the VCF format is unavailable(SNPEff eff). Please look into the problem and help me with it. Thank you\n\nGalaxy - Google Chrome 13-05-2022 20_42_051920\u00d71020 132 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/df2fb2eff7e07cd44ec2a618e25e052c1b4855ab.png)\n\nGalaxy - Google Chrome 13-05-2022 20_41_251920\u00d71020 126 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/aa189bd231c5dd2801928e62acdf2088566df6bc.png)"
    },
    {
      "role": "system",
      "text": "Yes, the GEMINI load tool produces a GEMINI database, which is not intended to be viewed, but queried. Please follow the tutorial, which explains a few ways how you can do that.\nThese tutorials: Identification of somatic and germline variants from tumor and normal sample pairs (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/somatic-variants/tutorial.html) and Calling variants in diploid systems (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html) demonstrate more options.\nThink of the GEMINI database as a computer-friendly version of a VCF. You can\u2019t read it, but queries are very efficient. The GEMINI load tool is essentially the converter that takes VCF and produces the database format."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a problem when trying to get genomic DNA sequences from a list of genomic coordinates. The file contains regions aroung ChIP-seq peaks and has been obtained trough several Galaxy functions (Map with Bowtie, MACS2 callpeak, compute and cut). It is correctly formatted (as .interval, but I have also tried as .bed) with columns corresponding to chr start end name and its genome version is indicated: [format:interval database:mm9].\nWhen I apply the function \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d selecting mm9 from the locally cached genomes I get an empty file with this warning:\n*622 warnings, 1st is: Chromosome by name \u2018chr19\u2019 was not found for build \u2018mm9\u2019. *\nSkipped 622 invalid lines, 1st is #1, \u201cchr19\t3204403\t3204904\tSAM-to-BAM_on_data_7__converted_BAM_peak_1\u201d.\nI get the same when I try to redo the analysis selecting mm10 instead of mm9.\nNote that in @link https://usegalaxy.org/ there are two instances of \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d, one is version 3.0.3 (the same available in @link https://usegalaxy.eu/) but there is also a version 2.2.4 which works perfectly fine on the same .interval file above and it returns the correct fasta file with all the sequences.\nMoreover, in @link https://usegalaxy.org/ the version 3.0.3 of \u201cExtract Genomic DNA using coordinates from assembled/unassembled genomes\u201d does not even display mm9 among the available cached genomes (even If I do the entire pipeline including mapping).\nI need to perform this step on @link https://usegalaxy.eu/, can you help me with this?\nBest,\nMattia"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis should now be fixed on @link http://usegalaxy.eu.\nCheers,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there!\nI am working with WGBS data and I want to extract the DMRs in a particular mutant.\nI used bwameth to align the sequencing data to mm10 genome. After that, I sorted the alignment file and used methyldackel to see the methylation status. I used the metilene tool to extract DMRs but this doesn\u2019t work. I get an error message like this\n\" null device 1 [WARNING] Tue Aug 9, 16:7:8, 2022 Input files need to be SORTED, i.e., use \u201cbedtools sort -i file >file.sorted\u201d. I sorted the methyldackel output files using Sortbed and ran metilene again. Same error message. I read from the manual for Metilene, that the input file has to be in a certain format and the input file can be prepared using bedtools unionbedg. But this tool is not present in @link http://usegalaxy.eu or .org. Can anyone help me to prepare the input files for metilene?\nThanks!!\nAbishek"
    },
    {
      "role": "system",
      "text": "Thanks for asking! Yes I did, by removing the last two columns and first row (description) of the Methyldackel output. It then did the trick and Metilene took them with no problem! The weird thing is that I didn\u2019t have this problem at all a year ago when I used the exact same files and process (using it for teaching a course).\nCl\u00e9ment"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI\u2019m discovering the galaxy world and I try to integrate some tools to galaxy.\nI started with @link https://sites.google.com/view/recentrifuge a tool for taxonomy visualization but I failed to generate the output.\nIn fact, the planemo test failed to compare because the output of the test is empty.\nThe recentrifuge code don\u2019t explicit an output but I tried with the -o option (give another name to the output) without success.\nSo I don\u2019t know how to deal to solve the no output problem.\nAs a minimal example with only my input file, output files and test code :\n  <command detect_errors=\"exit_code\"><![CDATA[\n        mkdir '$__tool_directory__/output_dir' &&\n        rcf\n        #if $db.db_selector == \"cached\"\n          -n '$db.cached_db.fields.path'\n        #end if\n\n        #if $file_type.filetype == \"centrifuge\"\n          -f '$input_file'\n        #else if $file_type.filetype == \"lmat\"\n          -l '$input_file'\n        #else if $file_type.filetype == \"clark\"\n          -r '$input_file'\n        #else if $file_type.filetype == \"kraken\"\n          -k '$input_file'\n        #end if\n\n        #if '$output_name.modify_name' == \"true\"\n          -o '$__tool_directory__/output_dir/$output_name.file_name'\n        #else\n          -o \"$__tool_directory__/output_dir/rcf.output\"\n        #end if\n\n        -e '$output_type'\n\n    ]]></command>\n<inputs>\n<param name=\"input_file\" type=\"data\" format=\"tabular\" label=\"Output from taxonomy assignation\" help=\"Select taxonomy file tabular formated\"/>\n</inputs>\n  <outputs>\n      <!-- HTML report -->\n      <data name=\"html_report\" format=\"html\" from_work_dir=\"output_dir\"/>\n\n      <data name=\"data_csv\" format=\"csv\" from_work_dir=\"output_dir\">\n         <filter>output_type and \"CSV\" in output_type</filter>\n      </data>\n      <data name=\"data_tsv\" format=\"tsv\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"TSV\" in output_type</filter>\n      </data>\n      <data name=\"stat_tsv\" format=\"tsv\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"TSV\" in output_type</filter>\n      </data>\n      <data name=\"xls_report\" format=\"xlsx\" from_work_dir=\"output_dir\">\n        <filter>output_type and \"XLSX\" in output_type or output_type and \"DYNOMICS\" in output_type </filter>\n      </data>\n        <data name=\"stat_csv\" format=\"csv\" from_work_dir=\"output_dir\">\n          <filter>output_type and \"CSV\" in output_type or output_type and \"MULTICSV\" in output_type </filter>\n      </data>\n    </outputs>\n<tests>\n      <test> <!-- Test 1: Basic test with kraken file-->\n        <conditional name=\"db\">\n          <param name=\"db_selector\" value=\"cached\"/>\n          <param name=\"cached_db\" value=\"test-db-2022\"/>\n        </conditional>\n          <param name=\"filetype\" value=\"kraken\"/>\n          <param name=\"input_file\" value=\"kraken.txt\"/>\n          <param name=\"output_type\" value=\"CSV\"/>\n          <param name=\"scoring\" value=\"KRAKEN\"/>\n          <param name=\"summary\" value=\"avoid\"/>\n\n        <output name=\"data_csv\" file=\"output_dir/rcf.output.data.csv\">\n        </output>\n        <output name=\"stat_csv\" file=\"output_dir/rcf.output.stat.csv\">\n        </output>\n        <output name=\"html_report\" file=\"output_dir/rcf.output\">\n        </output>\n      </test>\n    </tests>\n\nThe planemo report give me the code seem to be ok :\nmkdir '/home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir' && rcf -n '/home/pierre/.planemo/planemo_tmp_a_v6y1k9/test-data/test-db'  -k '/tmp/tmpnivasmvl/files/2/3/3/dataset_2334d82b-ddc1-4dc8-bb41-df026f8c1a34.dat'  -o \"/home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output\"  -e 'CSV'\n\nThe standard ouput from planemo report is okay, the softaware indicate generating of files and give me the output directory\nGenerating final plot /home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output OK!\nGenerating csv extra output /home/pierre/.planemo/planemo_tmp_a_v6y1k9/output_dir/rcf.output.]*.csv OK!\n"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI opened a PR; it fixes the problem with the outputs Fix outputs by gallardoalba \u00b7 Pull Request #1 \u00b7 mesocentre-clermont-auvergne/galaxy-tools \u00b7 GitHub (@link https://github.com/mesocentre-clermont-auvergne/galaxy-tools/pull/1/)\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI installed yesterday Docker on my Windows 8 PC.  I use Norton360 antivirus.\nI then installed Galaxy.\nI start Galaxy as follows:\ndocker run -d -p 8080:80 -p 8021:21 -p 8022:22 bgruening/galaxy-stable\nThe container creates, I wait a few minutes and I can see the logs\nI try to connect to Galaxy using Chrome throught the addresses @link http://localhost:8080/  and @link http://127.0.0.1:8080  -  in both cases I get\nThis site can\u2019t be reached\n127.0.0.1 refused to connect.\nERR_CONNECTION_REFUSED\nCan you let me know what do I need to do to be able to access my Galaxy?\nEnclosed the logs.\nThanks !\nGeorge Weingart\n$ docker logs -f awesome_elgamal\nEnable Galaxy reports authentification\nChecking /export\u2026\nDisable Galaxy Interactive Environments. Start with --privileged to enable IE\u2019s.\nStarting postgres\npostgresql: started\nChecking if database is up and running\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: @link http://sqlalche.me/e/f405)\nWaiting for database\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: @link http://sqlalche.me/e/f405)\nWaiting for database\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\npsycopg2.errors.UndefinedTable: relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/check_database.py\u201d, line 25, in \nquery.count()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3669, in count\nreturn self.from_self(col).scalar()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3393, in scalar\nret = self.one()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3360, in one\nret = self.one_or_none()\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3329, in one_or_none\nret = list(self)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3405, in iter\nreturn self._execute_and_instances(context)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\u201d, line 3430, in _execute_and_instances\nresult = conn.execute(querycontext.statement, self._params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 984, in execute\nreturn meth(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\u201d, line 293, in _execute_on_connection\nreturn connection._execute_clauseelement(self, multiparams, params)\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1103, in _execute_clauseelement\ndistilled_params,\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1288, in execute_context\ne, statement, parameters, cursor, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1482, in handle_dbapi_exception\nsqlalchemy_exception, with_traceback=exc_info[2], from=e\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/util/compat.py\u201d, line 178, in raise\nraise exception\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\u201d, line 1248, in _execute_context\ncursor, statement, parameters, context\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\u201d, line 590, in do_execute\ncursor.execute(statement, parameters)\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \u201cgalaxy_user\u201d does not exist\nLINE 3: FROM galaxy_user\n^\n[SQL: SELECT count(*) AS count_1\nFROM (SELECT galaxy_user.id AS galaxy_user_id, galaxy_user.create_time AS galaxy_user_create_time, galaxy_user.update_time AS galaxy_user_update_time, galaxy_user.email A\nS galaxy_user_email, galaxy_user.username AS galaxy_user_username, galaxy_user.password AS galaxy_user_password, galaxy_user.last_password_change AS galaxy_user_last_pass\nword_change, galaxy_user.external AS galaxy_user_external, galaxy_user.form_values_id AS galaxy_user_form_values_id, galaxy_user.deleted AS galaxy_user_deleted, galaxy_us\ner.purged AS galaxy_user_purged, galaxy_user.disk_usage AS galaxy_user_disk_usage, galaxy_user.active AS galaxy_user_active, galaxy_user.activation_token AS galaxy_user_a\nctivation_token\nFROM galaxy_user\nWHERE galaxy_user.email = %(email_1)s) AS anon_1]\n[parameters: {\u2018email_1\u2019: \u2018admin@galaxy.org\u2019}]\n(Background on this error at: @link http://sqlalche.me/e/f405)\nWaiting for database\nDatabase connected\nStarting cron\ncron: started\nStarting ProFTP\nproftpd: started\nStarting Galaxy reports webapp\nreports: started\nStarting nodejs\ngalaxy:galaxy_nodejs_proxy: started\nStarting condor\ncondor: started\nStarting slurmctld\nStarting slurmd\nCreating admin user admin with key fakekey and password password if not existing\n==> /home/galaxy/logs/handler0.log <==\ngalaxy.workflow.scheduling_manager INFO 2020-12-17 18:52:14,638 No workflow schedulers plugin config file defined, using default scheduler.\ngalaxy.workflow.scheduling_manager DEBUG 2020-12-17 18:52:14,638 Starting workflow schedulers\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,688 Binding and starting galaxy control worker for handler0\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,722 Queuing async task rebuild_toolbox_search_index for handler0.\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:52:14,808 handler0 is config watcher\ngalaxy.util.watcher DEBUG 2020-12-17 18:52:14,843 Watching for changes in directory (recursively): /galaxy-central/tool-data\ngalaxy.app INFO 2020-12-17 18:52:14,976 Galaxy app startup finished (21130.909 ms)\ngalaxy.web_stack INFO 2020-12-17 18:52:14,976 Galaxy server instance \u2018handler0\u2019 is running\ngalaxy.queue_worker INFO 2020-12-17 18:52:14,986 Instance \u2018handler0\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.queue_worker DEBUG 2020-12-17 18:52:14,986 App is not a webapp, not building a search index\n==> /home/galaxy/logs/handler1.log <==\ngalaxy.workflow.scheduling_manager INFO 2020-12-17 18:52:24,026 No workflow schedulers plugin config file defined, using default scheduler.\ngalaxy.workflow.scheduling_manager DEBUG 2020-12-17 18:52:24,026 Starting workflow schedulers\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,063 Binding and starting galaxy control worker for handler1\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,082 Queuing async task rebuild_toolbox_search_index for handler1.\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:52:24,148 handler1 is config watcher\ngalaxy.util.watcher DEBUG 2020-12-17 18:52:24,190 Watching for changes in directory (recursively): /galaxy-central/tool-data\ngalaxy.app INFO 2020-12-17 18:52:24,233 Galaxy app startup finished (11560.530 ms)\ngalaxy.web_stack INFO 2020-12-17 18:52:24,234 Galaxy server instance \u2018handler1\u2019 is running\ngalaxy.queue_worker INFO 2020-12-17 18:52:24,263 Instance \u2018handler1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.queue_worker DEBUG 2020-12-17 18:52:24,263 App is not a webapp, not building a search index\n==> /home/galaxy/logs/reports.log <==\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,580 Enabling \u2018history\u2019 controller, class: History\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,590 Enabling \u2018users\u2019 controller, class: Users\ngalaxy.web.framework.base DEBUG 2020-12-17 18:52:21,603 Enabling \u2018tools\u2019 controller, class: Tools\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,609 Enabling \u2018paste.httpexceptions\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,642 Enabling \u2018RecursiveMiddleware\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,661 Enabling \u2018ErrorMiddleware\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,670 Enabling \u2018TransLogger\u2019 middleware\ngalaxy.webapps.util DEBUG 2020-12-17 18:52:21,675 Enabling \u2018XForwardedHostMiddleware\u2019 middleware\nStarting server in PID 563.\nserving on @link http://127.0.0.1:9001\n==> /home/galaxy/logs/slurmctld.log <==\n[2020-12-17T18:52:43.624] No trigger state file (/tmp/slurm/trigger_state.old) to recover\n[2020-12-17T18:52:43.624] _preserve_plugins: backup_controller not specified\n[2020-12-17T18:52:43.624] Reinitializing job accounting state\n[2020-12-17T18:52:43.624] cons_res: select_p_reconfigure\n[2020-12-17T18:52:43.624] cons_res: select_p_node_init\n[2020-12-17T18:52:43.624] cons_res: preparing for 1 partitions\n[2020-12-17T18:52:43.624] Running as primary controller\n[2020-12-17T18:52:43.641] No parameter for mcs plugin, default values set\n[2020-12-17T18:52:43.641] mcs: MCSParameters = (null). ondemand set.\n[2020-12-17T18:52:46.664] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2\n==> /home/galaxy/logs/slurmd.log <==\n[2020-12-17T18:52:44.740] error: Domain socket directory /var/spool/slurmd: No such file or directory\n[2020-12-17T18:52:44.741] Message aggregation disabled\n[2020-12-17T18:52:44.741] CPU frequency setting not configured for this node\n[2020-12-17T18:52:44.839] slurmd version 17.11.2 started\n[2020-12-17T18:52:44.856] slurmd started on Thu, 17 Dec 2020 18:52:44 +0000\n[2020-12-17T18:52:44.859] CPUs=1 Boards=1 Sockets=1 Cores=1 Threads=1 Memory=985 TmpDisk=18275 Uptime=10758 CPUSpecList=(null) FeaturesAvail=(null) FeaturesActive=(null)\n==> /home/galaxy/logs/uwsgi.log <==\ngalaxy.web_stack DEBUG 2020-12-17 18:52:26,867 [p:644,w:1,m:0] [MainThread] Calling postfork function: \ngalaxy.web_stack DEBUG 2020-12-17 18:52:26,870 [p:644,w:1,m:0] [MainThread] Calling postfork function: <function postfork_setup at 0x7fa776f5c2f0>\ngalaxy.web_stack INFO 2020-12-17 18:52:26,871 [p:644,w:1,m:0] [MainThread] Galaxy server instance \u2018main.web.1\u2019 is running\nStarting server in PID 540.\nserving on @link http://127.0.0.1:8080\nserving on uwsgi://127.0.0.1:4001\ngalaxy.queue_worker INFO 2020-12-17 18:52:26,991 [p:644,w:1,m:0] [Thread-1] Instance \u2018main.web.1\u2019 received \u2018rebuild_toolbox_search_index\u2019 task, executing now.\ngalaxy.tools.search DEBUG 2020-12-17 18:52:26,991 [p:644,w:1,m:0] [Thread-1] Starting to build toolbox index.\ngalaxy.tools.search DEBUG 2020-12-17 18:52:27,113 [p:644,w:1,m:0] [Thread-1] Toolbox index finished (122.256 ms)\ngalaxy.tools.search DEBUG 2020-12-17 18:52:27,689 [p:647,w:2,m:0] [Thread-1] Toolbox index finished (836.592 ms)\n==> /home/galaxy/logs/handler0.log <==\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:53:14,860 handler0 is not config watcher\n==> /home/galaxy/logs/handler1.log <==\ngalaxy.model.database_heartbeat DEBUG 2020-12-17 18:53:24,201 handler1 is not config watcher\n==> /home/galaxy/logs/slurmctld.log <==\n[2020-12-17T18:57:43.858] error: Could not open job state file /tmp/slurm/job_state: No such file or directory\n[2020-12-17T18:57:43.859] error: NOTE: Trying backup state save file. Jobs may be lost!\n[2020-12-17T18:57:43.859] No job state file (/tmp/slurm/job_state.old) found"
    },
    {
      "role": "system",
      "text": "@quote\nWhere can I see a file with errors so I can investigate the problem? The files in /home/galaxy/logs don\u2019t show anything unusual.\nMaybe advanced logging will help? GitHub - bgruening/docker-galaxy-stable: Docker Images tracking the stable Galaxy releases. (@link https://github.com/bgruening/docker-galaxy-stable#Advanced-Logging)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi @system,\nThanks for your help before\u2013I was able to get those files uploaded!\nNow I am trying to FTP some larger files to @link http://usegalaxy.eu and am encountering a similar problem. I have successfully done this in the past to the main @link http://usegalaxy.org server, but for usegalaxy.edu, I am getting an error when I try to connect: Connection attempt failed with \u201cETIMEDOUT - Connection attempt timed out\u201d.\nI have used the tutorial on this page: @link https://galaxyproject.org/ftp-upload/\nand this information (@link https://galaxyproject.eu/ftp/) specific to @link http://usegalaxy.eu for guidance.\nBelow is a screenshot from FileZilla showing the information I input for the FTP (I blocked out my username).\nFrom the Grafana page you sent before, it doesn\u2019t look like now is a particularly \u2018bad\u2019 time to be connecting, so I wonder if I have entered something incorrectly. Do you have any suggestions for solving the issue? I tried increasing my \u201cTimeout in seconds\u201d setting within FileZilla but that didn\u2019t seem to help.\nThanks!\nKristen\n\nScreen Shot 2019-01-09 at 9.58.06 AM.png892\u00d7683 84.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/1aba9406f68c9b0d974366e457f7177b31917730.png)"
    },
    {
      "role": "system",
      "text": "@user can you please try @link ftp://galaxy.uni-freiburg.de as FTP server?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I haven\u2019t been able to get anything to run on Galaxy all morning. I got the following error message that suggests contacting an administrator. Thanks\n\"An error occurred while updating information with the server. Please contact a Galaxy administrator if the problem persists.\n\n\n{\n  \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"22.01\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"\",\n    \"responseJSON\": null,\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"2022-02-08T15:33:46.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n    \"traditional\": true,\n    \"data\": {\n      \"order\": \"hid\",\n      \"v\": \"dev\",\n      \"q\": [\n        \"update_time-ge\",\n        \"deleted\",\n        \"purged\"\n      ],\n      \"qv\": [\n        \"2022-02-08T15:33:46.000Z\",\n        \"False\",\n        \"False\"\n      ]\n    },\n    \"emulateHTTP\": false,\n    \"emulateJSON\": false,\n    \"textStatus\": \"error\",\n    \"errorThrown\": \"Internal Server Error\"\n  },\n  \"url\": \"https://usegalaxy.org/api/histories/eee48555debc811e/contents?order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2022-02-08T15%3A33%3A46.000Z&qv=False&qv=False\",\n  \"model\": [\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"Barcode Splitter on data 101 and data 100: Summary\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 103,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b56d01d266ccbc7ebf\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:07:12.507Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b574968129c4e2f3ec\",\n      \"id\": \"bbd44e69cb8906b574968129c4e2f3ec\",\n      \"extension\": \"tabular\",\n      \"update_time\": \"2022-02-08T20:16:35.347Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b574968129c4e2f3ec\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"download\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b574968129c4e2f3ec\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b574968129c4e2f3ec/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b574968129c4e2f3ec&metadata_name=\"\n      }\n    },\n    {\n      \"collection_type\": \"list\",\n      \"deleted\": false,\n      \"history_content_type\": \"dataset_collection\",\n      \"model_class\": \"HistoryDatasetCollectionAssociation\",\n      \"job_source_type\": \"Job\",\n      \"name\": \"Barcode Splitter on data 101 and data 100\",\n      \"element_count\": 97,\n      \"hid\": 102,\n      \"populated_state_message\": null,\n      \"url\": \"/api/histories/eee48555debc811e/contents/dataset_collections/5db9dc0679243c07\",\n      \"history_id\": \"eee48555debc811e\",\n      \"type\": \"collection\",\n      \"type_id\": \"dataset_collection-5db9dc0679243c07\",\n      \"populated_state\": \"ok\",\n      \"update_time\": \"2022-02-08T20:16:35.293Z\",\n      \"tags\": [],\n      \"job_source_id\": \"bbd44e69cb8906b5da1c4e487dc9f4e4\",\n      \"create_time\": \"2022-02-08T20:07:12.488Z\",\n      \"id\": \"5db9dc0679243c07\",\n      \"visible\": true,\n      \"contents_url\": \"/api/dataset_collections/5db9dc0679243c07/contents/234faa589298f537\",\n      \"elements\": []\n    },\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"Non Phi reads Bowtie2 on data 8 and data 5: unaligned reads (L)\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 101,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b58db42f51ccada7ad\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:06:22.860Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b53e081997149ac3c6\",\n      \"id\": \"bbd44e69cb8906b53e081997149ac3c6\",\n      \"extension\": \"fasta\",\n      \"update_time\": \"2022-02-08T20:07:12.503Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b53e081997149ac3c6\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"download\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b53e081997149ac3c6\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b53e081997149ac3c6/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b53e081997149ac3c6&metadata_name=\"\n      }\n    },\n    {\n      \"state\": \"ok\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"IL barcodes.txt\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"eee48555debc811e\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 100,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"dataset_id\": \"bbd44e69cb8906b59774e56bdfa98f55\",\n      \"type\": \"file\",\n      \"create_time\": \"2022-02-08T20:06:16.546Z\",\n      \"type_id\": \"dataset-bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"id\": \"bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"extension\": \"tabular\",\n      \"update_time\": \"2022-02-08T20:07:12.503Z\",\n      \"url\": \"/api/histories/eee48555debc811e/contents/datasets/bbd44e69cb8906b5d2eb20229bdea3f3\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"download\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b5d2eb20229bdea3f3\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b5d2eb20229bdea3f3/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b5d2eb20229bdea3f3&metadata_name=\"\n      }\n    }\n  ],\n  \"user\": {\n    \"id\": \"e9b380ca8aa5cec4\",\n    \"username\": \"bmaguire\",\n    \"total_disk_usage\": 93489887631,\n    \"nice_total_disk_usage\": \"87.1 GB\",\n    \"quota_percent\": 34,\n    \"is_admin\": false,\n    \"preferences\": {\n      \"dbkeys\": \"{\\\"test\\\": {\\\"name\\\": \\\"hopethisworks.txt\\\", \\\"fasta\\\": 62915310, \\\"len\\\": 62915623, \\\"linecount\\\": 62915624}}\"\n    },\n    \"tags_used\": [\n      \"cpg\",\n      \"exons\"\n    ],\n    \"deleted\": false,\n    \"purged\": false,\n    \"quota\": \"250.0 GB\"\n  }\n}\n"
    },
    {
      "role": "user",
      "text": "Hi Jenna, Thanks for your email. I haven\u2019t been checking recently, as after a few days of trying I got set up with a local instance instead, and that has been working well. Just went back and tried executing a trim on the main Galaxy and it ran OK, so the issue seems fixed. Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "The DESEQ results file is empty despite the other files being properly populated (i.e., VST, normalized, rlog, plots). When I used this tool last week, it worked (I am using Galaxy). What am I doing wrong here?"
    },
    {
      "role": "system",
      "text": "It works again for me.\nMany thanks to whoever fixed it."
    }
  ],
  [
    {
      "role": "user",
      "text": "2 lines\n\nTASK [usegalaxy_eu.certbot : Request certificate] ******************************\nfatal: [galaxyimu]: FAILED! => {\u201cchanged\u201d: true, \u201ccmd\u201d: [\u201c/opt/certbot/bin/certbot\u201d, \u201ccertonly\u201d, \u201c\u2013test-cert\u201d, \u201c\u2013non-interactive\u201d, \u201c\u2013webroot\u201d, \u201c\u2013register-unsafely-without-email\u201d, \u201c\u2013agree-tos\u201d, \u201c-w\u201d, \u201c/srv/nginx/_well-known_root\u201d, \u201c-d\u201d, \u201cgalaxyimu\u201d], \u201cdelta\u201d: \u201c0:00:02.063752\u201d, \u201cend\u201d: \u201c2022-06-27 19:06:29.794965\u201d, \u201cmsg\u201d: \u201cnon-zero return code\u201d, \u201crc\u201d: 1, \u201cstart\u201d: \u201c2022-06-27 19:06:27.731213\u201d, \u201cstderr\u201d: \u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\\nAn unexpected error occurred:\\nError creating new order :: Cannot issue for \"galaxyimu\": Domain name needs at least one dot\\nAsk for help or search for solutions at @link https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d, \u201cstderr_lines\u201d: [\u201cSaving debug log to /var/log/letsencrypt/letsencrypt.log\u201d, \u201cAn unexpected error occurred:\u201d, \u201cError creating new order :: Cannot issue for \"galaxyimu\": Domain name needs at least one dot\u201d, \u201cAsk for help or search for solutions at @link https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\u201d], \u201cstdout\u201d: \u201cRequesting a certificate for galaxyimu\u201d, \u201cstdout_lines\u201d: [\u201cRequesting a certificate for galaxyimu\u201d]}\n\nTASK [galaxyproject.nginx : Fail due to previous configuration installation or validation errors] ***\nfatal: [galaxyimu]: FAILED! => {\u201cchanged\u201d: false, \u201cmsg\u201d: \u201cThe new nginx configuration failed to install or validate, so the previous configuration has been restored. Please investigate the errors above for more information.\u201d}"
    },
    {
      "role": "system",
      "text": "\n\n@link https://en.wikipedia.org/wiki/Fully_qualified_domain_name\n\n\n\nFully qualified domain name (@link https://en.wikipedia.org/wiki/Fully_qualified_domain_name)\nA fully qualified domain name (FQDN), sometimes also referred to as an absolute domain name, is a domain name that specifies its exact location in the tree hierarchy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone. A fully qualified domain name is distinguished by its lack of ambiguity in terms of DNS zone location in the hierarchy of DNS labels: it can be interpreted only in one way.\n Domain names in DNS are read from right to lef...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Community Members\nThe genome that I need to map my reads to is not listed in the reference genome list while using HISAT2. Can anyone share with me the procedure to index my required reference in HISAT2 genome list? Is it alright if I directly upload a fasta file for the reference genome from my PC to the history and use the same as a reference genome?\nFollowing is the NCBI link for the reference genome of the microorganism that I am working with\n\n\n@link https://www.ncbi.nlm.nih.gov/nuccore/CP000112\n\n\n\nDesulfovibrio alaskensis G20, complete genome - Nucleotide - NCBI (@link https://www.ncbi.nlm.nih.gov/nuccore/CP000112)\n\n\n\n\n\nThank you for any help"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can just copy the Desulfovibio alaskensis reference genome FTP URL (@link ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/Desulfovibrio_alaskensis/representative/GCA_000012665.1_ASM1266v1) from the NCBI FTP server and paste it in the Galaxy uploader (Paste/Fetch data).\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Trinity genome-guided assembly produces empty files despite the job being completed successfully. I obtained the same unexpected results also on @link http://galaxy.org.\n\nrun log899\u00d7773 188 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4ea61aa567e76fb8ff5a59837dd5e76ae81d9b61.jpeg)\n\nUnexpected results277\u00d7593 37.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/eea9a7b4cf016265f85463a343b483ce03b9116a.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for all of the follow up!\nThe problem when running this in Galaxy is actually the wrapper (not the underlying tool version). Ticket: Trinity: Bug in output collection when using genome-guided mode \u00b7 Issue #3850 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/3850). And, I\u2019ve asked the Tools WG to take a look again. If you want to help make the changes, please comment on the ticket and reach out to the Tools working group (@link https://galaxyproject.org/community/wg/) for coordination.\nI thought it was fixed already, but it wasn\u2019t \u2013 sorry! I\u2019ve added your test history to the ticket with a few more manipulations/tests done by me. If you want to purge your copy to recover storage space in your account, that would be Ok now."
    }
  ],
  [
    {
      "role": "user",
      "text": "HI Everyone,\nAny job I run on Galaxy gives me the same error, \u2018FATAL:container creation failed\u2019.  Screenshot attached.\nThis happens since last night. Even the jobs I previously run successfully now give this error. Anyone else experiencing the same problems?\nThanks\n\nGalaxyError1001\u00d7467 25.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b9dc22d65e261ab0806ad21f73da1ed4cf12f1a2.png)"
    },
    {
      "role": "system",
      "text": "Hello @user\nA few jobs failed due to a problem with our CVMFS dependency last Friday at @link http://UseGalaxy.org. That was corrected. Please rerun any jobs that failed. Thanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey, I encountered an issue: I was working on Galaxy (@link http://huttenhower.sph.harvard.edu/galaxy/) and trying to run lefse but the output files were different when I changed the way microorganisms are named.\nHere is everything that I know:\nThe dataset is green, the job did not fail.\nThe lef_usual.txt is a input file whose first column contained many names of microorganisms, and these names had some special characters like \u201c()\u201d, \u201c-\u201d and \u201c'\u201d, like  \u201cClostridium_sp._DL-VIII\u201d.\nThe lef_usual_result.svg is the output file. There are 5 biomarkers for \u201cmeNorChild\u201d.\nThen, I replaced all the names of microorganisms in the first column in lef_usual.txt with another way of naming, such as s1, s2 and s3. The new input file was named lef_number.txt.\nThe  lef_number_result.svg is the new output file. There are 11 biomarkers for \u201cmeNorChild\u201d.\nThe relevant parameters of the two tasks are the same, only the naming method of the first column of the input file has changed.\nSo, why does just changing the way microorganisms are named affect the final results? What format of naming will give the most correct results?\nThe process screenshots, input files and output files have been uploaded.\nUploading: lef_number.png\u2026 (@link )\nUploading: lef_number_result.png\u2026 (@link )\nUploading: lef_usual.png\u2026 (@link )\nUploading: lef_usual_result.png\u2026 (@link )\nUploading: process.jpg\u2026 (@link )\nCould you help me?"
    },
    {
      "role": "system",
      "text": "@quote\nthe forum doesn\u2019t seem to be active\nThe forum has several Q&A threads from this week, and I see your post. You\u2019ll probably need to be patient for an answer. The way microorganisms are named affect the final results of lefse - LEfSe - The bioBakery help forum (@link https://forum.biobakery.org/t/the-way-microorganisms-are-named-affect-the-final-results-of-lefse/3876)\n@quote\nDo you have the email address of the author?\nThe homepage of their server has contact information for the lab. For Q&A, the forum is better."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, using galaxy for the first time on AWS through the genomics virtual lab on cloudlaunch. I understand that I cannot use the same username as when I use the public servers but my question is, how do I then use a workflow?\nI want to be able to have a workflow I can run every time I run galaxy on AWS but it seems that it creates a new blank server every time with no way to save a workflow for next time. Is this way of using galaxy only for exploratory analysis?\nThanks"
    },
    {
      "role": "system",
      "text": "Try downloading a copy of your workflow from @link http://usegalaxy.org. It will be in a file that ends with the extension .ga. That can then be uploaded to the cloud server into your new account.\nThe first time you open the workflow in the editor, warnings will come up if any tools/tool version are a mismatch for what is already installed on the default server. You can update tools in the workflow, install new tools on the server, etc until things match up. Installing new tool indexes is also possible.\nOnce you have customized your cloud server (if you need to), you can save an AMI image for reuse. This will cost money to maintain, so if you decide to do this, keeping the image as small as possible would be a good idea (save a copy that does not include full analysis, just what you need to start up a new analysis \u2013 permanently delete non-reusable account data, etc).\nWorkflow objects themselves use up very little space, so you could keep yours in the AMI. Or, download a copy of the workflow configured to work with the GVL default install or your custom AMI. Saving a backup copy is good idea anyway."
    }
  ],
  [
    {
      "role": "user",
      "text": "While mapping with bowtie2, in Galaxy I got the following error messages for each of the paired_end files. I also got alignment result. So, is my alignment result could run completely or they are just a part of the whole thing? Should I solve the problems using fastp and run again? Or I am good to go with the results?\nWarning: skipping mate #1 of read 'SRR14466480.24455260 24455260 length=1' because length (1) <= # seed mismatches (0)\nWarning: skipping mate #1 of read 'SRR14466480.24455260 24455260 length=1' because it was < 2 characters long\nWarning: skipping mate #1 of read 'SRR14466480.24461757 24461757 length=1' because length (1) <= # seed mismatches (0)\nWarning: skipping mate #1 of read 'SRR14466480.24461757 24461757 length=1' because it was < 2 characters long\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of 'int'\n(ERR): bowtie2-align died with signal 6 (ABRT) (core dumped)\n\nAlso got this one-\nScreenshot from 2021-08-30 14-48-231245\u00d7463 39.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/739856e669f2dcba52515d6c30981f4921c59309.png)"
    },
    {
      "role": "system",
      "text": "Could you try to filter out those short reads of only 2 bases? With a tool that is made for paired-end data like cutadapt?\nEDIT:\nThis is also possible with fastp but not default, you need to turn it on under \u201cLenght filtering options\u201d"
    }
  ],
  [
    {
      "role": "user",
      "text": "BLAST does not find my Galaxy-hosted and Galaxy-created database\nThere have been previous reports of problems with locally hosted databases, but this is for a Galaxy-hosted database.\nThe error message is\nLibCvmfs version 2.4, revision 25 BLAST Database error: No alias or index file found for nucleotide database [/scratch/03166/xcgalaxy/main/staging//29564501/inputs/dataset_43031652_files/blastdb] in search path [/scratch/03166/xcgalaxy/main/staging/29564\nThis database was created on Galaxy using the \u2018make BLAST database\u2019. I ran the make database program twice, in both cases the database appears to lack an alias/index file.\nIs this a Galaxy software problem?\nAdvice?"
    },
    {
      "role": "system",
      "text": "Just closing this out.\nAs communicated via email from the bug report, the fasta dataset had duplicated identifiers.\nAll >indentifer content must have a unique name within the same fasta to generate an index."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am repeatedly getting the error message \u201cRemote job server indicated a problem running or monitoring this job.\u201d when trying to run BWA for mapping short reads against a reference genome. I am using single read fastq data and a reference genome from my history.\nThe first time I ran the tool it worked on the first read, but none of the rest (4 others). The second time, it worked on none of the 5. Each time I get the same error message.\nIs there something I should do differently to avoid this?"
    },
    {
      "role": "system",
      "text": "Resolved by mapping with BWA-MEM instead."
    }
  ],
  [
    {
      "role": "user",
      "text": "Ran Tophat 5X on same data set that worked 2 days ago\u2026 Is Tophat down? I have deadlines to meet\u2026"
    },
    {
      "role": "system",
      "text": "Update: Fixed\nNote to all end users running mapping:  Using all defaults for the \u201cJob Resources\u201d parameter is a good choice now again. No need to target a particular cluster anymore. Allowing Galaxy to choose the target cluster based on the currently available resources, at the time of job submission, is the fastest way to run tools.\nTicket: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/257#issuecomment-540676708"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI am using usegalaxy to analyze RNAseq for the first time. Is there a tutorial for making Venn Diagram?\nI am using the tool \u201cVenn diagram on list\u201d, but keep failing to make it work.\nThanks"
    },
    {
      "role": "system",
      "text": "Dear @user,\nI am not super familiar with the tool and just had a quick look at the code. What I supect is a malformatted input file or you have chosen the wrong input files.\nFull dataset (with all identifiers) - Must contain all the identifiers (IDs) from the individual set files you will provide.\nMembers of set - Are the individual set files.\nMake sure the files have in the first column the identifiers. And also make sure the idenfiers of the Members of set are contained in the Full dataset (with all identifiers). I think the program couldn\u2019t find the identifier ENSMUSG00000025498.15.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "i want upload my data to galaxy.its a little heavy(3Gb). i treid it before and worked ,.but it doesnt now. i waited too long but the uploading  was 0% .where is the problem?"
    },
    {
      "role": "system",
      "text": "@quote\nPort number isnt important ?\nFTP clients will know the standard ports\nOne possible issue is that your network admin restricted access to FTP, you can contact them and ask."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI am using Galaxy server for genome assembly using SPAdes. But there is an error occurred:\nCommand line: /cvmfs/main.galaxyproject.org/deps/_conda/envs/__spades@3.12.0/bin/spades.py -o /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working --disable-gzip-output --only-assembler --careful -t 7 -m 288 -k 33,55,91 --cov-cutoff auto --pe1-fr --pe1-1 fastq:/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat --pe1-2 fastq:/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\nSystem information:\nSPAdes version: 3.12.0\nPython version: 3.8.1\nOS: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-glibc2.10\nOutput dir: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working\nMode: ONLY assembling (without read error correction)\nDebug mode is turned OFF\nDataset parameters:\nMulti-cell mode (you should set \u2018\u2013sc\u2019 flag if input data was obtained with MDA (single-cell) technology or --meta flag if processing metagenomic dataset)\nReads:\nLibrary number: 1, library type: paired-end\norientation: fr\nleft reads: [\u2019/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat\u2019]\nright reads: [\u2019/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\u2019]\ninterlaced reads: not specified\nsingle reads: not specified\nmerged reads: not specified\nAssembly parameters:\nk: [33, 55, 91]\nRepeat resolution is enabled\nMismatch careful mode is turned ON\nMismatchCorrector will be used\nCoverage cutoff is turned ON and threshold will be auto-detected\nOther parameters:\nDir for temp files: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/tmp\nThreads: 7\nMemory limit (in Gb): 288\n======= SPAdes pipeline started. Log can be found here: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/spades.log\n===== Assembling started.\n== Running assembler: K33\n0:00:00.000 4M / 4M INFO General (main.cpp : 74) Loaded config from /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/config.info\n0:00:00.001 4M / 4M INFO General (main.cpp : 74) Loaded config from /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/careful_mode.info\n0:00:00.002 4M / 4M INFO General (memory_limit.cpp : 49) Memory limit set to 288 Gb\n0:00:00.002 4M / 4M INFO General (main.cpp : 87) Starting SPAdes, built from N/A, git revision N/A\n0:00:00.002 4M / 4M INFO General (main.cpp : 88) Maximum k-mer length: 128\n0:00:00.002 4M / 4M INFO General (main.cpp : 89) Assembling dataset (/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/dataset.info) with K=33\n0:00:00.003 4M / 4M INFO General (main.cpp : 90) Maximum # of threads to use (adjusted due to OMP capabilities): 7\n0:00:00.003 4M / 4M INFO General (launch.hpp : 51) SPAdes started\n0:00:00.003 4M / 4M INFO General (launch.hpp : 58) Starting from stage: construction\n0:00:00.003 4M / 4M INFO General (launch.hpp : 65) Two-step RR enabled: 0\n0:00:00.003 4M / 4M INFO StageManager (stage.cpp : 132) STAGE == de Bruijn graph construction\n0:00:00.020 4M / 4M INFO General (read_converter.hpp : 77) Converting reads to binary format for library #0 (takes a while)\n0:00:00.020 4M / 4M INFO General (read_converter.hpp : 78) Converting paired reads\n0:00:00.574 100M / 100M INFO General (binary_converter.hpp : 93) 16384 reads processed\n0:00:00.972 116M / 116M INFO General (binary_converter.hpp : 93) 32768 reads processed\n0:00:01.746 144M / 144M INFO General (binary_converter.hpp : 93) 65536 reads processed\n0:00:03.283 204M / 204M INFO General (binary_converter.hpp : 93) 131072 reads processed\n0:00:06.374 320M / 320M INFO General (binary_converter.hpp : 93) 262144 reads processed\n0:00:12.521 548M / 548M INFO General (binary_converter.hpp : 93) 524288 reads processed\n0:00:34.567 908M / 908M INFO General (binary_converter.hpp : 93) 1048576 reads processed\n0:01:09.485 908M / 908M INFO General (binary_converter.hpp : 93) 2097152 reads processed\n0:02:18.535 912M / 912M INFO General (binary_converter.hpp : 93) 4194304 reads processed\n0:04:48.246 920M / 920M INFO General (binary_converter.hpp : 93) 8388608 reads processed\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 58) The number of left read-pairs is larger than the number of right read-pairs\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 60) Unequal number of read-pairs detected in the following files: /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39274260.dat /pylon5/mc48nsp/xcgalaxy/main/staging/28325065/inputs/dataset_39272829.dat\n== Error == system call for: \u201c[\u2019/cvmfs/main.galaxyproject.org/deps/_conda/pkgs/spades-3.12.0-1/share/spades-3.12.0-1/bin/spades-core\u2019, \u2018/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/config.info\u2019, \u2018/pylon5/mc48nsp/xcgalaxy/main/staging/28325065/working/K33/configs/careful_mode.info\u2019]\u201d finished abnormally, err code: 255\nWhat can I do ? Kindly help me to solve this problem.Thank You"
    },
    {
      "role": "system",
      "text": "Did you already checked the input files?\n\nThe number of left read-pairs is larger than the number of right read-pairs\n0:06:31.981 936M / 936M ERROR General (paired_readers.hpp : 60) Unequal number of read-pairs\n\nIf you work with paired-end data the forward and reverse reads need to be the same amount of reads."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI accidentally tool_conf.xml. However the galaxy still shows all the installed tools. Is there a way to restore it using galaxy? I tried installing some new package but it did not result in creation of tool_conf.xml.\nThanks in advance."
    },
    {
      "role": "system",
      "text": "@user The first thing that comes to my mind is to try with bioblend\n@link https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#bioblend.galaxy.tools.ToolClient.get_tools"
    }
  ],
  [
    {
      "role": "user",
      "text": "After using Shovill to assemble paired-end NGS reads for DNAseq data I am hoping to filter my FASTA output file based off the coverage of each assembled contig. The output for Shovill is a FASTA file that contains a unique contig ID and a coverage value. Using \u201cfilter FASTA\u201d on galaxy, I want to output a FASTA file with contigs that have a coverage greater than 1000. Can I use \u201cfilter FASTA\u201d for this and if so what is an example of the regular python expression that would work to filter for coverage?\nThank you very much in advance."
    },
    {
      "role": "system",
      "text": "@user\nOk, good!\nTry this expression to find reads with coverage at or above 500.0\n^.+cov=([5-9][0-9][0-9])|([1-9]\\d{4}\\d*)\\..+$"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am trying to align my sequences with the silva 138.1 version but the downloaded file is empty (same for version 132),\ncan anyone advise please?\nI am looking for bacterial and archaeal community in my samples amplified with 341F and 806 R for 16S V3V4 region.\nThank you"
    },
    {
      "role": "system",
      "text": "HI @user\nWell, there were some issues at the server last week. Those may have impacted your work. It looks like you have had a successful rerun by now with the same inputs that originally timed out and failed.\nApologies for the delayed reply \u2013 I also needed to wait for things to get back to (mostly) normal before reviewing/replying. All should be Ok now. Everyone should expect slightly longer job queue timing until the banner on the server clears, but everything else is fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "My account were active last night and running data, however this morning I tried logging in to find a message saying the account had been marked as deleted. How do I recover the data that was in the account?"
    },
    {
      "role": "system",
      "text": "Update Nov 1, 2019\nDuplicated accounts are detected as an ongoing process. If you found your account recently deleted, or have not but do have duplicates and want to avoid problems, write to us at @link mailto:galaxy-bugs@lists.galaxyproject.org and list the following:\n\nThe registered email for the account you wish to keep\nIf that email is from a public domain service, include your non-public primary email address. See \u201cReminders\u201d below if you do not understand the difference.\nList out your duplicated account email addresses. All, not just those you happen to be using right now.\nConfirm that you now understand the terms of usage, and will follow them going forward.\n\nReminders:\n\n\nOnly one account will ever be restored. Unfair usage means data loss in other accounts.\nTerms of usage can be found here: @link https://usegalaxy.org/static/terms.html\n\nThere is never a valid reason for multiple accounts\nIf we discover duplicated accounts before they are proactively declared, your data is at risk of being lost\nA public domain email address is from an anonymous service like Gmail, Hotmail, Nate, QQ, Yahoo or similar.\nA non-public primary email address is from your university or place of work.\nAct quickly and be honest.\nAccount terms are only re-clarified once per user, as a courtesy.\n\nREAD THIS FAQ: Getting an account at Galaxy Main (http://usegalaxy.org) (@link https://galaxyproject.org/support/account/) Creating a \u201cnew\u201d account will not solve duplicated account problems and instead cause more. You need to write in to get your issues resolved, correctly.\n\nThe simplified quota rule is one account per user per public Galaxy server.\nEnd users are permitted to create a single account at each other public Galaxy server \u2014 most if not all have a \u201cone account per end-user\u201d requirement.\nWhen presented with the activation link in the emails anyone received when creating an account, the account terms were very clearly explained, and you agreed to these terms.\nQuote:\n\nBy clicking on the above link and opening a Galaxy account, you are also confirming that you have read and agreed to Galaxy\u2019s Terms and Conditions for the use of this service (@link https://usegalaxy.org/static/terms.html). Terms include a quota limit of one account per user. Attempts to subvert this limit by creating multiple accounts or through any other method may result in the termination of all associated accounts and data.\n\nThanks for understanding and for helping us to keep Galaxy a free and fair public resource, for everyone, worldwide.\nJen & the Galaxy team"
    }
  ],
  [
    {
      "role": "user",
      "text": "This had been working fine 6 months ago. I am now analyzing a new data set and it looks like it is running fine. The plots come out and look just fine as well as normalized count files but the list for the results is empty. I have rerun it over the course of the last few days and each time the list is still empty."
    },
    {
      "role": "system",
      "text": "Thanks again for the report, this should be fixed with DESeq2 version 2.11.40.6+galaxy1 (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.6+galaxy1)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m currently using the following line (@link https://github.com/scholtalbers/galaxy-dataset-exporter/blob/master/dataset-exporter.xml#L17) to get tags of datasets to the underlying tool:\n#set tags = ','.join(str(t.value) for t in $dataset.tags)\n\nThis however this does not return the original tags, they get truncated (losing  #  and the  name:), converted to lowercase and special characters are removed. Is there a better way to get access to tags in the tool xml?"
    },
    {
      "role": "system",
      "text": "There\u2019s no sanitization anywhere, but tags are linked via lowercase, so maybe you\u2019d need to use\nuser_tname and  user_value"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone!\nI have used the Stacks series a few times before but now have an error that I can\u2019t seem to get around.\nI did the demultiplexing via Stacks V2.55, having three enzymes I activated the option \u201cDisable checking if the RAD site is intact\u201d and only entered 2 of them (one frequent and one rare cutter). For the rest, I used the same parameters as I used before. The demultiplexing went fine.\nNow, as for the assembly, the de novo wrapper always ends on \u201cUnable to finish job\u201d. Funny enough, all the files that are not lists (the summary statistics, the logs) are available and can even be downloaded. I can see the list of the loci and where there are consensus, for example. But as for the lists, they just do not exist, so I don\u2019t have the catalog of loci\u2019s actual sequences for example, which is quite a bother.\nIt took between one day and a week for the data to stop running. I have PE data, 2 species for which I have 14 and 10 individuals. I used only the default options.\nAnybody could have an idea of the problem?\nThank you in advance."
    },
    {
      "role": "user",
      "text": "Hello,\nIf anyone shares this problem, I \u201csolved\u201d it by using not the V2 of Stacks but the version 1.46, which apparently works for this type of situation."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to develop a tool based on R script. The tool should provided several methods for correlation calculation. Then there should be a parameter with options for different methods. I have set the parameter, but \u201cthere is no options available\u201d on my local galaxy.\nHere is my code:\n    <param name=\"method\" type=\"select\" label=\"methods for correlation calculation\"/>\n\n            <option value=\"pearson\">Pearson </option>\n            <option value=\"spearman\">Spearman  </option>\n            <option value=\"kendall\">kendall </option>\n    </param>\n\nIn the corresponding R script:\nspec_list$expr <- c(\u2018expr\u2019, \u2018f\u2019, \u20181\u2019, \u2018character\u2019)\nspec_list$method <- c(\u2018method\u2019, \u2018m\u2019, 1, \u2018character\u2019)\nspec_list$correlationMatrix <- c(\u2018correlationMatrix\u2019, \u2018c\u2019, 1, \u2018character\u2019)\nspec <- t(as.data.frame(spec_list))\nopt <- getopt(spec)\n\u2026\nif(!is.null(opt$method)){\ncorMatrix <- cor(expr, method=opt$method)\n}"
    },
    {
      "role": "system",
      "text": "<param name=\"method\" type=\"select\" label=\"methods for correlation calculation\"/>\n\nthere is a typo \"/ \"\nyou close the parameter block before the options"
    }
  ],
  [
    {
      "role": "user",
      "text": "I was trying to run a workflow but ran into some problems, which I think are related to featurecounts. Basically, once instance of featurecounts in a workflow>it runs. Two instances in a workflow>it does not run. Remove either featurecount from the workflow>it runs again. These kinds of problems do not occur if I use htseq instead, but I would prefer to use featurecounts because it counts more reads that are of interest to me."
    },
    {
      "role": "system",
      "text": "Hi Chaitanya,\nI\u2019ve built the workflow \u2018from scratch\u2019 in Workflow editor. Just add two input boxes, for alignment and annotation, add meaningful description, add tools, and you\u2019ll be fine.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am currently analyzing RNASeq data obtained from dog cell culture. I used HISAT2 for the alignment to the canis familiaris reference genome in Galaxy. Then, to count the reads, I tried both htseq-count (Galaxy Version 0.9.1) and featureCounts (Galaxy Version 1.6.4+galaxy1). As gene annotation file, I took this file from iGenomes: @link https://usegalaxy.eu/library/list#folders/Fcf0661b18aa5ad51\nIt contains 32461 lines, among them 15178 exons (feature type). As I run htseq-count/featureCounts with \u2018exon\u2019 as feature type, I expected 15178 lines in the outout of these tools. In contrast, I get both in htseq-count and featureCounts only 2023 lines, even though exons with 0 counts are included in the output.\nDo you have any idea what could have happened to the remaining exons?\nThank you.\nBest\nHannah"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe iGenomes version of the annotation is based on RefSeq from several years ago.\nAnnotation from Ensembl will be more current but will require some manipulations to have the chromosome identifiers match up with the built-in canFam3 indexes that are sourced from UCSC.\nOr, you can also use the matching Ensembl fasta version of the genome and use that as a Custom Reference Genome/Build with tools. Be aware that the dog genome is very large and may exceed server processing resources when used this way at a public Galaxy server. But you can try.\nDo not assign the \u201cdatabase\u201d as canFam3 if you decide to use all Ensembl data. You\u2019ll also need to remap. The Ensembl version of the data does not have the same chromosome identifiers as the UCSC version. Every step in an analysis needs to use data that is all based on the same exact genome/build.\nFAQs: @link https://galaxyproject.org/support/\n\nPreparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/)\nMismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Sir/Madam,\nThanks in advance for your help. I would like to share you my problem regarding collecting the unmapped contigs using Bowtie2 tool. I have tried several times and I am still facing the same problem (please see attached images of error report\n\nBowtie2-problem344\u00d7549 15.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/327a4148a9eea8ee8f4238d6b4e489f08e60c828.png)\nBowtie2 problem\n)"
    },
    {
      "role": "system",
      "text": "Hi, sorry for the wait. There was a bug in the way that SPAdes was running that prevented it from properly using the amount of memory allocated for the job. This has been corrected and you should be able to successfully run it now. Please give it a try and let us know if it\u2019s still not working for you."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m Yoon.\nEvery works on Galaxy eu are impossible.\nI tried \u2018Bowtie2 (with ATAC-seq data)\u2019, but it wasn\u2019t work.\nSo, i tried \u2018Faster Download and Extract Reads in FASTQ format from NCBI SRA\u2019 on Get Data, but it also wasn\u2019t work.\nOn @link https://usegalaxy.org/, i can do that work, but i need \u2018Genrich\u2019 program which is exist on only Galaxy eu.\nIs this a serve problem or it dosen\u2019t work because this is holliday?\nPlease let me know what happened, because i need to submit some data to professor.\nAlways thank you for your work!"
    },
    {
      "role": "user",
      "text": "Oh\u2026 it work now. thank you. Happy new year!"
    }
  ],
  [
    {
      "role": "user",
      "text": "im doing an rna seq analysis and after aligning with Star and filtering after Deseq2 a question remains. and that is:how to replace these ID with official gene names? i found a tool names annotatedmyIDs but it isnt working"
    },
    {
      "role": "system",
      "text": "Hi -\nSince the identifier is in the first column, Text transformation with sed will work without a complex expression. Try this:\ns/\\.[0-9]+//\n\nOther options include using regular expressions with tools like:\n\nText reformatting with awk\nReplace parts of text\nReplace Text in entire line\nReplace Text in a specific column\n\nOr, if you don\u2019t want to use a regular expression, the column can be isolated, the \u201cdot\u201d replaced with a \u201ctab\u201d, then all the data rearranged back again into one file. This could be put into a workflow if you plan to run it again. Example tool order: Cut > Convert delimiters to TAB > Paste > Cut.\nAlso, your IDs are Emsembl transcripts, so choose that as the input type with annotateMyIDs.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nIm running Trimmomatic Pair-end(separate data set) with multiple data sets. Every data was fine till two of them showed error.\nThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource.\nTrimmomaticPE: Started with arguments: -threads 6 -phred33 fastq_r1.fastqsanger.gz fastq_r2.fastqsanger.gz fastq_out_r1_paired.fastqsang\nwhat kind of error is this? and how can i fix it?\nThanks\nSihae"
    },
    {
      "role": "system",
      "text": "Dear Sihae,\nSorry for the late reply but I applied Trimmomatic and FastQC to the @link https://www.ebi.ac.uk/ena/browser/view/SRR10538402 data, which took quite some time to finish, and it worked for me. Can you please check that you have selected the newest version of the tools?\nYou actually know the sequence device and version from FastQC (first table, Sanger / Illumina 1.9 encoding) and it is also written that they have used the HiSeq X Ten device. So should be a newer Illumina version.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a local Galaxy installation that so far works fine.\nI now work with data collections.\nI have Illumina data (128 files) in one collection (each file around 20-30 Mb, so not very big).\nI now try to run jobs on that collection. For example FastQC.\nNow Galaxy behaves strange: When I select the collection as FastQC input and press \u201cSubmit\u201d, Galaxy starts to add the jobs into the History while the \u201cSending\u2026\u201d button is shown at the tool page so, I do not see the green \u201csubmitted\u201d page yet.\nHowever, after approx. 60 sec. Galaxy starts to submit the job again (while the \u201cSending\u2026\u201d button is still showing). The \u201cold\u201d job keeps being processed - it just adds a new one (with of course again 128 entries. After some time (~60sec) it adds it again and so on. This can easily add >10 jobs with over 1000 files from my inital list - that are of course all processed.\nThis strange behavior only stops at some point when Galaxy displays an error message saying the \u201cthe job submission failed\u201d (red box). From then on no \u201cnew\u201d jobs are added.\nThe thing is that even the first job submission was OK - it just needed some time to be processed due to the large number of files.\nTo me this looks like a timeout error that is triggered because the tool page is at the \u201csending\u2026\u201d level for to long.\nIs there any way how I can increase such a timeout to prevent this \u201cexplosion\u201d of jobs?\nMany thanks in advance! for any help!"
    },
    {
      "role": "system",
      "text": "If your using nginx as your proxy and you connect via the uwsgi protocol you can increase the timeout by setting uwsgi_read_timeout 300; which would set the timeout to 5 minutes. If you\u2019re using uwsgi without a proxy you should be able to set http-timeout: 300 in the uwsgi: section of galaxy.yml (you can find all valid options for uwsgi in @link https://uwsgi-docs.readthedocs.io/en/latest/Options.html#http-timeout), but I\u2019m less certain about that being the right option."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to install galaxy release_22.01 to our local server.  The server runs with centos 7, python 3.8, and postgresql 14.  The galaxy service can start with run.sh when the default configuration was used.  However, I couldn\u2019t get it switch to postgresql database.   When the configuration was changed to (database_connection: postgresql://galaxy:password@127.0.0.1:5432/), galaxy service (run.sh) didn\u2019t switch to postgresql database but still kept using default sqlite database.  In the command line, use \u201cpsql -U galaxy -d galaxy -h 127.0.0.1\u201d, the database can be accessed successfully with password.  Can anyone help with it?\nThank you!\nWei"
    },
    {
      "role": "system",
      "text": "Hi @user, that is a bug that we have to fix. For now you can fix this by activating galaxy\u2019s virtualenv (source .venv/bin/activate), and calling galaxyctl update --force. Your instance is still trying to start from galaxy.yml.sample. After that, restart your Galaxy instance and the changes should apply."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all\nI am trying to analyse my mRNAseq data using the galaxy reference based workflow (Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#counting-the-number-of-reads-per-annotated-gene)). I am very new to data analysis and also to galaxy. I tried two different approaches to align my data:\n1)RNA-STAR followed by feature counts: In this case the feature counts showed 0 counts.\n2) Next, I tried HISAT2 alignment\u2026and that showed just 177 reads\u2026\nI don\u2019t understand why the read number is so low\u2026do you have any suggestions?"
    },
    {
      "role": "system",
      "text": "Hi @system,\nI\u2019ll reproduce the analysis by using that datasets (@link https://ftp.ncbi.nlm.nih.gov/genomes/refseq/bacteria/Streptomyces_coelicolor/latest_assembly_versions/GCF_013363625.1_ASM1336362v1/). I\u2019ll inform you as soon as I get some results.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI was trying to map my sequences to the latest mouse genome assembly (GRCm39/mm39) but I couldn\u2019t find it on galaxy. How do I make a request for the mouse genome to be updated on galaxy?\nIn the meantime, I successfully got the fasta file of the GRCm39/mm39 from the NCBI FTP site and tried to map my sequences to it using Bowtie2, but it gave an error with no information about the error.\nPlease help, I\u2019m new to galaxy :sob:\nThank you."
    },
    {
      "role": "system",
      "text": "@link http://usegalaxy.eu has mm39 now included. I hope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI was advised to use $GALAXY_SLOTS variable in order to obtain number of processor cores. Workflow automation? - #2 by mvdbeek (@link https://help.galaxyproject.org/t/workflow-automation/5760/2) But when i print this variable in the tools, i get exactly one despite i have 16 - 48 logical processors.\nWhy it is not set automatically? E. g why not add\n elif [ `nproc` > 0 ]; then \n     GALAXY_SLOTS=`nproc`\n\ninto /data/galaxy/lib/galaxy/jobs/runners/util/job_script/CLUSTER_SLOTS_STATEMENT.sh ?"
    },
    {
      "role": "system",
      "text": "We assume and highly recommend that jobs submitted by Galaxy are run on a HPC system with a job scheduler (like SLURM, PBS, Torque, LSF) or kubernetes. You then configure the destinations in the job_conf.xml file to request the required resources, and you assign tools to the destination. If you don\u2019t have this available you can use local_slots as you found out, but you still need to map tools to destinations.\nNote that this is not recommended, as there is no way for Galaxy to know what resources are available, so you will start consuming more resources than are available if you queue up many jobs.\nConnecting Galaxy to a compute cluster (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html) walks through installing slurm using ansible on the head node \u2026 while that\u2019s still not ideal it is better than using the local job runner."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI tried running DESeq2 using output data from upstream Salmon analysis. The samples I\u2019m investigating are RNA-seq, paired-end. The run failed and the following error message came up when investigating the bug reports:\n\nimage720\u00d7290 11.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/45149094f9c7928a3caffc6e17c9b2a26ef1c782.png)\nI saw that other people encountered similar issues before, but with different errors (error message from DESeq2/EdgeR - #4 by jennaj (@link https://help.galaxyproject.org/t/error-message-from-deseq2-edger/1017/4)). Some recommendations for this other type of error were to try run datasets with option \u201cFiles have a header\u201d off.\nI tried that (just in case), but still had the same error message (only the line number changed). So maybe I did something wrong upstream.\n\nimage731\u00d7291 11.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e5b716a0e0d3c8ee1c5704486b42002e2546507f.png)\nAlso, not sure if this is of relevance here, but the upstream Salmon report mentioned that there are newer versions of Salmon, with bug fixes, etc. Could this be some sort of bug?\nIf somebody had any suggestions on how to sort this, that would much appreciated!"
    },
    {
      "role": "user",
      "text": "An update. I spent some time playing around with \u201ctranscript to gene\u201d table formats, swapping \u201ctranscript/gene stable ID versions\u201d with \u201ctranscript/gene IDs\u201d in various combinations, different ways of importing the table (TSV instead of CSV), converting the table to tab-delimited format, adjusting the titles, removing the titles etc.\nUnfortunately, none worked for DESeq2, despite upstream Salmon quantification working well. I suspect that perhaps there was some mismatch in ensembl transcriptome data in fasta format (maybe I messed up something when normalising it or used wrong reference dataset) and the BioMart table (I tried various versions).\nWhat worked was downloading ensembl reference transcriptome in gtf format. I used it unmodified in Salmon and downstream DESeq2 instead of the Transcript-to-Gene table. The result was this:\n\nimage1081\u00d7298 28.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5fd09723f3fba6fbfcdd4b314ff4f2a928d15624.png)\nI guess my next question to @system and other experts is the following: can I somehow validate if DESeq2 worked correctly? Of course, if something did not work, that will come out during downstream analyses. But maybe there is a tool or type of analysis that allows flagging up issues early?\nThank you!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have some files obtained by small non-coding RNA sequencing in humans. My aim is to identify the miRNA and do differential expression.\nI am following this protocol: Hands-on: miRNA differential expression analysis - YouTube (@link https://www.youtube.com/watch?v=TyWCXxzHZak&ab_channel=GalaxyProject)\nIt says that I need some reference files, but I\u2019m new in the field and i don\u2019t understand what\u2019s the difference between them or where I can download them. These files are:\n\nannotation.gtf\ntranscriptome.fasta\nstar_miRNA_seq.fasta\nmature_miRNA.fasta\nmiRNA_stem-loop_seq.fasta\n\nI searched in miRBase and I found some reference files, but I am not sure which are the correct ones or where are the ones missing. I attach a picture.\n\nCaptujra921\u00d7607 48.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/41769922f43288994918709e52a9435fc41d76a4.png)\nCan anyone tell me where I can download them and what\u2019s the difference among them (especially between annotation and transcriptome file, not only the format; also between star miRNA and the other two). Thank you so much! :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user,\nsome background on the files:\n\n\nAnnotation.gtf\nThe Gene Transfer Format is a file format used for describing genes and other features of DNA, RNA, and protein sequences. You can download it from ENCODE (@link https://www.gencodegenes.org/human/); this file corresponds to the Comprehensive gene annotation dataset.\n\n\nTranscriptome.fasta\nThis file includes the sequence of all the transcripts in the human genome. You can download it from ENCODE (@link https://www.gencodegenes.org/human/); this file corresponds to the Transcript sequences dataset.\n\n\nmature_miRNA.fasta\nThis file includes the functional miRNA sequences. You can download it from miRBase (@link https://mirbase.org/ftp.shtml); this file corresponds to the mature miRNA sequences dataset.\n\n\nmiRNA_stem_loop_seq.fasta\nThis file contains the hairpin structure of the precursor miRNA. You can download it from miRBase (@link https://mirbase.org/ftp.shtml); this file corresponds to the hairpin miRNA sequences dataset.\n\n\nstar_miRNA_seq.fasta\nThis file includes the sequences generated after processing the pre-miRNAs, which are assumed to be non-active. However, currently, this notation has been modified, since experimental results indicate that those sequences are usually functional. For this reason, star sequences are not currently available in miRBase (because have been integrated with the mature sequences). This file is optional.\n\n\nI suggest you to have a look at the training, since it can be useful for understanding the pipeline: miRNA data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/mirna-target-finder/tutorial.html#background-on-data).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Can anyone clarify what is expected in the following block of code in cli.py?\n\nif state is None:\nif ajs.job_wrapper.get_state() == model.Job.states.DELETED:\ncontinue\n        external_metadata = not asbool(ajs.job_wrapper.job_destination.params.get(\"embed_metadata_in_job\", DEFAULT_EMBED_METADATA_IN_JOB))\n        if external_metadata:\n            self._handle_metadata_externally(ajs.job_wrapper, resolve_requirements=True)\n\n        log.debug(\"(%s/%s) job not found in batch state check\" % (id_tag, external_job_id))\n        shell_params, job_params = self.parse_destination_params(ajs.job_destination.params)\n        shell, job_interface = self.get_cli_plugins(shell_params, job_params)\n        cmd_out = shell.execute(job_interface.get_single_status(external_job_id))\n        state = job_interface.parse_single_status(cmd_out.stdout, external_job_id)\n        if not state == model.Job.states.OK:\n            log.warning('(%s/%s) job not found in batch state check, but found in individual state check' % (id_tag, external_job_id))\n    if state != old_state:\n        log.debug(\"(%s/%s) state change: from %s to %s\" % (id_tag, external_job_id, old_state, state))\n        if not state == model.Job.states.OK:\n\n\n            # No need to change_state when the state is OK, this will be handled by `self.finish_job`\n            ajs.job_wrapper.change_state(state)\n    if state == model.Job.states.RUNNING and not ajs.running:\n        ajs.running = True\n    ajs.old_state = state\n    if state == model.Job.states.OK:\n        log.debug('(%s/%s) job execution finished, running job wrapper finish method' % (id_tag, external_job_id))\n        self.work_queue.put((self.finish_job, ajs))\n\nIf the galaxy job gets queued by the cluster Galaxy marks it as OKay and deletes the job directories\u2026  Log output:\n\ngalaxy.jobs.mapper DEBUG 2019-04-22 15:07:56,035 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Mapped job to destination id: LocalShell\ngalaxy.jobs.handler DEBUG 2019-04-22 15:07:56,063 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Dispatching to cli runner\ngalaxy.jobs DEBUG 2019-04-22 15:07:56,086 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Persisting job destination (destination id: LocalShell)\ngalaxy.jobs DEBUG 2019-04-22 15:07:56,087 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Working directory for job is: /Dedicated/clingalproddata/database/jobs_directory/000/594\ngalaxy.jobs.runners DEBUG 2019-04-22 15:07:56,103 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] Job [594] queued (40.204 ms)\ngalaxy.jobs.handler INFO 2019-04-22 15:07:56,123 [p:38344,w:0,m:2] [JobHandlerQueue.monitor_thread] (594) Job dispatched\ngalaxy.jobs.command_factory INFO 2019-04-22 15:07:56,246 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] Built script [/Dedicated/clingalproddata/database/jobs_directory/000/594/tool_script.sh] for tool command [echo \"Running with '{GALAXY_SLOTS:-1}' threads\" > \"/Dedicated/clingalproddata/database/files/000/dataset_671.dat\"]\ngalaxy.jobs.runners DEBUG 2019-04-22 15:07:56,320 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) command is: rm -rf working; mkdir -p working; cd working; /Dedicated/clingalproddata/database/jobs_directory/000/594/tool_script.sh; return_code=?; cd \u2018/Dedicated/clingalproddata/database/jobs_directory/000/594\u2019;\n[ \u201cGALAXY_VIRTUAL_ENV\" = \"None\" ] && GALAXY_VIRTUAL_ENV=\"_GALAXY_VIRTUAL_ENV\u201d; _galaxy_setup_environment True\npython \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/set_metadata_KNv21u.py\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/registry.xml\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/working/galaxy.json\u201d \u201c/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_in_HistoryDatasetAssociation_806_uXnJC8,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_kwds_HistoryDatasetAssociation_806_hQcOzn,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_out_HistoryDatasetAssociation_806_gc_BfH,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_results_HistoryDatasetAssociation_806_adwo6t,/Dedicated/clingalproddata/database/files/000/dataset_671.dat,/Dedicated/clingalproddata/database/jobs_directory/000/594/metadata_override_HistoryDatasetAssociation_806_Hi6XQ8\u201d 5242880; sh -c \u201cexit $return_code\u201d\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:07:56,341 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) submitting file: /Dedicated/clingalproddata/database/jobs_directory/000/594/galaxy_594.sh\ngalaxy.jobs.runners.cli INFO 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) queued with identifier: submitted\ngalaxy.jobs DEBUG 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) Persisting job destination (destination id: LocalShell)\n128.255.162.194 - - [22/Apr/2019:15:07:59 -0500] \u201cGET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A55.000Z&qv=False&qv=False HTTP/1.1\u201d 200 - \u201c@link https://clinical-galaxy.iihg.uiowa.edu/?job_id=64c391652c2ec90d&__identifer=m1o72f2talq\u201d \u201cMozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:60.0) Gecko/20100101 Firefox/60.0\u201d\n[pid: 38337|app: 0|req: 8/8] 128.255.162.194 () {46 vars in 2172 bytes} [Mon Apr 22 15:07:59 2019] GET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A55.000Z&qv=False&qv=False => generated 1304 bytes in 36 msecs (HTTP/1.1 200) 3 headers in 124 bytes (1 switches on core 6)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:03,060 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job not found in batch state check\n128.255.162.194 - - [22/Apr/2019:15:08:03 -0500] \u201cGET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A59.000Z&qv=False&qv=False HTTP/1.1\u201d 200 - \u201c@link https://clinical-galaxy.iihg.uiowa.edu/?job_id=64c391652c2ec90d&__identifer=m1o72f2talq\u201d \u201cMozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:60.0) Gecko/20100101 Firefox/60.0\u201d\n[pid: 38337|app: 0|req: 9/9] 128.255.162.194 () {46 vars in 2172 bytes} [Mon Apr 22 15:08:03 2019] GET /api/histories/3be00b2dc330bf21/contents?details=21f51087e2f8c9cb%2C820e8a68e593cae6%2Cb864d31a7696bc50%2C6905cb35e1e81f5a%2C2cd348c1fb24f9ad%2C00caf7494de597ee%2C3c35cd92a39bfa13%2C8ec91fbec3795e2f%2C82db617418e6bc1b%2Cfbd2dc1fb21882db%2C83e581a4b150a907%2C37dcab24bd9ca28b%2Cc5afba4dc1e58b9c%2Cb98bcf8e380dadee%2Cf66341e9f43e26d6%2C02234c079b2b140d%2Ccf0b1d1cc4735367%2Cc40b1fbe0607aa09&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2019-04-22T20%3A07%3A59.000Z&qv=False&qv=False => generated 2 bytes in 28 msecs (HTTP/1.1 200) 3 headers in 124 bytes (1 switches on core 0)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,281 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) state change: from new to ok\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,281 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job execution finished, running job wrapper finish method\ngalaxy.jobs.runners ERROR 2019-04-22 15:08:06,286 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594/submitted) Job output not returned from cluster: [Errno 2] No such file or directory: \u2018/Dedicated/clingalproddata/database/jobs_directory/000/594/galaxy_594.o\u2019\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:06,406 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594/submitted) User killed running job, but error encountered during termination:\n"
    },
    {
      "role": "system",
      "text": "It looks like your submit() method is outputting the full qsub output. The cli runner splits stdout on whitespace and uses the last element as the job id (@link https://github.com/galaxyproject/galaxy/blob/2c6e99a4a0f2141f3c21321bb2bd770db6bee251/lib/galaxy/jobs/runners/cli.py#L105), which in your case appears to be the word submitted:\ngalaxy.jobs.runners.cli INFO 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) queued with identifier: submitted\ngalaxy.jobs DEBUG 2019-04-22 15:07:59,556 [p:38344,w:0,m:2] [ShellRunner.work_thread-0] (594) Persisting job destination (destination id: LocalShell)\ngalaxy.jobs.runners.cli DEBUG 2019-04-22 15:08:03,060 [p:38344,w:0,m:2] [ShellRunner.monitor_thread] (594/submitted) job not found in batch state check\n\nIf you update your submit() method to output just the SGE job ID that should get you past the issue you\u2019re seeing here."
    }
  ],
  [
    {
      "role": "user",
      "text": "@link https://huttenhower.sph.harvard.edu/galaxy/\nHi Sir/Madam,\nWhen I try Lefse analysis, I often get this kind of error \u201cJob ran longer than the maximum allowed execution time (runtime: 0:15:15, limit: 00:15:00), please try different inputs or parameters\u201d.\nCan you advise how to fix it?\nlefse error"
    },
    {
      "role": "system",
      "text": "User opinion:\nIs your input that big?\nMy lefse runs on hutlab are always done in ~5min. Using ~200 features.\nLook at this: Job and Tool Error Help (@link https://galaxyproject.org/support/tool-error/#type-execution-exceeds-maximum-allowed-job-run-time-walltime).\nAlso, remember that hutlab is a 3rd party server."
    }
  ],
  [
    {
      "role": "user",
      "text": "hello, I have loaded my sequences in galaxy because I want to do 16S Microbial Analysis with Mothur, and in the step of make.contings, the files are not created, a message that says \u201cThis job is currently running\u201d appears, many hours have passed but Many, I do not know if I\u2019m doing something wrong or it\u2019s normal, could you help me please?"
    },
    {
      "role": "system",
      "text": "Which Galaxy server are you using? In general bioinformatic tools can run a long long time, even days. This is dependent on the tool in question, the size of the input data size and how busy the server is. So I wouldn\u2019t worry and wait a little bit longer.\nYou also might want to have a look at our training material about 16S analysis with Mothur (@link https://galaxyproject.github.io/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019m trying to do GO analysis using goseq but I get this error:\nScreenshot 2023-07-22 123611349\u00d7630 71.6 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/11579d5fa2666665450d1bc9d91d72deff76ee80.jpeg)\n<<Warning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: paral>>\nas input I had this:\nScreenshot 2023-07-22 124256384\u00d7580 72 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/334f570b08162955ca0c42460109282c0f00feea.jpeg)\nScreenshot 2023-07-22 124140327\u00d7529 33.7 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0db6c67ce6ba6d096e4e91d7222b59fe8ed3ca01.jpeg)\nTo add more details :\nDetails\nExecution resulted in the following messages:\nFatal error: An undefined error occured, please check your input carefully and contact your administrator.\nTool generated the following standard error:\nWarning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: parallel\nAttaching package: \u2018BiocGenerics\u2019\nThe following objects are masked from \u2018package:parallel\u2019:\nclusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\nclusterExport, clusterMap, parApply, parCapply, parLapply,\nparLapplyLB, parRapply, parSapply, parSapplyLB\n\nThe following objects are masked from \u2018package:dplyr\u2019:\ncombine, intersect, setdiff, union\n\nThe following objects are masked from \u2018package:stats\u2019:\nIQR, mad, sd, var, xtabs\n\nThe following objects are masked from \u2018package:base\u2019:\nanyDuplicated, append, as.data.frame, basename, cbind, colMeans,\ncolnames, colSums, dirname, do.call, duplicated, eval, evalq,\nFilter, Find, get, grep, grepl, intersect, is.unsorted, lapply,\nlengths, Map, mapply, match, mget, order, paste, pmax, pmax.int,\npmin, pmin.int, Position, rank, rbind, Reduce, rowMeans, rownames,\nrowSums, sapply, setdiff, sort, table, tapply, union, unique,\nunsplit, which, which.max, which.min\n\nLoading required package: Biobase\nWelcome to Bioconductor\nVignettes contain introductory material; view with\n'browseVignettes()'. To cite Bioconductor, see\n'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\nLoading required package: IRanges\nLoading required package: S4Vectors\nAttaching package: \u2018S4Vectors\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\nfirst, rename\n\nThe following object is masked from \u2018package:base\u2019:\nexpand.grid\n\nAttaching package: \u2018IRanges\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\ncollapse, desc, slice\n\nAttaching package: \u2018AnnotationDbi\u2019\nThe following object is masked from \u2018package:dplyr\u2019:\nselect\n\nUsing manually entered categories.\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: goseq \u2192 reversemapping \u2192 [ \u2192 [.table \u2192 NextMethod\nAny help would be much appreciated.\nedit: from what I noticed, the order of the genes in the two inputs(Gene IDs and differential expression, and Gene IDs and length) is different , could this be the problem? and if yes, how can I edit them so both inputs have the same order of genes? edit2: I saw a video on YouTube where the genes on the two inputs had different order and it still worked"
    },
    {
      "role": "system",
      "text": "@quote\nI am usinggalaxy.embl.deand is not listed under \u201cAvailable at these Galaxies\u201d that the tutorial has, so maybe this explains the error\nYes, maybe, if the installed tool has some configuration problem or is running an older version of the tool wrapper. You could also try running this same data through at a supported server to compare.\nBut first \u2013 I would try removing the .N version from the two files next. Maybe the dot is causing a mismatch. After removing the version, confirm that all lines have a unique gene.\nYou could also do a check to make sure all the genes in the first file are actually in the second file."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have tried to use the Download Client to download an authorized dataset from EGA. I set up my EGA login and password in the Galaxy Preferences menu. I tried to use DESeq2 tool but in the \u201ccount file\u201d section I read \"(unavailable) EGA Download Client: authorized datasets. Why?\nThanks\nBarbara"
    },
    {
      "role": "system",
      "text": "Hi @user,\nhave you uploaded the count table into Galaxy? If yes, click at the name of this dataset in Galaxy history and check the datatype (format: text_string_to_check). Does it say tabular? If the datatyep (format) is anything but tabular, e.g., \u2018data\u2019 or \u2018txt\u2019, change it to tabular by clicking at Edit Attributes (pencil icon) > Datatypes tab in the middle window > select \u2018tubular\u2019 in the pull-down menu.\nJust in case: you can import data from a remote site into Galaxy directly using URL. Copy the URL on the remote site, usually with right mouse click > copy YRL or link, and paste it into Galaxy Upload menu, Paste/Fetch data section.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy community,\nI am trying to build database for Mycobacterium tuberculosis H37Rv using GFF annotation. I tried to use .fna and .gff files from ncbi but am getting following error:Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/539/24539611/_job_tmp -Xmx7g  -Xms256m\nException in thread \u201cmain\u201d java.lang.ArrayIndexOutOfBoundsException: 1\nat org.snpeff.SnpEff.parseArgs(SnpEff.java:947)\nat org.snpeff.SnpEff.cm\nIs this something related to my computer or am I doing something wrong?\nI also tried .fasta and .gff3 files but came back with the same result.\nAlternatively, I tried to snpEff download and received following error message: Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/539/24539793/_job_tmp -Xmx7g  -Xms256m\n00:00:00\tSnpEff version SnpEff 4.3t (build 2017-11-24 10:18), by Pablo Cingolani\n00:00:00\tCommand: \u2018download\u2019\n00:00:00\tReading configuration fil\nI wonder if anyone can help me get through this.\nThank you"
    },
    {
      "role": "system",
      "text": "Hi Sanjay\nNice to meet you. It sounds like there might be a problem with the data that you are trying to analyse.\nFirstly when it comes to M. tuberculosis, the snpEff database to use is Mycobacterium_tuberculosis_h37rv. This corresponds to the NCBI NC000962.3 genome (i.e. H37Rv). You can either download this (using the snpEff download tool) and use it from your history or just use the text string in your snpEff.\nI don\u2019t know what you are using for variant calling, but if you use snippy with Genbank reference input (i.e. the one from here @link https://www.ncbi.nlm.nih.gov/nuccore/NC_000962.3) it will run snpEff on your VCF for you automatically. This works for me with the 4.3.6+galaxy2 snippy tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "hello, I performed the differential analysis using the following pipeline, trimmomatic \u2192 kallisto quant \u2192 deseq . I get the volcano plot on the screen below using the volcano plot tool with Significance threshold = 0.05 .\njk\nthere is a lot of genes at the top,  what\u2019s wrong with it ?  if it\u2019s normal what\u2019s the interpretation of it ?"
    },
    {
      "role": "system",
      "text": "According to the DESeq2 results, you could hypothesize that those genes are the most relevant from a biological point of view (the ones whose differential expression is more significant from a statistical point of view). But anyway, I suggest you consider in the downstream steps all those genes whose Adjusted p-value is lower than 0.05.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was trying to run count.seqs on my data but got this error:\n\nUntitled748\u00d7354 11.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e7962d828e9de9d4fda8f83624449edb2a808879.png)\nWhat does this error mean? How do I fix this?\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user,\nsince you are using make.groups for generating the groups file, it is necessary to collapse the FASTA collection into a single file. Here you can see what I mean: test history (@link https://usegalaxy.org/u/gallardoalba/h/mothur-test-history).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello guys, please excuse my english by advance but its not my main language.\nSo Im trying to \u201cinvoke\u201d or \u201crun\u201d a tool of my galaxy instance using the bioblend package. I\u2019ve already tried this with a very simple tool without any problem but now im trying with a more complex tool : NCBI-blastp\nHere is the error I got:\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.09.37\nfrom the python code:\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.12.192236\u00d71020 233 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9ac5fa800f7f1b8e5c55ab24aa0113ec57f67a80.jpeg)\nHere im trying to get a query fasta file and a big database fasta from differents history. (everything has been uploaded successfully before), it seems to work with the query but not the database\u2026 im using those weird method \u201cset param\u201d, but maybe should I try it from another way?\nHere is the config XML file of my tool:\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.10.451922\u00d71428 428 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5a9ee3c75d1a7fec34abccbb07db3d36ca33692e.jpeg)\nedit : here is a detailed error report\n\nCapture d\u2019\u00e9cran 2021-05-30 \u00e0 18.46.431952\u00d71454 325 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b62b9a296f8e2ccadd1f4d479442679bb29102cd.jpeg)\nIts very strange, Subject database/sequences is locked on \u201cdb\u201d, but I\u2019ve set the param on \u201cfile\u201d\u2026\nI\u2019ve try everything :\u2019( do someone already did this with success ?"
    },
    {
      "role": "user",
      "text": "Hello, I finally did it by invoking a workflow with the tool inside instead of a tool alone, hope it will help someone in the future :slight_smile:\nCapture d\u2019\u00e9cran 2021-05-31 \u00e0 15.00.06\n\nCapture d\u2019\u00e9cran 2021-05-31 \u00e0 14.55.381048\u00d7298 31 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1f3ba35f64894e8227ad6fb2ddb2dd8d79bc3869.png)\n\nCapture d\u2019\u00e9cran 2021-05-31 \u00e0 14.50.04279\u00d7531 24.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cc391b97ba83414fe85627e5c86adcf29ed4d8c5.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello all, I am new to ChIPseq and I\u2019m having problems with Diffbind analysis. My control (WT) and treated (3xTG) samples was not assigned properly for differential analysis. Under the replicate column, Diffbind calls WT and 3xTG as replicate 1,2,3,4 instead of replicate 1,2 for both conditions (as shown in the attached picture 1). In addition, under the caller column, it shows as raw instead of bed. I have included narrowpeak bed file called from MACs as well as the BAM files (shown in picture 2).\n\nPicture 1742\u00d7645 12.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/60db679b6bf68687d9e3c23bbdd886edfb243bdf.png)\n\nPicture 21154\u00d7686 34.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9de2929ef6be118f70082fd42a1b7b3439148fcd.png)\nI would really appreciate if anyone could show me how to arrange my samples so that the output would be WT (replicate 1 and 2) and 3xTG (replicate 1 and 2). Thank you!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nThe column set for the score column is not appropriate for files from MACS2. Expand your dataset to find the correct column, probably c5, unless you manipulated the files?\nThe Galaxy version of the tool requires sample replicates.\nQuote from the tool form down in the help section, sometimes missed.\n\nNote this DiffBind tool requires a minimum of four samples (two groups with two replicates each).\n\nIf you want to do \u201cdata exploration\u201d with only two samples, that might be possible with the command-line version of this tool. But check before you invest time in that \u2013 some of the other tools by the same authors had changes that require the use of replicates now, too, at the source (so impact command-line versions). All Bioconductor tools wrapped for Galaxy have always required replicates. Three replicates per condition is the minimum ideal for scientific reasons but two is enough to satisfy the technical requirements. Bioconductor Forum (@link https://support.bioconductor.org/post/search/?query=diffbind)\n@quote\nMy control (WT) and treated (3xTG) samples was not assigned properly for differential analysis. Under the replicate column, Diffbind calls WT and 3xTG as replicate 1,2,3,4 instead of replicate 1,2 for both conditions (as shown in the attached picture 1). In addition, under the caller column, it shows as raw instead of bed. I have included narrowpeak bed file called from MACs as well as the BAM files (shown in picture 2).\nAll of this is due to how the underlying tool is creating data structures in R. Your screenshot of the input design from the Dataset Details view explains how the data is logically input to this tool, and is correct. The issue is with missing replicates \u2013 in this view that would show up as two datasets in each section, and is the same as how you would input the data on the tool form.\nThe order per group matters for the peak-bam pairings but don\u2019t worry about that yet. If you get a job failure once inputting replicates, you can read about this in the help section and reorder the datasets in the history (copy dataset > new history, done in the order that you want the tool to use them). Or, bundle and process the samples within collections, and this \u201corder\u201d will work out automatically and not be as tedious. The good news: this is only tool with that \u201cdataset order in the history\u201d wrinkle and it is because of the pairing of the inputs. There wasn\u2019t another good way to model the data back when the wrapper was written. You job didn\u2019t have replicates yet so the pairings were fine.\nResources\n\n@link https://training.galaxyproject.org/training-material/search?query=collections\n\n@link https://training.galaxyproject.org/training-material/search?query=ChIP-seq \u2013 note this tool is not specifically covered but other related tools are and the usage is similar\nSearch results for 'diffbind order:likes' - Galaxy Community Help (@link https://help.galaxyproject.org/search?q=diffbind%20order%3Alikes)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI\u2019m using galaxy to trim and align data from targeted  DNA-seq. Using BWA-MEM I\u2019ve seen that between built-in genomes there is not the hg38 canonical you can find in bowtie2.\nHow is it possible to use that genome version to assembly my data?\nThank you in advantage.,\nG."
    },
    {
      "role": "system",
      "text": "Ah, yes, you\u2019re right (sorry for not reading the question carefully the first time).\nhg38 should be identical to hg38 Full, but it\u2019s different from hg38 Canonical.\nSo, no, you cannot use hg38 Canonical as the reference in a bwa-mem alignment.\nThat said, you should also not do that anyway. Aligning against just canonical chromosomes can cause misalignments of reads that originate from non-canonical sequences, simply because there is no better match for them than a stretch of canonical sequence.\nThe better approach is to align against the full genome, then eliminate non-canonical mappings by filtering or by using the canonical genome during variant calling. Agreed, this can be tricky depending on downstream tools. Some variant callers, for example, may refuse to work with sequences mentioned in the input BAM header that are not found in the reference genome, in which case you would have to reheader your BAM dataset first.\nAll in all, my recommendation would be to rerun your bowtie2 jobs using the full hg38 version, not to look for solutions for making bwa-mem work with the canonical version.\nAt the same time, it\u2019s probably true that we should offer hg38 Canonical for bwa-mem if we do so for bowtie2 so thanks for bringing this up."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI am trying to run the second step of quantification (i.e. quantifier) following the mapper module of mirDeep2. In the mapping step everything is fine and I get the collapsed reads to be given to the quantifier module. Then, when trying to run the quantifier module, providing mature, precursor and star sequences, the run ends with an error which apparently lacks any suggestions on where the error is.\nHere the error:\n\nFatal error: Exit code 1 ()\nTool generated the following standard error:\ngetting samples and corresponding read numbers\nConverting input files\nbuilding bowtie index\nmapping mature sequences against index\nmapping read sequences against index\nMapping statistics\n#desc\ttotal\tmapped\tunmapped\t%mapped\t%unmapped\ntotal: 12382657\t343630\t12039027\t0.028\t0.972\nseq: 12382657\t343630\t12039027\t0.028\t0.972\nmapping star sequences against index\nanalyzing data\nExpressed miRNAs are written to expression_analyses/expression_analyses_galaxy/miRNA_expressed.csv\nnot expressed miRNAs are written to expression_analyses/expression_analyses_galaxy/miRNA_not_expressed.csv\nCreating miRBase.mrd file\nmake_html2.pl -q expression_analyses/expression_analyses_galaxy/miRBase.mrd -k dataset_21574970.dat -y galaxy  -o -i expression_analyses/expression_analyses_galaxy/dataset_21574970.dat_mapped.arf -j expression_analyses/expression_analyses_galaxy/dataset_21574968.dat_mapped.arf -l  -M miRNAs_expressed_all_samples_galaxy.csv\nmiRNAs_expressed_all_samples_galaxy.csv file with miRNA expression values\nparsing miRBase.mrd file finished\ncreating PDF files\n\nAny clues on what\u2019s going on here? I see a lot of unmapped sequences but this is somewhat expected from a biological point of view as the miRNA fraction is not usually the most abundant in the tissue I am investigating (zebrafish gonads)\nThanks\nLuca"
    },
    {
      "role": "user",
      "text": "Found out, there were mismatches between fasta identifiers in the provided miRNAs (precursors, star and mature)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am looking to find differentially expressed genes in the disease severity sample groupings within the cell type clusters (scRNAseq with scanpy) rather than across all clusters. Using two separate Strings in the groupby section yields an error, so how can this specific subgrouping be specified through a file?"
    },
    {
      "role": "user",
      "text": "Thank you, I have changed some of the \u201cAdvanced Settings\u201d inputs so the command running is the following. This might result in what I require. I need DE genes expression tables rather than plots.\nscanpy-find-markers --save diffexp.tsv --n-genes \u2018500\u2019 --groupby \u2018CoVID-19 severity\u2019 --method \u2018t-test_overestim_var\u2019 --use-raw  --groups \u2018celltype\u2019 --reference \u2018rest\u2019 --filter-params \u2018min_in_group_fraction:0.25,max_out_group_fraction:0.5,min_fold_change:2.0\u2019   --input-format \u2018anndata\u2019 input.h5   --show-obj stdout --output-format anndata output.h5"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m trying to run annotatemyid on my featurecounts table. I put as input ID type Entrez, but a error occurs .\nFatal error: Exit code 1 ()\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :\nline 2 did not have 8 elements\nCalls: read.table \u2192 scan\nCan anyone help me?\nThanks"
    },
    {
      "role": "system",
      "text": "Update 11/16/2022\nThe indexes are restored at @link http://UseGalaxy.org. Prior failed runs should now complete successfully.\nThanks to all who reported the issue for their patience!\n\n\nUpdate: This was a bug. I opened a ticket for the correction here if interested: annotateMyIDs (several versions) is not really ignoring header line when option set \u00b7 Issue #2319 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/2319)\nUntil that is completed, continue to remove header lines from any inputs to this tool, no matter which server or tool version you may be using."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI wonder if it is possible to use a .txt file as input parameter for instrument which accepts text field parameter. I want to use this option in a workflow: 1) get user\u2019s simple text input, 2) process this text with some instruments, 3) use the processed text as input parameter for the text field of other instrument. Can I do that?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI wonder if it is possible to use a .txt file as input parameter for instrument which accepts text field parameter.\nYes. Please see the functionality described in the tutorial below for the options.\nUsing Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/) \u2192 Using Workflow Parameters (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/workflow-parameters/tutorial.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m on Galaxy Main trying to run a simple workflow that takes paired-end RNAseq reads and turns them into a count table via a process of cutadapt \u2192 mapping using RNA STAR \u2192 featureCounts, along with some quality control steps on the way. The workflow is designed to take two paired-end RNAseq reads and a gene annotation file and can be found here: @link https://usegalaxy.org/u/kevin-ma/w/paired-end-rnaseq-reads-to-counts-table\nWhenever I press \u201cRun Workflow\u201d like in the following screenshot, the button briefly changes to \u201cSending\u2026\u201d for a fraction of a second, and then absolutely nothing happens and the button returns to \u201cRun Workflow\u201d. Nothing appears in my history or my Workflow Invocations tab. Please let me know if more info would be helpful, and any help is appreciated, thank you!\n\nScreen Shot 2021-03-18 at 11.55.34 AM1792\u00d71350 293 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2b533fb4f89eeb3105b43451da4c284187508bae.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you try it again?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have 24 untrimmed fastq files for RNA-seq. They are paired files. I have them in a data collection and run fastQC on them with no problem, but when I try to aggregate fastQC results with multiQC it only reports two files; forward and reverse, instead of each of the 24 individual files. What I\u2019m I doing wrong? I\u2019ve followed the tutorials and have set all parameters per those tutorials but keep getting the same result."
    },
    {
      "role": "system",
      "text": "Hi @user\ntry \u201cflatten collection\u201d from Collection Operation section on collection of paired reads before the FastQC step. Datasets in flatten collection have unique names.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, I am working on galaxy and have a genome file in genbank format. To use featurecounts for my RNAseq, I need to convert the genbank format to a GTF format because that\u2019s the format the featurecounts tool in galaxy expects. Now, I tried to convert thse genbank to GFF3 using GenbaktoGFF3 tool in galaxy and managed to do so. Also I tried to use the gffread tool to convert the GFF3 file to GTF but this resulted in a GTF with no written lines and I can\u2019t understand why.\nIs there a way to directly convert genbank to GTF in galaxy (or even outside galaxy, but preferably in galaxy). Also, any idea why gffread is not working properly in my case? I am using default parameters (leaving everything as is) and only giving GTF as feature file output and track name as \u201cword\u201d.\nThanks,"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit seems that this problem has been fixed recently, but the new tool version isn\u2019t available in @link http://usegalaxy.org yet; I have requested to update it; meanwhile, I suggest yo  use @link http://usegalaxy.eu, since gffread works fine in that instance.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nApologies to bother you again. Another Salmon quant question. While checking the output files, I noticed that the number of lines in the output files (\u201cGene quantification\u201d and \u201cQuantification\u201d) was inconsistent. For instance:\nimage\nimage\nI understand that \u201cGene quantification\u201d vs \u201cQuantification\u201d output files show different things (@link https://biostar.galaxyproject.org/p/28404/): one the counts for genes while the other for transcripts. While that could possibly explain different numbers of rows, I am somewhat puzzled by higher numbers of lines in the \u201cGene quantification\u201d file - I would expect more transcripts (and their versions) than genes.\nAlso, some files have the same number of lines: ~190,000.\nSecond thing, when I was trying to set-up the pipeline for my analysis and ran DESeq2, the PCA spaced the datasets a bit weirdly: I expected them to group by test condition but they didn\u2019t. Of course, sample size was too small for PCA (only 4 datasets in total) but having noticed this difference in line numbers, I now began to worry that maybe I ended up incorrectly or partially mapping/aligning/referencing the datasets and/or maybe they are partially counted by Salmon and/or partially compared when running DESeq2.\nThird concerning thing was that my transcript-to-gene table I tried when testing the pipeline earlier had ~270,000 lines (probably because of transcript versions). I dropped that when DESeq2 did not work and switched to ensembl cDNA reference in .gtf format (~3,200,000 lines).\nProblem is, I did not notice this issue with differences in numbers of lines when running the pilots (they were all ~190,000 lines, irrespective of which dataset or output file it was). And I am sure I used the same parameters for all the datasets - pilot and the remaining ones.\nWould appreciate your opinion on whether this is a problem and if so, how you would recommend to solve it.\nThanks!"
    },
    {
      "role": "system",
      "text": "@quote\nThe interesting thing is that the second thread also mentions some mismatches between the fasta and .gtf files. Is this something you have ever run into when doing your analyses?\nThis is likely your problem, too. And many posts at this forum are about format/content issues with either fasta, GTF, or both when used together. Tools are literal \u2013 any identifier that is compared between files needs to exactly match between all inputs.\nShort list of absolute requirements for Salmon \u2192 DESeq2\n\nTranscript fasta\n\n\ndid the file completely Upload to Galaxy? Truncated fasta is very hard to detect. So, just try to reproduce it by loading the file again and comparing\nhas correct formatting \u2013 try running it through the tool NormalizeFasta using the options to wrap at 80 bases and checking the box to remove \u201ceverything after the first whitespace\u201d\ndoes not contain any duplicated transcripts_id values \u2013 these are on the > title lines\nif you get the fasta from a data provider, all in one file, it might have problems the first and second will address but you won\u2019t need to worry about third\n\n\n\nGTF\n\nshould be from the same data provider where you sourced the fasta\nmake sure it is also completed Uploaded, and use \u201cauto-detect\u201d for the datatype. If Galaxy doesn\u2019t guess gtf there is a problem to fix\nthe most common problem is header lines # at the top. Remove the headers, then \u201credetect\u201d the datatype until Galaxy guesses gtf. If it won\u2019t be produced, then you don\u2019t have a GTF (maybe is GFF3 which will not work well?)\ninspect the identifiers in that file \u2013 does the transcript_id attribute exactly match those in the fasta (minus the >)?\ndoes the file contain gene_id attributes? are those different from the transcript_id attributes? if they are the same value, you need to find a GTF that has actual gene_id values.\n\n\n\nExecute all Salmon runs with the same fasta, along with the same GTF that you plan to use with DESeq2.\n\n\nThese FAQs cover the \u201chow-to\u201d. The UCSC hosted Ensembl genes track is probably what you want to use. Get the fasta from the Table Browser but get the GTF from their downloads area (only \u2013 not the TB).\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fasta-datasets\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#detecting-the-datatype-file-format\n\nI also added some tags to your post that will find related prior topics here."
    }
  ],
  [
    {
      "role": "user",
      "text": "Is it possible to use have workflow \u2018inputs\u2019 that allow multiple sample selection?\ne.g. First step of \u2018Workflow A\u2019 uses starSOLO, which allows selection of multiple lanes. Both R1 files (and both R2) are handled by a single run of the tool (rather than a batch job run twice).\nimage\nIf I set it up with \u2018input datasets\u2019 (below), upon running the pipeline I can select either a single input from a dropdown, or, hit \u2018multiple datasets\u2019 which creates a batch job. (\" This is a batch mode input field. Separate jobs will be triggered for each dataset selection.\"). Neither of which is what I want.\nimage\nIf, instead, I leave the inputs out as so:\nimage\nI get the \u2018ugly\u2019 full version of the inputs rather than the nice parameterised summary (its petty, but this matters to me, I\u2019m trying to make it nice for useability), but I can select the multiple files to run correctly.\n\nimage1110\u00d7786 128 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7764afad298fd33ab2714b30e698ab5996f53d03.jpeg)\nHowever, that has knockon effects. My \u201cWorkflow A\u201d is essentially a subworkflow to be called by \u201cWorkflow B\u201d. In workflow B, I cannot connect any input to the R1/R2 inputs of workflow A, so when I launch it there is no box for this input. If I hit \u2018expland to full workflow form\u2019, right now* I can\u2019t see any further options to expland inputs for this subworkflow.\n\nimage1253\u00d775 11.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/227969f26830835a7fbe2f026e9996996448c73b.png)\n\nI thought i had previously seen this to give the \u2018single dropdown\u2019 or batch selection options(so unable to run correctly), but that might have been when I had the subworkflow A set to take an \u2018input\u2019.\n\nI\u2019m a fairly new galaxy user, so understand I might not be coming at this from the right angle.\nWhat is the ~galaxy-way~ tm of structuring this kind of input?"
    },
    {
      "role": "system",
      "text": "@quote\nSo I changed the input type in my workflow too. And then created a \u2018collection input\u2019 (from the inputs menu). By default, it wouldn\u2019t let me connect via noodle (fair, it needs a collection of pairs). So I changed the type in the collection to \u2018list:paired\u2019, and then I could noodle that into the starsolo input.\nFor this part, when changing the input type, disconnecting all the downstream noodles, then reconnecting in the order of processing will \u201creset\u201d the workflow metadata. I\u2019m not sure if that was what you did. If not yet, try that first. It solves many workflow-runtime issues.\n@quote\nBut if I configure this in my workflow as below, I get two independant starSOLO runs, one per lane.\nIf still a problem after reconnecting the noodles and trying a rerun, would you please post back a screenshot of the setting you want to use on the tool form? The one you want translated into the workflow? And maybe the section of that same tool form within your workflow (the side panel)? Sort of what you posted originally. These visually provide a lot of useful information :slight_smile:\nI don\u2019t think this tool \u201cpools\u201d samples but let\u2019s make sure."
    }
  ],
  [
    {
      "role": "user",
      "text": "On my workstation I use winzip to unpack these.  I hoped to unpack these with the online tools, but none of the ziptools recognized the fq.gz files.  How can I unpack these here on galaxy?  Or is it possible to analyze and assemble gz.files unpacked."
    },
    {
      "role": "system",
      "text": "Yes, you can just work with fastq.gz datasets. If you really want to uncompress the data, you can click on the dataset\u2019s pencil icon, switch to the Convert tab and select the uncompressed format as Target datatype."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run Extract Genomic DNA on a bed file in which I have an ID, the start coordinate and the end coordinate. The sequences I am targeting represent products for potential primers (~180 of them total).\nThe Coordinate file set up:\nChrom            Start          End\nchr1000         184143     184871\nThe file I am matching it to is a set of ORFs that I extracted from a Blast search in the following format:\n\nchr1\tGTGCTCACCGCCGCCGCGNNNNNNN\u2026\nchr10     CTGGCTGCGGCGCACCTACGGCCTCACCGTCGC\u2026\nchr100   ATGGTGGCCACCGCTCCCCCGGCGAACCGGGCCAGGATGGCACAG\netc\u2026\n\nI have both files assigned to my custom build for the organism. When I try to run this, however, I get the error:\n181 warnings, 1st is: Unable to fetch the sequence from \u2018184143\u2019 to \u2018171\u2019 for chrom 'chr1000 '.\nSkipped 181 invalid lines, 1st is #1, \u201cchr1000 184143 184314\u201d\nI think I have made the formats of the chromosome names as identical as they get, but why will this still not function? I was wondering if it came to a matter of tabs vs spaces?\nAny ideas are helpful.Thanks!\nEDIT: I tried converting my bed file to tab delimited (just in case even though I saved it as tab-delimited) and tried again, and it still is saying all my lines are invalid."
    },
    {
      "role": "user",
      "text": "Thank you @system for the suggestions. I was able to resolve the issue by the following:\n\n\nSplit my large file containing the ORFs of interest (~47 000 sequences) into separate files each containing 500 sequences each\n\n\nUsed one of the files produced from the above step to find putative primers and generate a bed file with the coordinates (Used Primer3).\n\n\nCreated a custom build on galaxy using the file (same one as selected for Step 2) that I used to find the primers. This same file was also used as the reference genome for the Extract Genomic DNA search.\n\n\nAssigned the bed file and the reference genome to the custom build.\n\n\nSuccess running Extract Genomic DNA.\n\n\nI actually didn\u2019t need to normalize the data following procedure, but that may have been happy coincidence.\nThanks again for the suggestions!\nAndrea"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nThe most strange thing happened. I aligned a sample of raw RNA-seq data with HISAT2 (default settings). Then I used the function [Filter SAM or BAM, output SAM or BAM files on FLAG MAPQ RG LN or by region] to cut a 20 kb region that I am interested in. I don\u2019t have a lot of storage space. So far so good. But when I opened it in IGV, there were more than 20 kb, way more. I could see the entire chromosome. I checked two more chromosomes and they were empty.\nThis happened with only one sample but I have 9 from the same batch that are processing right now.\nI have followed this protocol before and nothing like this happened. The only time when I could see more outside my region, it was solely because the ends of the exon were exceeding it. Which made sense.\nDo you have any opinions about it?\nBTW I checked and the region that I selected was the right one."
    },
    {
      "role": "system",
      "text": "Please try modifying your regions term.\nCurrently is similar to:\n\nchrX: 1-1,000\n\nWhen should be written as a contiguous term, with no space after the :\n\nchrX:1-1,000\n\nor this (the inclusion of commas , is optional):\n\nchrX:1-1000\n\nWith the extra space included, the filter is malformed, and effectively ending up with an applied filter based on only the chromosome name (the first \u201cword\u201d in the term).\nTool form option and help:\n\nfilter-bam-select-regions.png1276\u00d7194 17.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/f35d1db3b1ea38178d9d6eb1477fc138107ffea7.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone\nI am trying to install locally galaxy, I am using conda 4.14.0 with ubuntu (20.04.4 LTS). The installation seems correct after running ./run.sh it seems everything went on fine, and after a while the installation freeze at \u2018\u2018solving enviroment\u2019\u2019, i left it like that for hours and nothing happened.\nI saw in other post that this problem was fixed using virtualenv:\nI installed virtualenv, desactivate conda, activate virtualenv, and the run the ./run.sh and\nI get this error\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [5 lines of output]\n      running egg_info\n      writing mercurial.egg-info/PKG-INFO\n      writing dependency_links to mercurial.egg-info/dependency_links.txt\n      writing top-level names to mercurial.egg-info/top_level.txt\n      Python headers are required to build Mercurial but weren't found in /usr/include/python3.8/Python.h\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n\nother try show this\nERROR: Could not install packages due to an OSError: [Errno 13] Permiso denegado: '/home/julia/galaxy/.venv/lib/python3.8/site-packages/zipstream'\nCheck the permissions.\n\n\n"
    },
    {
      "role": "system",
      "text": "First of all to install with conda you only have to activate your base environment and run galaxy.\nconda activate base\nsh run.sh\n\nIf something has gone wrong earlier it sometimes helps to starts over, and remove the automaticly created conda environment.\nconda remove --name _galaxy_ --all\n\nAnd/or remove the .venv folder\nrm -rf .venv\n\nAs far as I know you don\u2019t need additional packages if you do it like this. My earlier answer was specifically about a python/venv environment.\nThe log itself does not look necessarily wrong from on top of my head. How did you test if galaxy works or not?\nOn the following locations you could find logs that could help.\ngalaxy/galaxy.log\ngalaxy/database/gravity/supervisor/supervisord.log\ngalaxy/database/gravity/log\n\nIf you execute sh run.sh you will always see info lines on your terminal. If you want to start galaxy and run it in the background you need to do sh run.sh --daemon with the command sh run.sh --stop-daemon you can stop it again."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I tried installing SLURM on my Ubuntu 22.04 server by following the tutorial from the Galaxy Training platform (Connecting Galaxy to a compute cluster (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html)) and ran into the following error during the installation:\n\nfatal: [192.168.123.108]: FAILED! => changed=false\nmsg: \u2018Failed to update apt cache: E:The repository \u2018\u2018Index of /natefoo/slurm-drmaa/ubuntu (@link http://ppa.launchpad.net/natefoo/slurm-drmaa/ubuntu) jammy Release\u2019\u2019 does not have a Release file., W:Updating from such a repository can\u2019\u2018t be done securely, and is therefore disabled by default., W:See apt-secure(8) manpage for repository creation and user configuration details., W:@link https://repos.influxdata.com/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details., W:@link https://packages.grafana.com/oss/deb/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u2019\n\nI was wondering if SLURM will be getting a release file for jammy and if so, if this is already under development. If not, are there any alternatives supported by Galaxy which could be used in a production environment?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "This should be fixed as of version 1.0.2 of the galaxyproject.slurm role."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi I have a similar problem. When I am adding a control Bam file to my sequence I am getting 0 narrow peaks and 0 bed. When I do not have a control I am getting the results. Could you please help me?"
    },
    {
      "role": "system",
      "text": "This comment makes me think that the person who reviewed the data thought it was too sparse to call peaks, so they suggested less aggressive filtering. That is certainly something to try, but all of this is a balance. If you tried something like filtering only the control at 30 (strict), and treatments at 10 (permissive), that may even produce results, but the scientific quality of the result would be questionable, and in a different way than not including a control at all (which already did produce questionable results for one of you).\n@quote\nI have had Mapq of 30 before but changed it according to the bioinformatician\u2019s recommendation at uni. I didn\u2019t get any results with Mapq 30 either.\nIf there is a suspected data content problem, these are the items to examine:\n\nRun FastQC (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html) on the reads before and after other QA is applied (trimming, etc) and review << if the data doesn\u2019t appear to have sufficient quality it may not be usable\nReview the BAM mapping rates << this is likely where the problem first starts to impact the analysis, I\u2019m guessing low coverage\nReview the BAM alignments in a genome browser (details in original post + consider adding in an annotation GTF \u2013 UCSC has several for each mouse build in Galaxy)\nTry different filtering methods: mapQ is not the only item, getting rid of optical duplicates is also important (also in original post, plus see protocol advice at other forums \u2013 one example Did you remove ChIP-seq duplicates (@link https://www.biostars.org/p/318974/))\nUse the most current version of the genome build for the species (mm10, not mm9)\nAnd, sometimes using a mapper like BLASTN+, with a subset of reads as a query against a target database that contains reads from many species (WGS) can give some information: are the reads really from the species you think they were originally from? Is there contamination of some type?\n\nOther items like checking that the control and treatment samples are not mixed up is part of this.\nGet the processed reads from archives like SRA, and not the submitted reads, unless you understand how to manipulate the submitted reads into the current Illumina format that most tools know how to interpret. The processed fastq reads from SRA are already in the right format (fastqsanger). You can test this by allowing Galaxy to guess the format (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#datatypes) and also by running FastQC after the data is loaded (the format is reported).\nNot all public datasets are high quality, and that can impact the ability to use reads for analysis. Reads that you created yourself or obtained from a collaborator could also have quality/usability problems. This is a judgement call on your part, and the help above and in the original post are the ways to explore data for quality/performance with tools leading up to the problems with MACS2.\nSmall note: The @link http://usegalaxy.org server is undergoing maintenance. I wouldn\u2019t expect the problem in the history I reviewed to be impacted \u2013 but this can double checked by running the same analysis at a different usegalaxy.* server: @link http://usegalaxy.eu or @link http://usegalaxy.org.au."
    }
  ],
  [
    {
      "role": "user",
      "text": "I wat to view my dataset on UCSC browser. I know I am supposed to expand the dataset to see the \u201cDisplay at UCSC browser main\u201d option. But this option isn\u2019t showing up at all. Please what might be the reason for this?\nI have attached an image of the expanded dataset.\nThanks\nimage"
    },
    {
      "role": "system",
      "text": "Hi @user\nhere the issue is in format: tabular does not work with the browser. It is too generic. Galaxy handles datasets based on labels (datatypes) assigned to files. For example, a BED file with txt, tabular and bed datatypes is treated differently: there is no columns in txt file, tabular is not compatible with the browser, hence no UCSC GB and IGV links.\nBased on content on the first four columns, dataset #7 contains information about intervals. If it is BED, change the format (datatype) from tabular to bed: Edit attributes (pencil icon) > Switch to Datatype tab in the middle window and select an appropriate datatype in the pull-down menu.\nJust in case: start positions in BED are zero based. Make sure you compensate for this in generic data manipulation.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have an existing dataset from nanopore run in fast5 format. I\u2019m using GALAXY to convert it into fastq format. The tools which I\u2019ve trialled from the tools list are : 1) extract fastq in tabular format from a set of fast5 files and 2) extract read in fasta or fastq format from nanopore files.\nI got either 1 line or 0 bytes from these runs,no output.\nDoes anyone has any idea why? or which step did i miss or input the wrong thing?\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nThis error indicates that a tar archive was loaded and used for the input. For many cases when a tar archive is uploaded, only the first file in the archive is loaded. That first file can be an index, which won\u2019t contain any reads.\nThese tools are working at @link http://UseGalaxy.org using the tool\u2019s test data (raw data URL link (@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/poretools/test-data/poretools-in1.tar.bz2?raw=true)). Review to see if/how your input dataset differs and make adjustments as needed. The input was directly loaded by URL with the datatype \u201cautodetected\u201d. Galaxy | Accessible History | nanopore tar upload test (@link https://usegalaxy.org/u/jen-galaxyproject/h/nanopore-tar-upload-test)\nIf you get odd results again after adjusting the input (as needed), or if you cannot figure out what adjustments are even needed, please write back and specify:\n\nWhich public Galaxy server you are working at (server URL)? The topic was posted under the @link http://UseGalaxy.org category, but sometimes that is misassigned.\n\nOr, if using a different type of Galaxy deployment, please describe. Galaxy version, how/where sourced and installed, and the last date it was updated.\nUpdate your own Galaxy if needed. There are point-updates between full releases \u2013 and those are usually important to capture (tend to be bug fixes important enough to include before the next full release) Releases \u2014 Galaxy Project 20.09 documentation (@link https://docs.galaxyproject.org/en/master/releases/index.html). Please be aware that the next full release is expected to be published in the next week or so (if all goes as planned!). The pre-release 21.01 is already installed at @link http://UseGalaxy.org for integration testing \u2013 but don\u2019t install that version yet on your own server.\nHow and from where tools were installed also matters. The most current version should be sourced from the Galaxy Main ToolShed @link https://toolshed.g2.bx.psu.edu/. Install directly from the Admin area of the Galaxy application and use the built-in dependency resolution function for the simplest way to make sure the install is complete.\n\n\nWhat is the \u201cdatatype\u201d currently assigned to the dataset when the tools failed to produce output?\nWas that \u201cdatatype\u201d autodetected during Upload? Or did you assign it directly? What datatype is assigned if you try to \u201cdetect datatype\u201d under the function: pencil icon > Edit attributes > Datatypes? Does a rerun work with that datatype assigned (if it differs)?\n\nLet\u2019s start troubleshooting more from there :slight_smile:. We may ask for you to generate a share link to the history containing the work so we can review it in more detail and provide the best feedback. That can be posted into a branched direct message (we\u2019ll start one up \u2013 as you won\u2019t be able to yet since are new to this forum) \u2013 unless you are Ok with posting the share link publically in a reply to this topic.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI have local Galaxy installed. Also I used Data Manager for creating indexes.\n\nGalaxy - 19.05\nHISAT2 - 2.1.0+galaxy5\n\nI have just tried to run Hisat2 on @link http://usegalaxy.org with one of my files and it run successfully.\nI uploaded this file to galaxy:\n\n@link http://212.150.245.226/~mali/Galaxy/RNA_seq/a1.fastq\n\nThen  I run HISAT2 using this file as input (single end) fastq and hg38 as index , that\u2019s all, very simple\nIn @link http://usegalaxy.org it works, in local galaxy it runs forever\u2026\nI don\u2019t have any error, just eternal job. This only one thing that I can found this message:\n\nCould not display BAM file, error was:\nfile has no sequences defined (mode=\u2018rb\u2019) - is it SAM/BAM format? Consider opening with check_sq=False\n\nAlso I try the same run in console and notice the same behavior."
    },
    {
      "role": "system",
      "text": "Thanks for the clarification about your Galaxy, @user\nSo you have tested in both a GVL cloud Galaxy instance and a 19.05 local version of Galaxy, same problem (job errors)? But the same job works at the public Galaxy server @link https://usegalaxy.org?\nOne of these could be going wrong:\n\n\nThe genome was not fully indexed. Any genome should have Data Managers run in this order: Indexing reference genomes with Data Managers: Resources, tutorials, troubleshooting (@link https://help.galaxyproject.org/t/indexing-reference-genomes-with-data-managers-resources-tutorials-troubleshooting/1223/2). The cloud version of Galaxy would have this genome already indexed. In a local Galaxy, you\u2019ll need to do the indexing.\n\n\nYour data is in fastqsanger format and the first read has an exact human genome match (hg19, hg38), so the problem doesn\u2019t seem to be with either of those \u2013 unless you manually assigned the datatype to be fastqsanger.gz (compressed fastq). When the assigned datatype is a mismatch for the actual data content, errors are produced.\n\nTry running FastQC to see if it was fully uploaded. If that tool fails, the dataset was likely truncated upon Upload. Loading again is usually how to fix that type of problem. It doesn\u2019t seem like that original dataset is truncated, if it worked at @link http://usegalaxy.org. Be sure to use \u201cautodetect\u201d for the datatype assignment.\n\n\n\nFAQs: @link https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format? (@link https://galaxyproject.org/support/fastqsanger/)\nUnderstanding compressed fastq data (fastq.gz) (@link https://galaxyproject.org/support/compressed-fastq/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage348\u00d7740 27.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/769dda2a22d305c839ec5059081c186cf0745dda.png)\nThe uploader on the website is not taking in any files. Can anyone point me to the issue or help me why it is not working.\nThanks!"
    },
    {
      "role": "system",
      "text": "Update 11/09/2022\nThe Upload issues are now resolved. Anyone that had an issue similar to the other descriptions, please try running your jobs again.\nThanks to all for the feedback!\n\n\nHi @system and @user\nThanks for the extra information! This is exactly what I found, too. We will be looking into this in a few hours US time tomorrow. Updates will post back to this thread until resolved.\n\nUpload (local file browsing)\nIt looks like this at first using the Upload tool. Stalls for hours\u2026 I haven\u2019t had an error result yet, but it will probably end up that way the same as yours did.\n\nupload-tool-stalls2016\u00d71276 230 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/730c51defd6b03748733347b038d8ec8aa726333.png)\nRemote file loading (SRA example)\nThis did already fail with a server-side error.\n\nremote-file-errors574\u00d71208 63.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4c45e7d67de76eec137fc80b68c115fc9940c0f0.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI have to fill parameters manually into my workflow which are mostly file names and sample names. I have to select from about 7 files about 20 times and enter similar but slightly different sample names (e. g. wn0552.A7CE75785, wn0552.A7CE75785_tumor, wn0552.A7CE75785_normal) for another 20 times. As you may guess, it is time consuming, error prone and in general not exactly the kind of work i dreamed of.\nBut i suspect i can automate this process. If it were linux command line, all i need were echo, cat, ln etc., and some rarely used ascii symbols. However in galaxy i found only cat.\nSo essentially i want:\n\n\nTool that receives one file and sends it to the output intact so i could connect its output to all steps using some specific file and set its name only once. In linux it may be cat or cp, but i would prefer ln or even variable definition to save disk space and time.\n\n\nTool that receives some string and output it intact like echo.\n\n\nTool that receives two string and concatenates them like echo $a$b . So i could set my sample name to e. g. wn0552.A7CE75785, derive other important strings from it and use these strings in all my steps using \u201cadd connection to module\u201d.\n\n\nAnd also tool that gives the number of processors on current machine to set the number of threads in some tools.\n\n\nIs this possible in galaxy? Or maybe there is an easier way to automate workflows? Or i have to write wrappers for all these tools myself?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Please check out Using Workflow Parameters (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/workflow-parameters/tutorial.html#introduction), I think it covers all of this, minus the last part. If tools can use multiple threads or cores they can consume $GALAXY_SLOTS, which is set as appropriate based on the resources the admin has configured."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nIs it possible to tell galaxy not to delete temporary tool working directory?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Have a look at this\n@link https://docs.galaxyproject.org/en/master/admin/options.html?highlight=amqp_internal_connection#cleanup-job"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello guys,\nI am running EGSEA, and it was working fine until I started to run into errors.\nI have the file in tabular format.\n\n\n\n\nGeneid\nFFSSCLCR3\nFFSSCLCR2\nFFSSCLCR1\niPSC1\niPSC2\niPSC3\n\n\n\n\n2018\n2867.300115\n4384.063805\n3697.321171\n0\n0\n0\n\n\n196047\n2139.832269\n3254.124667\n2938.558938\n0\n0\n3.544768899\n\n\n27063\n6174.085968\n2953.349501\n6630.032231\n0\n7.378716416\n5.317153348\n\n\n\nAnd have a factor as tabular format\n\n\n\n\nSample\nCell Type\n\n\n\n\nFFSSCLCR3\nFFSSCLC\n\n\nFFSSCLCR2\nFFSSCLC\n\n\nFFSSCLCR1\nFFSSCLC\n\n\niPSC1\niPSC\n\n\niPSC2\niPSC\n\n\niPSC3\niPSC\n\n\n\nand my annotation as tabular as well\n\n\n\n\nENTREZID\nSYMBOL\n\n\n\n\n100287102\nDDX11L1\n\n\n653635\nWASH7P\n\n\n102466751\nMIR6859-1\n\n\n100302278\nMIR1302-2\n\n\n645520\nFAM138A\n\n\n79501\nOR4F5\n\n\n729737\nLOC729737\n\n\n102725121\nDDX11L17\n\n\n\nBut\u2026 when I tried to run EGSEA, it says\ngroupGOTerms: \tGOBPTerm, GOMFTerm, GOCCTerm environments built.\nEGSEA analysis has started\nLog fold changes are estimated using limma package \u2026\nlimma DE analysis is carried out \u2026\nEGSEA is running on the provided data and h collection\n\u2026globaltestcamera.ora*\nEGSEA is running on the provided data and c5 collection\n\u2026cameraglobaltestora*\nEGSEA is running on the provided data and gsdbgo collection\n\u2026globaltestcamera.ora*\nEGSEA is running on the provided data and kegg collection\n\u2026globaltestcamera.ora*\nEGSEA analysis took 12.34 seconds.\nEGSEA analysis has completed\nEGSEA HTML report is being generated \u2026\nReport pages and figures are being generated for the h collection \u2026\nHeat maps are being generated for top-ranked gene sets\nbased on logFC \u2026\nError in plotHeatMapsLogFC(gene.sets = gsets.top, fc = logFC, limma.tops = limma.tops,  :\nAll featureIDs in the gs.annot list should map to\na valid gene symbol\nCalls: egsea.cnt \u2192 egsea\nand tried to troubleshoot, but I gave up after three days of trying\u2026\ncan anyone help me to figure out what I am doing wrong here?\nThank you in advance"
    },
    {
      "role": "system",
      "text": "@system @user\nThe problem is reported in the error message here:\n@quote\nError in plotHeatMapsLogFC(gene.sets = gsets.top, fc = logFC, limma.tops = limma.tops, :All featureIDs in the gs.annot list should map toa valid gene symbol\nTool form instructions:\n\nSymbols Mapping file\nA file containing the Gene Symbol for each Entrez Gene ID. The first column must be the Entrez Gene IDs and the second column must be the Gene Symbols. It is used for the heatmap visualization. The number of rows should match that of the Counts Matrix.\n\nChecking that both datasets contain the same number of rows is a basic check, but you could also compare the IDs and make sure they are a match. You might also need to adjust the header lines in the files \u2013 these deviate from the example data.\nSome tools have built-in expectations for how the data are labeled in headers that the Galaxy wrapper around the tool cannot auto-adjust. This can lead to spurious error messages. So, I\u2019d recommend following the same header labeling as in the examples just to eliminate that from being a problem, too. The sample names can differ of course, but columns of data representing gene/symbol information could have standardized labels in the first row.\nUsing the most current version of any tool is also usually important, to capture bug fixes and wrapper enhancements, and to make further troubleshooting easier. If you are not working at a usegalaxy.* server, you might want to try the tool at one of those as a comparison. @link http://UseGalaxy.org and @link http://UseGalaxy.eu are the best choices \u2013 @link http://UseGalaxy.org.au usually follows the same server (but not cluster) configuration as @link http://UseGalaxy.eu so testing/comparing between those is not normally needed.\nIf the tool still fails after those adjustments are made/confirmed, please confirm:\n\nThe URL of the usegalaxy.* server where you tested.\nThe complete tool name and version \u2013 find this info at the top of the tool form.\n\nLet\u2019s start there. We may ask for a history share link to help more, and that can be posted back if you are not concerned about keeping the data private, or let us know if you do want to keep it private and a moderator will start a private message thread for sharing. If there is some bug with the tool, we can help to sort that out versus a usage issue."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I launched a few assembly analyses with Unicycler on Monday and they are still queued (in grey).\nThe ones I did last week took no more than 2 hours, so I\u2019m starting to wonder if there is an issue here or a maintenance that prevents the analyses to keep going?\nOf note, I have been running other analyses (with QUAST) and they completed in minutes.\nThanks\nVanina"
    },
    {
      "role": "system",
      "text": "@user Please try a rerun. For any job failure, at least one rerun is recommended to eliminate cluster issues.\nIf those fail, then check the error messages and the inputs. There might be a problem that could be fixed, or the data could be too large to execute on the public server.\nThis FAQ helps with identifying and addressing the most common data input problems that lead to tool errors:\n\nMy job ended with an error. What can I do? (@link https://galaxyproject.org/support/tool-error/)\n\nMore comprehensive help, including links to GTN tutorials, is here. Unicycler is covered in a GTN tutorial under the topic \u201cAssembly\u201d.@quote\nCheck to see if there is a known usegalaxy.* server issue. \n\nServer Status:https://status.galaxyproject.org/If the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Greetings!\nI want to study ribosomal RNA expression and potential processing sites in bacteria. To such effect, I want to perform an RNA-Seq experiment in order to analyze potential rRNA transcripts isoforms and processing sites.\nConventional RNA-Seq experiments are not designed for the analysis of rRNA expression and generally include a RNA depletion step, which reduces rRNA signal. Notice rRNA in bacteria accounts for ~90% of total RNA.\nIn the context of studying rRNA processing, would the RNA-Seq approach be appropriate? If so, what rRNA treatment would you recommend to study rRNA expression levels and processing?\"\nThank you in advance."
    },
    {
      "role": "system",
      "text": "Sorry @user, I didn\u2019t express it properly; the cited protocol can be used for extracting total RNA, not just rRNA. Any protocol for RNA extraction will be fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "Jennaj,\nI am running into problems with the Unique.seq tool in Galaxy.  I use Galaxy online to teach a course and we use the 16S Metagenomics (Mouse) dataset as the training module. I have been using this approach for a couple of years now, but we have run into a Fatal Error when running the first step of the Unique.seq tool.  We have tried to run Unique.seqs from multiple accounts (students and mine), but always using Galaxy online. I have tried it from our computers at work (faster internet) and from home. Again, over 400 students in the class have done this over the years and we have not encountered the problem until now. Any suggestions? Belwo is the error messages we get from Unique.seq.  There is an error for each of the three output files from Unique.seqs.\nAn error occurred with this dataset:\nformat mothur.names database ? (@link https://usegalaxy.org/datasets/f9cad7b01a4721357efd443d15d14e99/details#)\ne[He[J"
    },
    {
      "role": "system",
      "text": "Update April, 2023\nThe tool has been adjusted. Please rerun prior technical failures.\nThanks to everyone who reported the issue, and to all that helped fix it :partying_face:\n\n\nUpdate:\nThe Unique.seqs tool has a bug at @link http://UseGalaxy.org (only).\nDetails are here Mothur Unique.seqs -- fails when an optional input is not provided at ORG \u00b7 Issue #5163 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/5163)\nThe choices are to use @link http://UseGalaxy.eu instead, or to supply the optional input until the fix is live on the other server.\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have started managing my own Galaxy instance using a virtual machine (virtualbox, not docker). I would like to add an administrator and a user, but I cannot use any SMTP server. How can I register those two users?\nBest."
    },
    {
      "role": "system",
      "text": "SMTP server has nothing to do with user registration, you do not need it. You can go to the URL where your Galaxy is running and click on \u2018Register\u2019 in the top menu."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nThe FATSQC job has been waiting to run for a while and I am not sure why. I have added the link to my history below.\nurl: https://usegalaxy.org/u/nourmahfel/h/chip-seqfinal (@link https://usegalaxy.org/u/nourmahfel/h/chip-seqfinal)"
    },
    {
      "role": "system",
      "text": "\nMy jobs aren\u2019t Running! (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running)\nUnderstanding job states (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses)\nWill my jobs keep running? (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#will-my-jobs-keep-running)\n\n\nThe most effective strategy is to queue up analysis jobs, then to let them complete.\n\nPublic servers are shared resources. Jobs will run when resources become available.\nQueue and execution time depends on the tool and how busy the server is.\nPlease avoid deleting and rerunning jobs in hopes that they will run quicker. It has the opposite result: the new job is added back at the end of the queue extending wait time.\nA wait of a few hours to a few days can be normal for computationally expensive tools.\n\n:rocket: Tool forms and workflows include options to send yourself notifications by email.\n\n\nDataset Collections and Workflows are worth learning about.\n\n\nMost Galaxy Training Network (@link https://training.galaxyproject.org/) (GTN) tutorials include at least one associated Workflow to get your started.\n\n\nNot sure how to use, create, import, or customize Collections and Workflows?\n\nUsing Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\nA collection of microtutorials explaining various features of the Galaxy user interface and manipulating data within Galaxy.\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a problem while performing a Deseq2 in galaxy web.\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in read.table(file = file, header = header, sep = sep, quote = quote,  :\nno lines available in input\nCalls: get_deseq_dataset \u2192 lapply \u2192 lapply \u2192 FUN \u2192 read.table\n\nScreenshot_2022-10-31-16-04-00-06720\u00d71105 121 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/728637543130593a258cdea0919572c2ed27795a.jpeg)\n\nScreenshot_2022-10-31-16-05-18-67720\u00d7902 125 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f185d41805a696784212aef11010c547f327b5c9.jpeg)"
    },
    {
      "role": "system",
      "text": "@quote\nWhy they were empty i don\u2019t understand.May I proceed to next Deseq2 step without selecting Control 2&4 as input?\nYou should find out why there are no results from the upstream tool. Maybe it can be fixed.\nWhether to drop datasets from the analysis is a scientific decision. Just keep in mind that this tool technically requires at least two factor levels and at least two count files per factor level. With a minimum of three per factor level considered best to generate meaningful results."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nWe are trying to analyze our files following this RNA Seq tutorial: @link https://galaxyproject.org/tutorials/nt_rnaseq/\nOur DESeq2 outputs, however, are giving us MSTRG gene IDs, which are not very useful as the labels are only relevant internally. In researching how to convert MSTRG to a gene name, we found that a better \u201creference file to guide assembly\u201d should be used to run StringTie so a gene name is outputted. However, the galaxy StringTie job fails when we use the Homo_sapiens_NCBI_GrCh38.tar.gz reference file. We have tried unzipping it to .tar and also using a DEXSeq annotation as the reference file, then ran StringTie, but this also failed. Also, the option to use a built in reference file is disabled. Currently, the only method that successfully outputs a DESeq2 file is when no reference file is used, however we are unsure how to convert the MSTRG gene ID it outputs to a gene name/prevent MSTRG outputs.\nAny help is greatly appreciated; thanks in advance!\n-Preformatted textAnanya"
    },
    {
      "role": "system",
      "text": "Hello @user @system\nSorry the original question got missed.\nThis FAQ explains how/where to get a human reference annotation dataset that will work with these tools: @link https://galaxyproject.org/support/diff-expression/\nI also added some tags to your post that includes other Q&A about this. In short, you need a GTF dataset that matches the UCSC version of the genome/build (if you are mapping against the built-in genome indexes). Correct format matters or tools will fail. The FAQ and linked FAQs explain with full details and common sources.\nGRCh38 is the same genome as UCSC\u2019s hg38.\nGRCh37 is the same genome as UCSC\u2019s hg19.\nBut the \u201cbuild\u201d may differ between sources. Check your chromosome identifiers and make sure they are a match.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "gives an error message: Uncaught exception in exposed API method"
    },
    {
      "role": "system",
      "text": "Update\nThe account registration in function has been restored at @link http://UseGalaxy.org.\nFor everyone reading, please see the banner on the server and let us know about any other odd behavior.\n\nGalaxy has recently been updated to a pre-release of the next Galaxy stable release (22.05), and you might encounter problems as we iron out the wrinkles. Please report any issues you encounter using the bug icon on error (red) datasets, or email @link mailto:galaxy-bugs@galaxyproject.org.\n\n\nThanks.\nHave you tried again yet? If not, please do.\nThe server was undergoing maintenance and was just updated again later today to the 22.05 pre-release. This function is working from tests.\nIf you have more problems, please explain a bit more about your situation.\nCorrection \u2013 registration is failing with this error at @link http://UseGalaxy.org. We are looking into the issue. Updates will post back here.\nFor now, you can work at @link http://UseGalaxy.eu and/or @link http://UseGalaxy.org.au. Having one account at each public server is common and grants access to distinct resources.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nAfter updating my (Ubuntu 22.04) server, I suddenly get the following error when running the ansible-playbook galaxy.yml command:\n\nTASK [galaxyproject.nginx : Set additional config options] ***********************************************************************************************************\nfatal: [192.168.123.108]: FAILED! => changed=false\n  checksum: 3cb6cfac4f721ecabef8f03be6774942e82eeb0f\n  msg: Destination directory /etc/nginx/conf.d does not exist\n\n\nI never got this error before the update. After checking the /etc/nginx directory, I found out that the directory was completely gone. I ran the ansible-galaxy install requirements.yml command again just in case, but this ofcourse didn\u2019t change anything. I am stuck not knowing where to go from here. Any help is appreciated!"
    },
    {
      "role": "user",
      "text": "The error was ultimately fixed by running the following commands as root (sudo su):\napt-get purge nginx nginx-common nginx-full\n\nFollowed by:\nansible-galaxy install requirements.yml\n\nAnd finally:\napt-get install nginx\n\nIt is still unknown what caused the error, but this is what fixed it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI would like to create an authentication system with username and password (without storing the email of the user, for security reason). So I just send by mail the username and password to the user that contact me to create the account (but i don\u2019t store the email itself). Is there a way?"
    },
    {
      "role": "system",
      "text": "The email field of a user record is required and must be unique, but is only required to be valid for the following scenarios:\n\nYou have enabled new account activation by email in the Galaxy config.\nResetting forgotten passwords.\nBug reporting: the user email is used as the return address (but users can specify a valid email with the bug report if they wish.\nFeatures such as emailing on workflow step completion (there\u2019s no workaround for this).\n\nThere may be cases I\u2019m forgetting, but most of Galaxy\u2019s functionality should work fine without a valid email address."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to  input a file from kallisto and I keep getting a key input error on the annotate my IDs function. My headers are called target_id_eightfiveoneseven and est_countseightfiveoneseven. The file has two rows; the first column  is the transcript ID from ensembl and the second is the est count column. The transcript names look odd for example one looks like ENST00000631435.1. Even though I select the ensembl transcript ID it still doesn\u2019t seem to recognize the input.since I get a key error.\nError in .testForValidKeys(x, keys, keytype, fks) :\nNone of the keys entered are valid keys for \u2018ENSEMBLTRANS\u2019. Please use the keys method to see a listing of valid arguments.\nCalls: select \u2026 select -> .select -> testSelectArgs -> .testForValidKeys\nMy table headers are unique and are called target_id_eightfiveoneseven and est_counts_eightfiveoneseven.\nDoes anyone spot errors in the names of the input files?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hello @user\nTry removing the .N (where N is some number) from the identifiers.\nTool choices include: Replace parts of text\n@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to ask help for the following problem with a workflow: I need to connect two tools. The first it outputs a list of files and I would like to connect it to a second tool that needs in input only a specific file from the list generated from the previous tool. Is there a way to do it, or a link to a comprehensive documentation on the workflows?\nHave a nice day, and many thanks."
    },
    {
      "role": "system",
      "text": "Then you can use the Extract dataset from a list (@link https://usegalaxy.eu/tool_runner?tool_id=__EXTRACT_DATASET__) tool. This tool doesn\u2019t set the datatype in advance, so if you\u2019re connecting this to a downstream step you also need to change the datatype to a compatible datatype.\n\nScreen Shot 2019-07-17 at 14.51.47.png951\u00d7810 69.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/3c74ddb089c8ed3ec76b9eeb093caad2d2f392a6.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want my local Galaxy instance allow all users(anonymous and registered) free to browse around and see what\u2019s available without registering, but only the registers can upload data or run jobs.\nAccording to @link https://github.com/bgruening  suggestion, I could set limits in my job_conf.xml file and configure resources for users that are not registered. Below is the configuration of my job_conf.xml file, but it doesn\u2019t work. Could anyone help me see what went wrong?Thank you.\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n    </plugins>\n    <destinations>\n        <destination id=\"local\" runner=\"local\"/>\n    </destinations>\n    <limits>\n        <limit type=\"registered_user_concurrent_jobs\">2</limit>\n        <limit type=\"anonymous_user_concurrent_jobs\" id=\"local\">0</limit>\n    </limits>\n</job_conf>\n"
    },
    {
      "role": "system",
      "text": "It looks like there may be a bug with setting the limit to 0. In lieu of this, you can also create a dynamic job rule (@link https://docs.galaxyproject.org/en/latest/admin/jobs.html#dynamic-destination-mapping-python-method) that will cause any jobs submitted by anonymous users to fail.\nThe job configuration would look like:\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n    </plugins>\n    <destinations default=\"require_login\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"require_login\" runner=\"dynamic\">\n            <param id=\"type\">python</param>\n            <param id=\"function\">require_login</param>\n        </destination>\n    </destinations>\n    <limits>\n        <limit type=\"registered_user_concurrent_jobs\">2</limit>\n    </limits>\n</job_conf>\n\nAnd then create a file <galaxy_root>/lib/galaxy/jobs/rules/require_login.py with contents:\nfrom galaxy.jobs.mapper import JobMappingException\n\n\ndef require_login(user_email):\n    if user_email is None:\n        raise JobMappingException('Please register an account to use this tool')\n    return 'local'\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have recently installed a local version of Galaxy and installed FreeBayes 1.3.6 from the tool shed. After indexing the genomes hg19 and hg38 as explained in many posts, I ran HiSat2 to generate the BAM files for input in to FreeBayes (version 1.3.6). This version of FreeBayes fails to recognize the BAM files generated by HiSat2 and gives the following error: \u201cSequences are not currently available for the specified build\u201d. The @link http://Galaxy.eu server version of FreeBayes is able to identify and process the BAM files generated by HiSat2 without any issues.\nWhen I downgrade the FreeBayes version to 1.3.1 on the local instance of Galaxy, FreeBayes is able to recognize the BAM files and proceeds to process them. I am wondering if this anomalous behaviour is related to the latest version and how can I fix it (other than downgrading the FreeBayes version)?\nThe FreeBayes version that I am using on the @link http://Galaxy.eu server is 1.1.0.46\nThank you"
    },
    {
      "role": "system",
      "text": "@quote\nindexes were built on the Ensembl human genome fasta file (hg38) downloaded separately\nThe problem is probably with the format of that fasta. Specifically, the > lines don\u2019t match what UCSC used, so sequences lengths cannot be found. That is already part of the \u201ccomes with Galaxy\u201d master list of dbkeys (includes length-per-identifier).\nEither get that fasta\n\nfrom UCSC with the DM tool (the original source of hg38) by starting over for that specific dbkey\nor use CVMFS\nor give the Ensembl version of the assembly a distinct dkbey\n\nFor any, you\u2019ll need to strip out your existing indexes (most of that is trial and error) or start over with a fresh Galaxy install first.\n@quote\nJennifer Hillman Jackson\nThis is me :sweat_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nPlease I want to call the peaks from bigwig file, so I converted it to bedgraph formate, and run MACS, it worked really fine and I got the output, however suddenly it turned red and gave me this message:\nThe job creating this dataset has been resubmitted\n NFO  @ Sun, 02 Jun 2019 12:45:55: \n# Command line: callpeak -t /data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat --name bigWigToBedGraph_on_data_13 --format BAM --gsize 2700000000 --keep-dup 1 --qvalue 0.05 --mfold 5 50 --bw 300\n# ARGUMENTS LIST:\n\nI have no idea what happened! and how to fix it!\nCould you please help me!\nThank you in advance!\nSham"
    },
    {
      "role": "system",
      "text": "Hi @user\nWhenever you have errors like these, we recommend submitting a bug report. That\u2019s often a fast way to get help.\nLooking at the (i) information icon, and checking the stderr and stdout, I see the following.\nINFO  @ Sun, 02 Jun 2019 12:45:55: \n# Command line: callpeak -t /data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat --name bigWigToBedGraph_on_data_13 --format BAM --gsize 2700000000 --keep-dup 1 --qvalue 0.05 --mfold 5 50 --bw 300\n# ARGUMENTS LIST:\n# name = bigWigToBedGraph_on_data_13\n# format = BAM\n# ChIP-seq file = ['/data/dnb02/galaxy_db/files/009/613/dataset_9613622.dat']\n# control file = None\n# effective genome size = 2.70e+09\n# band width = 300\n# model fold = [5, 50]\n# qvalue cutoff = 5.00e-02\n# Larger dataset will be scaled towards smaller dataset.\n# Range for calculating regional lambda is: 10000 bps\n# Broad region calling is off\n# Paired-End mode is off\n \nINFO  @ Sun, 02 Jun 2019 12:45:55: #1 read tag files... \nINFO  @ Sun, 02 Jun 2019 12:45:55: #1 read treatment tags... \nException struct.error: 'unpack requires a string argument of length 4' in 'MACS2.IO.Parser.BAMParser.tsize' ignored\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/bin/macs2\", line 4, in <module>\n    __import__('pkg_resources').run_script('MACS2==2.1.1.20160309', 'macs2')\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 750, in run_script\n    self.require(requires)[0].run_script(script_name, ns)\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1534, in run_script\n    exec(script_code, namespace, namespace)\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 617, in <module>\n    \n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 57, in main\n    \n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/MACS2/callpeak_cmd.py\", line 73, in run\n  File \"/usr/local/tools/_conda/envs/mulled-v1-9745eaf08708c75d5cb939b005b2fbbcc13ccefcb86602af9e1bf012d47be274/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/MACS2/callpeak_cmd.py\", line 399, in load_tag_files_options\n  File \"MACS2/IO/Parser.pyx\", line 880, in MACS2.IO.Parser.BAMParser.build_fwtrack (MACS2/IO/Parser.c:13703)\n  File \"MACS2/IO/Parser.pyx\", line 893, in MACS2.IO.Parser.BAMParser.build_fwtrack (MACS2/IO/Parser.c:13111)\n  File \"MACS2/IO/Parser.pyx\", line 877, in MACS2.IO.Parser.BAMParser.get_references (MACS2/IO/Parser.c:12886)\nstruct.error: unpack requires a string argument of length 4\n\nmaybe this is helpful to you?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am new at using Galaxy and trying to follow the tutorials of Immune Repertoire pipeline and SHM & CSR pipeline using tutorial test data sets. But the analyses ended up with some errors as below. How can I fix these??\nThanks,\nERROR from Immune Repertoire pipeline:\ncat: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/samples.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 55: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/productive_counting.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 149: /var/lib/galaxy/database/jobs_directory/008/8936/dataset_656640_files/AAMedianBySample.txt: No such file or directory\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 344: 7za: command not found\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/argalaxy_tools/33412e85e669/argalaxy_tools/report_clonality/r_wrapper.sh: line 350: 7za: command not found\n\nERROR from SHM & CSR pipeline:\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/shm_csr/f387cc1580c6/shm_csr/baseline/script_imgt.py:20: DeprecationWarning: 'U' mode is deprecated\n  with open(args.ref, 'rU') as ref:\n/var/lib/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/davidvanzessen/shm_csr/f387cc1580c6/shm_csr/baseline/script_imgt.py:20: DeprecationWarning: 'U' mode is deprecated\n  with open(args.ref, 'rU') as ref:\n"
    },
    {
      "role": "system",
      "text": "Then you may get a better and faster response if you contact that maintainer. The error looks like some technical issues but I can never be sure."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nFollowing the tutorial \u201cFrom peaks to Genes\u201d I have an error uploading data with UCSC Main. The error is: The uploaded file contains invalid HTML content.\nThe settings used are those:\nimage1280\u00d7747 43.7 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/57301bc077971d528183449c04cff89c780f560f.png)\nimage1280\u00d7638 31.5 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/93a15496817cf786e58fa5ba70890cdf69c2cadd.png)\nThanks for any help!"
    },
    {
      "role": "system",
      "text": "Update:\nAs of today, UCSC has rolled back the change and all Table Browser queries should work as they do normally. Meaning, no need to download/upload as separate steps.\nThe change will now be implemented in about two weeks. It involves some server-level modifications to allow the direct data transfer. How end-users experience these functions will not change.\nThanks to everyone to reported the problem & helped to get it solved!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, all there,\nI\u2019m trying to build a local galaxy that anybody can access with different devices on the same internet, so I set the uwsgi part of galaxy.yml as \u2018http: 127.0.0.1:8080\u2019, then it showed :\n#####################################\nGalaxy server instance \u2018main.web.1\u2019 is running\nStarting server in PID 9063.\nserving on @link http://127.0.0.1:8080\n#####################################\nBut with this setting, we could not access galaxy from the web browser on any other PC.\nCould you give me some advice on how to set of my galaxy or linux environment?"
    },
    {
      "role": "system",
      "text": "Generally your computer has to be reachable over the network for the other devices to be able to see your services like Galaxy. That means it has to have an IP address that the others know and can send requests too. This is very much out of scope of what Galaxy community can help you with since we have no knowledge of your network\u2019s topology. I recommend you contact your network administrator and ask for help."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am attempting to follow the Galaxy tutorial for reference-based RNA-sequencing (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#visualization-of-the-differentially-expressed-genes).\nUsing Galaxy, I have successfully (1) extracted the most DE genes from the normalized counts table and (2) extracted the normalized counts into a table. Currently, I am attempting to visualize my differential expressed genes. I have been unable to plot the heatmap with my normalized counts using the tool heatmap2. I could not find others in the community with a similar issue. Any help would be greatly appreciated!\nI am using the tool heatmap2 (toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_heatmap2/ggplot2_heatmap2/2.2.1+galaxy1).\nParameters used were:\nData transformation - Log2(value) transform my data\nEnable data clustering - True\nLabeling columns and rows - Label columns and not rows\nColoring groups - Blue to white to red\nData scaling - Do not scale my data\nError Details\nTool generated the following standard error:\nFatal error: Exit code 1 ()\nAttaching package: \u2018gplots\u2019\nThe following object is masked from \u2018package:stats\u2019:\nlowess\nError in hclust(x, method = \u201ccomplete\u201d) :\nNA/NaN/Inf in foreign function call (arg 11)\nCalls: heatmap.2 -> hclustfun -> hclust"
    },
    {
      "role": "system",
      "text": "I met the same error and changed the parameter \u201cData transformation\u201d to - Log2(value+1) transform my data. Finally it plotted the heatmap successively. I think there should be \u201czero\u201d in your data too. So after \u201cLog2(value)\u201d it got the Inf value in the data.\n(Sorry for my poor English.)"
    }
  ],
  [
    {
      "role": "user",
      "text": "If I download a file, fasta or zip of lets say 3GB the downloaded file is always max 1GB. Where do I need to look to solve this?\nI tried to add proxy_max_temp_file_size 0; to my nginx.conf but that is not working.\nIn my galaxy.log I get the following error:\nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n\nEDIT:\nI am still searching for a solution (and a cause) so here an update. My problem is not solved yet.\nAt the moment I have two galaxy servers running, both a different server. Using x-accel-redirect solves my problem only for one of the servers. The difference between them is that on the one were it does not work the database/files folder is located on a mounted volume. If I click the save icon to download some files I get a sort of page not found error, I will check the actual error in a moment because it is slightly different. I already found that it can have to do with reading rights but I gave the full rights to every one on the database/files folder.\nEDIT 2:\nI just mounted a volume on the galaxy where I did not had problems and everything still works. It is clearly not a galaxy problem. If I find the cause I will post it.\nEDIT 3: The solution and cause\nI still not fully understand it myself but it is solved and I know the cause. So in my case the path to the files folder looks like this: /media/galaxy/database/files.\nThe galaxy folder had the rights drwxrwxrwx (777), I just changed the rights to drwxr-xr-x (755) and now it works."
    },
    {
      "role": "user",
      "text": "First of all thanks for your reply, I will edit my original question with an update and maybe you or some one else can still point me in the right direction. I am working on the x-accel-redirect solution"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to install the Frogs version @link https://toolshed.g2.bx.psu.edu/view/frogs/frogs_2_0_0/76c750c5f0d1\nUnfortunately, i am not able to resolve following dependencies\nFROGS Demultiplex reads frogs\nperl-io-zlib\nperl-io-gzip\nperl\nKindly help me to resolve the problem faced for my Galaxy installation."
    },
    {
      "role": "system",
      "text": "Hi,\nI encountered the same problem. Installing version 3.1.0 solved dependencies problem.\nCheers"
    }
  ],
  [
    {
      "role": "user",
      "text": "Please note that I have already posted this message below since Oct. 20 at the\n@link http://usegalaxy.org support, but so far I got no response. thus, I am reposting it here in case someone could help. Many thanks in advance.\nInitial post in @link http://usegalaxy.org support:\nHello,\nI am using as input for the Volcano plot tool a file derived from DESeq2. The whole pipeline works great and I do get the corresponding Valcano graph but I have two little issues:\n\nFew genes are highly expressed and at the same time have p-value equal to 0; thus, these genes are depicted in the corresponding Volcano at the very top:\n\nimage678\u00d7682 74.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/fb0b8c45e836a35598384834125edcf4b81401ba.png)\n\n\nMy question is: How should I treat those genes? I definitely want to have them due to are highly expressed, but at the same time, the plot doesn`t look proper in the way they are presented. I am not saying that is wrong, just saying that is odd. Should I exclude those genes? Should I do anything else with them?\n\nOn some occasions the DESeq2 file that I am using as an input does show for LogFC, p-value, and p-adjust of a few genes the entry NA, I guess standing for Not Applicable, instead of any real value. Should I exclude these particular genes from the input that I am going to use for the Volcano?\n\nI will highly appreciate any help,\nGreetings"
    },
    {
      "role": "system",
      "text": "Update\nI can reproduce your graph clumping up near the top by faking extremely small p-values (e100, e200, e300, e400 \u2026 e900).\nIt seems this is a known limitation of R data representation - R largest/smallest representable numbers - Stack Overflow (@link https://stackoverflow.com/questions/38165221/r-largest-smallest-representable-numbers)\nAnd the Bioconductor tool authors are aware, example topic (warning, older posts!) P Value of 0? (@link https://support.bioconductor.org/p/64787/) but you can also search there for how others have scientifically interpreted these tiny values and any current additional advice.\nGalaxy history that uses the inputs and workflow from the tutorial, plus an adjusted input with \u201cextreme-pvalues\u201d for the test. Everything is tagged and I\u2019ll leave it shared as a reference. Galaxy | Accessible History | test rna-seq-viz-with-volcanoplot (@link https://usegalaxy.org/u/jen/h/test-rna-seq-viz-with-volcanoplot)\nHope that helps!\n\n\n@quote\nthat tool/step had a problem interpreting the content of the values evaluated.\nIt looks like you are losing significant digits in the table (Excel?).\n@quote\nA p-value of 0 should be filtered out for not being significant.\nTools can report all kinds of odd values when there is a problem. A pvalue that is actually just \u201c0\u201d means that something went wrong. In your case, this seems to be related to losing significant digits in intermediate applications and that should be addressed.\nTry running these tools in Galaxy if you are not already, and see what results. You won\u2019t lose significant digits."
    }
  ],
  [
    {
      "role": "user",
      "text": "Does a job using CVMFS take a long time? I implemented CVMFS configuration on my local galaxy with reference to @link https://galaxyproject.org/admin/reference-data-repo/  Then I executed bwa job for test using  hg38, but it is still running after about 6 hours. Smaller one such as dm3 finished but it took about 50 minutes. It seems to be slow for the size of the data.  In both cases, input data is the same, paired-end fasq filest about 400MB each and the second run with dm3 finished  less than a minute.\nI think it\u2019s not a problem of our network because downloading data from other sites, such as UCSC chromosome data, comes out to 2-3MB/s. I would like to know if the galaxy with CVMFS performance is normal, and if there is a setting that would improve performance.\nThank you,\nYukie"
    },
    {
      "role": "user",
      "text": "Follow-up comment. I tried to change Stratum1 server and it led to improved performance.  Bwa job with same input files and dm3 finished less than 10 minutes. Thanks for the advice, both of you.\nBest,\nYukie"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone! Which appliance on Galaxy would you recomend for Eukaryotic genomic annotation? It would be even more helpful if there was one capable of intersecting or cross-linking information from two or more Exomes. The Galaxy tutorials used GEMINI and Funannotate but those appear to no longer be availiable.\nThank you in advance, Anna"
    },
    {
      "role": "system",
      "text": "Hi @user\nI believe if you intersect annotated VCF files, the output file will have the annotations present in the input file(s).\nGalaxy servers usually have several intersection tools. Maybe try different tools.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m Yoon.\nI\u2019ve tried to do DEseq process and featureCount process, but it still not downloaded.\nI tried it since 2 days ago, and i tried it repeatedly.\nAlso, i checked Internet state and storage.\nAs i write, i checked some possible causes.\nIt still not work(it presents gray color), and it is not move on to the next level(yellow or green) .\nPlease let me know what should i do and what is the problems.\nThank you for your work."
    },
    {
      "role": "system",
      "text": "Hi all \u2013\nIf you are working at @link http://UseGalaxy.org, there is one very busy cluster. A new banner on the server explains:\n\nTools impacted include by the busiest cluster: Unicycler, SPAdes, and Trinity\n\nOther clusters that run different tools are also very busy. Leave jobs queued for the quickest processing and to not lose your place in the queue. Three hours is not very long to wait \u2013 give it a few days \u2013 most jobs will process sooner than that, but the queue time is dependent on the server load as @system stated.\nThe above applies to jobs that have valid (successful) inputs. It may be worth checking those upstream inputs for problems. If any are in an error or paused state, that could lead to downstream jobs pausing until the input is corrected \u2013 if you need to make a change to an input, that would be the exception for waiting. You should correct the inputs, then rerun.\nFAQs: Galaxy Support (@link https://galaxyproject.org/support/)\n\nDatasets and how jobs execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\n\nNote: When one public Galaxy server is very busy, you can try working at another. @link http://UseGalaxy.eu is one alternative. It is common to have one account at each distinct public Galaxy server. Each utilizes different processing resources.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI have local installed Galaxy 19.05 on t3.2xlarge instance (8 CPU and 32Gb RAM) and sometimes when I try update page or follow to another job I get next error:\n\nAn error occurred while updating with the server. Please contact a Galaxy administrator if the problem persist.\n\nFor me only helps close and open new tab on browser, but this very uncomfortable.\nDetails of error:\n\n{\n\u201cuserAgent\u201d: \u201cMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\u201d,\n\u201conLine\u201d: true,\n\u201cversion\u201d: \u201c19.05\u201d,\n\u201cxhr\u201d: {\n\u201creadyState\u201d: 4,\n\u201cresponseText\u201d: \u201c{\u201cerr_msg\u201d: \u201cUncaught exception in exposed API method:\u201d, \u201cerr_code\u201d: 0}\u201d,\n\u201cresponseJSON\u201d: {\n\u201cerr_msg\u201d: \u201cUncaught exception in exposed API method:\u201d,\n\u201cerr_code\u201d: 0\n},\n\u201cstatus\u201d: 500,\n\u201cstatusText\u201d: \u201cInternal Server Error\u201d\n},\n\u201coptions\u201d: {\n\u201cparse\u201d: true,\n\u201csilent\u201d: true,\n\u201climit\u201d: 500,\n\u201coffset\u201d: 0,\n\u201cdetails\u201d: \u201c92b83968e0b52980\u201d,\n\u201ctraditional\u201d: true,\n\u201cdata\u201d: {\n\u201climit\u201d: 500,\n\u201coffset\u201d: 0,\n\u201cdetails\u201d: \u201c92b83968e0b52980\u201d,\n\u201corder\u201d: \u201chid\u201d,\n\u201cv\u201d: \u201cdev\u201d,\n\u201cq\u201d: [\n\u201cdeleted\u201d,\n\u201cpurged\u201d,\n\u201cvisible\u201d\n],\n\u201cqv\u201d: [\n\u201cFalse\u201d,\n\u201cFalse\u201d,\n\u201cTrue\u201d\n]\n},\n\u201cemulateHTTP\u201d: false,\n\u201cemulateJSON\u201d: false,\n\u201ctextStatus\u201d: \u201cerror\u201d,\n\u201cerrorThrown\u201d: \u201cInternal Server Error\u201d\n},\n\u201curl\u201d: \u201c@link http://18.130.115.156/api/histories/f597429621d6eb2b/contents?limit=500&offset=0&details=92b83968e0b52980&order=hid&v=dev&q=deleted&q=purged&q=visible&qv=False&qv=False&qv=True\u201d,\n\u201cmodel\u201d: [],\n\u201cuser\u201d: {\n\u201cid\u201d: \u201cf2db41e1fa331b3e\u201d,\n\u201cusername\u201d: \u201cadministrator\u201d,\n\u201ctotal_disk_usage\u201d: 881918595,\n\u201cnice_total_disk_usage\u201d: \u201c841.1 MB\u201d,\n\u201cquota_percent\u201d: null,\n\u201cis_admin\u201d: true,\n\u201cpreferences\u201d: {},\n\u201ctags_used\u201d: [\n\u201cname:tissueA\u201d,\n\u201cname:tissueB\u201d\n],\n\u201cdeleted\u201d: false,\n\u201cpurged\u201d: false,\n\u201cquota\u201d: null\n}\n}\n\nI use Nginx as frontend HTTP server and use SQL lite by default.\nThanks!"
    },
    {
      "role": "user",
      "text": "Thanks you for answer!\nI noticed in log next line:\n\n/srv/galaxy/.venv/local/lib/python2.7/site-packages/Cheetah/Compiler.py:1630: UserWarning:\nYou don\u2019t have the C version of NameMapper installed! I\u2019m disabling Cheetah\u2019s useStackFrames option as it is painfully slow with the Python version of NameMapper. You should get a copy of Cheetah with compiled C version of NameMapper.\n\"\\nYou don\u2019t have the C version of NameMapper installed! \"\n\nSo I installed cheetah3:\n\npip install cheetah3\n\nAnd now it works well!\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I ran a test file (xxx.txt) containing fasta formatted sequences that I uploaded from my desktop. There are only <10 sequences in the file. I wanted to test this because I was having issues with running megaBLAST on a larger dataset collection. At any rate, I think there is something happening with this python script that is causing megaBLAST to fail with large or small files.\nThis was the error message:\nFile \u201c/project/galaxy/galaxy-py3/galaxy-base/database/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/megablast_wrapper/fb2e0e1dac89/megablast_wrapper/megablast_wrapper.py\u201d, line 68\nprint megablast_command\n^\nSyntaxError: Missing parentheses in call to \u2018print\u2019. Did you mean print(megablast_command)?\nI also tried to upload a pic of the error screen, not sure if it made it\u2026\n\nerror screen1377\u00d7841 140 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/17e3ac517fa7d2500fcb38e44803c302890b0da1.jpeg)\nThis was the input file (for some reason it\u2019s not seeing the >'s at the start of each sequence title, but they are on there):\n\nA00129:980:H5TVGDSX2:4:1107:23547:158121:N:0:TGTCGCAT+AAGCTGAC\nTAGTAGATTTAAAACGAAGGTCTTGGGGGGTGGTACTTCCCTCAATTCCAATAACCTAAAATAAAATTAAAATTAATAAGTATTTAAAATTTAATATTACGCGCTAGCGTAACATCTGACATCCATTTCTTCCCATGTGATAAGGT\nA00129:980:H5TVGDSX2:4:1114:6777:151701:N:0:TGTCGCAT+AAGCTGAC\nGTCGGAATGGAATCGTAAATTGGACGGGACTTATACTAAATATTCTAGTCTTACCAGGTATTCTGCATTTTTCCACAGGCACACATGTATCTTTCAAACAATCCGTGAATATAGTGTGAGGCTTTATTCCCTTCTTCCTCATTAACTGAG\nA00129:980:H5TVGDSX2:4:1124:1524:98311:N:0:TGTCGCAT+AAGCTGAC\nCACCTAACCTAGAGTACCACTTTAATCGTAACTAAAACTACTCTACTACGGAGCTGAGTCTTTAGAGGCAACTCAACCTTCCTCACATATAAGCTAAAAGTGGTGGTCTAATCCACTCTGCTTTTAGACTCTTATATGCAAGTTCACTCC\nA00129:980:H5TVGDSX2:4:1126:11279:367461:N:0:TGTCGCAT+AAGCTGAC\nACCCCATCCGAATGCCAACTCTAGCGCTTGTTTAGCATTCTCAATGGTTGCTACTCGACGACCCAATCCTCGAGCATGTGTCCAATTGGTTGTTCCTTCTATAGAAACATTATCCAGATTGGCTAGAAACACGGGTCTTGTTGGATGTTT\nA00129:980:H5TVGDSX2:4:1127:7699:341781:N:0:TGTCGCAT+AAGCTGAC\nCGGATAAAGGCAAATCAGTAATACCTAACCAAGCTAACCTAATTAACAAACAATTTGAAATTGTATTCAATATGTCCGTTATTGGAGAACCTGATGGAATTCCGCAAGGTACTCGGTACACCAAATCGCGACATAGATGACTAGGCG\nA00129:980:H5TVGDSX2:4:1140:5041:243771:N:0:TGTCGCAT+AAGCTGAC\nCCACCGTAACAAACTAGCACGACATGTCTAGAAAATTCGGATAAAGGCAAATCAGTAATACCTTGCCAAGCCAATCGAATTAACAAACAATTCGAAATAGTATTCAAAATGTCCGTAATTGGTGATCCAGAAGGAATACCGCATAGTACG\nA00129:980:H5TVGDSX2:4:1169:13883:227481:N:0:TGTCGCAT+AAGCTGAC\nTATATTTTTCTTTCTTACTCCAATTACTTTCTTCTACGGCAAATACAAGTTATTTCATTTCTTTGCGCTTTATAAGTAACCGTTGTAACTTAGTAACTCTTAGTTCGAGGAATTATTTAGAAAGCTTTTTAAGTAACAATATATTT\n"
    },
    {
      "role": "user",
      "text": "@system\nI was using an old version of megaBLAST (circled in red). I am using the USDA SciNet Galaxy, and this is an option still.\n\nCapture megaBLAST1679\u00d7520 102 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/170737e028aa0de0266fa072ecfcf6613c507897.jpeg)\nI don\u2019t see it on the @link http://usegalaxy.org (non-USDA SciNet) page. I guess they need to deprecate/retire/take down that option. But I still don\u2019t know why it wouldn\u2019t work correctly. Maybe something associated with that tool needs updating or whatever.\nAt any rate, thank you for taking time out for this.\nV"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone, when trying to import a dataset (large .csv file) from my active galaxy history to Rstudio with gx_get(19056) I get the following error:\n\nDEBUG:root:Host IP determined to be b\u2019172.17.0.1\\n\u2019\nDEBUG:root:TestURL url=https://usegalaxy.eu obj=False\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): @link http://usegalaxy.eu:443\nDEBUG:urllib3.connectionpool:@link https://usegalaxy.eu:443 \u201cGET /api/histories?key=###HTTP/1.1\u201d 200 None\nDEBUG:root:TestURL url=https://usegalaxy.eu state=success\nDEBUG:root:Downloading gx=GalaxyInstance object for Galaxy at @link https://usegalaxy.eu history=0cd1246fe45a8830 dataset=19056\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): @link http://usegalaxy.eu:443\nDEBUG:urllib3.connectionpool:@link https://usegalaxy.eu:443 \u201cGET /api/histories/0cd1246fe45a8830/contents?key=###HTTP/1.1\u201d 200 None\nTraceback (most recent call last):\nFile \u201c/usr/local/bin/get\u201d, line 19, in \nresults = get( args.id, args.identifier_type, args.history_id )\nFile \u201c/usr/local/lib/python3.8/dist-packages/galaxy_ie_helpers/init.py\u201d, line 199, in get\ndatatypes_all.append({ds[identifier_type]: ds[\u2018extension\u2019] for ds in history})\nFile \u201c/usr/local/lib/python3.8/dist-packages/galaxy_ie_helpers/init.py\u201d, line 199, in \ndatatypes_all.append({ds[identifier_type]: ds[\u2018extension\u2019] for ds in history})\nKeyError: \u2018extension\u2019\n\nThe dataset loads normally in my locally installed Rstudio.\nAny suggestions?"
    },
    {
      "role": "system",
      "text": "Glad worked out! No idea what was wrong originally but seems working on .org and .eu now. Maybe it was a data compression issue for you \u2013 or maybe the IE environment got corrupted. But if working now considering it all good.\nI was doing something else (and will be doing more today). That involves these same commands. I\u2019ll also use a fresh session.\nThanks for all the followup! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there,\nI am an amateur user of Galaxy and have been trying to convert a bedgraph file generated from the Methyldackel tool into a bigwig format for visualisation using the Wig/Bedgraph - to - Bigwig converter tool. I am receiving this error message -\n\u201cUnrecognized line 1 of stdin:\nchr1 3000826 3000828 80 4 1\u201d\nCan you please tell me what is wrong and how I can solve it?\nThanks!!\nAbishek"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe problem was that the converter requires a BedGraph with only four columns; so you in your case you need to generate three different bedgraph files, one for each track.\nTest history: Galaxy | Accessible History | imported: MIWI2 WGBS_abi (@link https://usegalaxy.org/u/gallardoalba/h/imported-miwi2-wgbsabi)\nSorry for the late solution, regards!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to build a local Galaxy server (basically followed the training materials up to step 7 (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html), version 22.01).\nMy problem is that when I run BWA/BWA-MEM I receive the following error:\n\nimage1305\u00d7658 38.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/406ea35abaa9f379d593d86e220c3953c5fb1c79.png)\nAs far as I can tell the data tables and CVMFS seem to work properly.\n\nimage1390\u00d7288 21.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/393738b481d47b699b4576d971ba32293168501b.png)\nubuntu@galaxy-server:~$ ls /cvmfs/data.galaxyproject.org/byhand/hg38/hg38full/bwa_index_v0.7.10-r789/\nhg38full.fa  hg38full.fa.amb  hg38full.fa.ann  hg38full.fa.bwt  hg38full.fa.pac  hg38full.fa.sa\n\nAny help is appreciated!"
    },
    {
      "role": "user",
      "text": "Hello,\nI\u2019ve managed to track down the issue to Singularity. I\u2019ve rolled back my configuration to the CVMFS tutorial (@link https://training.galaxyproject.org/archive/2022-05-01/topics/admin/tutorials/cvmfs/tutorial.html) and BWA works properly. It\u2019s after I apply the changes from the Singularity tutorial (@link https://training.galaxyproject.org/archive/2022-05-01/topics/admin/tutorials/singularity/tutorial.html) that it fails to locate the index files. I assume that the /cvmfs directory isn\u2019t accessible to the containers.\nSeems like adding < env id=\u201cSINGULARITY_BIND\u201d>/cvmfs< /env> did the trick."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am the admin of local galaxy installation. Files more than 2GB fail to upload and run.\nHow to upload data from a local disk directly so that I can continue with my work."
    },
    {
      "role": "system",
      "text": "The 2GB browser upload soft limit is long gone, it was more of a browser\u2019s issue than Galaxy\u2019s. If you already have data local to Galaxy instance the best way is to not upload them, just to import them with Data Libraries using links instead of copying. This is how: @link https://galaxyproject.org/data-libraries/#user-folder"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using Galaxy 20.09 (last pull January) and found a strange error when trying to open some shared workflows (trace below). The workflows themselves work fine, both execution and loading the editor from the \u201cworkflow\u201d menu. But from the \u201cshared workflow\u201d menu you get 500 if you click on it. I cleared the template cache to no avail. This effect only some workflows, other shared workflows on the same instance can be opened. New basic workflow can be opened as well. If I download the problematic workflow and import it the error will occur again. If I import it to another instance of Galaxy, it will also cause server 500 error. One of the failing workflow can be seen exported here: 500.ga \u00b7 GitHub (@link https://gist.github.com/martenson/ce81f0e33c9148d2834d380c011d9327)\ntrace:\n86.49.250.134 - - [10/Mar/2021:15:14:35 +0000] \"GET /u/marten/w/mzmlconvert HTTP/1.1\" 500 - \"https://umsa.cerit-sc.cz/workflows/list_published\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36\"\nTraceback (most recent call last):\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/error.py\", line 153, in __call__\n    app_iter = self.application(environ, sr_checker)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/paste/recursive.py\", line 85, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/statsd.py\", line 33, in __call__\n    req = self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/paste/httpexceptions.py\", line 640, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/base.py\", line 141, in __call__\n    return self.handle_request(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/base.py\", line 220, in handle_request\n    body = method(trans, **kwargs)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/galaxy/controllers/workflow.py\", line 230, in display_by_username_and_slug\n    return self._display(trans, stored_workflow)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/galaxy/controllers/workflow.py\", line 272, in _display\n    user_is_owner=user_is_owner)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/webapps/base/webapp.py\", line 972, in fill_template_mako\n    return template.render(**data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/template.py\", line 476, in render\n    return runtime._render(self, self.callable_, args, data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 883, in _render\n    **_kwargs_for_callable(callable_, data)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 920, in _render_context\n    _exec_template(inherit, lclcontext, args=args, kwargs=kwargs)\n  File \"/mnt/volume/shared/galaxy/py3_venv/lib/python3.6/site-packages/mako/runtime.py\", line 947, in _exec_template\n    callable_(context, *args, **kwargs)\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/base/base_panels.mako.py\", line 124, in render_body\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/display_base.mako.py\", line 234, in render_center_panel\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/display_base.mako.py\", line 269, in render_render_content\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 256, in render_render_item\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 227, in do_inputs\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 134, in render_do_inputs\nKeyError: 'input_files'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/batch.py\", line 80, in __call__\n    return self.application(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/request_id.py\", line 15, in __call__\n    return self.app(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/xforwardedhost.py\", line 23, in __call__\n    return self.app(environ, start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/translogger.py\", line 70, in __call__\n    return self.application(environ, replacement_start_response)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/error.py\", line 163, in __call__\n    exc_info)\n  File \"/mnt/volume/shared/galaxy/server/lib/galaxy/web/framework/middleware/translogger.py\", line 69, in replacement_start_response\n    return start_response(status, headers, exc_info)\nSystemError: <built-in function uwsgi_spit> returned a result with an error set\n\nanother workflow has the same trace except the middle part is:\n  File \"/mnt/volume/shared/galaxy/var/cache/template_cache/workflow/display.mako.py\", line 134, in render_do_inputs\nKeyError: 'input_smiles'\n\nbut there is no input_smiles value when looking at the .ga file"
    },
    {
      "role": "user",
      "text": "@quote\nSounds like a variation of[20.09] Incorrect tool version saved in workflow \u00b7 Issue #10914 \u00b7 galaxyproject/galaxy \u00b7 GitHub?\nThis was the right direction. To add to it you also need to delete all problematic nodes from the workflow and re-add them. :confused:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been attempting to use the Mothur pipeline to conduct my 16S community analysis. I was onto the beta-diversity step, using the Dist.shared tool. However, when running this tool I received the following error message, \u201cFatal error: Exit code 139 (), The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\u201d\nI was unable to locate the empty dataset in any of my input datasets or any of the datasets used in the proceeding tools that may have caused the issue. After attempting to re-run the tool a number of times to no avail, I attempted to conduct my entire analysis again.\nHowever this time, when running summary.seqs tool after the sequence alignment step was completed using the align.seqs tool, this gave the same error. I assumed something went wrong in the align.seqs tool so I reattempted to run this again with the same data set but then this gave me the same error message again even thought I successfully ran the align.seqs tool with the same dataset that worked a few hours earlier.\nUnsure why this is happening as I have successfully used Mothur in the past with no issues, any help would be fantastic, Ryan"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis message usually means that one of the inputs is empty (possibly has only headers and no data lines) or sometimes that the one or more input datasets was entered more than once on the tool form.\n@quote\n\u201cFatal error: Exit code 139 (), The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\u201d\nIf you still need help, you can post back a share link to your history publicly as a reply, and we can try to help. If you\u2019d rather share the link privately instead, please confirm the URL of the public server where you are working (actually @link http://UseGalaxy.eu?) and state that you\u2019d like a moderator to start up a direct message to post the history share link in.\nFAQs\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting \u2013 the \u201cDataset Information\u201d view ( :information_source: icon within any dataset) is a good way to review the inputs and parameters for a job in a summary. This is the same place where the job stderr/stdout logs are located for any dataset, errored or successful.\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n\nTutorial reference @link https://training.galaxyproject.org/training-material/search?query=mothur\nLet\u2019s start there! Thanks"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have Chipseq data and 2 replicates for each condition. I did MACS2 peak calling and tried to perform DiffBind. But I always get the same error. [1] \u201cen_US.UTF-8\u201d\nCan you please help me with this.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nplease have a look at this entry; it covers your case and provide some additional info about how to use Diffbind.\n@quote\nHi@systemAt least two replicates per condition are required. And for Diffbind, the first condition must be specifically named \u201cCondition\u201d (quirk in how the underlying wrapped tool works). \nFAQ:https://galaxyproject.org/support/Extended Help for Differential Expression Analysis Tools\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nCapture 21620\u00d7877 60.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9d1a73351b42e58cae4cbdb9fc963058efcec47b.png)\nCapture\nFastQC job created yesterday and only half processed. The rest of the jobs are in the waiting or unprocessed mode. First time encountering this as similar jobs were processed within 4 hours earlier this week."
    },
    {
      "role": "system",
      "text": "Hi @user,\nsorry for the late response; I just saw that the jobs finished succefully.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am new to Galaxy and I am trying to learn how to use the Galaxy API with the BioBlend package.\nWhen describing the Galaxy API, Galaxy mentions that it is useful to use the API and tools such a BioBlend in the following cases:\n\n\nRunning a workflow against multiple datasets (which can be done with the web interface, but is tedious)\n\n\nWhen the analysis involves complex control, such as looping and branching\n\n\nI was therefore hoping that I would be able to programmatically create workflows with BioBlend, not only invoke them. however, the documentation for BioBlend doesn\u2019t seem to indicate that I can make workflows. Am I correct in thinking that I cannot create workflows? Is this something that Parsec might be able to do?\nAn example of I was hoping to do. I want to automate RNAseq differential expression starting from a single matrix produced with featureCounts. I have 3 groups I need to compare (so 3 inputs for edgeR). However, in the Galaxy GUI, I have been unable to seperate my matrix count into 3 noodles going into edgeR. In other words, if I want edgeR to have 3 input groups, I need to create 3 separate matrix counts. See image below. I was hoping to be able to programmatically create a workflow with BioBlend that would help me around this problem.\n\nScreenshot from 2018-12-17 11-47-41.png1064\u00d7925 106 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/1a6c1ccec43cad96006fcd12483460af55956eec.png)\nAny help would be deeply appreciated."
    },
    {
      "role": "user",
      "text": "It seems it now is possible to use a single matrix as input - so need more need to have one matrix per sample.\n\nScreenshot from 2019-02-01 14-05-34.png905\u00d7258 17.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/44bbb863ef9c7ad34c32f4f65c84557f2e441b48.png)\nHowever, I am having trouble using a single factor file for a multifactorial analysis (but I have posted a separate question about this)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to upload files via FileZilla onto my Human Cell Atlas instance, which I have done before without any issue as recently as a few days ago. However, now when I log on and click \u2018Upload Data\u2019 I only have an option for \u2018Choose Remote Files\u2019, which does eventually give me access to my FileZilla folders, but would involve manually selecting each file individually from subfolders. What I used to be able to do, and I can see this is still available on @link http://usegalaxy.org, is click on an option called \u2018Use FTP files\u2019, which lets me upload the entire FileZilla folder at once. The \u2018Choose Remote Files\u2019 button is now showing instead of this.\nCould you please advise how to get this \u2018Use FTP files\u2019 option back within the Human Cell Atlas instance?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe interface has changed a little bit as we now offer much more options to get your data.\n\ngrafik949\u00d7639 78.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7d0a3f4925000b1893cebc02a089b6a364895491.png)\nPlease see this dialog and choose the top option \u201cFTP Directory\u201d.\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am just getting started with developing Galaxy.\nI cloned the repository onto my local computer - MacOS running High Sierra 10.13.6.\nNext, I created a virtual environment, activated it, and then tried the run.sh script.\nA lot of things were downloaded and installed, but then the script died with this error:\nRequirement already satisfied: setuptools in ./.venv/lib/python2.7/site-packages (from sphinx==1.7.2->-r ./lib/galaxy/dependencies/dev-requirements.txt (line 32)) (41.0.1)\nUnsetting $PYTHONPATH\nActivating virtualenv at .venv\ngalaxy.tools.deps DEBUG 2019-05-30 17:32:26,193 Unable to find config file './dependency_resolvers_conf.xml'\nexecuting: .venv/bin/uwsgi --module 'galaxy.webapps.galaxy.buildapp:uwsgi_app()' --virtualenv /Users/aloraine/galaxy/.venv --pythonpath lib --threads 4 --buffer-size 16384 --http localhost:8080 --static-map /static/style=/Users/aloraine/galaxy/static/style/blue --static-map /static=/Users/aloraine/galaxy/static --die-on-term --hook-master-start 'unix_signal:2 gracefully_kill_them_all' --hook-master-start 'unix_signal:15 gracefully_kill_them_all' --enable-threads --py-call-osafterfork \nTraceback (most recent call last):\n  File \".venv/bin/uwsgi\", line 6, in <module>\n    from uwsgi import run\nImportError: dlopen(/Users/aloraine/galaxy/.venv/lib/python2.7/site-packages/uwsgi.so, 2): Library not loaded: /usr/local/opt/pcre/lib/libpcre.1.dylib\n  Referenced from: /Users/aloraine/galaxy/.venv/lib/python2.7/site-packages/uwsgi.so\n  Reason: image not found\n\nThe python I am running in my virtual environment (called \u201cgalaxy\u201d):\n(galaxy) $ python\nPython 2.7.10 (default, Oct 6 2017, 22:29:07)\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)] on darwin\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.\nI used pip to install wsgi in my environment, and it seems to have worked:\n(galaxy) $ pip install uwsgi\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won\u2019t be maintained after that date. A future version of pip will drop support for Python 2.7.\nRequirement already satisfied: uwsgi in /Users/aloraine/.virtualenvs/galaxy/lib/python2.7/site-packages (2.0.18)\nTips on what to do next would be great!\nMany thanks,\nAnn Loraine"
    },
    {
      "role": "system",
      "text": "The most current Galaxy release can always be found here:\n@link https://docs.galaxyproject.org/en/master/releases/index.html\n\nExample step-by-step for a basic local Galaxy install with an admin account created\n\nGo to home directory using Terminal on MAC OSX\n\n\ncd\n\n\nSet up a place to keep the local galaxy. I often need to do testing with fresh checkouts, so tend to use the version as the dir name. Wherever you put/name this, avoid nesting the location too much, or the path will get long and possibly present with Conda errors later on (it doesn\u2019t play well with longer paths).\n\n\n$ mkdir 1901\n\n\n$ cd 1901\n\n\nClone Galaxy\n\n\n$ git clone -b release_19.01 @link https://github.com/galaxyproject/galaxy.git\n\n\nLet that run to completion ^^. Then go into the galaxy dir.\n\n\n$ cd galaxy\n\n\nSet up the virtualenv\n\n\n$ virtualenv -p /usr/bin/python .venv\n\n\nInitial startup of Galaxy\n\n\n$ sh run.sh\n\n\n\nThen that runs using all default settings, ending with the server hosted at the local URL. Will then create my \u201cadmin\u201d account in that web browser (but it won\u2019t be admin yet!).\n\n\nAfter that will shut it down, change the galaxy.yml to become admin, start it up again, and log into my account in the web browser to make sure that I set up the admin config correctly.\n\n\nMore can be done after that to customize your instance.\n\n\nI hope this works for you, too :slight_smile:\n\n\n\nYou probably know where these resources are, but for others reading:\n\n@link https://galaxyproject.org/admin/get-galaxy/\n@link https://docs.galaxyproject.org/en/master/admin/index.html\n@link https://github.com/galaxyproject/dagobah-training\n\nIn particular, have a look at:\n\n@link https://docs.galaxyproject.org/en/master/admin/framework_dependencies.html\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "My workflow does not execute any step after being successfully invoked. Under \u201cStep scheduling\u201d there is written: Loading step state summary and under \u201cJob execution\u201d there is written: Loading job summary. Nothing has changed for over 40 hours and the single steps that are included in workflow do work properly on my input dataset, so there is no mistake in the workflow nor in the input data. It just does not \u201ckick off\u201d.\nPlease help me,\nMonika"
    },
    {
      "role": "system",
      "text": "I had the same issue. I came to this thread after google search. I had to restart the galaxy server and all the workflow steps were loaded and ran fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!\nI tried running a basic workflow of Quality Control and Mapping for RNAseq, with Cutadapt, RNA star and HTseq, with FASTQC/MULTIQC and qualimap. But everytime I try to run it, it gives me this error and Cutadapt does not run\nimage1305\u00d7685 28.5 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2dd39b24441f47fbba00b9e3af2898ddee5dfd18.png)"
    },
    {
      "role": "system",
      "text": "Update:\nTurns out that Cutadapt is working fine in a workflow now. Based on that result, reconnecting the workflow \u201cnoodles\u201d will likely solve your entire issue as well. Make sure you are incorporating the most current tool version (all steps/tools) to avoid older-but-fixed problems.\nTest history and workflow if you want to compare your work to our test. I\u2019ll leave this shared for a while, may help others, too.\n\nGalaxy | Accessible History | test cutadapt listpaired (@link https://usegalaxy.org/u/jen-galaxyproject/h/test-cutadapt-listpaired)\nGalaxy | Accessible Workflow | Workflow constructed from history 'test cutadapt listpaired' (@link https://usegalaxy.org/u/jen-galaxyproject/w/workflow-constructed-from-history-test-cutadapt-listpaired)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Has anybody had issues running goseq on galaxy? the first time I ran goseq it failed after 3 days (error message said it exceeded max. run time) so I fixed an issue with my dataset and hit execute again. It is still running and has been for 20 hours? Is there an issue I am missing or something I can do to speed it up? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry Biomart. The query will look something like the screenshot below. Export/download in TSV format (tabular) from this site then update the file to Galaxy.\n\nOnly these two columns (Gene and GO term) should be input to the Goseq tool for the \u201cCategories\u201d reference file.\nDo not input any lines (genes) that do not have a GO term. Remove those first. There are a few ways to filter out genes that do not have a GO assignment, this is one:\n\nTool Select lines that match an expression\n\nOption: Matching\n\nRegular expression: ^E.+\\sG.+$\n\n\n\nMore related data can be exported if you want, for later use.\n\nTools like Cut and Join and others in the Data Manipulation tool group can be used.\n\n\n\n\n\nensembl-to-go-terms-biomart2172\u00d71108 316 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/ea30ac0d000e9f1f45f6dae6b15c24ffb712a652.png)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nWhen using the write unaligned reads (in fastq format) to separate file(s) option in the Bowtie2 tool (triggering the --un-conc parameter) on a paired-end library, I get two unaligned read output files: unaligned reads L and unaligned reads R. I assume that these separate files are generated to distinguish which file contains the unaligned reads from the R1 and R2 input files but can anyone clarify which file (L or R) refers to which input file (R1 and R2)?\nThanks!\nminor admin reformat for clarity"
    },
    {
      "role": "system",
      "text": "Hi @user\nUnmapped L reads are from the R1 input (first input on the tool form)\nUnmapped R reads are from the R2 input (second input on the tool form)\nThe read names can confirm this.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI submitted a job to unicycler over 24 hours ago to the main USA galaxy server and it is still grayed out waiting to run. How long does this typically take?\nI am prepping for a course I am teaching and I was going to have my students do the \u201cUnicycler assembly of SARS-CoV-2 genome with preprocessing to remove human genome reads\u201d tutorial and I want to ensure they have enough time to perform it."
    },
    {
      "role": "system",
      "text": "@user have seen your post here only now. Read your corresponding message in the Galaxy Training Network chat, but your issue wasn\u2019t so clearly described there.\nI cannot comment on Unicycler jobs on @link http://usegalaxy.org. There may indeed be a long queue you are in there, though 4 days is really long.\nWhat I do know is that people have followed that very same tutorial successfully on @link http://usegalaxy.eu last week so maybe you have more luck there.\nIn general, however, it is a good idea when planning to do a tutorial like this with a challenging tool like Unicycler (I think the training material mentions this bottleneck explicitly) to create an example history before the actual course.\nDuring the course let participants perform all steps up to that challenging one, but then explain them what computational complexity is hiding behind that next step and that this cannot be scaled reliably to a large number of participants. At this point, you can share your prepared history with them, they can copy the dataset they need from yours into their history, and continue.\nPlease also note that @link http://usegalaxy.eu offers TIaaS (@link https://galaxyproject.eu/tiaas), which makes trainings a lot more reliable though in the end it\u2019s always good to have a strategy for when things don\u2019t work as they should."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone, I\u2019m trying to run LoFreq to call variants on sorted BAM, with default settings.\nI\u2019ve used it for long time and it\u2019s always worked fine\nSince a couple days it keeps crashing with this error message:\n\u201ccat: can\u2019t open \u2018/jetstream2/scratch/main/jobs/51231981/_job_tmp/lofreq2_call_parallel*/*.log\u2019: No such file or directory\u201d\nany clue? what\u2019s wrong?\nthx\nLuca"
    },
    {
      "role": "system",
      "text": "We\u2019ve had a misconfiguration of the temporary directories for containerized jobs on @link http://usegalaxy.org. It should be resolved now, please try running again. Sorry for the inconvenience and thanks for using Galaxy."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone, I am trying to run MimodD extract variant sites tool with default settings in @link http://Galaxy.org and I got the below error for a week ago. I have used this tool many times and it has worked\u2026\nFatal error: Exit code 127 ()\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 10: mimodd: command not found\nI first aligned my fastq file using Map with BWA-MEM tool, then ran FilterBAM, MarkDuplicates, CleanSAM, Realign reads, MimodD Variant calling, and then tried to MimodD extract variant sites.\nI re-ran it several times, but got same error, tried to run this tool with the data that worked in the past, but got same error now.\nCould you help me?\nThank you!\nHere is the bug report\nDetailed Job Information\nJob environment and execution information is available at the job info page (@link https://usegalaxy.org/datasets/f9cad7b01a472135b38cb4f13b806d30/details).\n\n\n\n\nJob ID\n51284803 (bbd44e69cb8906b5ca7ad95501fbcc85)\n\n\n\n\nTool ID\n@link http://toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varextract/0.1.9\n\n\nTool Version\n0.1.9\n\n\nJob PID or DRM id\n42890615\n\n\nJob Tool Version\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 9: mimodd: command not found\n\n\n\nJob Execution and Failure Information\nCommand Line\nmimodd varextract \u2018/corral4/main/objects/3/a/b/dataset_3ab6f1f4-e4f9-4f97-9ef1-f70de1dbe8c5.dat\u2019 --ofile \u2018/corral4/main/objects/6/0/6/dataset_6069d9c4-28b2-40b3-868f-cba502a72c30.dat\u2019 --verbose\nstderr\n/corral4/main/jobs/051/284/51284803/tool_script.sh: line 10: mimodd: command not found\nstdout\nJob Information\nNone\nJob Traceback\nNone"
    },
    {
      "role": "system",
      "text": "This should now be working at @link http://usegalaxy.org again. Sorry for the trouble, and please let us know if you continue to encounter issues."
    }
  ],
  [
    {
      "role": "user",
      "text": "We\u2019ve been experiencing persistent error pasted below with our lab\u2019s cloud-based installation of the Galaxy suite via the Genomics Virtual Laboratory 4.4.0.  Specifically, whenever we attempt a Trinity de novo assembly of a collection of Illumina TruSeq stranded mRNA paired end reads, we get the errors below.  We\u2019ve had no problems with these same data sets when using other tools like HiSat2 and StringTie.   We\u2019ve poked around in the \u201cmanage tools\u201d and \u201cmanage dependencies\u201d area of the Galaxy Admin interface and updated to the latest version of Trinity and checked dependencies, and all seems to be OK; however, we must confess to being rank amateurs.  Can anyone point us toward some other things we might try?\n\nFatal error: Exit code 2 ()\nPossible unintended interpolation of @2 in string at\n/mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 46.\n\n\nTool Errors\nInternal Tool Error log\n\n\n\n\nTime\nPhase\nFile\nError\n\n\n\n\n2019-02-14 03:03:05.105596\nTool Loading\n./tools/phenotype_association/sift.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/sift_db.loc\u2019\n\n\n2019-02-14 03:03:05.104204\nTool Loading\n./tools/evolution/add_scores.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/add_scores.loc\u2019\n\n\n2019-02-14 03:03:05.103428\nTool Loading\n./tools/evolution/codingSnps.xml\n[Errno 2] No such file or directory: \u2018/mnt/galaxy/galaxy-app/tool-data/codingSnps.loc\u2019\n\n\n2019-02-14 03:03:05.100147\nTool Loading\n./tools/extract/liftOver_wrapper.xml\n\u2018liftOver\u2019\n\n\n\n\n\nAlso found additional error information via the Galaxy | Reports interface:\n> Error, 1 threads errored out at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/opt/trinity-2.8.4/util/insilico_read_normalization.pl line 963.\n> Error, cmd: /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/opt/trinity-2.8.4/util/insilico_read_normalization.pl --seqType fq --JM 1G  --max_cov 200 --min_cov 1 --CPU 4 --output /mnt/galaxy/tmp/job_working_directory/000/104/working/trinity_out_dir/insilico_read_normalization   --max_pct_stdev 10000  --SS_lib_type FR  --left /mnt/galaxy/files/000/dataset_64.dat,/mnt/galaxy/files/000/dataset_67.dat,/mnt/galaxy/files/000/dataset_70.dat,/mnt/galaxy/files/000/dataset_73.dat --right /mnt/galaxy/files/000/dataset_65.dat,/mnt/galaxy/files/000/dataset_68.dat,/mnt/galaxy/files/000/dataset_71.dat,/mnt/galaxy/files/000/dataset_74.dat --pairs_together --PARALLEL_STATS   died with ret 6400 at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 2689.\n> \tmain::process_cmd(\"/mnt/galaxy/tool_dependencies/_conda/envs/__trinity\\@2.8.4/opt\"...) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 3235\n> \tmain::normalize(\"/mnt/galaxy/tmp/job_working_directory/000/104/working/trinity\"..., 200, ARRAY(0x55a0498484e8), ARRAY(0x55a0498484d0)) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 3182\n> \tmain::run_normalization(200, ARRAY(0x55a0498484e8), ARRAY(0x55a0498484d0)) called at /mnt/galaxy/tool_dependencies/_conda/envs/__trinity@2.8.4/bin/Trinity line 1319\n"
    },
    {
      "role": "system",
      "text": "This looks like a tool wrapper bug (conda path quoting).\nIt might present in some Trinity tool wrapper versions and not in others.\nGVL 4.4.0 was released in September 2018 (@link https://galaxyproject.org/news/2018-09-28-gvl-440/) and is based on the Galaxy release 18.05. The most current version of the Trinity wrapper from the IUC was released in October 2018 (@link https://toolshed.g2.bx.psu.edu/view/iuc/trinity/c9cfec002f71). The most current Galaxy version is now 19.01.\nI suggest updating the Trinity wrapper to the most current IUC version and testing that out. If the latest version still presents with the problem, please write back and we can help to get a ticket opened to review the issue, learn the (actual, not guessed) root cause, and get it fixed."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nIve been trying to download a large collection ~32GB of alignments, and despite attempts, ive not been able to download more than a GB before it cuts out. this includes browser and wget options.\nAny advice?"
    },
    {
      "role": "system",
      "text": "Yes, this could be a network issue. A slower connection can drop. Your internet provider could have download limits. Other connection issues are possible but those are two most common reasons we have had reported when this has come up before.\nTry adding this argument to your wget string: -c\nFor more explanations, Google \u201cresume curl\u201d or \u201cresume wget\u201d for much discussion. Plus, the man page for each tool will have official argument descriptions. Both can resume but in slightly different ways that can also differ by OS. You\u2019ll need to test what works for you. If none work, confirming what your IP provider data transfer/time limits are is the next step.\nExample, this Q&A gives a good line-liner to \"resume transfer for (@link https://stackoverflow.com/questions/19728930/how-to-resume-interrupted-download-automatically-in-curl) curl\"."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have worked through the Single-Batch Processing hand-on tutorial three times and consistently get different results than what the tutorial says. After the UMI-tool count (last step of the Single-Batch Processing), the tutorial states there should be 2,140 lines in the matrix and 180 columns, but I am consistently getting 1,972 lines and 50 columns.\nIs anyone else experiencing this?\nDoes anyone have any ideas why this is happening? I made sure to use the Tutorial Mode on galaxy, so when I would select the next step it took me to the appropriate tool and version."
    },
    {
      "role": "system",
      "text": "Dear @user - I am back from holiday, thanks for sharing your history.\nCan you try the analysis again, this time using slightly different input FASTQ files:\nhttps://zenodo.org/record/3253142/files/SRR5683689_1.fastq\nhttps://zenodo.org/record/3253142/files/SRR5683689_2.fastq\n\n(note the lack of \u201c_subset\u201d prefix)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to connect to galaxy via filezilla but i am getting this error message:\n\n|Status:      |Resolving address of usegalaxy.org|\n\n|\u2014|---|\n|Status:      |Connecting to 129.114.60.60:21\u2026|\n|Status:      |Connection established, waiting for welcome message\u2026|\n|Status:      |Initializing TLS\u2026|\n|Status:      |Verifying certificate\u2026|\n|Status:      |TLS connection established.|\n|Status:      |Logged in|\n|Status:      |Retrieving directory listing\u2026|\n|Command:|PWD|\n|Response: |257 \u201c/\u201d is the current directory|\n|Command:|TYPE I|\n|Response: |200 Type set to I|\n|Command:|PASV|\n|Response: |227 Entering Passive Mode (129,114,60,60,118,163).|\n|Command:|MLSD|\n|Error:        |Connection timed out after 20 seconds of inactivity|\n|Error:        |Failed to retrieve directory listing|\n|Status:      |Disconnected from server|\n|Status:      |Resolving address of usegalaxy.org|\n|Status:      |Connecting to 129.114.60.56:21\u2026|\n|Status:      |Connection established, waiting for welcome message\u2026|\n|Status:      |Initializing TLS\u2026|\n|Status:      |Verifying certificate\u2026|\n|Status:      |TLS connection established.|\n|Status:      |Logged in|\n|Status:      |Retrieving directory listing\u2026|\n|Command:|PWD|\n|Response: |257 \u201c/\u201d is the current directory|\n|Command:|TYPE I|\n|Response: |200 Type set to I|\n|Command:|PASV|\n|Response: |227 Entering Passive Mode (129,114,60,56,120,136).|\n|Command:|MLSD|\n|Error:        |Connection timed out after 20 seconds of inactivity|\n|Error:        |Failed to retrieve directory listing|\n"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe service is definitely up.\n@quote\nFAQ:https://galaxyproject.org/ftp-upload/\nThe FAQ has examples, but this is some more troubleshooting help that usually solves problems:\n\n\nMake sure you are using the latest version of Filezilla. The tool updates quite frequently and will prompt you to update when the app is opened. Those updates install quickly. I tend to just replace my current version with the update \u2013 but I also don\u2019t set up any special configurations (on purpose) \u2013 so I can\u2019t tell you if those are preserved or not after an update with the latest version. You could test it out or review/ask for help from those app developers if this is a concern. There are many FTP clients, and it can be used command-line \u2013 the FAQ I sent has more help about that.\n\n\nTry using all default settings. The connection will still be secure when connecting to @link http://usegalaxy.org. You\u2019ll be prompted to accept the security certificate when the connection starts up.\n\n\nMake sure to connect with your Galaxy registered email address as your Filezilla \u201cusername\u201d. Your Galaxy \u201cpublic name\u201d won\u2019t connect. The \u201cpassword\u201d is the same as used when logging into that same exact Galaxy server for that same account.\n\n\nThe FTP server name aka \u201chost\u201d will vary by the Galaxy server \u2013 sometimes it is the server\u2019s base URL (and that is true for \u201c@link http://usegalaxy.org\u201d) \u2013 and sometimes it is different. What to use is likely posted within the Upload tool for the server in the button > pop-up for the FTP function (tool used move FTP\u2019d files into a history as datasets). Another place to check is the server\u2019s homepage. If you cannot find it \u2013 you can ask here (we might know based on the public Galaxy server\u2019s URL) or you can contact those server administrators directly \u2013 not all follow this forum. Contact information for public Galaxy servers is often on the server\u2019s home page and/or in their directory listing here: @link https://galaxyproject.org/use/\n\n\nDo not enter a \u201cport\u201d for most cases (including @link http://usegalaxy.org). Advanced configuration is different \u2013 and the FAQ linked before has details about that. But you don\u2019t need to do that \u2013 defaults work fine for most people.\n\n\nIf the data transfer stalls out while in progress, which files failed is noted near the bottom of the Filezilla app in the tabs. Timeouts can happen with slower connections or larger datasets and it is possible to resume the transfer where it left off within Filezilla. Note that this will usually prompt another security certificate pop-up. Click to accept each time.\n\n\nIf you are transferring many files and having problems, try transferring less files at a time. The transfer will only go as fast as your internet connection allows. University and business wifi connection speed is usually much faster than home wifi or cellular connection speeds. For personal internet/cell, most ISPs maximize \u201cdown\u201d transfer speeds \u2013 and \u201cup\u201d data transfers can be much slower \u2013 for the same service. There are many \u201cspeedtests\u201d available \u2013 an internet search will find them if you want to check yours.\n\n\nSome universities or businesses block sending data outside of the \u201cintranet\u201d entirely and others may only block some external sites for security purposes. If you think that @link http://usegalaxy.org is being blocked, contact your local administrators.\n\n\nSome home/cell ISPs may also block or throttle where and/or how much data can be transferred. Total or within a given time window. Contact your ISP for help if you think that is the problem.\n\n\nReview prior Q&A about FTP at this forum. Search with the keyword \u201cftp\u201d to find those topics. You can also review prior Q&A at our older forum. The second post in this search is an identical issue to what you have, and using \u201call defaults\u201d solved their problem: @link https://galaxyproject.org/support/ >>  Galaxy Biostars: retired Q&A archive (@link https://biostar.usegalaxy.org/) >> add \u201cftp\u201d (or \u201cmsld\u201d, part of the error message) into the keyword search already including \u201cbiostar\u201d >> @link https://galaxyproject.org/search/?q=biostar+MLSD#gsc.tab=0&gsc.q=biostar%20MLSD&gsc.page=1 >> @link https://biostar.usegalaxy.org/p/24134.1.html\n\n\nHope that helps! I don\u2019t think everything was in one post before, so maybe will help others too."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI was trying to run a pipeline these days. Unfortunately, I don\u2019t know the strandedness of the sample (fastq files) I have. Is there any way to check this? I tried already to convert my fastqs to BAMs with bowtie and then use Infer Experiment to check for that. I was wondering if there is any other way.\nThank you,\nAthanasios"
    },
    {
      "role": "system",
      "text": "Hi @user and @system\nTo check actual strandedness, as a QA/QC step for data with unknown protocols, or your own data with a known (to confirm), try this:\n\n\nMap the reads to the reference genome as unstranded\n\nRun the tool Infer Experiment on the resulting bam\n\n\n\nNote this will require a bed dataset with 12 columns\n\n\nDepending on how your obtained/created the bed data, it may have the datatype bed or bed12 assigned. Either is Ok, but check to make sure the data really has 12 columns\n\n\nUCSC\u2019s Table Brower is one source for complete bed data (tool: Get Data > UCSC main).\n\n\nIf your genome is not supported by UCSC, or you want to base the gene model on some other data provider\u2019s genome annotation, you can convert any gtf dataset to a bed12 dataset (tool: Convert GTF to BED12)\n\nDo not obtain gtf data from the UCSC Table Browser for most purposes\nUCSC has a gtf appropriate for RNA-seq tools for selected genomes in their Downloads area\nOr, you can choose a gtf from another source\nFAQ: @link https://galaxyproject.org/support/diff-expression/ (help here applies to any reference annotation usage. Format, content, and \u201cmatched\u201d inputs are important \u2013 or expected problems.\n\n\n\n\n\nMap the reads again to your reference genome setting the strand correctly based on the results of Infer Experiment. Use this bam result for analysis.\n\nMore Help:\n\n\nUCSC Table Browser options to get a bed with 12 columns\n\nSet the \u201cclade + genome + assembly\u201d\nPick a track from the \u201cgroup\u201d Gene and Gene Predictions\n\nSet \u201cregion\u201d = genome\n\nSet \u201coutput format\u201d = bed and check the box to send the output to Galaxy\nSubmit the query by clicking on the button \u201cget output\u201d\nNot done yet! There will be another sub-form presented next to specify more details\u2026\nChoose \u201cCreate one BED record per\u201d > \u201cWhole Gene\u201d to output a bed with 12 columns\nFinish by clicking on the button for \u201cget bed\u201d\nIf you are logged into a public Galaxy server known by UCSC (@link http://usegalaxy.org, @link http://usegalaxy.eu, and @link http://usegalaxy.org.eu are \u201cknown\u201d), the output will be sent to your active History.\nIf working somewhere else, or you want a downloaded copy, you can use the Table Browser directly (@link https://genome.ucsc.edu/cgi-bin/hgTables) and not check the box to send the output to Galaxy. Once the file is downloaded, use the Upload tool to get it into Galaxy.\n\n\n\n\n Generating and Interpreting the results from `Infer Experiment`\nSee these \u201cGalaxy Training Network\u201d (GTN) tutorials:\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html#qc-strandness\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#mapping\n\nMore More Help:\n@quote\nThese FAQs cover datatypes, expected formats, and related help, including how to make sure all of your inputs are a \u201cmatch\u201d. Using the same exact genome build/version for all analysis steps is important, otherwise tools can error or produce unexpected/scientifically incorrect results.Start here:https://galaxyproject.org/support/>>https://galaxyproject.org/support/#getting-inputs-rightMore help is also at this forum. Searching with keywords like \u201creference\u201d or \u201cannotation\u201d or even \u201chg38\u201d will find prior Q&A similar to yours\nA search with the keyword \u201cgtf\u201d will find more topics, too.\n\n@quote\nI tried already to convert my fastqs to BAMs with bowtie and then use Infer Experiment to check for that. I was wondering if there is any other way.\nConverting Fastq > BAM produces a bam dataset without any mapping information. If you actually mapped already with Bowtie (this wasn\u2019t clear to me!), maybe there was some other problem. RNA-seq data might not map well enough with a DNA mapping tool \u2013 use an RNA mapping tool like HISAT2 instead to see if that helps.\nTry the method above to produce a bam with mapping results for QA/QC purposes, run Infer Experiment on that, interpret the results, then run the analysis mapping with strand settings that match your data.\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have 3 muscles I am trying to perform differential expression with.\nFor each muscle type, I have 3 replicates.\nHow can I get pairwise comparisons between muscle 1 vs muscle 2, muscle 1 vs muscle 3, muscle 2 vs muscle 3. Within the DESeq2 tool I have factor set to muscle, then level 1 as muscle 1, level 2 and muscle 2, etc. Then within each level/,muscle type, I select the three replicate data count files.\nI read that DESeq2 takes the first input as the reference (so that would be muscle 1) and then compares everything to that. But that\u2019s not necessarily what I want. I have been using Cuffdiff to perform analyses but wanted to use a more updated tool but I can\u2019t seem to set it up as it needs to be. Can anyone give me some guidance?"
    },
    {
      "role": "user",
      "text": "It almost seems that it\u2019s not available as part of the Galaxy wrapper. Ultimately, I ended up ditching DESeq2 and ended up using edgeR as I found that their set up made more sense to me. I also found it easier to manipulate."
    }
  ],
  [
    {
      "role": "user",
      "text": "hi there I just installed a local galaxy server on my computer, and I  loaded several fastq files with no problems while I have issues uploading 2 fastaq. I used these fastq in other analysis and they work just fine. this is the error I get:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 326, in \nmain()\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 319, in main\nmetadata.append(add_file(dataset, registry, output_path))\nFile \u201c/Users/azimmer/galaxy/tools/data_source/upload.py\u201d, line 128, in add_file\nconvert_spaces_to_tabs=dataset.space_to_tab,\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/datatypes/upload_util.py\u201d, line 54, in handle_upload\nconvert_spaces_to_tabs=convert_spaces_to_tabs,\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/datatypes/sniff.py\u201d, line 759, in handle_uploaded_dataset_file_internal\nif not is_binary and check_content and check_html(converted_path):\nFile \u201c/Users/azimmer/galaxy/lib/galaxy/util/checkers.py\u201d, line 28, in check_html\ntemp = open(file_path, mode=\u2018rb\u2019)\nIOError: [Errno 2] No such file or directory: u\u2019/Users/azimmer/galaxy/database/tmp/f2db41e1fa331b3e-1570157510288-2428886529\u2019\nthanks for any help!"
    },
    {
      "role": "system",
      "text": "@system  You have a space in your path\u2026 Change it to something like:\n/Users/viboud_1/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there an issue with the servers? It\u2019s taking me a whole day just to upload some files, another day to run HISAT2, and I\u2019ve been waiting since yesterday afternoon for my Stringtie data."
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nIt\u2019s taking me a whole day just to upload some files\nUploading data will take longer than usual if you are now working from a home internet connection instead of an institution/commercial connection. Home internet services are almost always optimized for data download speeds, not upload speeds.\nTry running a \u201cspeedtest\u201d to find out what your connection speeds are. Contact your ISP provider to find out what options are available to increase \u201cup\u201d speeds. There are several \u201cspeedtest\u201d options \u2013 an internet search will find those appropriate for where you are located, or you can ask your ISP provider for guidance.\nThe internet, in general, is a bit slower for most right now, or at least not what it was before. More people working over home wifi plus phone connections, more people streaming entertainment during the day, etc.\n@quote\nanother day to run HISAT2, and I\u2019ve been waiting since yesterday afternoon for my Stringtie data.\nThe Galaxy Main @link https://usegalaxy.org server is under very heavy load. This means that jobs will stay in the queued stage (grey dataset) longer. The execution stage (yellow/peach dataset) is about the same as before for most tools. There are simply more people using the server, for a variety of reasons, including distance learning.\nThe best strategy is to queue up jobs, then allow them to complete. Avoid deleting and restarting jobs in hopes of job running faster. Restarting a job places it at the back of the queue again, extending wait time. Only delete and rerun jobs if you already know there is some problem with the inputs/settings originally used. If you haven\u2019t used Workflows before, this might be a good time to learn/start using them.\nFAQ for jobs/datasets: Datasets and how jobs execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nMany GTN Tutorials include workflows in the tutorial itself and most have a workflow that runs through all steps (except for data loading) that can be used as examples/templates. Those under the topic \u201cIntroduction\u201d cover simple cases for the \u201chow-to\u201d. Other topics cover intermediate/advanced methods for customization.\n@quote\nGalaxy Training Network Tutorials: SomeGTNtutorials are appropriate forGalaxy Mainand some are not. Where you can run each is noted per tutorial \u2013 click on theGalaxy instancesgear iconto review thePublic Galaxyserver choices. If a tutorial is supported by a pre-configuredGalaxy Dockertraining image, instructions for how to get it will be listed below the tutorial listings, per category.\nNote: There are a few tools running with reduced memory resources. These are listed in the top banner on the server. The banner will update or be removed as the situation changes. Reduced memory doesn\u2019t impact how long a job takes to queue or execute but it can result in larger jobs failing for resources (even if they ran Ok before).\nIf work is urgent or you just want jobs to execute run faster, setting up your own Galaxy server is an option. Public servers are not the only choice. Cloudman is popular for scientists as it requires very little administration, resources can be scaled up for your own needs, and all of that (except for the initial step) is GUI based with built-in help. Galaxy itself is always free but commercial storage and computational resources are not. AWS has always offered grants for learning/research work, and they have extended that (or did last time I checked). I added a few tags to your post about Cloudman \u2013 please review for more details if this interests you, or see the links below for all choices.\n\n@link https://galaxyproject.org/choices/\n@link https://galaxyproject.org/use/\n\nDelays are frustrating, but I hope this helps to explain the factors involved in the different kinds of delays most public Galaxy servers are experiencing right now (not just @link http://usegalaxy.org), plus what you may choose to do to mitigate delays.\nGood question! Hopefully, the Q&A helps others to understand the current circumstances.\nBest, Jen"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have multiple VCF files corresponding to 40 different patients. I want to run a batch annotation and GEMINI analysis on them. Can I merge/concatenate these files and run 1 single VCF file keeping patient ID information?"
    },
    {
      "role": "system",
      "text": "@system has added vcf_bgzip to gemini, so that should work starting with version 0.20.1+galaxy2 which is already available on @link http://usegalaxy.eu"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was wondering could I get suggestions on how to align multiple protein sequences that are in an excel file to a reference protein fasta sequence. I want to get an output of which align and which do not align.\nThanks for any tool suggestions."
    },
    {
      "role": "system",
      "text": "Hi @user,\nI cannot reproduce that error, which Galaxy instance are you using? This tool requires the reference as a sequence of nucleotides.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nCan you tell me how can I solve this problem? \ndeseq821\u00d7401 24.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/61e83ad22bed3c2c78f349c4abe9f2f87d0f5327.png)"
    },
    {
      "role": "user",
      "text": "Hi,\nI solved this by rounding numbers in RStudio, and then uploading data back on server. It worked! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am currently enrolled in the intro to galaxy class with coursera and registered for a galaxy account. I tried to get shared data from the tab on the top and there is a blank screen.\nthanks"
    },
    {
      "role": "user",
      "text": "up and running\nthanks again :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI recently noticed that the je-demultiplex tool is no longer accessible. Is there any explanation for why it was taken down, and will it be put back up? I see a few other je suite tools, such as je-demultiplex-Illu and je-clips, but I\u2019ve become reliant on je-demultiplex and would like to use it again, if possible.\nThanks for any information\nJake"
    },
    {
      "role": "system",
      "text": "@user sorry for the problem. There was a Galaxy bug in the new 19.09 that prevented to load this tool. This was fixed in the meantime and everything should work now.\nThanks for reporting.\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI noticed that upon running FastQC on my list of pairs containing PE Illumina reads, all the generated reports have the filename of the input reads set to either \u2018forward\u2019 or \u2018reverse\u2019, depending which set of reads it corresponds to. This isn\u2019t too big of an issue since the output of FastQC is another list of pairs itself, so I can easily figure out which \u2018forward\u2019 and \u2018reverse\u2019 it is referring to, however when I take the FastQC rawdata outputs and use them for MultiQC, the tool cannot summarize all my data as the FastQC files all refer to the same \u2018filename\u2019 even though they\u2019re all from different SRAs.\nI used the fasterqdump tool from the sra-tools to import many SRAs and that\u2019s how the list of pairs of my reads was created.\nWould appreciate any advice on how I can work around this, ideally so that FastQC actually picks up the correct filename for each read!\nCheers."
    },
    {
      "role": "user",
      "text": "I did some more digging, and the issue only arises when using list of pairs PE reads. If I just select one individual read file, the value for $input_file.element_identifier is indeed its proper name SRR0000-forward.fastq.gz, while if I have a collection, the element_identifier for all reads is either just forward or reverse. This seems to be an issue (feature?) inherent to Galaxy itself.\nNot quite sure how to move on from here, other than maybe running FastQC individually on each reads file in a list, rather than on the list of pairs itself.\nEDIT: flattening the list seems to solve the issue! easy enough workaround!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to follow the Hsp90 tutorial from this paper: Intuitive, reproducible high-throughput molecular dynamics in Galaxy: a tutorial | Journal of Cheminformatics | Full Text (@link https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00451-6). However, in step 5, I cannot find a tool called \u201cMerge GROMACS topologies\u201d. I have tried looking through all the tools that are available and cannot find the one used in the tutorial. I would greatly appreciate any help!"
    },
    {
      "role": "system",
      "text": "Hi @user, can you switch the tool version to 3.4.3+galaxy0?\nBest wishes,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, when I used freebayes to call SNPs with input filter parameters, there is a error in my local galaxy. when I closed the input filter parameters everything is OK. I just want to know how can I solve the problem.\nthe following is the error details\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 243, in prepare_job\njob_wrapper.prepare()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1161, in prepare\nself.command_line, self.extra_filenames, self.environment_variables = tool_evaluator.build()\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 462, in build\nraise e\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 458, in build\nself.__build_command_line()\nFile \u201clib/galaxy/tools/evaluation.py\u201d, line 483, in __build_command_line\ncommand_line = fill_template(command, context=param_dict, python_template_version=self.tool.python_template_version)\nFile \u201clib/galaxy/util/template.py\u201d, line 107, in fill_template\nraise first_exception or e\nFile \u201clib/galaxy/util/template.py\u201d, line 80, in fill_template\nreturn unicodify(t, log_exception=False)\nFile \u201clib/galaxy/util/init.py\u201d, line 1060, in unicodify\nvalue = str(value)\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/Template.py\u201d, line 1053, in unicode\nreturn getattr(self, mainMethName)()\nFile \u201ccheetah_DynamicallyCompiledCheetahTemplate_1638720981_7731435_86611.py\u201d, line 438, in respond\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/NameMapper.py\u201d, line 293, in valueFromSearchList\n_raiseNotFoundException(key, searchList)\nFile \u201c/home/ychen1/galaxy/galaxy-test/galaxy/.venv/lib/python3.7/site-packages/Cheetah/NameMapper.py\u201d, line 178, in _raiseNotFoundException\nraise NotFound(excString)\nCheetah.NameMapper.NotFound: cannot find \u2018standard_filters\u2019"
    },
    {
      "role": "user",
      "text": "thanks for your help, I re-install freebayes and I can use the input-filter parameters successfully. It works well now!\nYangchen"
    }
  ],
  [
    {
      "role": "user",
      "text": "how can i turn a single end data, into the paired end data and opposite( paired end into single end data)\n?"
    },
    {
      "role": "system",
      "text": "@quote\nFLASH\nThank you @system for adding in this detail. Agree with everything you state.\nFor some cases where the reads actually overlap, this type of micro \u201cassembly\u201d to generate a consensus is appropriate. And yes, that would not technically result in \u201csingle-end\u201d data and shouldn\u2019t be used that way.\n@user To learn more about FLASH, see the tool form. A short description is in the help section and link to the publication is at the bottom. :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I noticed workflow versions (v1,v2\u2026) in workflowhub and myexperiments but it\u2019s not available to the galaxy shared workflow.\nIs it possible to get all versions of a particular workflow??"
    },
    {
      "role": "system",
      "text": "@user are you talking about publishing a workflow on a Galaxy instance so that others can find it under \u201cShared Data\u201d \u2192 \u201cWorkflows\u201d?\nIf so, then the answer is: when people are importing a Galaxy-published workflow into their own account they will import only the specific state the workflow is in at that moment. Importing is a copy operation so afterwards the other user owns the copy and can start modifying, which will create new versions of their workflow.\nIf you modify the original workflow later, this will be reflected directly in the published workflow in Shared Data, but will not affect the other user\u2019s copy. If they want to get your latest changes, they will need to re-import the published workflow, i.e. create another copy.\nThe best way to deal with public-facing workflow versions is to export your workflow file and deposit it in a public version-control system. Dockstore and workflowhub are repositories tailored specifically to scientific workflows, but you can use, for example, a github repo to host your workflow and people can use Galaxy\u2019s workflow import functionality to get the workflow directly from there. This way everyone\u2019s aware of your versions and can refer to them properly."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, Galaxy! :wave: :sparkles:\nI am running a local instance of Galaxy 22.01 on my institution\u2019s slurm cluster via DRMAA. I am configuring it to send jobs to our partition, where each job is given one node and all the cpus on that node (in job_conf.xml: param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=32 --partition=vgl</param>). For more background on this cluster, when I am submitting jobs via sbatch and I need a tool from a conda environment, I need to either a) activate the conda env from the interactive head node, then submit the job, or b) have the job script explicitly init my conda, then source ~/.bashrc, then activate the required env, within the job script. Otherwise, the job can fail because it did not find the tool.\nOnto the local Galaxy install!\nSometimes when I run a workflow, a tool works once but not another time, like in this screenshot:\n\nimage1357\u00d7458 40.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d443164daf3ba4441d009ffe6c8db9fe9b52ac2b.png)\nOther times, a tool will not work when I try to invoke a workflow using it. These two BWA MEM jobs failed for different reasons in the same workflow. But then I tried invoking the workflow again, and then they both ran fine.\n\nimage (1)1404\u00d71014 100 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/03f0cc434b9764c6e583839b6b8d0a6793e1dd55.png)\nAnother instance of BWA MEM working once then not again within the same invokation:\n\nimage (2)1366\u00d7344 38 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/aa704db31d2eb79de482266466d541344e688f87.png)\nI have also been seeing this with BUSCO and QUAST. Here is an example of the workflow-invoked BUSCO failing, but then it ran fine on the same dataset when I clicked \u201crerun job\u201d:\n\nScreen Shot 2022-04-18 at 3.14.38 PM1866\u00d71312 280 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/188074e5be69a669be7c3cab1c4da1c68f916f30.png)\nSometimes for BWA MEM I get a database locked error (might be unrelated to the previous ones\u2026):\nTraceback (most recent call last):\n  File \"/lustre/fs5/vgl/scratch/labueg/galaxy_22.01/galaxy/.venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1803, in _execute_context\n    cursor, statement, parameters, context\n  File \"/lustre/fs5/vgl/scratch/labueg/galaxy_22.01/galaxy/.venv/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 732, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: database is locked\n\nFor the above database locked error, this was when I was running the bwa-mem workflow a couple times to try to debug it, so maybe it is coming from trying to run the workflow twice at once or something? I have only started seeing this error recently, like last night when trying to run it a couple times.\nFor tools that I have not run into this issue with: meryl, merqury, and hifiasm have all been running without a hitch, I have not seen any of the above errors with them.\nThe workflows I am using are found here: HiC workflow (with BWA MEM) (@link https://github.com/Delphine-L/iwc/blob/VGP/workflows/VGP-assembly-v2/VGP-HiC/Galaxy-Workflow-VGP_HiC.ga) and [iwc/Galaxy-Workflow-Long_read_assembly_with_Hifiasm_and_HiC_data.ga at VGP \u00b7 Delphine-L/iwc \u00b7 GitHub (@link https://github.com/Delphine-L/iwc/blob/VGP/workflows/VGP-assembly-v2/VGP-Hifiasm-HiC-assembly/Galaxy-Workflow-Long_read_assembly_with_Hifiasm_and_HiC_data.ga)](@link https://Hifiasm-HiC workflow (with BUSCO/QUAST)). Here is the rest of my job_conf.xml in case it is helpful:\n<?xml version=\"1.0\"?>\n<!-- A sample job config that explicitly configures job running the way it is\n     configured by default (if there is no explicit config). -->\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/>\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n            <param id=\"drmaa_library_path\">/vggpfs/fs3/vgl/store/labueg/programs/slurm-drmaa/slurm-drmaa-1.1.3/lib/libdrmaa.so</param>\n        </plugin>\n    </plugins>\n    <destinations default=\"slurm-vgl\">\n        <destination id=\"local\" runner=\"local\"/>\n        <destination id=\"slurm-vgl\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=32 --partition=vgl</param>\n        </destination>\n        <destination id=\"slurm-bigmem\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=64 --partition=vgl_bigmem</param>\n        </destination>\n    </destinations>\n</job_conf>\n\n\nAny help is appreciated, and please let me know if any more details would help! Thank you for your time! :smile:"
    },
    {
      "role": "system",
      "text": "@user if the file-not-found error only appears randomly, can it be that a few nodes in your cluster do not have access to your conda environment? Every cluster node that receives jobs need to have access to the conda environments. Alternatively, you can try to use Docker or Singularity containers.\nThe database is locked is due to the sqlite database that you are using. This is not meant for production or heavy load. Normally you will use a proper database like postgresql instead.\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Greetings!  We would like to run blasts with fairly high stringency, e.g. e values <e-50\nWhen we enter values in the \u201cSet expectation value cutoff\u201d field, it seems we are restricted to using strings of zeros.  Is there a way we can enter cutoffs in scientific notation."
    },
    {
      "role": "user",
      "text": "Thanks much Jenna.   This scientific notation for the cutoff e-value worked perfectly.  I am not sure whether there is a limit, but I tried both e-41 and e-60.  What was critical is that I needed to include the1 (as in 1e-50)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have put the download papAnu.fa to history as my reference genome, but why it does not work when I do hisat2 for alignment with other fastq files"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe statistics are there, for every HISAT2 run by default. But if for some reason there were many \u201cwarnings\u201d or processing details, the statistic report will be after those lines, and you won\u2019t see it in the expanded dataset view. Only the first few lines of stderr are captured here, as a \u201cpeek\u201d view.\nTechnically, the underlying HISAT2 tool reports the alignment statistics in a part of the job output named stderr. That\u2019s just how this particular tool works, and it doesn\u2019t necessarily indicate that there was an actual error/problem even though the name of that output includes \u201cerr\u201d (abbreviation for \u201cerror\u201d) in it. All tools have a stdout and stderr output by default, although not all tool authors report meaningful data in those places. The Galaxy wrapper around a tool can capture and (sometimes) reformat or add more information to the default stdout and/or stderr to provide more details. And some tool wrappers, like this one, have an option to report the interesting parts of those metadata as a distinct dataset in your history (just the statistics).\nHow to review the full stderr message (any tool).\nFor HISAT2, this is where the alignment statistics are captured (at the end, any content above it is worth reviewing but doesn\u2019t necessarily mean that anything went wrong. In my example, the warnings are simply informing that there were some very short reads in my fastqsanger inputs. Too short for the tool to map. A few of these kinds of warnings not unusual \u2013 but if my reads didn\u2019t map well overall, could explain one reason why (perhaps QA was too aggressive or the data was not of high quality originally).\nA job\u2019s stdout can also be reviewed this way. Plus, this is a method to review the tool parameters originally set/submitted in a simplified format (scroll further into the report than what is shown in the graphics below to find this information). This is a very useful report to learn how to review, and not only when something goes wrong (error or unexpected results).\n\nExpand the result bam dataset and click on the \u201cView details\u201d icon. This example happens to have too many warnings to show the statistics, and is probably similar to one of your jobs.\n\n\nhisat2-review-alignments-in-stderr-1.png632\u00d71026 119 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/907f5a05e7ad036c0870727ea11c0f2c7e40278d.png)\n\nThe \u201cJob details\u201d report will display in the center pane. Locate and click on the link to the stderr output.\n\n\nhisat2-review-alignments-in-stderr-2.png1218\u00d71252 125 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6c25da7e586c3666fb8b2e71d6a7f0886c7307d8.png)\n\nOn the stdout report, scroll down past the warnings to find the statistics at the end. In this example, all of my warnings except for the last one before the actual statistics were not captured in the screenshot, on purpose, to keep the graphic smaller. The last line is from the wrapper and is just a processing detail that can be ignored.\n\n\nhisat2-review-alignments-in-stderr-3.png1224\u00d7678 67.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c80f5bba52d810c070e55755d576d061b2c483a8.png)\nHow to capture only the HISAT2 alignment statistics from the full stderr message into a distinct dataset report into the history.\nOn the HISAT2 tool form, expand the section \u201cSummary Options\u201d to access the parameter to \u201cPrint alignment summary to a file.\u201d It happens to be set to \u201cNo\u201d by default. In the graphic, I changed the option to be \u201cYes\u201d, and the extra output dataset was created during job execution. It contains just the alignment statistics \u2013 in a cleaned-up format (no \u201cwarnings\u201d or technical run details). The statistics content is exactly the same as those found at the end of the stderr output above.\n\nhisat2-print-alignments-to-a-dataset.png1136\u00d7540 40.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d5bafd5c864440cfaedf706509674bece4caf2db.png)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nWhen using \u201cVCF to pg_snp\u201d (with idea to reformat later to gd_snp), Galaxy fails. The tool does not accept that some alleles have alternatives:\nAn error occurred with this dataset:\nformat interval database ? (@link https://usegalaxy.org/#)\nbad variant nt AAGA,AATG for nt 2 at /cvmfs/main.galaxyproject.org/migrated_tools/toolshed.g2.bx.psu.edu/repos/devteam/vcf2\nPlease advise on how to proceed, or on alternative tools. Thank you!"
    },
    {
      "role": "system",
      "text": "All of the Genome Diversity tools have been deprecated, and are no longer supported."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nTrying to use DESEQ2 for a couple of days now, I have the output of FeatureCounts, I also used Limma Voom successfully. However I need the tables with the p-value and the fold difference in expression so I try to get them via DESEQ2. I tried to compare the Control with each mutant (4) but I think I got it wrong. I\u2019m not sure how to import the files correctly and I couldn\u2019t find any proper explanation of the factors. Please help or an idea where to start from? :slight_smile:"
    },
    {
      "role": "system",
      "text": "Dear Sammy,\nOk where to start. Well \u2026 first here are the DESeq papers: DESeq2 (@link https://doi.org/10.1186/s13059-014-0550-8) and DeSeq (@link https://doi.org/10.1186/gb-2010-11-10-r106). Then I would recommend to look into one of the Galaxy training tutorial that uses DESeq: Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html). There is also a good biconductor post (@link http://www.bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) and a thorough document from the created and author here (@link https://pure.mpg.de/rest/items/item_1983209/component/file_1983208/content). Ah and for a visual and audio-tutorial you can watch this DESeq2 series (@link https://www.youtube.com/watch?v=UFB993xufUU), which is quite intuitively explained.\nFrom your explanation I just guess that you do not received any p-vlaues with DESeq2? If that is the case than the reason for that is probably that you only have one replicate for either mutant or control, or both. DESeq2 needs at least two replicates two calculate a p-value. This has something to do with the variance estimation.\nI hope I could help you and have a nice day!\nFlorian\n."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have the following workflow: @link https://galaxy.embl.de/u/descoste/w/tesyatacseqmm10\nIt processes paired-end atac-seq data using mm10.\nThis is a copy of an existing functional workflow to which I just modified trimmomatic to trim galore. I imagine that my problem is coming from this tool.\nIndeed, the workflow blocks after bowtie2 and does not load all steps in the history.\nMy question is, is there any log file where I can see where is the problem coming from?\nIs the only way to solve the problem to rebuild the entire thing step by step?\nThx,\nNicolas"
    },
    {
      "role": "system",
      "text": "Welcome, @user!\nThe server/shared link is not publically accessible, so we can\u2019t help to review the problem in detail.\nI can let you know that certain earlier versions of Trimmomatic didn\u2019t work well in workflows for some use cases. Changes included in recent Galaxy releases (most current version is 19.01 (@link https://docs.galaxyproject.org/en/master/releases/index.html)) and to the Trimmomatic tool itself (most current version 0.36.6 (@link https://toolshed.g2.bx.psu.edu/view/pjbriggs/trimmomatic/51b771646466)) resolved those issues.\nI would suggest contacting the administrators of that server to make sure Galaxy/Trimmomatic are at the most current versions, then modify the workflow to include that updated version of Trimmomatic. If that doesn\u2019t resolve the problem, then also ask them to update the other tools included your workflow (including Bowtie2), if not at the most current version available in the ToolShed (@link https://usegalaxy.org/toolshed), to eliminate other potential technical issues as being a factor.\nOther items to note:\n\n\nWorkflows populate output datasets into the history in the order of execution. This effectively means that not all steps will appear at the very start, but will show up as the processing proceeds over time. Now, this is somewhat dependent on the server resources, the tools invoked, and whether or not dataset collections are included. So might want to check your history now and see if more content has populated since you first wrote in.\n\n\nYou could also ask the administrators if the server is busy, causing a delay at the mapping step with Bowtie2. This is a computationally intensive tool and usually takes longer to execute than Trimmomatic.\n\n\nThere is a known issue regarding \u201cinput\u201d changes and workflow execution. The issue ticket is here, and you could try the workaround steps if you think this might be a factor: @link https://github.com/galaxyproject/galaxy/issues/7431.\n\n\nWe are working to resolve the issue above and discussing ways to make workflow execution issues/conflicts clearer for ends users to aid with troubleshooting, directly in the interface, with priority.\n\n\nFinal option:\n\n\nTest at the public server Galaxy Main @link https://usegalaxy.org to compare results.\n\n\nDownload/import the workflow, update tool versions (as needed), create a history with sample input data, and test it out to see what results. You might need to edit the workflow to remove any tools not available at this server (likely to be downstream of Trimmomatic/Bowtie2).\n\n\nIf it works at Galaxy Main, that confirms the issue is at the other server.\n\n\nIf it fails at Galaxy Main in the same way, a share link to that version of the workflow could be posted back, along with the shared history link, and we can review/help more that way.\n\n\nHope that gives some actionable choices to get this resolved!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have encountered a problem with Galaxy analyses of RNAseq data when I want to analyze the differential expression using DESeq2 or EdgeR. Below are the typical error messages. I have used HISAT2 for as the alignment program, Stringtie for assembly, and both htseq count and featurecounts to count the reads. I have tried to use the output from either htseq count, featurecounts and Stringtie into DESeq2 or EdgeR with error messages popping up after a few minutes. The original data are from Illumina next seq, paired end. I am working with Galaxy Main @link https://usegalaxy.org.\nBelow is from DeSeq2\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator. Error in data.frame(\u2026, check.names = FALSE) : arguments imply differing number of rows: 51\nBelow from EdgeR\nFatal error: Exit code 1 () Error in data.frame(\u2026, check.names = FALSE) : arguments imply differing number of rows: 51329, 58444, 53946 Calls: do.call -> cbind -> cbind -> data.frame"
    },
    {
      "role": "system",
      "text": "if your header was correct\u2026 maybe you make a mistake in loading your data in Deseq2 or EdgeR. you cant use a data twice in one run of Deseq2 or EdgeR. i think you do something wrong in loading Steps.take a Sc of your analyse data"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m unable to upload simple small (400 bytes) test file (fasta).\nThe spinner keeps spinning and nothing gets uploaded.\nI\u2019m getting 504 gateway time-out error from \u201c/api/upload/resumable_upload/\u201d. The method works on \u201c@link http://usegalaxy.org\u201d. I\u2019ve tried different browsers and internet providers, I\u2019ve also tried to switch autodetect and fasta format, but with no success.\nIs there anything I can do on my end (pasting file content is no-go for me, as actual files are much larger).\nThank you"
    },
    {
      "role": "user",
      "text": "As of now, the upload works.\nThanks"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have posted the original question here, if you\u2019d like to answer from there too:\n@link https://www.biostars.org/p/9509800/\nI have an (old) Galaxy installation at a cluster that makes use of default python 2.7. I can see that there are 2 symlinks called python2 and python2.7 that maps to a python binary inside /galaxy/.venv/bin and calling it with --version returns\n2.7.14\nI recently installed a tool using tool.xml that has the following directive:\n<configfiles>\n  <inputs name=\"inputJSON\" filename=\"myConfig.json\" data_style=\"paths\"></inputs>\n<configfiles>\n\nThe above directive tells galaxy to parse the entire UI from XML and create a JSON representation of the user choices, which is then passed to a perl script.\nWhen I ran the tool I got the following error:\nImportError: No module named xml.dom.minidom\nSince the tool and the wrappers are written in node and perl, I am guessing this has something to do with galaxy dependencies that is required to parse the xml directive above.\nI checked the default python from $PATH and it\u2019s version is 2.7.14 as well. When I go to /usr/lib64/python2.7 or /usr/lib64/python I can see that there is a folder structure xml -> dom -> minidom.py. So it means I already have this module installed.\nNext I went to galaxy/.venv and I see there is a folder called lib and another called lib64 which points to lib itself. Inside the lib there are many python scripts that are symlinks inside /usr/lib64/python2.7/. For example galaxy/.venv/lib/types.py maps to /usr/lib64/python2.7/types.py .\nAt this point I have some questions:\n\nHow can solve this dependency issue?\n\nShall I create a folder symlink called xml inside galaxy/.venv/lib that maps to /usr/lib64/python2.7/xml ? Would that be sufficient ? - Tried and this did not help\n\nShouldn\u2019t these dependency symlinks be refreshed/created when galaxy is restarted? Because when I restart galaxy, I still get the same error, meaning galaxy is still not aware that this module exists.\n\n\nAdditional information:\n\n\nlsb_release -a yields SUSE 15.1\n\nI also have a local clone of galaxy (on Ubuntu) from the repo that uses python 3.8 and CANNOT reproduce the error above\nIn both galaxy instances (the local one that uses 3.8 and one at SUSE that uses 2.7) there is a file at location galaxy/lib/galaxy/util/__init__.py and here I can see both galaxy instances are trying to import  xml.dom.minidom. However, in the older galaxy\u2019s __init__.py there is an additional line at the start of the script: from __future__ import absolute_import. Would this affect the search path or cause the problem in this question?\n"
    },
    {
      "role": "system",
      "text": "I don\u2019t really know. xml.dom is a standardmodule, so should be available. Do you have a manipulated PYTHONPATH somewhere? For example an xml.py file somewhere which confuses Python?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m trying to run AnnotateMyIDs but I get this error; please what does it mean? and what should I do, I am still a beginner.\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 9: R: not found\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 10: Rscript: not found"
    },
    {
      "role": "system",
      "text": "Update 11/16/2022\nThe indexes are restored at UseGalaxy.org (@link http://usegalaxy.org/). Prior failed runs should now complete successfully.\nThanks to all who reported the issue for their patience!\n\n\nHi @user\n@quote\n/corral4/main/jobs/046/311/46311947/tool_script.sh: line 9: R: not found/corral4/main/jobs/046/311/46311947/tool_script.sh: line 10: Rscript: not found\nTechnically, this means that the tool did not have any valid input to send to sub-functions the tool is executing. This can be due to input problems or a transient cluster issue.\nIf you are actually working at @link http://UseGalaxy.org (I see a bug report about this same issue) \u2013  the built-in annotation for this tool and the upstream Featurecounts tool is an open known issue we are still working on. The problem started about 3 weeks ago, and will likely not be fixed until later next week. I\u2019ll mark this topic for an update.\nYou can try at @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au instead for now OR supply a reference annotation GTF from the history instead of using the built-in annotation.\nTutorial\n@link https://training.galaxyproject.org/training-material/search?query=annotatemyids"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI submitted a job to produce a Circos Plot over 72 hours ago and it is still grey and waiting to run. It\u2019s a fair amount of information, and I\u2019ve read the other topics on this and so understand the process but this seems even longer than most. Could you advise please?\nThanks!"
    },
    {
      "role": "system",
      "text": "It should work now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear community,\ni run meryl for k-mer coverage counting due to the following workflow: Galaxy | Europe | Accessible Workflow | Kcov (@link https://assembly.usegalaxy.eu/u/delphine-l/w/kcov) .\nWhen doing so with a paired end Illumina dataset it first work well, i got results.\nWhen repating the same workflow with more data the job does not failed (no error message) but the resulting .meryldb file is not suitable for histogram creation within the next step, it resulted in an empty file.\nDoes anybody know what the problem is? Too much data? Too less quota?\n\nAll the Best\nThomas\nMeryl\nDataset Information\nNumber 22\nName Meryl on data 19: read-db.meryldb\nCreated Monday Jul 5th 4:38:13 2021 UTC\nFilesize 6.9 MB\nDbkey ?\nFormat meryldb\nFile contents contents\nHistory Content API ID\n11ac94870d0bb33a65184c01b231c955\nHistory API ID\ne0706b5a535dc122\nUUID a86e5aae-b668-409d-9f9e-1bdec2668a33\nFull Path /data/dnb03/galaxy_db/files/a/8/6/dataset_a86e5aae-b668-409d-9f9e-1bdec2668a33.dat\nTool Parameters\nInput Parameter Value\nOperation type selector count-kmers\nCount operations Count: count the occurrences of canonical k-mers\nInput sequences\n19: Collapse Collection on data 18, data 17, and others \n\nK-mer size selector provide\nK-mer size 21\nJob Information\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/meryl/meryl/1.3+galaxy3\nCommand Line\nexport GALAXY_MEMORY_GB=$((${GALAXY_MEMORY_MB:-8192}/1024)) && meryl count k=21 memory=$GALAXY_MEMORY_GB threads=${GALAXY_SLOTS:-1} /data/dnb03/galaxy_db/files/d/0/9/dataset_d09368a3-8058-42f6-bd3e-0ba9db38611a.dat output read-db.meryl && echo \u2018K-mer size: 21\u2019 && tar -zcf read-db.meryldb read-db.meryl\nTool Standard Output\nK-mer size: 21\nTool Standard Error\nFound 1 command tree.\nCounting 111 (estimated) billion canonical 21-mers from 1 input file:\nsequence-file: /data/dnb03/galaxy_db/files/d/0/9/dataset_d09368a3-8058-42f6-bd3e-0ba9db38611a.dat\n\nSIMPLE MODE\n21-mers\n\u2192 4398046511104 entries for counts up to 65535.\n\u2192 64 Tbits memory used\n119496818480 input bases\n\u2192 expected max count of 477987273, needing 14 extra bits.\n\u2192 56 Tbits memory used\n15 TB memory needed\n\nCOMPLEX MODE\nprefix # of struct kmers/ segs/ min data total\nbits prefix memory prefix prefix memory memory memory\n 1     2  P   285 MB    13 GM    17 MS  8192  B   142 GB   142 GB\n\n 2     4  P   278 MB  7122 MM  8905 kS    16 kB   139 GB   139 GB\n\n 3     8  P   271 MB  3561 MM  4341 kS    32 kB   135 GB   135 GB\n\n 4    16  P   264 MB  1780 MM  2115 kS    64 kB   132 GB   132 GB\n\n 5    32  P   257 MB   890 MM  1030 kS   128 kB   128 GB   129 GB\n\n 6    64  P   250 MB   445 MM   500 kS   256 kB   125 GB   125 GB\n\n 7   128  P   243 MB   222 MM   243 kS   512 kB   121 GB   121 GB\n\n 8   256  P   237 MB   111 MM   118 kS  1024 kB   118 GB   118 GB\n\n 9   512  P   231 MB    55 MM    57 kS  2048 kB   114 GB   115 GB\n\n10  1024  P   225 MB    27 MM    27 kS  4096 kB   111 GB   111 GB\n\n11  2048  P   221 MB    13 MM    13 kS  8192 kB   107 GB   108 GB\n\n12  4096  P   221 MB  7122 kM  6680  S    16 MB   104 GB   104 GB\n\n13  8192  P   227 MB  3561 kM  3231  S    32 MB   100 GB   101 GB\n\n14    16 kP   245 MB  1780 kM  1559  S    64 MB    97 GB    97 GB\n\n15    32 kP   289 MB   890 kM   752  S   128 MB    94 GB    94 GB\n\n16    64 kP   383 MB   445 kM   362  S   256 MB    90 GB    90 GB\n\n17   128 kP   578 MB   222 kM   174  S   512 MB    87 GB    87 GB\n\n18   256 kP   976 MB   111 kM    84  S  1024 MB    84 GB    84 GB\n\n19   512 kP  1780 MB    55 kM    41  S  2048 MB    82 GB    83 GB\n\n20  1024 kP  3392 MB    27 kM    20  S  4096 MB    80 GB    83 GB  Best Value!\n\n21  2048 kP  6624 MB    13 kM    10  S  8192 MB    80 GB    86 GB\n\n22  4096 kP    12 GB  7123  M     5  S    16 GB    80 GB    92 GB\n\n23  8192 kP    25 GB  3562  M     3  S    32 GB    96 GB   121 GB\n\n24    16 MP    50 GB  1781  M     1  S    64 GB    64 GB   114 GB\n\n25    32 MP   101 GB   891  M     1  S   128 GB   128 GB   229 GB\n\n26    64 MP   202 GB   446  M     1  S   256 GB   256 GB   458 GB\n\n27   128 MP   405 GB   223  M     1  S   512 GB   512 GB   917 GB\n\n28   256 MP   810 GB   112  M     1  S  1024 GB  1024 GB  1834 GB\n\n\nFINAL CONFIGURATION\nEstimated to require 107 GB memory out of 110 GB allowed.\nEstimated to require 4 batches.\nConfigured complex mode for 107.359 GB memory per batch, and up to 4 batches.\nStart counting with THREADED method.\nUsed 10.589 GB / 109.922 GB to store 0 kmers; need 0.000 GB to sort 0 kmers\nInput complete. Writing results to \u2018read-db.meryl\u2019, using 10 threads.\nfinishIteration()\u2013\nFinished counting.\nCleaning up.\nBye.\nTool Exit Code: 0\nJob API ID: 11ac94870d0bb33ad73bde806337fc6c\nDataset Storage\nThis dataset is stored in a Galaxy object store with id files10.\nInheritance Chain\nMeryl on data 19: read-db.meryldb\nJob Metrics\ncgroup\nMemory softlimit on cgroup 0 bytes\nWas OOM Killer active? No\nOOM Control enabled No\nMax memory usage (MEM+SWP) 106.9 GB\nMemory limit on cgroup (MEM+SWP) 8.0 EB\nMax memory usage (MEM) 106.9 GB\nMemory limit on cgroup (MEM) 110.0 GB\nFailed to allocate memory count 0\nCPU Time 21 minutes\ncore\nJob Runtime (Wall Clock) 26 minutes\nJob End Time 2021-07-05 19:04:55\nJob Start Time 2021-07-05 18:38:25\nMemory Allocated (MB) 112640\nCores Allocated 10\nhostname\nhostname vgcnbwc-worker-c125m425-7047.novalocal\nAWS estimate\n1.06 USD\nThis job requested 10 cores and 110 Gb. Given this, the smallest EC2 machine we could find is m4.10xlarge (160 GB / 40 vCPUs / Intel Xeon E5-2676 v3 (Haswell)). That instance is priced at 2.4 USD/hour.\nPlease note, that those numbers are only estimates, all jobs are always free of charge for all users.\nDataset peek\nCompressed binary file\ncancel (@link https://help.galaxyproject.org/tags/c/usegalaxy-eu-support/6/error/l/posted)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe error seems to be related to the compressed files; I opened an issue about it Meryl: wrong outputs with compressed FASTQ files \u00b7 Issue #3801 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/3801#issue-942263693). Could you try to decompress the fastq.gz files and relaunch the analysis? You can uncompress your files in that way:\n\n\n\n (@link https://www.youtube.com/watch?v=AfAxsbyh5P4)\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have a fastq file with this header.\n@1/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@2/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@3/1\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\nBut I need to replace the header from \u201c/1\u201d to \u201c/2\u201d\nso it looks like this.\n@1/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@2/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\n@3/2\nNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n#############################\nHow can I replace this to every line?\nIf the Galaxy tool is \u201cManipulate FASTQ\u201d, I wanna know the string syntax. Many thanks!\nMiae"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe @link http://usegalaxy.eu server is pending some changes to adapt to Python 3. This error is related to that. Other tools may also fail for this same reason until addressed \u2013 possibly any written in Python. Ping @system @system @system\nFor your purposes, try Sed instead. This is the \u201cprogram\u201d substitution expression to use. It is specific enough that quality scores will not be modified, just the format of your particular fastq sequence headers.\ns/(^@[0-9]+)\\/1$/\\1\\/2/ \n\nScreenshot:\nmanipulate-fasta-seq-name-sed1486\u00d7422 25.3 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e1a816949f40a0288910cf825e6fae5285deec5a.png)\nPlease give that a try. Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Now, I installed Galaxy Docker 20.05 and tried to join two datasets using \u2019 Join Two Datasets side by side 2.1.3\u2019. I can\u2019t get the tool to run it. Error like this : 'No module named \u2018galaxy\u2019. After searching previous list, I noticed that there were technical problem in this module and fixed it now. But, this fault is not fixed in Docker image 20.05. So, I tried to download other version of \u2018join two datasets side by side\u2019 on tool shed and didn\u2019t find anything. Are there any other option to solve this problem?"
    },
    {
      "role": "system",
      "text": "Update:\nThe Galaxy Docker (@link https://hub.docker.com/r/bgruening/galaxy-stable/) release has been updated since this question was originally asked, and dependencies for this tool would be managed correctly in that environment now using default settings.\nThe later question didn\u2019t involve a Docker image but a stand-alone Galaxy instance. The problem was due to a configuration issue that has been discussed in this Gitter chat (@link https://gitter.im/galaxyproject/Lobby?at=5f062456f6b7416284201e6b). In short, a dependency was missing.\n\n@system  First, make sure that you installed the most current Galaxy release (@link https://docs.galaxyproject.org/en/master/releases/index.html), that you setup the installation directory correctly, and review that Gitter chat help. You are probably missing that same dependency.\nFor a very simple local Galaxy intended to be used by a single person, these instructions are still valid, and it it would be easiest to install tools from the ToolShed using managed dependencies, especially if you are new to server administration.\n@quote\nHello, \nI am just getting started with developing Galaxy. \nI cloned the repository onto my local computer - MacOS running High Sierra 10.13.6. \nNext, I created a virtual environment, activated it, and then tried the run.sh script. \nA lot of things were downloaded and installed, but then the script died with this error: \nRequirement already satisfied: setuptools in ./.venv/lib/python2.7/site-packages (from sphinx==1.7.2->-r ./lib/galaxy/dependencies/dev-requirements.txt (line 32)) (41.0.1)\nUnsett\u2026\nIf that does not resolve the problem, and you need more help, please ask your new question in a new topic and include enough details so we can help. This would include at a minimum: the version/source of Galaxy, details about the local environment (OS, python version), and any admin configuration changes (@link https://docs.galaxyproject.org/en/master/admin/index.html) you have done so far, including tool installs and how those were done. We might need to bring in expertise from the Galaxy Admin working group (@link https://galaxyproject.org/community/wg/), but it is fine to get that started here."
    }
  ],
  [
    {
      "role": "user",
      "text": "how can i detach my collection.  i turn my pair end data into collection because its easier for analyses. but now i what to detach them to use them separately in limma. is there tool or options to do that ?"
    },
    {
      "role": "system",
      "text": "Have a look at this presentation from yesterday at GCC2019 (@link https://galaxyproject.org/events/gcc2019)\nSlides: @link http://bit.ly/complextutorial\nIt covers many collection operations, tutorials that focus on collections, workflow features, and more. Most of this is fairly new and it links up to the newest GTN tutorials that cover the same concepts/functions.\nIn short, it is worth learning how to construct your collections with metadata that makes it easier to sort out the data downstream.\n@quote\nits too hard too search them one by one in there\nFor this part \u2013 maybe unhide the datasets and use the search at the top of the history panel? Searches will be easier if your collections have more metadata but perhaps will help for now? Or use the tools in the group \u201cCollection Operations\u201d. All collection datasets have element identifiers and there are tools to find out what that metadata currently is plus then manipulate it and/or extra datasets based on that info.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nDoes anyone else have trouble running the PAP structural workflow?\nFor me it has been running for days now, and then it is now halted at #14: \u2018MGA to GFF3\u2019\nI have just uploaded my fasta file and run the workflow, with all default settings except for the \u2018Create or Update organism\u2019.\n/Lene"
    },
    {
      "role": "system",
      "text": "This might be the workflow you are using: @link https://cpt.tamu.edu/galaxy-pub/u/cory-maughmer/w/pap-2019-structural-workflow-v810\nIf the workflow is erroring at different steps, the jobs may be exceeding the computational resources where you are working. Or something else is wrong. You\u2019ll need to contact their support \u2013 many of these tools are custom for this domain-specific server.\nEach public Galaxy is independently administered. If that server accepts bug reports, send one in. Or you can contact them directly. Some Galaxy servers post contact on the home page and/or here: @link https://galaxyproject.org/use/.\nContact for this server (and the workflow author) happen to be posted on the homepage of the server: @link https://cpt.tamu.edu/galaxy-pub plus also here @link https://galaxyproject.org/use/center-for-phage-technology-cpt/."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI would like to delete all lines from my vcf, where ALT=\".\" in order to only have the variants represented in my vcf. I thought I could youse the genotype filter in the tool vcffilter, but I actually want to keep those lines where an ALT allele is reported, even though GT=\"./.\".\nI would be glad for help.\nThanks a lot!\nRose"
    },
    {
      "role": "system",
      "text": "The main problem with your pattern is that .+\\t acts greedily, i.e. it tries to match as much of each line as possible while still enabling a match to the rest of the pattern. That is, the first .+\\t may actually already span several columns.\nIn your previous pattern with ANN=, you prevent this by anchoring the match at that string (provided that ANN= occurs only once per line in the INFO column). In your current pattern, however, the [.]+\\t is too ambiguous since you may have other columns after ALT ending in a . (my guess would be the FILTER column).\nThis is why my revised suggestion defines a column explicitly as at least one character, which is not a TAB and which is followed by a TAB. The complete pattern then requires four such columns followed by a character that is not a . (or, alternatively, a # as the first character on the line) followed by additional characters.\nIf you prefer, you could also use the following NOT Matching pattern, which is slightly shorter: ^([^\\t]+\\t){4}\\. and simply says: skip all lines that are starting with four tab-separated columns and a . at the start of the 5th.\nIn general, you can test your patterns locally in a Linux terminal with:\ngrep -P 'your_pattern_in_single_quotes' input_file   # for a Matching pattern\ngrep -vP 'your_pattern_in_single_quotes' input_file   # for a NOT Matching pattern\nSimilarly use grep -cP or grep -cvP if you just want to count the number of lines you\u2019d keep.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I want to transfer a history that someone shared with me. I imported it into a Galaxy account I had on one server (temporary- as part of a class I did). Now I want to import it into my regular Galaxy account. I tried using the email option, but then it says I can\u2019t transfer to myself, so then I tried creating and copying the URL, but nothing appears in my Histories or Shared Histories. I used Import from File to do this - wrong way? Thanks."
    },
    {
      "role": "system",
      "text": "If the server is still up and public, or you have downloaded the history archive, then moving the history to another server is possible.\nYou state that the accounts are on different Galaxy servers. This is why the share by email didn\u2019t work. To share with another user by email, both accounts must be on the same server. All accounts on the same Galaxy server will have distinct registered email addresses.\nGalaxy FAQs:\nOptions for Data Sharing (@link https://galaxyproject.org/support/#data-options)\nLoading/Downloading Data (@link https://galaxyproject.org/support/#basics)\nTips:\n\nMake sure the history is set as \u201cShared\u201d at the original server. This is required.\nClick on the link in the archiving message (blue box) to get an updated status until noted as being completed and ready for import/download. The larger the history, the longer it will take the archive to build. Permanently deleting any data not needed will make the history smaller and the archive creation faster.\nOnce the archive is ready, clicking on the link a second time will launch a download dialogue (where to save it, etc). This can be dismissed if you are not going to download it. Copy the URL instead for a direct Galaxy-to-Galaxy transfer.\nYou can navigate away from the export page and do other thing while waiting (including logging out). But be sure to watch it, archived histories remain on servers for a set length of time (varies by the server, generally about a day or so).\nIf clicking on \u201cExport to File\u201d starts the archive creation again instead of taking you back to the original archive already in progress, the server might not support archive creation or you may have waited too long to check back.\n\nDownloading a history archive locally from one server, then uploading it to another, works for almost all cases regardless of firewalls and other server configuration. If transferring by URL fails, try that instead. Keeping a backup of your work is a good idea anyway, plus these archives can be uncompressed to use the included datasets for other purposes.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I need help. I have the genome of lens ervoides (legume) and need to find genes and symbiosis protein sequences associated with rhizobia using known protein sequences in other legumes. How do i start? How is the work flow?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI think that the best approach would be to download the gene sequences of the most closely related species from the NCBI (@link https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&BLAST_SPEC=&LINK_LOC=blasttab&LAST_PAGE=blastx&QUERY=AJ575246.1) (in that case, by restrincting the search to the Fabaceae family), and then use the NCBI BLAST+ blastn (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastn_wrapper/2.10.1+galaxy0) tool in Galaxy to identify the target sequence in the reference genome. I performed a similar analysis (@link https://usegalaxy.eu/u/gallardoalba/h/orthologous-test) which can be useful to guide you.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody. I\u2019m trying to upload my tool in the public tool shed following this guide (not using Planemo): How to publish a tool in the Tool Shed - Galaxy Community Hub (@link https://galaxyproject.org/toolshed/publish-tool/)\nIn my local instance of Galaxy, in the admin tab I can see the repository but it says that it is not possible to install it, which is confirmed also by the repository tab in the tool shed.\nI just loaded a tar file with the xml and the python code of the tool, am I missing something else?\nThank you for the help"
    },
    {
      "role": "user",
      "text": "I found the solution by using the following command for lint:\nplanemo shed_lint --tools --fail_level error\n\nIn this way it will not compile only in presence of errors."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello when i use fastq spliter to separate the forward and reverse strand no error but the final result contain no sequence shows 0 byte result"
    },
    {
      "role": "system",
      "text": "Update: I was able to find your account at the Galaxy Main @link https://usegalaxy.org public server. No more information is needed.\nWhat to do:\nThe data is an interleaved/interlaced fastq dataset from NBCI. The tool you want to use instead is the Fastq De-interlacer \u2013 this will output two datasets, one representing the forward reads and one representing the reverse reads.\nFull details are in the Galaxy support FAQs: @link https://galaxyproject.org/support/#getting-inputs-right\n\nPlease see: Reformatting fastq data loaded with NCBI SRA (@link https://galaxyproject.org/support/ncbi-sra-fastq/). Specifically, here: @link https://galaxyproject.org/support/ncbi-sra-fastq/#interlaced-forward-and-reverse-reads\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nCould the admin for this repo (@link https://depot.galaxyproject.org/yum/package/slurm/18.08/7/x86_64/) help to recompile\n\nslurm-drmaa.x86_64 : DRMAA for Slurm\nslurm-drmaa-debuginfo.x86_64 : Debug information for package slurm-drmaa\n\nThe DRMAA rpm is breaking as it is not meant for the slurm version in this repo.\nThe slurm rpm in the repo provides .so.33 and drmaa in this repo requires .so.31\n\nError: Package: slurm-drmaa-1.1.0-1.el7.x86_64 (slurm-18.08)\nRequires: libslurm.so.31()(64bit)\nError: Package: slurm-drmaa-1.1.0-1.el7.x86_64 (slurm-18.08)\nRequires: libslurmdb.so.31()(64bit)\nYou could try using --skip-broken to work around the problem\n\nThank you."
    },
    {
      "role": "system",
      "text": "This should be fixed now, sorry for the delay."
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to perform an analysis of non-templated adenosine residues added to the end of RNAs in bacteria. In bacteria poly(A) sequences, if they are present, are generally quite short. What tools could I use to identify the location and length distribution of such non-templated residues using RNA-seq data?"
    },
    {
      "role": "system",
      "text": "Hi @user\nfrom a quick look, public Galaxy servers have tools used in the paper: bowtie1 (Map with Bowtie for Illumina), cutadapt, trimming tool from fastx (Trim sequences)., SAMtools, bedtools.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I recently tried to replicate the results of a Galaxy tutorial for variant analysis (@link https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/somatic-variants/tutorial.html#variant-calling-and-classification) using @link http://usegalaxy.eu server.\nBut when almost half of the analysis was done, I came to know that some tools used in the tutorial are only found at @link http://usegalaxy.org server. In fact all the analysis was done on @link http://usegalaxy.org server. Now I am doing again all the analysis on @link http://usegalaxy.org.\nIs there any way that I can copy all the completed steps of @link http://usegalaxy.eu on @link http://usegalaxy.org, so that I don\u2019t have to run again all the steps, coz it is taking so much time for me.\nCan please someone guide me about this problem? Or is there is any way I can \u201ctransfer\u201d (use) the tools found on @link http://usegalaxy.org to @link http://usegalaxy.eu server?\nWaiting for your kind responses,\nRegards,\n~Huzaifa"
    },
    {
      "role": "system",
      "text": "Hi @user\nSeems your topic post was missed by the community. In case you still need help, or someone else wants more clarification about your questions later on (are likely things other new Galaxy users are interested in) \u2026 replies are below. Thanks!\n\nQuestion 1: Why are not all tools in each GTN tutorial hosted at all public Galaxy servers? How can I find out which server to use, per tutorial?\nHelp 1: Servers host different tools. You can check to see which public Galaxy servers are good choices per tutorial. These can change over time, and a server may actually host the tools but not be listed because the tool version was updated (meaning, the tool can be used but perhaps not exactly as described in the tutorial). How to use an updated tool is usually possible to figure out \u2013 but if you are not sure, can always ask \u2013 here (new topic) or even better at the GTN Gitter chat: @link https://gitter.im/Galaxy-Training-Network/Lobby (asking there may even get the tutorial updated).\nNotes: The particular tutorial you reference has a dedicated Docker image and isn\u2019t technically/exactly supported by any public Galaxy server \u2013 but you may still be able to use it at some of them.\nScreenshots and more info below:\n@quote\nBut when almost half of the analysis was done, I came to know that some tools used in the tutorial are only found atusegalaxy.orgserver. In fact all the analysis was done onusegalaxy.orgserver. Now I am doing again all the analysis onusegalaxy.org.\n@quote\nGalaxy Training Network Tutorials: SomeGTNtutorials are appropriate forGalaxy Mainand some are not. Where you can run each is noted per tutorial \u2013 click on theGalaxy instancesglobe icon menuand review thePublic Galaxyserver choices. If a tutorial is supported by a pre-configuredGalaxy Dockertraining image, instructions for how to get it will be listed below the tutorial listings, per category.\nScreenshot of the Hands-on for the tutorial you mention (the \u201cglobe\u201d icon menu is also here, for each tutorial):\ngtn-tutorial-available-public-galaxy-servers-within-hands-on1910\u00d71402 375 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d3a05d78b6d3941219dc6f8d7edd5cdf08261737.png)\n\nQuestion 2: Can I personally move data from one Galaxy server to another? Including public Galaxy servers?\nHelp 2: Yes. Datasets, Histories, Workflows can all be transferred directly (or downloaded to a file then uploaded to a public/private Galaxy again). For Datasets, click on the \u201cdisk\u201d icon in the expanded dataset, copy the URL, and paste that into the Upload tool at the other server. For entire Histories, create a History archive (option in the History menu \u2013 accessed from gear icon at the top right in the \u201cAnalyze data\u201d view) \u2013 once done, copy the link and paste it into the User > Histories \u201cImport from file\u201d button\u2019s function. For Workflows, go to Workflows, click on the workflow\u2019s menu and choose Share \u2013 on the next screen under Export, click on the Download button to generate a URL \u2013 paste that URL into the other server under Workflow > Import.\nNotes: On some \u201cfrom\u201d servers, Datasets may need to be in a History in a \u201cShared by URL\u201d state \u2013 and be sure to also check the box to share included datasets!. History archives always require the History (and included datasets) to be in a \u201cShared by URL\u201d state (do this before generating the archive). Histories \u2013 the History may need to be in a \u201cShared by URL\u201d state (another option in the History Menu) \u2013 be sure to also check the box to share included datasets!\n@quote\nIs there any way that I can copy all the completed steps ofusegalaxy.euonusegalaxy.org, so that I don\u2019t have to run again all the steps, coz it is taking so much time for me.Can please someone guide me about this problem?\n\nQuestion 3: Can I personally move tools from one public Galaxy server to another?\nHelp 3: No but you can request that tools are installed at other servers. Or set up your own Galaxy and install any tools you want (since you\u2019ll be the administrator!). The tools at any public Galaxy server are determined by the server administrators.\nNotes: What is the tool not available at @link http://usegalaxy.eu? Include the full name/version from the top of the tool form. The tool may be no longer be supported, have an updated version, or be replaced. Or, maybe it has simply not been added yet. We can let you know and ask the @link http://usegalaxy.eu administrators if some tool install seems needed/possible \u2013 but no promises, they make the final decision.\n@quote\nOr is there is any way I can \u201ctransfer\u201d (use) the tools found onusegalaxy.orgtousegalaxy.euserver?\n\nThere are other ways to transfer/move data between Galaxy servers or to download data as a backup. I\u2019ve added a few more tags to your post with much more Q&A help that covers a broad range of use cases around this.\nWe also just added in new features that touch on these functions \u2013 so all of the above will go into a new FAQ after the next release and older FAQs will be updated. Tutorials may also be updated or added.\nBest!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have problems with FastQ interlacer,\nthis work is taking too long to be completed"
    },
    {
      "role": "system",
      "text": "Hello @user\nPlease see the advice in this prior Q&A. It would apply for most public Galaxy servers, not just @link https://usegalaxy.org.\n@quote\nHello@systemQueue times can vary by time of day, day of week, the tool, and how many other people are running that same tool at the same time. The best strategy is to start jobs up then allow them to run. Avoid deleting and rerunning in hopes of jobs running faster, as that only puts your job back at the end of the queue, extending wait time. \nIf there are server issues, this is where to check:https://galaxyproject.statuspage.io/. Right now there are some issues with Support & Test Servers, bu\u2026\nIf you still need to contact the administrators of a different public Galaxy server, contact information is usually on the home page of the server itself or noted in their directory listing here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello:\nI would like to filter a set of reads that were aligned using Bowtie in a few different ways. 1. Filter reads that map to a certain class of genes (e.g., CDs or tRNAs). 2. Filter reads that contain a single mismatch. 3. Filter reads that contain a specific mismatch (e.g. T to C). What tools would be suitable to perform these operations?\nThank you!"
    },
    {
      "role": "system",
      "text": "Dear @user,\n(1) You can use bedtools intersect (bedtools Intersect intervals find overlapping intervals in various ways) it works with bam files. You would need to provide an annotation for your CDS or tRNAs.\n(2) You can use BAM filter Removes reads from a BAM file based on criteria\n(3) This is actually not that straightforward (to my knowledge) and would need samtools (see this post Extracting Reads Containing A Specific Variant From A Bam File (@link https://www.biostars.org/p/86199/)). The question that comes immediately to my mind is, why you want to filter reads with a specific mismatch? Because if you want to investigate specific variants and count how many reads support which variant, then this goes into variant callling (see here Galaxy Training! (@link https://galaxyproject.github.io/training-material/topics/variant-analysis/)).\nBest wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage1225\u00d7385 4.32 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c5a73721be44f9a6aff8a6e8005171024c796e3a.png)\nDe novo transcriptome assembly, annotation, and differential expression analysis\nAuthors: Anthony Bretaudeau Erwan Corre. I am following this protocol for my sample, after trimmomatic  while using trinity de nova RNA transcriptome tool its running for  1 day and it show error how do I ratify the error"
    },
    {
      "role": "system",
      "text": "Hi @user and @system,\ncould you try to run the analysis in the @link http://usegalaxy.eu server?\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Galaxy738-D)_Plot_Cladogram_on_data_7371200\u00d7900 233 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/69c353f2e3d7c74d8f5b2a8d2f2fc695a5c596e1.jpeg) Dear All,\nMy Lefse cladogram has an issue. I have too many taxa in the legend and I cannot see all of them. It is like the legend has been cut cause the plot dimension is too small. If you see on the right, my legend stops at a9 Tyzeerella, so I ma missing all taxa till c8, so around 20 taxa missing. Do you know which parameter I should change to increase the dimension of my cladogram plot and make possible to see all taxa in legend? Thanks very much."
    },
    {
      "role": "system",
      "text": "Hi @user\nAdjusting the text size has helped other people to resolve this type of issue.\nI added a tag to this topic \u201clefse\u201d that points to the prior Q&A. Review and see if that helps. This is a good one to start with: Pictures of lefse on line has incomplete display (@link https://help.galaxyproject.org/t/pictures-of-lefse-on-line-has-incomplete-display/548)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am currently doing RNA-seq analysis for tomato. I have managed to map my trimmed reads using both RNA STAR and HISAT2. However, I have failed to get past featureCounts because it\u2019s return an error every time I try. I have also tried mapping with two different reference genomes (Index of /genomes/all/GCF/000/188/115/GCF_000188115.2_SL2.40 (@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/188/115/GCF_000188115.2_SL2.40/) and Index of /ftp/tomato_genome/Heinz1706/annotation/ITAG2.3_release/ (@link https://solgenomics.net/ftp/tomato_genome/Heinz1706/annotation/ITAG2.3_release/)) but all in vain. Could I be getting the reference genome files wrong? Or what could be the problem with my analysis?\nKindly help."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis is the problem:\n\nScreenshot from 2022-04-21 15-18-57727\u00d7115 7.53 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5e02ee162a37730c6aec297a25bcfe6beedc8f30.png)\nIn order to fix it you can use the tool Text transformation (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/1.1.1) with the following expression: s/Parent/parent/g on the GFF file. Then you should use the new file as input for FeatureCounts.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey there!\nI converted FASTQ files to FASTA with the \u201cFASTQ to FASTA\u201d converter (on galaxy) as well as with the \u201cany2fasta\u201d tool I installed on my computer in order to run the \u201cABRicate\u201d tool on it. But after the conversion ABRicate doesn\u2019t give results. I\u2019m sure that ABRicate should give results because when running MEGAHIT bfore running ABRIcate it also works. But the process is supposed to work without MEGAHIT. Do you have any idea, why the other conversion do not work?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe problems is not related with the headers. It seems that ABRicate is not able to identify resistant genes when the raw reads are provided as input; it requires to assembly the reads into contigs (e.g. as you did with MEGAHIT) as previous step.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI deleted the unnecessary files and histories, purged them as well but it still shows that I used 250 GB storage, which is not accurate. I trimmed my files but could not align due to this problem. Please help!!!\nThank you"
    },
    {
      "role": "system",
      "text": "Hi @user\nLook under User \u2192 Histories. Open the advanced search and set \u201cstatus = all\u201d. The size of each history is the amount each is using.\nIf any of the histories are only deleted, they need to be permanently deleted (purged) to free up quota space. To find all stray deleted datasets across histories, or entire deleted histories, go to User \u2192 Preferences \u2192 Storage Dashboard and use the functions.\nAfter purging, go back to the top level page for the Storage Dashboard and click on the \u201crefresh\u201d button to recalculate the usage. You could also log out then back in again to refresh."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, since Monday I have been trying to upload data to galaxy europe. My data is 2.2 GB in total size, and it is compound by two files each one of 1.1GB (they`re compressed). The warning that came out from galaxy is: Request Entity Too Large (413) (this happens wether I tried to upload one file of 1.1 GB or two). How can I solve this problem?"
    },
    {
      "role": "system",
      "text": "Hi Natalia,\nthere is a limit of 1GB file size uploading through the website.\nYou can use the FTP service for larger datasets.\nPlease follow the instructions at Galaxy Europe | FTP service instructions (@link https://galaxyproject.eu/ftp)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone! I\u2019m a newer in bioinformatic and galaxy - sorry :mask:\nI trying to use RNA STAR tool. I have full human transcriptom (RNAseq) in 8files (with R1 and R2 reads). So I have to get names of genes and their characteristics (pathogenic etc) in the end for clinicians.\nI want to create vcf file for 1-snpsift variant type and 2-snpsift annotate SNPs from dbsnp. And after this I wanted download this vcf file (for ClinVar annotation etc).\nI dont need the Trimmomatic so I was trying to do next:\nFastq\u2013>RNA-STAR(hg38)\u2013>Samtoolsmerge(for mapped.bam)\u2014>FreeBayes\nBut FreeBayes show me the error:\n\n{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 101 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 101, \u201ctype\u201d: \u201cexit_code\u201d }\nJob API ID: bbd44e69cb8906b5339b2fad2d9562d3\n\nPlease explain me what I do wrong and which tools I need to use for my purpose?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI dont need the Trimmomatic so I was trying to do next:Fastq\u2013>RNA-STAR(hg38)\u2013>Samtoolsmerge(for mapped.bam)\u2014>FreeBayes\nYou should at least run FastQCand probably Fastq Info on your read data to make sure there is not a content/format problem.\nThese prior topics may also be helpful: Search results for 'FreeBayes order:likes' - Galaxy Community Help (@link https://help.galaxyproject.org/search?q=FreeBayes%20order%3Alikes)\n@quote\nBut FreeBayes show me the error:{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 101 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 101, \u201ctype\u201d: \u201cexit_code\u201d }\nDoes the Dataset Details view contain any other informative messages? If not, check the prior topics for common help with this protocol, along with the tutorials below.\nIf you need more help, please post back a share link to the history containing this error. Please leave all inputs and outputs undeleted, along with the outputs from the QA checks. Sharing your History (@link https://training.galaxyproject.org/training-material/faqs/galaxy/histories_sharing.html)\n\nTutorials\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\n\n\nGalaxy Training: NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nAnalyses of sequences\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\n\n\nGalaxy Training: Variant Analysis (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, My understanding from other posts is that the bowtie2 \u201cFatal error: Exit code 1 ()\u201d is likely an issue with FASTQ inputs; I have run these paired-end FASTQ files through bowtie2 before without issue, but this time I processed reads with fastp before bowtie2 so I am confident the error has something to do with fastp output. How do I figure out with is wrong with fastp output and how do I correct it?\nThanks,\nJustin"
    },
    {
      "role": "system",
      "text": "Thanks for pasting your error report, for some reason I haven\u2019t received it via email. I checked the job and it\u2019s very odd, there\u2019s no output from Galaxy\u2019s job scripts or the tool at all, and it only ran on the cluster for 6 seconds. It\u2019s too late now but if the same thing happens again in the future, please give the job another try and report the error.\nRegarding fastp -> bowtie2, I don\u2019t know as I\u2019m not familiar with the science side of things, but someone else will hopefully provide some insight on that."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy team,\nWe are interested in deploying and running our own Galaxy server at University Computing Centre in Zagreb. Our users expressed the desire for it, so we were hoping that you can assist us in this matter.\nWe are providing resources for all Croatian scientists so deployment should be done in scalable manner. We tested CloudMan/CloudLaunch but that one is discontinued now: @link https://galaxyproject.org/blog/2021-10-sunsetting-cloudlaunch/. We already have a cloud infrastructure based on OpenStack. Could you please recommend what would be the optimal way of deploying Galaxy on top of such infrastructure?\nWe would prefer to have a single Galaxy instance where users would be authenticated with their national AAI credentials. Ideally users would spawn their own instances through such interface, similar to JupyterLab infrastructure. Is this an option with Galaxy?\nFrom performance side, how does Galaxy perform with its tools against regular job running in terminal? Eg. bwa-mem2 aligning or processing large GWAS datasets?"
    },
    {
      "role": "system",
      "text": "HI Marko,\nthis really depends. Usually its one week hands-on. But we also had a few days online, or a week-long online training. We also had one month stay in Freiburg and set up your server with us as part of an exchange project.\nPlease also search here for \u201cAdmin\u201d: Galaxy Event Horizon - Galaxy Community Hub (@link https://galaxyproject.org/events/)\nCheers,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nIs it possible to download the exact mm10 reference file used by galaxy directly into my computer? I\u2019m trying to run a variant caller called MuTect and the error I am getting is that the mm10 file I am supplying and my aligned files (which i aligned on galaxy using BowTie and the built-in mm10 database) do not have the same order of contigs (ERROR MESSAGE: Input files reads and reference have incompatible contigs: Order of contigs differences, which is unsafe). I figured out that a few chromosomes are out of order, which means the mm10 on my computer isn\u2019t exactly the same as the mm10 used by galaxy. So the easiest way to fix this error message is to get the exact mm10 file used by galaxy and supply it to MuTect instead of re-aligning everything to the current mm10 file I have on my computer.\nThanks."
    },
    {
      "role": "system",
      "text": "Reference file for what tool? E.g. hisat2 is here: @link http://datacache.galaxyproject.org/managed/hisat2_index/mm10/\nYou can browse others at @link http://datacache.galaxyproject.org/managed/ and @link http://datacache.galaxyproject.org/indexes/mm10/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nTrying to upload dataset from 7 accessions in SRA using the \u2018(faster) download and extract reads in fastq format from NCBI RSA\u2019 tool. The only thing that is returned are 2 empty collections. The pasted entry file that I used for input nicely shows 7 ERR accesions, 1 per line. Both single-end data and paired-end data , however, returned empty. Anybody idea what went wrong?\nHenk"
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease try again now. NCBI SRA itself was very busy over the last week and we also noticed odd job failures for accessions that should have processed fine at both servers. Those tests passed for us yesterday.\nA new test history where your single example accession + a list input containing your three example accessions also ran correctly later yesterday: Galaxy | Europe | Accessible History | SRR8307998 (@link https://usegalaxy.eu/u/jenj/h/srr8307998)\nThanks for reporting the problem and please let us know if you get another odd error (please specify the server and accession, per error). Both servers and all accessions should perform the same, but details often help with troubleshooting.\ncc @system"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am analysing the RNAseq data of the mosquitoe Aedes aegypti. I need to map my reads using the reference genome AaegL5.0_GCA_002204515.1.\nIn the RNA STAR tab, there is no A.aegypti ref genome and it was mentioned below \u201cIf your genome of interest is not listed, contact the Galaxy team (\u2013genomeDir)\u201d. I couldn\u2019t find a way to do it.\nany advice? how can I solve this? Is there a way to place the ref genome in here."
    },
    {
      "role": "system",
      "text": "Hi @user\nthe tutorial is somewhat old. I believe ftp is not supported anymore. Very often the reference genomes come in \u201cready to use\u201d form. I never used NormalizeFasta, but I guess it would not hurt. Make sure NormalizeFasta does not change the sequence names. The gene annotation file must use the same sequence names, as the reference genome. Try mapping and read counting steps on one sample first, to make sure it works for you. Upload the reference genome and gene annotations using URLs (paste URLs into Galaxy Upload menu >Paste/Fetch Data tab), examine the files using preview (eye icon) and try mapping with, say, HiSAT2 followed by read counting using featureCounts.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI\u2019m trying to assemble and polish my sequences from MinION.\nFirst I used Flye to assemble, then I used Minimap2, I set the configuration of the alignment parameters to the preset option \u201cOxford Nanopore all-vs-all overlap mapping\u2026\u201d and as I don\u2019t have a reference genome, I used the same reads (my fasta file I got from Flye) as reference genome and as dataset. Then I converted the BAM output  to SAM with \u201cBAM-to-SAM\u201d.\nAfter that, I used Racon and I always got the same error: \u201cempty overlap set!\u201d. To use Racon I use three inputs: my raw sequence in fastq, the overlaps I got from Minimap2 in SAM and the output I got from Flye in fasta.\nI tried to use the ovelaps in paf. format but It didn\u2019t work either.\nDoes anyone know why I always get the same error?\nThank you.\nimage"
    },
    {
      "role": "user",
      "text": "Hi, @system,\nyou were right, I mapped reads to contigs in Minimap2.\nThe problem that when I used Flye I applied the option \u201cPerform metagenomic assembly\u201d, now I\u2019ve tried turning that option false and applying what you told me and It worked.\nThank you so much."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI tried run Trinity on server of @link http://Galaxy.org and it suddenly finished with error message below.\n\u201cThis tools currently disabled, please see Galaxy Help site at @link http://help.galaxyproject.org for assistance.\u201d\nAnd to see the detail of error, I opened the window.\n\nScreenshot from 2022-06-21 15-48-591920\u00d71080 302 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d29493e0f0bcb924c1665f84f2b8cd52ccef2f7b.png)\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2.\nThe used file type was fastqsanger.gz and the tetrameter was like the second picture.\n\nScreenshot from 2022-06-21 15-51-341920\u00d71080 342 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/65f3e2b5c9be5affd671a06fbd78d7acb61cb748.png)\nDo anyone have a solution for this problem?\nThanks"
    },
    {
      "role": "system",
      "text": "We are experiencing some memory issues with Trinity; until the problem is corrected, I suggest you to use rnaSPAdes.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nThe tool I am developing runs without any error on command line. I also check the return code ($?). Within Galaxy the process generates all the outputs. But, there is an error that is raised and renders the results with a red background instead of a green one. Strangely enough, I can view the results by clicking on the eye. I have no clue since. The only strange thing is that the stderr starts with \u201cFatal error:\u201d and continues with all the log messages (more than one hundred lines). Any tip?\nSamuel"
    },
    {
      "role": "user",
      "text": "Thanks for your tips. While I solved an intermittent error, the problem is still present. Finally I discovered that Galaxy adds a \u2018Fatal Error:\u2019 line at the beginning of the stderr if there is \u2018ERROR\u2019 or \u2018error\u2019 somewhere in the stderr of the tool. Try it yourself with the following code in a simple tool.\nHope this help.\n<command detect_errors=\"aggressive\">\n<![CDATA[\necho << EOF 1>&2\n[15:34:56] error: blablabla\n[15:34:56] error: bliblibli\nEOF\necho Done!\n]]>\n</command>"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m having trouble installing packages to RStudio on @link http://galaxy.eu.\nI would like to install @link https://www.bioconductor.org/packages/release/bioc/html/SNPRelate.html  and its dependent package @link https://www.bioconductor.org/packages/release/bioc/html/gdsfmt.html. I tried following the instructions in RStudio to use conda install : conda install -c bioconda bioconductor-gdsfmt but got the following error : Error: unexpected symbol in \u201cconda install\u201d\nNot sure what I\u2019m doing wrong here.\nI also tried installing the package using biocManager: BiocManager::install(\"gdsfmt\")\nbut that led to the following error:\n/bin/sh: 1: x86_64-conda-linux-gnu-c++: not found\nmake: *** [/opt/miniconda/lib/R/etc/Makeconf:177: R_CoreArray.o] Error 127\nERROR: compilation failed for package \u2018gdsfmt\u2019\n\nremoving \u2018/opt/miniconda/lib/R/library/gdsfmt\u2019\n\nI appreciate any advice on how to install these packages that I need for my analysis.\nThanks!"
    },
    {
      "role": "user",
      "text": "I unfortunately never got an answer so decided to do this on my local version of RStudio."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nUntitled11.png1098\u00d7532 43.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/9a13692827ffa63045d91df48db095274c968d28.png)\nIn my history you can see what i mean. So they are all the same right '/? i dont thing they have diffrence with each other"
    },
    {
      "role": "system",
      "text": "@user Has this happened again since first reported? It seems like a cluster glitch.\nThe EU server will rerun failed jobs a few times automatically. Maybe some part of that was a transient problem. If persistent still now, we can ping that server\u2019s admins again to report the problem."
    }
  ],
  [
    {
      "role": "user",
      "text": "I can import a single workflow, but I need to import multiple workflows from shared workflow list to my workflow list.\nNB: I know how to import single workflow."
    },
    {
      "role": "system",
      "text": "Hi @user, that is not supported in the graphical interface. What is your use-case?\nIf you really need to import a very large number, so that it isn\u2019t convenient to click multiple times, and you know Python, you can use BioBlend:\nfor workflow_id in workflow_ids:\n    gi.workflows.import_shared_workflow(workflow_id)\n\nBest wishes,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have noticed that many new reference genomes have appeared in RNA STAR. Would be great if Dog ((UU_Cfam_GSD_1.0/canFam4) Mar.2020) could be added to this collection, then I can complete all my mappings with the same pipeline.\nThanks in advance for your help!"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncanFam6 from UCSC is now available for various tools including STAR on @link http://usegalaxy.eu.\nPlease test and let us know if everything works as expected.\nThanks,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy Community,\nIm doing a RNASeq Analysis and having problems with the tool featureCounts. I will give a short overview of what Ive done so far and what error I get at the end.\n\nI checked the quality of my rnaseq files with FASTQC tool\nSince I Work with Pseudomonas I downloaded the genome reference (fasta) and annotated genome (gtf) from Pseudomonas Genome Database (@link https://www.pseudomonas.com/strain/download)\n2b. I unpacked the files since they are in .x.gz format\nI used the tool hisat2 for the alignment of reads to the mentioned genome reference (fasta)\nI tried to link the aligned counts (hisat2 files) to genes with the annotated genome (gtf) and the tool featureCounts\nThe job was cancelled and I got following error code: \"Fatal error: Exit code 255 (). ERROR: no features were loaded in format GTF. The annotation format can be specified by the \u2018-F\u2019 option, and the required feature type can be specified by the \u2018-t\u2019 option\u2026\nThe porgram has to terminate. \"\nThe error code mentions something with format GTF. I double checked and the annotated reference genome was in the correct format. Maybe I did something along the line that I was not supposed to do.\nIm relatively new to galaxy and I would very much appreciate your help.\n\nCheers,\nMarcel"
    },
    {
      "role": "system",
      "text": "Hi @user and @system,\nthe problem is that by default featureCounts set exon as feature filter (Advanced options/GFF feature type filter), but this feature doesn\u2019t appear in the Pseudomonas annotation file (third column). You need to specify CDS as feature type in featureCounts.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using @link https://usegalaxy.org/\nI performed bwa-mem on a dataset, and then flagstat on the output dataset to check results. The flagstat output is also a dataset, in which I can review the results of each file individually. Is there an easy way to output flagstat to just one file, and look at the results of the entire dataset combined? Is there a better way to be able to visualize the mapping?\nAlexa"
    },
    {
      "role": "user",
      "text": "Thank you that worked perfectly!\nI appreciate it!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI used Unicycler on Galaxy in the past but with sequences from Illumina only.\nThis time I am using it with long reads obtained from a MinION.\nI tried to run it twice but the two times I got the same error message:\nRemote job server indicated a problem running or monitoring this job.\nIs it a problem with the server or is it a problem with my data?\nThanks for help,\nVanina"
    },
    {
      "role": "system",
      "text": "@quote\nrun Unicycler always got \u201c/jetstream2/scratch/main/jobs/47842391/command.sh: line 110: unicycler: command not found\u201d \nI am wondering if there are something wrong with my input data or there is something wrong with the Unicycler? \nThanks.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Recently the old instance of galaxy that we host has went through some heavy utilization and crashed the server and database several times. I am currently stuck at looking at several upload jobs for one user that will not error out or go away even after a reboot.\nUnder \u201cAdmin\u201d Manage jobs I see all their jobs listed but there\u2019s no check box for me to select it and submit it for stopping. Is there a way to do this in the terminal of the server?\nthank you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nThere is a really neat Galaxy Admin utility called gxadmin which can help you out. It contains a bunch of querys and scripts that can look at jobs, fail jobs manually etc.\nYou can get it from: @link https://github.com/usegalaxy-eu/gxadmin\nThe particular query you want to run is: gxadmin mutate fail-job <job_id> where <job_id> is the job number of the stalled jobs in your database.\nThere is a little bit of setup required, mostly so that gxadmin knows how to talk to your database.\nI hope this helps.\nSimon."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.I am currently doing project in exome sequencing. My data is paired sample and when i do fastqc i get 4 outputs(one for forward and another for reverse i.e., raw data and webpage).\n\nimage1366\u00d7768 116 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e9743b744f29eb6f77d8a062533c5fb0103e0979.png)\nwhen i perform multiqc all files should be combined and it should show all the samples but it is showing only forward and reverse. I  need your help.\n\nimage1366\u00d7768 114 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4eaf49837ec8e6cdb74df2826226ba95c32d04ee.png)"
    },
    {
      "role": "system",
      "text": "@quote\nI already saw the page but i don\u2019t understand what is meant by \u201cflatten collection\u201d\nHi @user\n\u201cFlatten collection\u201d is a Collection Operation tool.\nFind the tool at @link http://UseGalaxy.eu here: Galaxy | Europe (@link https://usegalaxy.eu/root?tool_id=__FLATTEN__)\nFind it at most public Galaxy servers by either navigating to the tool panel section \u201cCollection Operations\u201d or by searching tool panel with the keyword \u201cflatten\u201d.\nGeneral process:\n\nRun the original read collection through the tool \u201cFlatten collection\u201d\nRun FastQC on the \u201cflattened\u201d read collection\nRun MultiQC on the FastQC outputs\nUse the original read collection for other downstream tools.\n\n\nNote: The original and flattened read collections both represent the same data content. The only difference is the technical formatting.\n\nTutorials for Collection Operations tools:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/search?query=collections)\n\n\nGalaxy Training: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=collections)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\n\nGalaxy Training: Using Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m currently analyzing a dataset produced from DESeq2 in Galaxy. I have two factors, each with three factor levels:\nFactor 1: Hour (0, 5, 24 hrs) Factor 2: Tissue (A, B, C)\nThe summary plots and results files only compare two factor levels at a time, i.e. 24 hrs vs 0 hrs, and tissue C vs tissue A. How can I get the results file for other comparisons, such as 5 hrs vs 24 hrs, or tissue B vs tissue A? I know in R, using the \u201ccontrast\u201d argument can do this. Is this possible in Galaxy?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\nTo directly use the contrast function, try edgeR or limma instead.\nDeSeq2 has this function (partially) batched in the Galaxy wrapped tool version. The parameter is Output all levels vs all levels of primary factor (use when you have >2 levels for primary factor). That said, this probably isn\u2019t what you want, but you can compare it.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI am creating a workflow with a tool that receives text as input.\nThese options allow me to give text as input inside the workflow creation. I am trying to change that with\n\u201cSimple inputs used for workflow logic\u201d in order to provide the text input before running the workflow and not inside the workflow creation. I change the tool configuration like this:\nThis way does not allow me to connect the 2 tools inside the workflow.\nimage\nAm I doing something wrong?\nThanks a lot!"
    },
    {
      "role": "system",
      "text": "Yeah, that\u2019s been fixed in @link https://github.com/galaxyproject/galaxy/pull/7799, I can open a PR to backport this."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey all,\nI think there is a problem in Galaxy. I am trying to feed some data into my workflow but it doesn\u2019t work. Let me explain. So I have a collection of paired-end data (fastq-dump). The following options in HISAT2 are: Single-end, Paired-end, Paired-end Dataset Collection, Paired-end data from single interleaved dataset.\nI chose Paired-end Dataset Collection in HISAT2 to build my workflow\n\nCapture 4.PNG944\u00d7471 14.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/2a8a7769014c98685829ee301529c21494887f4f.png)\nBut I can\u2019t connect them. It says: Incompatible collection type(s) for attachment.\nHowever, outside the workflow, I don\u2019t have a problem uploading the fastq-dump in HISAT2 with the Paired-end Dataset Collection setting. Why this discrepancy?Is it because of the fastq-dump?\nAlso, is there any difference like this: (instead of choosing Paired-end Dataset Collection)\n\nCapture 5.PNG991\u00d7533 15 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/7752eb192b930a27674e42912da4eacdb5114658.png)\nAnyway, one more point: feeding collection into HISAT2 workflow is almost impossible.\nSituation 1:\n\nCapture.PNG953\u00d7504 39 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/b29a90bfcdcf994085393210cb46af403a176eab.png)\nMy paired-end collection is not seen\n\nCapture 2.PNG1460\u00d7551 34.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/4c401e63b06754e354b898e83b0bb46d29d570b0.png)\nSituation 2:\n\ncapture 6.PNG1024\u00d7438 42.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/bbf8f75209dba21c21a863c800670b6bb164bb11.png)\nSame. It won\u2019t let me select my data.\nThe only way that I found so far to have an workflow with HISAT2 was to choose Paired-end data from single interleaved dataset. But this means that I have to use Fastq-Interlacer on my raw samples before aligning them which takes so much time. I decided to give up on that.\nI should add that my workflow is not really complex. I have the input --> HISAT2 --> Filter SAM/BAM tool --> Merging BAMs. (The last step is not essential. It may or may not be there for now at least).\nSorry for the long post. I hope is understandable and is making sense."
    },
    {
      "role": "system",
      "text": "@quote\nBut I can\u2019t connect them. It says: Incompatible collection type(s) for attachment.\nThen that is probably true. Make sure you set the input collection to either \u201cpaired\u201d or \u201clist:paired\u201d, not \u201clist\u201d on the right hand side\n\nScreen Shot 2019-05-20 at 09.51.21.png1370\u00d7296 38.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/966f5344a4910b9f39a35618adf5d4fc2dc8b000.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am doing RNA-seq analysis, the pipeline is as follows: upload data > FastQC > Trimmomatic > HISAT2 > Feature counts > Deseq 2 > Annotate Deseq 2 output tables. The reference I used for annotation is \u201cmm10. refgene.gtf.\u201d However, the output shows just NA, like picture showed below, what is the problem?\n\nimage1472\u00d7245 27.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6fb3ed2ffbeff44f17bdd9850fd8773eee356d26.png)\nThanks for help\uff01"
    },
    {
      "role": "system",
      "text": "Dear @user,\nHave you used just one sample (replicate) for DESeq 2? DESeq2 does not calculate p-values (signifiance) if you just use one replicate.\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am following the galaxy with ansible installation tutorial (Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)) but fail when running the playbook (Ubunutu 22.04.2). It stops at the step \u201crun miniconda installer\u201d and prints the following error that I don\u2019t understand. What can I do?\nfatal: [localhost]: FAILED! => changed=true \n  cmd:\n  - /bin/sh\n  - /tmp/ansible-miniconda-installer.rnn9nixs.sh\n  - -b\n  - -p\n  - /srv/galaxy/var/dependencies/_conda\n  delta: '0:00:01.715882'\n  end: '2023-08-08 08:05:18.684320'\n  msg: non-zero return code\n  rc: 1\n  start: '2023-08-08 08:05:16.968438'\n  stderr: |2-\n...\nstderr_lines: <omitted>\n  stdout: |-\n    PREFIX=/srv/galaxy/var/dependencies/_conda\n    Unpacking payload ...\n  stdout_lines: <omitted>\n\n\nimage5112\u00d71736 637 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/26b024bbbb6637c9f1bfe998960ea6a892aa130f.png)"
    },
    {
      "role": "user",
      "text": "In case anyone else has a simlar issues: This PR solved the issue for me: Proposed changes to job-destinations and ansible-galaxy admin tutorials to prevent ansible-playbook errors. by Edmontosaurus \u00b7 Pull Request #4308 \u00b7 galaxyproject/training-material \u00b7 GitHub (@link https://github.com/galaxyproject/training-material/pull/4308)\nShould result in the following output:\nimage2090\u00d7810 147 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cacad230ed8c7f6f97658dc66377691304d6b966.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Community,\nI have recently doing LefSe analysis on line. Now i am facing a problem. The output picture of the lefse is incomplete. To avoid such problem, do I need to set some parameters in LefSe or anything else?\nimage\nThanks for your kindly help."
    },
    {
      "role": "user",
      "text": "Sorry, I didn\u2019t get many labels as you. I adjusted \u201cRation of the horizontal space to be given to the right side (for adjusting for label legend)\u201d in \u201cSet some graphical options to personalize the output\u201d, and got a whole figure."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m trying to run DESeq2 with Sailfish files. I have two factors (three files for each factor) and I\u2019m running the program with my own gene mapping file. I have been getting the below error:\nError in read.table(tx2gene, header = hasHeader) :\nmore columns than column names\nCalls: get_deseq_dataset -> read.table\nHas anyone seen this? I\u2019m not quite sure what the problem is."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe DESeq2 tool in Galaxy can work with count files that may or may not have a header line.\nIf your file starts with\nName\tLength\tEffectiveLength\tTPM\tNumReads\n\nyou need to select Files have header? -> Yes.\nFor data from sailfish you need to select\nChoice of Input data -> TPM values\nProgram used to generate TPMs > Sailfish\nand then you need to provide the GTF/GFF file or Tabular file with Transcript-ID to Gene-ID mapping.\nThis should be the same file you used as input to sailfish.\nFinally note that the successor software to Sailfish is Salmon, which is also available in Galaxy."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I am working with my organization to set up a Linux virtual machine (RedHat) to run a local Galaxy instance. Our IT department will make sure the required Python is installed. This VM will run behind a firewall and they\u2019d need to whitelist any access to the outside world. In order to install the Galaxy instance and tools, I was wondering which sites I would need to request to be white listed. I have Github and the toolshed site on my list. Do you happen to know which other sites I\u2019ll need to include? Thank you so much and I\u2019m sorry if this is a silly question."
    },
    {
      "role": "user",
      "text": "Thank you all so much! This has been really helpful. I am planning to use the ARTIC workflow on Galaxy to process SARS-CoV-2 data so I had to also include the Galaxy project site, auspice, nextstrain/nextclade, GISAID/Epicov, ENA, and the NCBI sites. I am sure I\u2019ll run into more problems with the firewall but this is a great start."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.\nI am trying to login and get data using \u2018@link http://usegalaxy.org\u2019.\nHowever, I was not able to login and get message as  \u2018Uncaught exception in exposed API method:\u2019\nAnd the downloaded data does not run and kept on gray block for an hour now.\nCould you help me with this issue?"
    },
    {
      "role": "system",
      "text": "We\u2019ve had an unexpected downtime, details: @link https://status.galaxyproject.org/incidents/f48lrj68k4hb. It should be ok now, sorry for the inconvenience."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am the developer of Pharokka and am trying to use Pharokka v 1.2.1 on the EU server (the adding of this to the toolshed was extremely helpfully handled by Paul Zierep! thanks Paul!)\nWhen I go to run Pharokka, there is no Database available to run - would you be able to add it?\nGeorge"
    },
    {
      "role": "user",
      "text": "Looks like it works now  @system (someone in the background fixed it so thanks Galaxy support)!\nI\u2019m not responsible for the Galaxy installation, but I am happy to help you install it locally in future if you run into troubles."
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried to install some tools and got this error below. Not sure why since a month ago I could still install tools. Galaxy version 20.05 on Ubuntu.\nraise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: @link https://toolshed.g2.bx.psu.edu/api/repositories/get_repository_revision_install_info?name=deeptools_plot_pca&owner=bgruening&changeset_revision=69264da0ce77\nCan someone please help? Thank you!"
    },
    {
      "role": "user",
      "text": "Hi, this was a problem with the toolshed and they fixed it pretty fast. Hope this worked out for you too."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nwe have a tool which creates many (>100) output files, and would like to achieve a single history item (or whatever you would call it) from the tool with the list of output files with one download button.\nWe have managed to configure the tool to create a list (dataset) but in addition we get all the 100 history items, one per output file in addition. See the screenshot.\n\nimage_2021_05_12T11_48_21_775Z277\u00d7901 23 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a084b2f3198dfbb18fb8e3a8c43d7e71f5880181.png)\nThe relevant lines in  the tool are\n <collection name=\"output_MC\" type=\"list\" label=\"Conventional Analysis for MC\">\n              <filter>input == \"MC\"</filter>\n             <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/MC/\" />\n</collection>\n\nHow to only get the List called \u201cConventional Analysis for MC\u201d in the screenshot, and not all the rest of the hist.MC. items in History?\nHelp much appreciated!\nThanks,\nMaiken"
    },
    {
      "role": "system",
      "text": "Hi @user,\n@quote\n<outputs>\n\t <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/\" recurse=\"true\" visible=\"false\" />\n </collection>\nThat is not a valid output section and the tool would fail to load at this point.\nIt should probably be\n<outputs>\n     </collection>\n         <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.root\" directory=\"Histograms/\" recurse=\"true\" visible=\"false\" />\n     </collection>\n</outputs>\n\nYour initial tool looks correct and the filter is supposed to work.\nvisible=\"false\" is not needed, that is the default setting, although it doesn\u2019t hurt adding.\nI\u2019d strongly suggest linting and testing tools with @link https://planemo.readthedocs.io/en/latest/readme.html#basics-galaxy"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am currently analysing data acquired from RNA Sequence from a specific cell type (microglia) from mouse brain. I have performed differential expression gene analysis by using DESeq2 between my control and my treatment. I would like now to perform some type of gene set enrichment analysis. According to the tutorials I can either perform fgsea or egsea.\n\nAre there any other tools that I can use?\nIf I want to use fgsea:\n\n\nwhat rank can is it better to use? I was thinking to use either the simple logFC or the signed fold change (ranked by foldFC multiply with the p values). However for the second option I don\u2019t know how to do it without R\n-Moreover, I am only working with only a cell type, will I be able to use the mouse hallmark set, or any other online available set?\n\n\nI have similar question also for the egsea tool. Working only with a cell type, can I use the MSigDB gene set which is available in the tool?\n"
    },
    {
      "role": "system",
      "text": "Hi @user\nMaybe these tutorials will help?\n@link https://training.galaxyproject.org/training-material/search?query=fgsea\nYou can use R in Galaxy, these cover how:\n@link https://training.galaxyproject.org/training-material/search?query=rstudio\nFor your questions \u2192 the single cell tutorials and transcriptomics tutorials have some overlap. You can certainly mix methods: QA and sorting/counting steps are distinct from higher levels of data reduction as far as I know. The complex indexes you mentions are used for both. Maybe try a few different ways and compare if you are not sure if some index is a fit for your data. The species involved, and not the sequencing methods, will matter after a certain point.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am a new Galaxy user doing my first RNA-seq analysis. I had my data in a collection and aligned it using HISAT2. This generated a collection titled \u201cHISAT2 on \u2026: aligned reads (BAM)\u201d with a list that I then attempted to enter into MultiQC, but the HISAT2 collection does not appear in the option (individual HISAT2 files for each sample can be selected). When I ran HISAT2 tool I had these parameters for output:\n\n\n\n\nOutput alignment summary in a more machine-friendly style.\nTrue\n\n\n\n\nPrint alignment summary to a file.\nFalse\n\n\n\nDo I need a printed alignment summary in order to be able to run MultiQC?\nI would really appreciate any help on this topic, thanks!"
    },
    {
      "role": "system",
      "text": "Hello @system @user @system\nPlease see this prior Q&A:\n@quote\nTo output statistics directly fromHISAT2, that output option needs to be set to \u201cYes\u201d on the tool form. Find it nested under the section \u201cSummary Options\u201d. This would allow you to useMultiQCto compare multiple BAM results together in a single report.\nGalaxy has had some interface style updates since the original post, but the optional output toggle is still in the same place on the tool form, and will look like this:\n\nhisat2-summary-output-toggle-for-multiqc892\u00d7212 17.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d347859633cb25a681dd9a484827cf3011a8dc9f.png)\nHope that helps everyone!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m trying to install Galaxy local on a server with Ubuntu 18.04.4 and python 3.10.2 .\nFollowing the steps on the Get Galaxy page, I successfully cloned the git repository. Now the problem is that when I try to run galaxy for the first time using sh run.sh (or sudo sh run.sh) I get the following error:\nCreating Python virtual environment for Galaxy: .venv\nusing Python: python3\nTo avoid this, use the --no-create-venv flag or set $GALAXY_VIRTUAL_ENV to an\nexisting environment before starting Galaxy.\nFetching https://files.pythonhosted.org/packages/source/v/virtualenv/virtualenv-16.7.9.tar.gz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 5001k  100 5001k    0     0  25.3M      0 --:--:-- --:--:-- --:--:-- 25.3M\nVerifying /tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9.tar.gz checksum is 0d62c70883c0342d59c11d0ddac0d954d0431321a41ab20851facf2b222598f3\nUsing base prefix '/usr'\nNew python executable in /home/andreaf/galaxy/.venv/bin/python3\nAlso creating executable in /home/andreaf/galaxy/.venv/bin/python\nTraceback (most recent call last):\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 2634, in <module>\n    main()\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 860, in main\n    create_environment(\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1162, in create_environment\n    install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages=site_packages, clear=clear, symlink=symlink)\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1721, in install_python\n    fix_local_scheme(home_dir, symlink)\n  File \"/tmp/galaxy-virtualenv-pIquIq/virtualenv-16.7.9/virtualenv.py\", line 1807, in fix_local_scheme\n    if sysconfig._get_default_scheme() == \"posix_local\":\nAttributeError: module 'sysconfig' has no attribute '_get_default_scheme'. Did you mean: 'get_default_scheme'?\n\nHow can I fix it?\nThank you for the help"
    },
    {
      "role": "user",
      "text": "Ok, was just following the guide in the first link and they say to put that code in the requirements.yml file so I don\u2019t get what\u2019s wrong\u2026 Should I use something like\n---\nroles:\n - src: galaxyproject.galaxy\n   version: 0.9.16\n - src: galaxyproject.nginx\n   version: 0.7.0\n - src: galaxyproject.postgresql\n   version: 1.0.3\n - src: natefoo.postgresql_objects\n   version: 1.1\n - src: geerlingguy.pip\n   version: 2.0.0\n - src: uchida.miniconda\n   version: 0.3.0\n - src: usegalaxy_eu.certbot\n   version: 0.1.5\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I found a website called tavaxy which claims that it\u2019s platform can convert taverna workflows to galaxy but all the post and the website are pretty old(from 2011 and 2012) and I couldn\u2019t find a way to open the platform.\nany updates on this?\ndoes Taverna to galaxy is possible? and how would that work?"
    },
    {
      "role": "system",
      "text": "Hello - Tavaxy (@link http://www.tavaxy.org/) was developed by Moustafa Ghanem when he was at the Center for Informatics Sciences, Nile University, Giza, Egypt, in around 2011-12. In 2013 the work (and the dept) was disrupted by political events in Egypt. In 2014 Moustafa tragically died and AFAIK work ceased.\nThe Taverna team are concentrating their efforts on the The Common Workflow Language as an interoperability platform for workflow systems. Galaxy is part of that initiative. @link http://www.commonwl.org"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi colleagues\nUnfortunately, I have tried several times to use your website  ( @link https://main.g2.bx.psu.edu/). However, no respond. Is there any temporarily problem?\nRegards\nAdnan"
    },
    {
      "role": "system",
      "text": "Hi Adnan,\ncan it be that your country is blocking the access of this website? Can you try if you reach @link http://usegalaxy.eu?\nCiao,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, when I tried to extract MAF blocks from a set of regions of the human  2013 GRCh38/hg38 assembly, the message \u201cNo options available\u201d popped out under \u2018Choose species\u2019. This operation works on the 2009 assembly GRCh37/hg19. However, if I send MAF alignments from the UCSC Table Browser using the 2013 GRCh38/hg38 assembly, I can actually join them and choose species! But, these are whole alignments, not blocks, so it doesn\u2019t help getting MAF blocks of specific regions, ie exons.\nIt would be great if the MAF blocks from the GRCh38/hg38 assembly could be extracted directly from genome coordinates of that assembly. I hope this can be fixed soon.\nThanks!"
    },
    {
      "role": "system",
      "text": "See updated topic here: Attempting to use Stitch MAF blocks tool, but lack of documentation (@link https://help.galaxyproject.org/t/attempting-to-use-stitch-maf-blocks-tool-but-lack-of-documentation/10091)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI would like to use the HyPhy-aBSREL tool to run selection tests on several genes. Is it possible to include an option to set the \u201cmultiple-hits\u201d option (not currently available) and set the \u201c\u2013branches\u201d to labeled in addition to the current options?\nThanks\nVInny"
    },
    {
      "role": "system",
      "text": "I don\u2019t have any direct control over @link http://usegalaxy.eu or other public servers, but the new wrapper is merged and I\u2019ll be updating @link http://usegalaxy.org with it as soon as it\u2019s deployed to the toolsheds, which should only be another 15-20 minutes. If you\u2019re running a local or cloud galaxy, you\u2019ll be able to update your installed version in the same time."
    }
  ],
  [
    {
      "role": "user",
      "text": "Do we have any platform or workflow for analyzing single cell  atac-seq data?\nthank you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthere is a fantastic training on ATAC-seq analysis (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html) in the Galaxy Training Platform.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nAm a new user for this platform.\nI have a paired end sequences samples from two groups of soils (farm B and farm C), each group has a sub-set of five samples B1,B2, B3, B4 and B5 for farm B and C1, C2, C3, C4, C5 for farm C. Is their a workflow show how to:\n\nset the sub-set of each group as forward and reverse read?\nmerge the two groups , so it will be easy to manipulate with my data.\nmake groups that the Galaxy know which sub-set is belong to which group.\n\nI will be appreciate your help"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI recommend you to have a look at this tutorials:\n\nRule Based Uploader (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/upload-rules/tutorial.html)\nRule Based Uploader: Advanced (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/upload-rules-advanced/tutorial.html)\n\nIt explain how to organize your input datasets in different groups.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi team, is your \u201cmain tool shed\u201d down at the moment? I\u2019ve tried the \u201cInstall new tools\u201d button from the Admin panel of 3x different Galaxy instances (all in different OS\u2019s and remote locations) and they all timeout at \u201cWaiting for @link http://toolshed.g2.bx.psu.edu\u2026\u201d\nNothing to be found in terms of scheduled maintenance/outages at:\n@link https://galaxyproject.org/main/notices/\n@link https://galaxyproject.org/news/\ngalaxy-dev Digests Vol 149, Issues 9-11"
    },
    {
      "role": "system",
      "text": "toolshed is back up now: @link https://toolshed.g2.bx.psu.edu/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I could not display the full taxon name of the legend labels on left side of graph while drawing the plot cladogram in LEFse analysis. Some of the indicator/legend labels are as follows. How can I do this?\ncladogram_indicator"
    },
    {
      "role": "system",
      "text": "You are working at the public Galaxy server in the quoted text below, correct?\nThey have custom versions of the Lefse tools, so contacting them is the best way to report potential bugs. Or, you can just try a different setting and see what happens. It seems your input choice on the C) Plot LEfSe Results (Galaxy Version 1.0) tool form should be valid as described within the form, but then down in the help section below it is described differently (?). I\u2019m guessing that the second is the correct usage since that seems to match the type of output you are actually getting in the result.\nTool form advanced setting:\n\nNumber of label levels to be displayed (-1 means all)\n\nTool form help section:\n\nText and label options\n\nNumber of label levels to be displayed: when dealing with hierarchical features this option sets the level to be displayed in the labels (-1 means the last level only, 1 means the highest level, 2 means the first two levels and so on).\n\n\nHuttenhower Galaxy server info. You can do both \u2013 modify the usage yourself for a quicker result but also report the conflict in the instruction to them. Maybe the usage changed at some point and they missed changing the instructions in both places.\n@quote\nThe tool you are using is specific to thehttp://huttenhower.sph.harvard.edu/galaxy/domain-specific public Galaxy server.The tools and protocols hosted there differ significantly from theLefsetools hosted at most other public Galaxy servers (including usegalaxy.* servers).The administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).Contact:Huttenhower LabHelp/tutorials: (also linked from the server\u2019s home page, with example data available):lefse \u2013 The Huttenhower Lablefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHub"
    }
  ],
  [
    {
      "role": "user",
      "text": "I ran Trinity for RNA seq assembly yesterday. It has been gone for nearly 24 hours but is still operational. Could you please tell me if Trinity is operational in galaxy Euorpe?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nrun time depends on amount of data,  quality and resources allocated for the job,  and some Trinity assembly take long time. Also, sometimes Galaxy does not refresh history panel, so completed jobs are displayed  as running. Click at Galaxy Europe banner in the top left corner to refresh the web page and refresh the history. I don\u2019t know the current wall time for Galaxy Europe, but in they we very generous in the past. You may also consider alternative tools, like rnaSPAdes.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI recently upgraded our local Galaxy instance from 21.09 to 22.05 and I noticed two things:\n\n\nAs recommended in the documentation, I switched from supervisor to systemd using the sample config file provided (@link https://docs.galaxyproject.org/en/release_22.05/admin/scaling.html?highlight=systemd#systemd): however, when Galaxy is started this way, tools give errors such as:\njobs_directory/052/52678/galaxy_52678.sh: line 9: mkdir: command not found\nas if the PATH was not right\u2026\nIf I start Galaxy manually from the virtualenv using galaxyctl start, no such error appears.\n\n\nI had set up Galaxy reports on \u201c/reports\u201d, but could not find a way to start it again on a prefix in the new version. Gravity states it handles Galaxy Reports, but it does not with the (modified) sample config file, and if I start it using run_reports.sh, it still only listens on \u201chost:9001/\u201d, not \u201chost:9001/reports\u201d. Setting galaxy_url_prefix does not change anything.\n\n\nAm I missing something?"
    },
    {
      "role": "system",
      "text": "Hi @user\nIt is recommended to upgrade Galaxy in the order of releases. Did you first upgrade to 22.01, then 22.05? Releases \u2014 Galaxy Project 22.05.1 documentation (@link https://docs.galaxyproject.org/en/master/releases/index.html)\nIf not, the developers will likely ask for you to try that first. But let\u2019s see if they have advice now. I posted your question to the Matrix admin chat. They may reply here or there, and feel free to join the chat. You're invited to talk on Matrix (@link https://matrix.to/#/!rfLDbcWEWZapZrujix:gitter.im/%24A00M8h5vzZA22cGf52mpi5qNWI88PyQLDmCEAQR2ElU?via=gitter.im&via=matrix.org)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having an issue with mapping using HISAT2 and Bowtie 2. I am trying to map my genome to a reference genome. Both programs run, but the BAM files only show is QNAME and all the other components of the table are blank. I have already gone through trimming and QC and the quality scores are all above 30.\nThis is example of an error in mapping stats:\nWarning: skipping mate #2 of read \u2018V300007992L3C001R0141268281/2\u2019 because it was < 2 characters long Warning: skipping mate #1 of read \u2018V300007992L3C001R020207830/1\u2019 because length (1) <= # seed mismatches"
    },
    {
      "role": "system",
      "text": "This is when viewing the BAM dataset, correct? Only the first line will display immediately. BAMs are compressed data. Viewing these is a special function in Galaxy and it takes some time to load the data up in the application. I was able to look at one of your smaller results and it took a few minutes. The first line loaded first, then the header (fairly large from the custom genome), then finally the data lines after scrolling down the page.\nTo view or work with the content in an uncompressed format, convert with BAM-to-SAM. To generate statistics on a BAM dataset, there are many tools \u2013 search the tool panel with the keyword \u201cbam\u201d and/or review the tools under the group SAM/BAM."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to use the \u2018qiime2 tools import\u2019 for SampleData[SequencesWithQuality] with \u2018Single Lane Per Sample Single End Fastq Directory Format\u2019.\nI think I\u2019ve correctly build the manifest and metada files. But I keep getting the same persistent error regarding the element \u2018name\u2019.\nThis parameter asks me for \u201cFilename to import the data as. Must match regex: .+_.+_L[0-9][0-9][0-9]_R[12]_001.fastq.gz\u201d but it doesn\u2019t matter which name I try it always seems to fail, so Galaxy doesn\u2019t even execute my job.\nI\u2019ve also tried to look up RegEx matchers to know if my created name would be suitable but these don\u2019t seem to be helpful.\nCan you help me in this matter? I\u2019m not really a coder hahaha but I really want to understand this.\np.s. everything runs perfectly when i\u2019m using QIIME2 on command line, the problem seems to be nested in galaxy\u2026 (which I prefer because buttons)"
    },
    {
      "role": "user",
      "text": "Meanwhile in the Matrix people already helped me with this problem.\nSo for the data \u201cARA.SOIL.1fastq.gz\u201d, the name \u201cARA_S1_L001_R1_001.fastq.gz\u201d doesn\u2019t create an error.\nHope this helps anyone having the same problem."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, for several years I have been using Galaxy to host my bigwig files, which I then visualize in the UCSC genome browser. Just recently, I have found that Galaxy doesn\u2019t allow this feature anymore. To elaborate: Typically, I would upload my bigwig file to Galaxy, and then I could simply click the bigwig file in my history to open a small box which displayed an option that said (maybe with different wording) \u201cview in UCSC.\u201d Clicking this option would immediately take me to the UCSC genome browser, with my bigwig file being displayed as browser tracks. This was great, and was exactly what I wanted. But in recent weeks, this option to \u201cview in UCSC\u201d has disappeared from my bigwig files being hosted on Galaxy. There is no longer a clickable link that says \u201cview in UCSC.\u201d Could you help me understand what is going on? Is this feature totally gone from Galaxy? As in, do I need to find another host for bigwig files which I want to view in the UCSC genome browser? Or is this feature still available in Galaxy, but just re-worked in a way that I haven\u2019t realized? To try an alternative approach, I clicked the \u201ccopy link\u201d icon for my bigwig on Galaxy, and tried pasting that link when I manually upload a custom track to the UCSC genome browser, but this approach is also not working (it prompts an error from UCSC saying that they could not open the file being specified by the link which I copied from Galaxy for my particular bigwig). It seems strange that Galaxy would still be okay with hosting our files, but not be okay with making those files (a bigwig file, in my case) viewable/accessible to other sites like the UCSC genome browser."
    },
    {
      "role": "system",
      "text": "Hello!\nYep, you guessed it \u2013 it\u2019s still in Galaxy but accessed a different way.  We\u2019ve consolidated all the visualization options to a single interface, and if you click on the visualization icon for the dataset in question:\nimage\nAnd then in the middle panel at the top, this should be what you\u2019re looking for:\n\nimage732\u00d7416 25.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2ad2e2d6a9e845865fe0238962fca8109a1602ad.png)\nLet me know if this isn\u2019t working correctly for you and I can dig deeper.  Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI would like to request adding the latest tomato genome assembly to Galaxy Main.\nThe latest genome assembly is called \u201cSL4.0\u201d and also S_lycopersicum_Feb_2019.\nHere is the sequence from the tomato genome Web site:\n@link ftp://ftp.solgenomics.net/tomato_genome/assembly/build_4.00/S_lycopersicum_chromosomes.4.00.fa.gz\nMany thanks,\nAnn Loraine (IGB project)"
    },
    {
      "role": "system",
      "text": "Hi,\nsorry for the late reply. We install this Tomato genome on @link http://usegalaxy.eu\nCheers,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy community, I am new to Galaxy and had a question while developing my local instance and running my files through the local instance.\nWhat I have tried so far:\nSo I created a local Galaxy instance with default configurations, and after becoming an admin following the guidelines in this tutorial: Get Galaxy (@link https://galaxyproject.org/admin/get-galaxy/) I created a simple workflow by following this tutorial while I am still an admin in my local instance Create and share workflows in Galaxy - YouTube (@link https://www.youtube.com/watch?v=m2u4sG3uYqc&ab_channel=BISCNG)\nand wanted to share this with my end user as a service for RNA-Seq analysis keeping these rules in mind Ten Simple Rules for Setting up a Galaxy Instance as a Service (@link https://galaxyproject.org/admin/ten-simple-steps-galaxy-as-a-service/)\nChallenge:\nBut once I log out of the web instance in which I was the admin and log back in as a non admin user into the same local instance I created, I am not able to see the workflow I created. I guess I was assuming that once the workflow is created, anyone who has access to the local instance can just access the workflow and run their files through it.\nTo summarize:\nI am interested in sharing this workflow with my end user so that they can simply open up this galaxy instance, input their files and the files run through the galaxy instance which gives them the desired output in the end! How do I accomplish this?\nThank you in advance!\n-Aditya Natu"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry this:\n\nLog into your account with the workflow.\nShare the workflow with all other users working on that same Galaxy server by publishing it.\nPublishing creates a URL that can be shared with others, but also places the shared object directly under \u201cShared Data > Workflows\u201d.All users working on that same Galaxy server will have access, and they can import and use the workflow in their own accounts.\nAny account can \u201cpublish\u201d into Shared Data: 1) Histories (and the Datasets inside of them), 2) Workflows, 3) Visualizations, 4) Pages.\nAny account can also \u201cshare\u201d work. Directly with the other person\u2019s account email address, or by generating a share link URL and giving that URL to them (however you communicate: text, slack, email, etc)\nOnly admin accounts can add data into Shared Data: 1) Data Libraries. It is a different way of organizing commonly used data and can save you some storage space since data in a Data Library imported by a user is just a linked clone of the original dataset.\nWorkflows do not consume any space at all in any account in terms of quota usage, but these files of course do consume a tiny amount of space on disk. That said, Datasets and Histories are much larger and matter more in terms of managing the overall disk size of your instance.\n\nFAQs: Galaxy Support (@link https://galaxyproject.org/support/)\n\nSharing and Publishing your work (@link https://galaxyproject.org/learn/share/)\nData Privacy Features (@link https://galaxyproject.org/learn/privacy-features/)\n\nTutorials that include share functions: Galaxy Training! (@link https://training.galaxyproject.org/training-material/search?query=share)\nWebinars. The first and fourth will probably be of the most immediate use for you: Galaxy-ELIXIR webinars series: Advanced Features | ELIXIR (@link https://elixir-europe.org/events/galaxy-elixir-webinars-series-advanced-features)\nNote: The FAQs and Tutorials show a slightly older version of Galaxy (the application changes fast!), but you should still be able to find the \u201cShare or Publish\u201d form for any Galaxy object and share/set permissions how you would prefer to. In practical terms, you might want to not grant editing permissions the workflows if you want the users to import and run the workflow exactly how it is published. The Webinar series was new this month, and includes operations using the most current Galaxy version.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "this invoked workflow has been stuck at what should\u2019ve been a relatively short step. It\u2019s been \u201crunning\u201d for a few weeks. Ran this on @link http://usegalaxy.eu\nJob API ID:\t11ac94870d0bb33a1b57076a9eb8139e"
    },
    {
      "role": "system",
      "text": "Hi @user\nI hope things are resolved by now. If not, please try reaching out to this dedicated chat usegalaxy-eu/Lobby - Gitter (@link https://gitter.im/usegalaxy-eu/Lobby).\nRelated Workflow invocation scheduling failed - Galaxy EU (@link https://help.galaxyproject.org/t/workflow-invocation-scheduling-failed-galaxy-eu/9251)"
    }
  ],
  [
    {
      "role": "user",
      "text": "We just finished the upgrade to Galaxy 20.01 (from 19.09) and everything works well, except that all uploaded files default to the data type of ZIP. I\u2019ve tested CSV, TSV, FASTA, TXT, and a couple more, and they all have \u201cformat: zip\u201d once uploaded/processed and in my history.\nI can, of course, manually change each item\u2019s format using the \u201cedit attributes\u201d option, but this is not desirable.\nAny ideas why this may be happening? Thanks in advance."
    },
    {
      "role": "system",
      "text": "Can you share an example of such a dataset as you try to upload it ? Note that zip archives should not ever be auto-selected on a normal release of 20.01, we have introduced a zip sniffer only in Galaxy release 20.05. Do you have a custom datatype_conf.xml file or a zip datatype installed from the tool shed ?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m the administrator for a local Galaxy instance for a few users on a dedicated server in my lab  [v19.05, Apache proxy, Linux Debian 10]\nFor some time now, we\u2019ve run into an annoying and mystifying issue :\nwhen anyone (myself included) tries to login, the site cycles back to the welcome page, without taking authentication into account\nHowever, it remains possible to:\n\nhave a wrong user/password combination rejected\nreset a user\u2019s password\nregister a new user\n\nBesides, AFAIK, nothing has changed on the server ! (Issue started before recent Debian upgrade from 10.2 to 10.4)\nAnd I cannot find any relevant error in the logs (Galaxy, Apache or PostgreSQL) :exploding_head:\nAny suggestion/possible test would be appreciated\u2026\nThanks in advance\nPS : Of course, no amount of service restart(s), or even server reboot, helped"
    },
    {
      "role": "user",
      "text": "OK, final update :\nThe issue was eventually resolved when I was able to use \u201cproper\u201d SSL certificates for Apache (i.e. instead of self-signed ones), that were supplied by my University\u2019s IT department.\nNo more login issues, including with the latest browser versions :sunglasses:\nHope this might help someone else\u2026"
    }
  ],
  [
    {
      "role": "user",
      "text": "Limited by the resources of our own platform, we accepted the suggestions to use the @link http://usegalaxy.org.\nBut when we using the @link http://usegalaxy.org, some tools are unreachable. So we upload the tools needed by us to the Repositories, but it still can\u2019t be used in @link http://usegalaxy.org.\nMay I ask how the tools uploaded by myself can be included in the public tools shed? Thanks!"
    },
    {
      "role": "system",
      "text": "Galaxy administrator chooses what tools to install in their Galaxies. The Main Galaxy at @link http://usegalaxy.org does not have all the tools from the Toolshed installed, rather it only has a curated set of tools most useful to the community."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!\nI assembled raw and trimmed reads (Illumina PE150) using metaSPAdes and then clustered into bins with MaxBin2. Now I want to annotate these bins by using Prokka but I\u2019ve got this error:\nArgument \u201c1.7.8\u201d isn\u2019t numeric in numeric lt (<) at /usr/local/bin/prokka line 259.\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/759/40759167/_job_tmp -Xmx28g -Xms256m\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/7\nI\u2019ve uploaded the bins obtained from MaxBin2 in fasta format.\nCould anyone help me with that?\n\nerror661\u00d7653 87.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/17c387538a0bdb0700ac7955818ab678380b0b80.jpeg)\nThank you in advance!"
    },
    {
      "role": "system",
      "text": "Update:\nThe tool is now running correctly. Please try again.\nThank you to everyone who reported the problem :slight_smile:\n\nHi @user\nThe tool has a new known configuration issue specific to @link http://UseGalaxy.org. The fix is in progress but not completely done yet. Tracking ticket: Prokka 1.14.6+galaxy1 = missing dependency \u00b7 Issue #354 \u00b7 galaxyproject/usegalaxy-playbook \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/354)\nMeanwhile, please run the tool at a different public Galaxy server. Both of these are good choices and host a toolset similar to the US server: @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au.\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve been successfully using galaxy.auth.providers.ldap_ad with auto account creation for over a year, and today it started throwing an error. There is a chance this is the first new user to log in since I upgraded to release_19.05.  Here is the log output:\ngalaxy.auth.providers.ldap_ad DEBUG 2019-09-26 16:03:06,064 LDAP authentication successful\ngalaxy.auth.util DEBUG 2019-09-26 16:03:06,071 Email: someone@example.com, auto-register with username: someone\ngalaxy.web.framework.decorators ERROR 2019-09-26 16:03:06,097 Uncaught exception in exposed API method:\nTraceback (most recent call last):\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/web/framework/decorators.py\", line 282, in decorator\n    rval = func(self, trans, *args, **kwargs)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 116, in login\n    return self.__validate_login(trans, payload, **kwd)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 137, in __validate_login\n    message, user = self.__autoregistration(trans, login, password)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/user.py\", line 93, in __autoregistration\n    user = self.user_manager.create(email=email, username=username, password=\"\")\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/managers/users.py\", line 99, in create\n    self.session().flush()\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py\", line 162, in do\n    return getattr(self.registry(), name)(*args, **kwargs)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2446, in flush\n    self._flush(objects)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2584, in _flush\n    transaction.rollback(_capture_exception=True)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py\", line 67, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py\", line 2544, in _flush\n    flush_context.execute()\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 416, in execute\n    rec.execute(self)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py\", line 583, in execute\n    uow,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 245, in save_obj\n    insert,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 1116, in _emit_insert_statements\n    statement, params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 980, in execute\n    return meth(self, multiparams, params)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 273, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1099, in _execute_clauseelement\n    distilled_params,\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1174, in _execute_context\n    e, util.text_type(statement), parameters, None, None\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1458, in _handle_dbapi_exception\n    util.raise_from_cause(sqlalchemy_exception, exc_info)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py\", line 296, in raise_from_cause\n    reraise(type(exception), exception, tb=exc_tb, cause=cause)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1171, in _execute_context\n    context = constructor(dialect, self, conn, *args)\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 719, in _init_compiled\n    for key in compiled_params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 719, in <genexpr>\n    for key in compiled_params\n  File \"/home/galaxy/galaxy-dist/.venv/local/lib/python2.7/site-packages/sqlalchemy/sql/type_api.py\", line 1194, in process\n    return process_param(value, dialect)\n  File \"/home/galaxy/galaxy-dist/lib/galaxy/model/custom_types.py\", line 373, in process_bind_param\n    value = value[0:self.impl.length]\nStatementError: (exceptions.TypeError) '_hashlib.HASH' object has no attribute '__getitem__' [SQL: u'INSERT INTO galaxy_user (create_time, update_time, email, username, password, last_password_change, external, form_values_id, deleted, purged, disk_usage, active, activation_token) VALUES (%(create_time)s, %(update_time)s, %(email)s, %(username)s, %(password)s, %(last_password_change)s, %(external)s, %(form_values_id)s, %(deleted)s, %(purged)s, %(disk_usage)s, %(active)s, %(activation_token)s) RETURNING galaxy_user.id'] [parameters: [{'username': 'someone', 'disk_usage': None, 'form_values_id': None, 'deleted': False, 'email': 'someone@example.com', 'last_password_change': datetime.datetime(2019, 9, 26, 16, 3, 6, 92741), 'external': False, 'active': True, 'password': <sha1 HASH object @ 0x7f9950dda620>, 'purged': False, 'activation_token': None}]]\n\nPlease let me know if you know what broke. I checked the github issue tracker but didn\u2019t see anything that seemed related.\nthanks,\n-Will"
    },
    {
      "role": "system",
      "text": "@user Thanks for the compliment, your report and the traceback were very helpful in tracking the issue down. It should be fixed by @link https://github.com/galaxyproject/galaxy/pull/8739 , let us know if you want to have this backported to 19.05."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hola,\nWe got a problem with \tvelvet toolshed.g2.bx.psu.edu/repos/devteam/velvet/velveth/1.2.10.0\nThe tool standard error is \u201cError running velveth Killed\u201d\nWe use a virtual machine with 4go RAM for the moment.\nPlease help\nThanks"
    },
    {
      "role": "system",
      "text": "Update: If still having problems with Velvet, try an alternative assembler:\n@quote\nWe recommendUnicyclerfor small genome assembly.GTN Tutorial:http://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m using Galaxy to get terminator regions for all genes in a genome and I\u2019m using the \u2018get flanks\u2019 tool.\nQuestion: does get flanks define the 3\u2019 end of the gene on the sense strand as the \u2018Around end\u2019 regardless of the direction of transcription? So if the gene is transcribed from the antisense strand, get flanks would be taking the flanking region around the transcription start site\u2026?\nI hope this makes sense, but let me know otherwise.\nThanks,\nDavid"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf you include the strand in the BED6 input, the tool will extract upstream/downstream correctly. For any BED dataset (Genome Browser FAQ (@link https://genome.ucsc.edu/FAQ/FAQformat.html#format1)), the coordinate numbers are always with respect to the forward strand but which bases those represent depend on the strand assignment.\nScroll down on the tool form for the examples: both are extracting the same coordinates on different strands as output. Those are based on different stranded inputs combined with some same parameters and some different parameters set on the form.\nIf you are not sure or want to check settings against your data: try running the tool on a small sample that includes both strands and visualize the results in IGV.\nThanks @system for linking to two posts together :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi.  It\u2019s been a couple years since I used Galaxy regularly so I\u2019m a bit rusty.  I have just uploaded a custom genomic scaffold file in fasta format to my history and I need to know how to process the file so that I can query it looking for promoter regions upstream of two common genes (cytoplasmic actin, polyubiquitin) using the coding sequence from closely related species.  Thanks for any help!"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI tried opening the file in IGV but it tells me that I\u2019m missing a .fai index file and it can\u2019t recapitulate that file.\nGalaxy creates fa.fai indexes at runtime (when needed).\nYou can also create the index directly and have it show up as a dataset in your history.\nThe help in this topic covers all the use cases (any fasta, not just from Unicycler): Opening Unicycler assemblies with IGV local (@link https://help.galaxyproject.org/t/opening-unicycler-assemblies-with-igv-local/2653)\nHope that helps :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy team,\nI need to include new variable modifications which are absent in the list. Is it possible to add these modifications to Galaxy? Like in the MaxQuant configuration tab?\nWith best regards,\nArman"
    },
    {
      "role": "system",
      "text": "Yes,\nin that case, you need to open a Pull Request in order to include those modifications in this file tools-galaxyp/macros.xml at master \u00b7 galaxyproteomics/tools-galaxyp \u00b7 GitHub (@link https://github.com/galaxyproteomics/tools-galaxyp/blob/master/tools/maxquant/macros.xml#L256). Let me know if you need any additional help.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Why is this error happening:\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/bin/extract_aln_ends.py\", line 102, in <module>\n    alns_bedpe = alns.bam_to_bed(bedpe=True, mate1=True, ed=True)\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/bedtool.py\", line 806, in decorated\n    result = method(self, *args, **kwargs)\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/bedtool.py\", line 337, in wrapped\n    decode_output=decode_output,\n  File \"/usr/local/tools/_conda/envs/__bctools@0.2.2/lib/python3.5/site-packages/pybedtools/helpers.py\", line 356, in call_bedtools\n    raise BEDToolsError(subprocess.list2cmdline(cmds), stderr)\npybedtools.helpers.BEDToolsError: \nCommand was:\n\n\tbedtools bamtobed -mate1 -i /data/jwd/main/038/753/38753581/tmp/tmpahcdouwa/fixedmates.bam -ed -bedpe\n\nError message was:\nThe edit distance tag (NM) was not found in the BAM file.  Please disable -ed.  Exiting\n"
    },
    {
      "role": "system",
      "text": "It depends on the tool used for mapping the samples; e.g. in order to include the NM tag when using RNA STAR (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.8a+galaxy0), it should be specified in the BAM output format option.\n\nSelection_001881\u00d7432 66.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2040961c8bae42c1d275b4a69c8f8bd6b08db1b6.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Apologies if this is too chatty, not an actual problem, just an anomaly, and from experience anomalies are often signalling a hidden problem.\nBasically, my local Galaxy installation (runs within my University firewall) installation processes far faster than when I run the equivalent on a faster stand alone machine (native applications). Worries me as to why? I doubt there will be a specific answer?\nSo running 20Gb FASTQ files through a workflow that concatenates across lanes then Hisat2>samtools sort/view>stringtie\u2026\nOn my Galaxy server (ubuntu on an iMac Intel i3 4-core 24GB RAM) takes about 12hrs.\nOn my much newer iMac (Big Sur) Intel i9 8-core (16 virtual) 48GB RAM this takes days.  As far as I can see the command line options are the same except that the sever version ADDs FASTQ groomer and FastQC.\nAny thoughts? It seems the Samsort bundled within Galaxy\u2019s Hisat2 >bam is the place where they are so different.\nKindest Regards\nRichard"
    },
    {
      "role": "system",
      "text": "I guess I misunderstood, I thought you are comparing workflow invocation amongst two Galaxy installations.\nBut in this case it seems we are actually debugging your iMac software configuration that is mimicking the Galaxy workflow\u2019s dependency stack. This is possibly significantly more complex. I\u2019d start with validating the results \u2013 are they the same?\nIf they\u2019re not I\u2019d probably install a Galaxy instance on your iMac and duplicate the workflow invocation properly. This will be rather easy compared to building the same dependency stack because in Galaxy each step is executed in an isolated environment \u2013 i.e. you can have different versions of the same dependency invoked during the workflow.\nControlling the number of resources with GALAXY_SLOTS has also possible significant effect on the speed of computation."
    }
  ],
  [
    {
      "role": "user",
      "text": "So I\u2019m trying to wrap tools into galaxy using planemo, however I\u2019m getting variations of this error: \u201cFatal error: Exit code 1 ()\nFATAL(lofreq_call.c|main_call:1293): Cowardly refusing to overwrite file \u2018/mnt/galaxy/files/000/208/dataset_208081.dat\u2019. Exiting\u2026\u201d\nthe .dat is the output eg \u201clofreq call --ref \u201c/mnt/galaxy/files/000/208/dataset_208056.dat\u201d \u201c/mnt/galaxy/files/000/208/dataset_208052.dat\u201d -o \u201c/mnt/galaxy/files/000/208/dataset_208081.dat\u201d\u201d\nhere is the XML @link https://gist.github.com/Micropathology/be429cade2224f3107317c1e5b2d521a\nIs there something about handling outputs that I\u2019m missing?"
    },
    {
      "role": "system",
      "text": "i know it\u2019s a work-around but have you tried to give a name to the output and say to galaxy to take it from the working dir? for example:\nas command :\n\n<![CDATA[ lofreq indelqual --dindel --ref \"$input1\" \"$input2\" --out foo_bar.bam ]]>\n\n in the output block\n <data  format=\"bam\" name=\"output\" from_work_dir=\"foo_bar.bam\"/>"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a local instance of Galaxy installed. I found out that the jobs_directory file is 71.6 GB. It has a file named \u201c/galaxy/database/jobs_directory/000/119/working/splitby.nameOrCount.dat.extra.temp\u201d which is 68.6 GB alone. The workflows I use in galaxy only takes about 6.7 GB. What is the function of this file? Will deleting it cause any problems?"
    },
    {
      "role": "system",
      "text": "If your cleanup policy in the galaxy config is not \u2018always\u2019 then the jobs_directory will slowly grow.\nIf you need to have your cleanup policy set to anything else then you will need to use something like @link https://linux.die.net/man/8/tmpwatch to clean up the folder. This can be automated with cron.\nThere is also this issue (@link https://github.com/galaxyproject/galaxy/issues/8242) that causes the jobs_directory to grow quickly.\nIf you are running out of space while running a workflow, I recommend enabling the \u201cOutput cleanup\u201d option in tools that it makes sense for the workflow."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am at the step of using the tophat tool for aligning my rna seq reads to the genome. I have paired end reads. The tophat tool asks me for \u201cMean Inner Distance between Mate Pairs\u201d. How do I calculate this? I know my sequence length is 101bp. I do not know anything more. Do I need to ask the sequencing company for this information?"
    },
    {
      "role": "system",
      "text": "Hi @user, I recommend you to test different values (e.g. 100, 50 and 0), and evaluate the alignments in order to infer the optimal parameter value.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "In the last two weeks the tool join two datasets has been upgraded from version 2.1.2. to 2.1.3 and now I can\u2019t get the tool to run.it keeps failing with the error Traceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nIt also now says that This tool requires galaxy-util (Version 19.9.0). Click here (@link https://galaxyproject.org/tools/requirements/) for more information.\nCan any one please advise as to what I need to do please to get this tool to work.  Many thanks"
    },
    {
      "role": "system",
      "text": "@system @user\nThank you for reporting the problem!\nThis tool is undergoing a correction as are a few other text manipulation tools. Please see this prior post linked below for details. I am going to close this topic out to keep ongoing Q&A all on the same topic.\n@quote\nI am following along with a tutorial given on the coursera genomic data science course. It involved first joining two datasets on a column column to produce another data set. Then it involves grouping the resultant dataset on a column. In the video it shows that there is a drop down list of column names from the dataset being grouped to pick from. No such drop down appears. When I tried typing the column name in from the dataset it seems to have ignored it when executing the task with an error i\u2026\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have several jobs waiting to run in RNA STAR for already days. Is there any trouble with this tool?\nThanks"
    },
    {
      "role": "system",
      "text": "There were some delays in handling larger jobs in the past 12 hours or so, but everything should be moving now. Sorry for the trouble - if your jobs still have not started, please post your @link http://usegalaxy.org username, or if you prefer to keep it private, email to @link mailto:galaxy-bugs@galaxyproject.org, and I can check the state of your jobs."
    }
  ],
  [
    {
      "role": "user",
      "text": "what is the differences between this two tools, and which one i should use for pseudoalignment.\nafter alignment which tool can i use for normalization?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou should use Kallisto quant; Kallisto pseudo runs only the pseudoalignment step and is meant for usage in single-cell RNA-seq. If you want to perform a differential expression analysis you do not need to normalize it . You can feed DESeq2 with the output generated by Kallisto.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to do alignment using trimmed, paired (by Trimmomatic) R1 and R2 fastq files. I\u2019m using appropriate reference file (.fa). While executing the HISAT2 operation I\u2019m getting the following error.\ne[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nI don\u2019t understand what the issue is about. Please help me to solve this issue."
    },
    {
      "role": "system",
      "text": "Hi @user\nWe communicated about this via email already, but for others reading:\nThis was a transient cluster error at @link http://UseGalaxy.org that is now corrected. Any jobs that failed with this error should be rerun.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI tried to upload data in tabular file. It began to be load and after few seconds it stops and I have a message error \u201cInternal Server Error (500)\u201d.\nCould you please tell me how to fix it?\nThank you very much and have a good day,\nBest regards,\nKathy"
    },
    {
      "role": "system",
      "text": "Hi all, @system @system @user @system\nIf the failures were at @link http://UseGalaxy.org, it seems to be working now and maybe there was a transient problem that resolved itself. I just tested uploading with the different methods with a broad range of datatypes, specifically: \u201cChoose local file\u201d (file on the user\u2019s computer),  \u201cChoose remote files\u201d (file from a public URL), and \u201cPaste/Fetch data\u201d (file uploaded via FTP first , and, from a hosted directory). All were successful.\nPlease try again now if you haven\u2019t already. If problems persists, please explain with more details about:\n\nWhat exactly is going wrong, including which upload method was used\nFile datatype if different from the original question\u2019s tabular file\nWhere this is happening (server URL)\n\nThanks for reporting the problem and hopefully it is resolved for everyone now :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I can\u2019t get featureCounts to complete on my GVL/Galaxy instance running on AWS. If I download the same HISAT2 output and use the same built-in reference (hg19) on the public server, the job completes just fine. Any thoughts?"
    },
    {
      "role": "system",
      "text": "So just to verify, the upload issue has been resolved? Was that related to the size of the node?\nRegarding the console not launching, are you able to access the cloudman console?\n\nimage1920\u00d71063 220 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d2e698b6468f423327f3a9b8ae1d0fb55f641d26.png)\nIf so, you can edit the Galaxy app settings (3 dot icon on the Galaxy app panel), and specify a different timeout for the k8s_walltime_limit in job_conf.yml.\n\nimage1019\u00d7666 73.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8fa21c4b19c8002197b46bb0d635c94fc1202a77.png)\nYou can try bumping that up to a larger number."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I tried to process my data set with the help of the information and tutorials in Clustering 3K PBMCs with Scanpy, but every time I got an error when using the Import Anndata and loom tool.\nI fixed the defects that I found myself and edited my gene files\nAnd now I have uploaded the data in the Import Anndata and loom tool.\nIt is possible for you to take a look at my history and tell me if there is a problem with the files or options selected in the Import Annadata and loom tool.\nI must say that the processing of this data is very important and essential for me and I will be very happy if you help me.\nHistory link:\n@link https://usegalaxy.eu/u/gh.maaryaam1996/h/unnamed-history-1"
    },
    {
      "role": "system",
      "text": "Hi @user\nEverything looks Ok from a quick review.\nIf the current job fails, try removing the header line from the tabular dataset \u201cnew 2.txt\u201d, then a rerun.\nPs: When you delete datasets, others cannot see them in the shared history view. So, if you want the best feedback, please leave all inputs and outputs undeleted."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have some very basic questions regarding @link http://usegalaxy.eu\n\nCan I somehow set up new tools on my own on @link http://usegalaxy.eu? For example MetaMaps?\nCan I upload large databases? (Up to more than several hundred GB)\nIf yes, what is the entry point to set up a new tool?\nIf no, I there any other service providing this options or do I have to set up my own machine to do this?\n\nThanks you!"
    },
    {
      "role": "system",
      "text": "1.+ 3. No, but you can ask the admin, for a trusted and popular tool I think you have a good chance. Admin of every instance has special interface and API to install tools.\n2. Most public instances have storage quota to ensure fair use. Hundreds of GB won\u2019t fit in most public Galaxy instances I know.\n4. Setting up Galaxy is convenient (@link http://getgalaxy.org/), but given the size of data you want to process and possible analyses you can run keep in mind you will need the corresponding hardware."
    }
  ],
  [
    {
      "role": "user",
      "text": "Would it be possible for galaxy to include  Camelus dromedarius reference genome in built in index ?"
    },
    {
      "role": "system",
      "text": "Ok, great, thanks for posting back.\nUCSC hosts many of the genomes as well, so that is probably where we will get it. That is where you could get it as well to avoid having to merge chromosome file together. @link https://hgdownload.soe.ucsc.edu/hubs/UCSC_GI.assemblyHubList.txt\n\nGCF_000803125.2\tCamDro3\tCamelus dromedarius\tArabian camel (Drom800 2019 refseq)\t9838\tmammals\n\nI\u2019ll mark this for updates later. Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "This problem applies to both web-based galaxy and 19.05 local.\nWhat\u2019s the current way to rename output based on input name?\ncurrent workflow: input database -> Trim Galore! (paired-end)\nI tried to rename output as #{input1}_trimmed, #{input_name}_trimmed, #{input1}_trimmed, and for all of them I got _trimmed as new file name. What should I put inside { }?\n@link https://galaxyproject.org/learn/advanced-workflow/variables/\nSince I am trying to assemble about 100 sequences, default naming does not allow me to keep track of them."
    },
    {
      "role": "system",
      "text": "Hi @user\nYou may have figured this out already \u2013 different tools name the \u201cvalid input variables\u201d in different ways. The workflow editor prompts with the proper variable(s), per tool.\nTwo screenshots as examples. The first is the tool you are using:\n\nworkflow-rename-example1950\u00d7608 68.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c24e965bf54199803b3c6f7d356d31d791d4e435.png)\n\nworkflow-rename-example2822\u00d7568 56.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/90ffd71af0752e4cf65b2ee9b85c201b34fc9793.png)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI deleted some histories permanently, but didn\u2019t give back the storage space. What can I do? Thanks!"
    },
    {
      "role": "system",
      "text": "Welcome, @user!\nPlease try logging out of Galaxy then log back in. This will reset the quota usage quicker.\nEventually, the server will update if you stay logged in, but that is a scheduled task that can sometimes take longer when the server is busy.\nFAQ: * The account usage quota seems incorrect (@link https://galaxyproject.org/support/account-quotas/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have been attempting to identify polymorphic DNA sites according to this workflow: @link https://usegalaxy.org/u/coursera/p/genomic-data-science-with-galaxyidentify-polymorphic-sites.\nI am trying to use Freebayes to identify polymorphic sites from the SAM file that I cleaned up. However, I keep getting an \u201cunable to find FASTA index\u201d error irrespective of the version I use. Any feedback would be helpful. Thanks!\n\nScreen Shot 2020-03-29 at 12.07.16 PM2560\u00d71600 554 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/82b32bf467682ee7f6e0920c3be54c6901a85a08.png) \nScreen Shot 2020-03-29 at 12.05.51 PM2560\u00d71600 565 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6180d6e7fe4aa4f1d3d913da196eab3e3b51fe30.png) \nScreen Shot 2020-03-29 at 12.15.11 PM2560\u00d71600 697 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4d1d2ee884dbf0effd402b9d2115420f25cb040c.png)"
    },
    {
      "role": "system",
      "text": "Hi and welcome,\none suggestion first: it is way more helpful if, for reporting errors, you click on that little bug icon on the failed datset and copy/paste the complete error message you find there, instead of posting screenshots.\nNow for your actual issue: the error is not about a missing FASTA index, but about specific entries in there. In other words, one or more of the chromosome names listed in your SAM/BAM inputs could not be found in the reference genome you\u2019ve been trying to call variants against.\nFor your dataset 31 that makes lots of sense since Group1 is most definitely not a chromosome identifier anywhere. For dataset 32, the name is cut off in the screenshot, but it is up to you anyway to check whether this is a valid chromosome name in your analysis.\nBest wishes,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI used Import Annadata and loom to process my single cell data\nMy genes file included 28 thousand gene symbols and ensemble IDs\nNow, although the output of this tool is green, the number of genes is considered zero\nAnd in the matrix he created, my genes are not there\nwhat is the reason?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nMy genes file included 28 thousand gene symbols and ensemble IDsNow, although the output of this tool is green, the number of genes is considered zeroAnd in the matrix he created, my genes are not there\nSome tools do just output nothing if there are technical \u201cmismatches\u201d.\nItems to check for:\n\nUnexpected content/format in your input datasets versus the tool form parameters (usually involves datatype issues).\nDifferences in ID formats contained in different input datasets and/or server indexed reference data (important \u201ckeys\u201d like gene names).\nThe tool form may be set up to look for specific attributes that are missing from the inputs (example: loom inputs have several default keys set for data parsing-- does your data contain those same keys?).\n\nAnd, there is a version of the Import Anndata and loom tool at that should be avoided. AnnData Import/ AnnData Manipulate not working? (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-scanpy-pbmc3k/faqs/anndata_not_working.html).\nSingle cell tutorials are here:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\nIf you need more help, please share more details. Considering sharing your history \u2013 as this tool processes several different input/output types and those details matter. Leave any inputs and outputs undeleted, and note the output dataset number that is unexpectedly empty.\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors \u2013 covers how to review logs for any job, not only red/failed\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages \u2013 covers the same top-level technical data problems we would check for at this forum\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history \u2013 public is best (more people can help) but if you would rather share this privately, state so and we can set up a direct message thread to use\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am going through the tutorials of DNA methylation analysis and once I have selected the bwa-meth for alignment. There is no option available to select the refence genome. I am also attaching a screeshot.\n\ngalaxy_problem1336\u00d7522 47.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4e1c2d23378945acbb7600cd0f562afbb3ccf1de.png)"
    },
    {
      "role": "user",
      "text": "Thank you. It has been resolved. I was using wrong server @link http://usegalaxy.org"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am following galaxy 101 tutorial.\nWhich exon had the highest number of repeats? How many repeats were there?\n\n\n\n\nchr22\n19150024\n19150283\nENST00000086933.2_cds_2_0_chr22_19150025_r\n0\n-\n\n\n\n\nchr22\n27796762\n27800543\nENST00000302326.5_cds_1_0_chr22_27796763_r\n0\n-\n\n\nchr22\n50603840\n50605064\nENST00000329492.6_cds_4_0_chr22_50603841_f\n0\n+\n\n\nchr22\n17119390\n17121127\nENST00000331437.4_cds_0_0_chr22_17119391_r\n0\n-\n\n\nchr22\n19766388\n19766867\nENST00000332710.8_cds_8_0_chr22_19766389_f\n0\n+\n\n\n\nHow many repeats were there?\n3\nIs the answer above is correct? I have shared my history.\n@link https://usegalaxy.org/u/ashgourd/h/galaxy101-continued-25mar22\nAppreciate your help. Thanks."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe dataset posted represent just the \u201cexons\u201d.\nWhat tutorial are you following? The answer is probably in dataset 20, not dataset 21, and the tutorial should include the answer. But that\u2019s just a guess \u2013 it looks like you may have missed a step, creating empty datasets in the middle of the history.\nThis is a different version (from GTN) for comparison: Galaxy 101 (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101/tutorial.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nThis is a follow-up question to the topic I had asked last week here (@link https://help.galaxyproject.org/t/freebayes-errors-out-in-v-1-3-6-vs-1-3-1/9186):\nAs per the guidelines by @system in that topic, I started with a fresh install of Galaxy and enabled access to reference genomes and indexes via CVMFS. I can now access reference genomes and indexes for almost all tools except that twobit does not show the hg38 genome (for the tool \u201cTranslate BED transcripts\u201d) in the dropdown options.\nI looked at data tables and looked in to the twobit folder and I see hg19 only for human plus the number of options are also quite less (~47 entries). Is this related to the CVMFS cache size (set at 500)? Should I be seeing a lot more options in the twobit folder?\nThank you"
    },
    {
      "role": "system",
      "text": "Just run the extra 2bit indexer. Choose the existing hg38 database as the source."
    }
  ],
  [
    {
      "role": "user",
      "text": "Help! I\u2019m running a local dev instance of Galaxy purely to write a wrapper for an in-house R script and I have a failed job preventing Galaxy from loading at all, at least I think that\u2019s the issue:\n\ngalaxy.jobs.handler ERROR 2021-01-14 13:54:24,285 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] failure running job 621\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 408, in __handle_waiting_jobs\nself.dispatcher.put(self.job_wrappers.pop(job.id))\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 1007, in put\nself.job_runners[runner_name].put(job_wrapper)\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 152, in put\njob_wrapper.enqueue()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1456, in enqueue\nself._set_object_store_ids(job)\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1478, in _set_object_store_ids\nobject_store_populator.set_object_store_id(dataset)\nFile \u201clib/galaxy/objectstore/init.py\u201d, line 1082, in set_object_store_id\ndata.dataset.object_store_id = self.object_store_id\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018dataset\u2019\ngalaxy.jobs.mapper DEBUG 2021-01-14 13:54:24,305 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2021-01-14 13:54:24,318 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Dispatching to local runner\ngalaxy.jobs DEBUG 2021-01-14 13:54:24,464 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] (622) Persisting job destination (destination id: local)\ngalaxy.jobs.handler ERROR 2021-01-14 13:54:24,473 [p:90,w:1,m:0] [JobHandlerQueue.monitor_thread] failure running job 622\nTraceback (most recent call last):\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 408, in __handle_waiting_jobs\nself.dispatcher.put(self.job_wrappers.pop(job.id))\nFile \u201clib/galaxy/jobs/handler.py\u201d, line 1007, in put\nself.job_runners[runner_name].put(job_wrapper)\nFile \u201clib/galaxy/jobs/runners/init.py\u201d, line 152, in put\njob_wrapper.enqueue()\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1456, in enqueue\nself._set_object_store_ids(job)\nFile \u201clib/galaxy/jobs/init.py\u201d, line 1478, in _set_object_store_ids\nobject_store_populator.set_object_store_id(dataset)\nFile \u201clib/galaxy/objectstore/init.py\u201d, line 1082, in set_object_store_id\ndata.dataset.object_store_id = self.object_store_id\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018dataset\u2019\n\nAs recommended here: Killing jobs or dead processes after a database crash on galaxy - #3 by Slugger70 (@link https://help.galaxyproject.org/t/killing-jobs-or-dead-processes-after-a-database-crash-on-galaxy/1360/3), I tried installing and running gxadmin but of course I haven\u2019t set up a postgresql db for my dev instance, so I get this failure:\n\ngxadmin mutate fail-job 621\n/usr/bin/gxadmin: line 258: psql: command not found\n\nI am completely stuck. Do I just need to install psql even though Galaxy is using SQLite?  We\u2019re going to have actual professionals help with Galaxy admin in about a month or two but I\u2019d really like to use my local instance before then.  Any recommendations would be great.  I\u2019m this close to gnawing off my own leg to get out of the Galaxy trap (i.e. clean reinstall)."
    },
    {
      "role": "system",
      "text": "Hey @user I think when we suggested gxadmin to you, we didn\u2019t know you were running with sqlite!  This makes a lot of sense.\nI think for that specific error you\u2019ll need to run the sql in your sqlite3 database potentially.\nSomething like\nsqlite3 universe.sqlite # Or wherever your DB is\nupdate job set state='failed' where id = 621;\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m new on processing raw data from RNA-seq experiments but my recent project requieres me to get it on.\nSo, I\u2019m using the 12 runs from the ENSEMBL accession number: E-MTAB-4929;\nI made the trimming using Cutadapt, also I had to figure out if the reads were stranded or not, so I used Infer Experiment for that and they are paired-end unstranded.\nFor the alignment I used RNA-STAR, I downloaded the reference genome from Genome NCBI (Glycine max (soybean)) as well as the annotation file in gff version 3 format.\nMy QC, made with MultiQC, from the alignment seem to me OK, here is a ss:\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.47.011088\u00d71052 99.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/07a26e18f2152f4e0ff3bfb5effb72ac2e1af48f.png)\nThe problem is when I run the featureCounts;\nmy input files are the BAM files from the alignment and the anotation file gff version 3 of the Glycine max genome.\nFor the extra info., I followed the tutorial: Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/topics/transcriptomics/tutorials/ref-based/tutorial.html#counting-the-number-of-reads-per-annotated-gene)\nI made the QC on the summary file [one output file of featureCounts] and I obtained this:\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.47.441248\u00d7972 79.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/21590f66935ecb5eea7e18712f8a4927e77d8765.png)\n\nCaptura de Pantalla 2021-05-06 a la(s) 16.48.111248\u00d7972 57.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9912d3afa762763cd54f880715a03f51cf5557ae.png)\nI\u2019m getting less than 50% of the total fragments mapped and I don\u2019t know why :S\n\u00bfCan someone please help me?"
    },
    {
      "role": "system",
      "text": "Dear Nati2208,\nYou can select in featureCount under advanced options different modes for ambiguitiy Allow reads to map to multiple features (.e.g, -O) together with the option Largest overlap to Yes. Test it and see what will happen.\nBest wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi team,\nI\u2019m trying to download my Galaxy histories to free up room for more analysis. However when I select \u2018Export History to File\u2019 and get a link to use with wget or curl (On MacOS and I\u2019ve attempted both) it only downloads an HTML file instead of a tar.gz file of the datasets in the history.\nThe file looks like this:\n ==> export_archive?id=XXXXXXXXXX <==\n<!DOCTYPE HTML>\n<html>\n<!\u2013base.mako\u2013>\nHope that this makes sense,\nBest,\nTim.\nadmin edit: remove archive ID"
    },
    {
      "role": "system",
      "text": "Hello,\nTry downloading the archive from the web page directly. Curl/wget will only work for datasets, with collection datasets requiring that your Galaxy API key is included.\nReference FAQ for anyone else reading: * Downloading Data (@link https://galaxyproject.org/support/download-data/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m not getting the stats when running my Hisat2 alignment, I\u2019ve already followed the suggestion of checking the stdout and stderr sections but there is nothing there either. The statistics of bam.iobio tell me I have a good quality alignment, though. Can I trust those results or what should I do?"
    },
    {
      "role": "system",
      "text": "Hi @user\nApologies for the delay.\nThe message about reads not being paired is just a warning. It will not cause a mapping job using HISAT2 to fail.\nIf you haven\u2019t tried a rerun yet, do that now, this may have been a cluster or server issue now resolved.\nIf the jobs failed again, then you are actually not getting any hits for some reason. Maybe the reads are too short \u2013 the parameters are too strict \u2013 or the wrong database was mapped against? This tool is designed to find \u201cexact matches\u201d when all default settings are used but the details can be adjusted under \u201cAdvanced settings\u201d.\nHope you worked this out!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI had a question about the default settings for HISAT2 on Galaxy, and whether soft-clipping is applied? Normally HISAT2 applies soft-clipping but I\u2019m unsure of whether this is the case on Galaxy. As I haven\u2019t trimmed for adaptor sequences, I am wanting to ensure that soft-clipping is applied. I can see for HISAT2, if you select \u2018Advanced Settings\u2019 you can choose whether soft-clipping is applied under Scoring Options, does this mean by default (e.g. not using any advanced options) it is not applied?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nyes, presence of S means soft clipping was used. It is used only for reads with soft clipped nucleotides. M means match, well, match or mis-match, to be precise. You can check description of CIGAR string in SAM/BAM documentation.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Starting last night, we had a series of failures of blastx.  These included with two new subject dbs, and with one we had used successfully previously.  When we try to run blastx, the message is \" Job submission failed\"  and also \u201cThe server could not complete the request. Please contact the Galaxy Team if this error persists. Action requires account activation.\u201d\nIs the blast server functioning?"
    },
    {
      "role": "system",
      "text": "Hi @user\nHi Tom, we talked about this over email but I am posted back the highlights for others that may run into similar problems.\n@quote\nAction requires account activation.\nCreate an account and log into it to run compute-intensive tools (User > Register). FAQ Registering Accounts (@link https://galaxyproject.org/support/account/)\n@quote\nI was unable to complete the uploads and received an error that the account hadn\u2019t been activated. I went to the activation email, and now got an error message that the link was not valid, and that perhaps the account was already active.\nAccounts have an activation step to confirm the registered email address is valid and belongs to you. Activation links are sent via email and are active for about six hours. Resend if needed using the password reset function (User > Login).\nAs you noticed, activation emails can be sorted into Spam or Trash folders depending on email filters, so if others run into the same problem check in those locations or search your email account with the keyword \u201cgalaxy\u201d if it seems to be missing.\nIf you never get an activation email (can take a few hours to arrive, depending on how busy the server is), your email service may be rejecting the email. We\u2019ve seen more of this occurring with institutional email services over the last year as updated security was implemented. Contact your institution\u2019s administrators to request that emails from the @lists.galaxyproject.org and @galaxyproject.org domains are accepted. Or, use a public email service instead. Please delete any accounts that will no longer be used (User > Preferences) to avoid multiple account issues. Account quotas at most public Galaxy servers are one account per person per public Galaxy server. Meaning, it is fine to have a single account at each but you shouldn\u2019t have more than one at any particular server. FAQ Registering Accounts (@link https://galaxyproject.org/support/account/)\n\nMore help about using custom fasta datasets indexed with makeblastdb\nFormatting of the fasta dataset matters. The usual custom genome format rules apply Datatypes (@link https://galaxyproject.org/learn/datatypes/#fasta), plus BLAST has more options for parsing line/identifier content than other mapping tools\n\n\nProblems with format usually do not show up until the database index created from the original fasta is mapped against. Meaning, the job output from makeblastdb may appear to be successful (green), but the index is corrupted, and running blastn, blastx, etc will fail when mapping against that index. The stderr will report that the index cannot be found. The error will start with text like this and means that you need to correct the fasta format and rebuild the index.\n\nstderr\nBLAST Database error: No alias or index file found for \u2026 (more details specific to your data)\n\n\n\nRemove description line content. Tool: NormalizeFasta using the option to \u201cremove content after the first whitespace\u201d. Wrap the sequence length to a value between 40-80 bases for the best results with most tools, including BLAST.\n\nnormalize_fasta_custom_genome1180\u00d7558 44.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/eb90f788deca408e0dfd9ce8750acdc880be9dc2.png)\n\n\nThe fasta identifiers can include alphanumeric characters (a-z, A-Z, 0-9), underscores (_), dots (.), and dashes (-). If you want to keep the description content, replace other characters with one of those. The goal is to produce sequence identifiers that are all one \u201cword\u201d with no whitespace (tabs, spaces) or other characters that BLAST cannot parse. Avoid very long identifiers (over ~30 characters), or expect problems.\nSearch the tool panel with the \u201creplace\u201d keyword to find all tool options. Replace Text in entire line should work with most data.\nAlternatively, a tool like Text transformation with sed can also be used within Galaxy. For example, a command like this will replace all commas with a dash in the entire file.\ns/,/-/g\n\n\nPipes (|) can be Ok, but the fasta data containing these characters are best sourced from NCBI and the option Parse the sequence identifiers should be used with the makeblastdb tool when creating the database index to map against.\n\nmakeblastdb-header936\u00d7108 9.48 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/342fffce64d8f8ae28cf289cb1f65105c34c5119.png)\n\u2026 other options, then find this option and set it to \"Yes:\n\nmakeblastdb_NCBI_parse_identifiers1204\u00d7148 14.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1ae1d328d02b5a1e5ad96c39fc0b1adb90b0a0d3.png)\n\n\n\nThanks for reporting the error, glad this is working for you now, and hope these tips help others :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI am new to Galaxy. I want to use Galaxy to teach students in 90 mins how to analyze cancer NGS data in an easy way without the need to install command line tools or use R (which I am familiar with).\nIs there any way to use Galaxy for that purpose? Any recommended tutorials or workflow that can help?\nI appreciate your response.\nBest Wishes,\nMohamed"
    },
    {
      "role": "system",
      "text": "Update: The somatic variants tutorial I mentioned before is available now at @link https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/somatic-variants/tutorial.html.\nWorking through it will take substantially longer than the 90 mins you gave as your requirement, but if you run all the steps before the actual variant calling prior to your exercises with your students, and have them start from BAM datasets of mapped and postprocessed sequencing reads, you may be able to just squeeze it into your time frame."
    }
  ],
  [
    {
      "role": "user",
      "text": "ive look for Compute tool in toolshed but i cant find it.\nis it deleted in new version of galaxy or just the name,  changed ?"
    },
    {
      "role": "system",
      "text": "Hi Amir,\nThe compute tool can be found under @link https://toolshed.g2.bx.psu.edu/view/devteam/column_maker/464b9305180e in the Toolshed."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI wonder if there is any option to use just one file from composite dataset (e.g. pbed) without downloading it if I want to use this file in automatic workflow."
    },
    {
      "role": "user",
      "text": "Thank you for the prompt replies.\nThe fastest way to solve my problems was to create custom galaxy tools - 1) for renaming files in composite dataset and 2) for extraction of files from composite dataset. It seemed to be not so easy at first but I succeeded in the end."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am completely new to bioinformatics but was hoping to use Salmon in Galaxy to analyze gene-level expression in a time-series RNA-seq dataset. Based on the parameters I set and files I have included (transcriptome FASTA and annotation GTF) I am able to get quantification of both transcripts and genes. I was wondering if there is a way to obtain information on the mapping rate? I know that alignment methods such as HISAT2 or STAR produce alignment rates to assess how well the read mapped to the genome and subsequent assignment rates when counting with a method such as featureCounts. Is there a similar output for mapping rate to assess how well my pair-end reads were assigned to a transcript/gene? Thank you in advance!"
    },
    {
      "role": "system",
      "text": "@quote\nGiven these results is it safe to assume that the counts Salmon is generating (quantification and gene quantification results) are based on everything that mapped (and includes reads that mapped either on their own or as a part of a paired-end fragment)?\nYes, this is all true as far as I know when using the default settings (mostly \u2013 coverage of a read against a transcript must be at least >= 31 bases to count).\nTo make the counting even more permissive, you could toggle either or both of these parameters to yes: \u201cAllow Dovetail\u201d or \u201cRecover Orphans\u201d.\nTo make what is counted up by Salmon stricter (less permissive), toggle one or more of the other parameters on the form to yes.\nGlad the prior info helped, and hope this does too :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All, I used Kallisto to generate transcript abundance files for RNASeq data and am trying to run it through DESeq2. My input is 4 .tsv files as well as a .gene_trans_map file, and the following error message is generated:\nNote: importing  abundance.h5  is typically faster than  abundance.tsv  reading in files with read.delim (install \u2018readr\u2019 package for speed up) 1 2 3 4 removing duplicated transcript rows from tx2gene Error in  $<-.data.frame ( *tmp* , \u201cTXNAME\u201d, value = character(0)) : replacement has 0 rows, data has 510059 Calls: get_deseq_dataset -> <- -> <-.data.frame\nI have tried changing a variety of parameters, including setting header to false, and I have also tried only using one dataset per factor level, and nothing is working to fix this error. I have also tried using a .gff3 file rather than a .gene_trans_map file, and that also is not making a difference. Any advice would be much appreciated!\nThank you."
    },
    {
      "role": "system",
      "text": "Welcome, @user\nI see your bug report sent in from Galaxy Main @link https://usegalaxy.org.\nThe problem is that your transcript fasta dataset identifiers do not match up with the transcript-to-gene mapping dataset.\nThe fasta (data 14) and TMP (data 356 + 357 + 358 + 362) transcript identifiers are formatted as:\nTR100009|c0_g1_i1|m.500685\n\nThe tabular transcript-tab-gene dataset (data 347) has two problems: 1) truncated transcript and gene identifier formats that do not match the TMP inputs, plus 2) it looks as if the order might be reversed (gene-tab-transcript)? As long as the genes are named consistently, it doesn\u2019t matter what they are, but the transcript names do need to match the other inputs.\nTR1|c0_g1\tTR1|c0_g1_i1\n\nIn short, primary IDs need to be in the same exact format across all inputs or tools cannot match data up. This is true for any tool, not just DESeq2.\nThe gff3 dataset is missing a header line: ##gff-version 3. That is why it was given the more generic gff datatype during Upload, and why the tool form does not recognize it as a valid input.\nAlso, replicates are required with DESeq2 in Galaxy.\nFAQ: @link https://galaxyproject.org/support/\n\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to extract BCR data from single (B) cell RNA sequencing data (SmartSeq, Nextera, HiSeq4000) to compare clonotypes between two samples. I\u2019m particularly interested in comparing % germline mutation to look at affinity maturation.\nThese are the settings I have used:\n\nimage435\u00d7718 19.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9069ececbff64e8ef995607f69dea33b8f9bd8b3.png)\nI have tried running one of my samples (= 81 paired reads, i.e. 81 B cells) with MiXCR with the shotgun analyze approach for non-enriched libraries, and it seems to run without any errors but all the clonotype tables are empty.\nimage\nThe report for the same sample shows this in the final section which makes it look like contigs have been generated. Could you please advise how to get these to export?\nimage"
    },
    {
      "role": "system",
      "text": "@quote\nSmartSeq\nHi, also MiXCR now supports SmartSeq2 protocol out of the box (and other single-cell protocols). There is a single command that only requires to set the species and provide input files:\nmixcr analyze smartseq2-vdj-full-length \\\n    -s hsa \\\n    161014_SN163_0729_AH3VW7BCXY_L1_{{CELL0ROW:a}}-{{CELL0COL:a}}_R1.fastq.gz \\\n    161014_SN163_0729_AH3VW7BCXY_L1_{{CELL0ROW:a}}-{{CELL0COL:a}}_R2.fastq.gz \\\n    result\n\nYou can read more on that here:\n\n\n@link https://mixcr.com/mixcr/reference/overview-built-in-presets/#smart-seq2\n\n\nBuilt-in presets - Docs @ MiLaboratories (@link https://mixcr.com/mixcr/reference/overview-built-in-presets/#smart-seq2)\nMiXCR: ultimate software platform for analysis of Next-Generation Sequencing (NGS) data for immune profiling.\n\n\n\n\n\nAlso, let me know if you need any help running the software.\nDisclaimer: I am one of the developers of MiXCR"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have been using Unicycler for assemblies and have been following this tutorial:\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html\n\n\n\nGalaxy Training: Unicycler Assembly (@link https://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\n\nHowever, when I try to open my assembly using \u201cdisplay with IGV local\u201d (figure 14 of the tutorial) I get an error message:\n\nScreen Shot 2019-12-12 at 4.35.44 PM824\u00d7293 20.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0936358ab22641fc5f9f5cee4add0dd139008c80.png)\nIt seems to say that the fasta file is available but the fai file is missing.\nI do not have jobs still running, all my assemblies are showing as green, so I don\u2019t see why it says \u201cadditional datasets require to be be generated\u201d.\nDid I miss a step when trying to use IGV local?\nThanks for help\nVanina"
    },
    {
      "role": "system",
      "text": "The assembly is done, but moving the data into any 3rd party external application can require some reformatting or the creation of additional files that the other application is expecting \u2013 or they can require none \u2013 it depends on the external application.\nIn this case (IGV), a fasta index is required for new genomes. This would be true even if you decided to load the fasta directly into IGV as a new genome (instead of loading it directly from Galaxy). The \u201cGalaxy-to-Local IGV\u201d data web-based loading for fasta data is a convenience feature in Galaxy that skips the need for you to create the fasta index as a distinct step, download the fasta and fasta index, then load both into IGV directly.\nTo avoid this step, you could pre-load your assembly as a new genome in IGV. Then promote the assembly in Galaxy to a \u201cCustom Build\u201d. Use the same exact \u201cdatabase\u201d aka \u201cdbkey\u201d name in both places, and assign that \u201cdatabase\u201d to datasets you wish to visualize in IGV against that new genome/assembly.\nAny fasta file can be used directly as a \u201cCustom Genome\u201d with tools wrapped in Galaxy natively, without any fasta or tool-specific indexing (that is done at job runtime, when needed). But, sometimes promoting a fasta/Custom genome to a \u201cCustom Build\u201d is helpful for other reasons \u2013 there are tools in Galaxy that also require an assigned \u201cdatabase\u201d aka \u201cdbkey\u201d.\nHow to create/use a Custom Genome/Build in Galaxy:\n\nPreparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/)\n\nIGV is a different application. The methods to create a \u201cNew Genome\u201d in your own IGV are below. The preparations steps will require that you index the fasta with samtools as part of that process. Or you can create the fasta index in Galaxy and download that along with the fasta.\n\n\nIGV: @link https://software.broadinstitute.org/software/igv/LoadGenome\n\n\nCreating a fasta index in Galaxy that can be downloaded as a distinct dataset: click into the pencil-icon for a fasta dataset to reach the Edit Attributes forms. On the tab for \u201cConvert\u201d, pick the option to create an index:\n\n\n\ncreate-fasta-index-in-galaxy1444\u00d7468 34.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/3d1b6be04c2036708c1cfb5102ff2468b3942566.png)\n\n\nNote: Certain other datatypes already have an attached index. An example is BAM data. The download icon for BAM data will contain two datasets: the .bam (mapping results) and the .bam.bai (index).\n\nWhether or not to spend the extra effort to directly index your fasta in IGV as a new genome and create a Custom Build in Galaxy so that you can assign that custom \u201cdatabase\u201d to other datasets is up to you. That will somewhat depend on how often you plan to use it as a reference genome for other analyses.\nHope that helps to explain the options :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello today i was using HiSAT2 to fo a mapping of my fastq paired-end secuences to the reference genome of my organism (Hydractinia symbiolongicarpus) and the software returned this error:\nTotal time for call to driver() for forward index: 00:07:18\n(ERR): mkfifo(/tmp/39861.inpipe1) failed.\nExiting now \u2026\nsamtools sort: failed to read header from \u201c-\u201d\n[main_samview] fail to read the header from \u201c-\u201d.\nI dont know exactly what it means. This is the link to the history im working on:\n\n@link https://usegalaxy.org/published/history?id=e6a0775e581ce894\n\n\nGalaxy (@link https://usegalaxy.org/published/history?id=e6a0775e581ce894)\nGalaxy is a community-driven web-based analysis platform for life science research.\n\n\n\n\n\n"
    },
    {
      "role": "system",
      "text": "Hi @user\nI successfully tested HiSAT2 with a custom genome. I don\u2019t know why your jobs have failed. I converted your genome and reads into plain text formats, fasta and fastqsanger, and completed a test job with 1k PE reads. It is possible that the reads were compressed in non-standard option, and HiSAT2 cannot unpack the data. However, I tested only 1k reads, so there might be an issue with the data. It is very unusual situation.\nJust in case, procedure for conversion of formats: click at Edit Attributes (pencil icon) > in the middle window switch to Convert tab and select an appropriate option from the pull-down menu.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nHope you all do well in Galaxy.\nI have a problem of getting shared histories from the other Galaxy users. Her account showed the histories have been shared, but it shows 0 in the \"History shared with you\" item in my account.\nI am a little confused about this, Looking forward to suggestions.\n\nimage1273\u00d7224 11.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5bacd717351479102a8f38b9e15d72ee90df9c7a.png)\nThanks a lot.\nTing"
    },
    {
      "role": "system",
      "text": "@user\nWe\u2019ve been discussing this over email for the last week or so.\n@quote\nI think we are both in the same Galaxy server.Because I can share my histories with her using the same account, but when she tried to share the histories with me, it showedmy account is not a valid Galaxy user\nMany people have one account at several public Galaxy servers. Accounts on different servers are different accounts, even though they may use the same email address for registration. You really need to confirm the server is the same for both of you for this function to work.\nThe advice that @system offers are the same as what I have shared. If you want to share a History by email, you will need to:\n\nConfirm that both people have an active account on the same exact Galaxy server. How to know which server(s) are being used? Compare the URLs.\nConfirm that the correct email address is used \u2013 account email addresses are case sensitive. How to check your email address? Look under User > Preferences.\n\nPlus we discussed how to generate/import a history archive (to transfer histories between Galaxy servers). Most of that information was already in this forum and I added more details based on our conversion earlier today:\n@quote\nIf the server is still up and public, or you have downloaded the history archive, then moving the history to another server is possible. \nYou state that the accounts are on different Galaxy servers. This is why the share by email didn\u2019t work. To share with another user by email, both accounts must be on the same server. All accounts on the same Galaxy server will have distinct registered email addresses. \nGalaxy FAQs:Options for Data SharingLoading/Downloading DataTips: \n\nMake sure the his\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "An error occurred while running the tool data manager gatk picard index builder (the only version of the manager 0.0.1) - I was just trying to build the indexes for the GATK tools, and I didn\u2019t succed\n\nScreenshot from 2022-12-18 19-58-58875\u00d7410 25.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7f3baa296be18f7359edff477037c21ca6095497.png)\n(galaxy 22.05 was installed locally on a ubuntu server)\nDoes anyone can help me?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis data manager has not been functional for some time now, was based on an earlier version of GATK (2), included a tool dependency model that is not supported anymore, and the tools it was creating indexes for, and this DM itself, are all considered deprecated. None are hosted at any of the public servers for these same reasons."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m following along the RNA-Seq reads to counts tutorial with my own data and am up to the Generating a QC Summary Report step.\nI\u2019ve followed the instructions (imported the QC report workflow, downloaded the RefSeq BED file) but in the workflow I cannot seem to select my BAM files - it only gives me the option to select MultiQC files.\nAny help would be appreciated! Thanks"
    },
    {
      "role": "system",
      "text": "@quote\nhowever my issue is I can\u2019t select any BAM files to enter\nHi - are you still having this problem?\nIf so, some items to check:\n\nThe BAM inputs should be in a dataset collection.\nThat collection needs to be in the same history where the workflow is launched (the \u201cactive\u201d history).\nAnd, this workflow is run after the completion of the other workflow (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/workflows/) associated with this tutorial.\n\nIf you want to share a link to your tutorial history back, we can help to test more. FAQ @link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have setup a local Galaxy server on a headless ubuntu server follow this website (@link https://galaxyproject.org/admin/get-galaxy/).\nLocally (@link http://localhost:8080), everything works fine. I can upload data en install tools. Currently, I am trying to make the Galaxy server available through reverse proxy using this website (@link https://docs.galaxyproject.org/en/master/admin/nginx.html). The Galaxy server is reachable through @link https://galaxy.mydomain.com but now if I want to upload data or install tools nothing happens.\nI have tried proxying using Nginx Proxy Manager and by just using nginx but nothings works.\nAny ideas?\nThanks in advance!\nErik"
    },
    {
      "role": "user",
      "text": "Okay. Fixed this. It was the same as this issue (@link https://help.galaxyproject.org/t/galaxy-gunicorn-cant-connect-to-socket-file/8851/9):\nproxy_set_header X-Forwarded-Proto $scheme; causes Chrome to block the upload request. The solution was amazingly simple: proxy_set_header X-Forwarded-Proto \"hppts\";"
    }
  ],
  [
    {
      "role": "user",
      "text": "We\u2019re in the middle of setting up a new Galaxy server (18.09) and currently installing tools - mixed ToolShed and locally developed code.\nQuite a lot of our tools are currently failing with error:\nConda dependency seemingly installed but failed to build job environment.\nThis includes BCFTools, GATK (3.x), and others.  Looking through the manage dependencies panel, the ones with issues often have multiple dependency requirements, with Conda as the resolver and separate environment paths listed. Other, working tools with multiple requirements via conda give the same mulled-v1-HASH environment path for all of them. Now, we don\u2019t have the mulled beta enabled right now, and I\u2019m not sure we ever did, so I don\u2019t know how these were sourced. We also aren\u2019t caching the dependencies, although maybe we need to.\nHas anyone seen this behavior, or have any troubleshooting suggestions? Is there a latency factor here - could it fail due to time taken to build the env across filesystems?"
    },
    {
      "role": "system",
      "text": "No, it\u2019s all fine. Make sure all your tools that have multiple requirements are using a Conda environment that starts with mulled-v1-. This should happen automatically, but can be very slow. The installation progress is logged in your log file. If for some reason this doesn\u2019t happen you can trigger the installation in the Manage dependencies menu. Have an eye on the logs when you do this, you\u2019ll see the commands that will be executed."
    }
  ],
  [
    {
      "role": "user",
      "text": "When running sh run.sh, the following error occurs:\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f42e08aa0b8>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/adal/\nERROR: Could not find a version that satisfies the requirement adal==1.2.5 (from versions: none)\nERROR: No matching distribution found for adal==1.2.5 \n\nI tried to instal adal version 1.2.5 to galaxy environment but the error remained.\nThe OS I\u2019m using is WSL. Could this be the root of the problem?"
    },
    {
      "role": "user",
      "text": "A strange thing have happened as I restarted my computer - now it works like a charm and has no errors. Prior to that, I started to try few solutions and made few actions:\n\nActivated hyper-v via power shell as described here: Enable Hyper-V on Windows 10 | Microsoft Docs (@link https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v)\n\nEnabled Virtual Machine Platform via: Settings \u2192 Apps ->Programs and features \u2192 Turn windows features on or of\n\nThese steps (including the restart of the computer) might have no influence on resolving this problem so I am not sure if we can call it as a solution.\nYet, the problem somehow has been solved."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi I\u2019m doing an RNA-seq analysis and I\u2019m having problems identifying some of the genes in the output of the DESeq2 tool because the list of genes obtained are in Entrez id, for context the pipeline used was Fastp > HISAT2 > feature counts > DESeq2. I used g: profiler and annotateMyIDs to identify the genes by the Entrez id, but there are still some that appear as NA in the list, so I tried to identify them manually by searching the Entrez id in NCBI and then took the coordinates from there and looking if there was a match in my samples using the visualizer UCSC and IGB, in parallel some of the genes in NCBI appears as updated and have another Entrez id associated to a gene, that gene is already identified in the list that I have, the question is can I keep the gene that is so far in the list and discard the obsolete one that had been updated?. Other than that can I discard the ones that haven\u2019t been updated and remains as NA?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis prior Q&A is similar to your question: Should i change de EntrezID of my genes if it has changed in NCBI? (@link https://help.galaxyproject.org/t/should-i-change-de-entrezid-of-my-genes-if-it-has-changed-in-ncbi/6681)\nYou could use the UCSC\u2019s GTF annotation instead (incorporate it at the first step you incorporate the other annotation). Where to find it plus some related tips: RNA STAR error on trimmomtaic files - #6 by jennaj (@link https://help.galaxyproject.org/t/rna-star-error-on-trimmomtaic-files/6670/6)\nWhether to keep data that doesn\u2019t have any known annotation yet (NA) is your choice. If the research goal is to discover novel features, then you would probably want to keep those. If you are not hunting for novel features, then you can restrict the analysis to known features upstream during mapping.\nHISAT2 has an option to only report hits with known features: Advanced options >> Spliced alignment options\n\nGTF file with known splice sites (input the UCSC annotation)\nTranscriptome assembly reporting (set to: \u201cReport only those alignments within known transcripts\u201d)\n\nHope that helps! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI want to add a reference genome for RNA-seq analysis to use on HISAT2 and FeatureCounts.\nGenome assembly ARS-UI_Ramb_v2.0\n\n\nNCBI (@link https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_016772045.1/)\n\n\nOvis aries genome assembly ARS-UI_Ramb_v2.0 (@link https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_016772045.1/)\nNone - Ovis aries (sheep)\n\n\n\n\n\nHow do I go about doing this and what file formats do the they need to be for each program?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user yes, you need a gene annotation file. Usually annotation files are available from the same source, as genome assembly. During setup of featureCounts job change Gene annotation file to \u2018in your history\u2019. Make sure the gene annotation is for the assembly used for read mapping. Inspect the gene annotation file and double check that values used for read counting are present in the file. By default, reads are counted against exons and aggregated for gene_id attribute - see Advanced options in featureCounts.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good morning. It is my first time using Galaxy/hutlab. I have uploaded a  text tab-delimited file and it apears in my history. I am trying to use it to perform the first step of LEfSe (A. Format data for LEfSe) but i am not able to select any file as the drop-down menu does not work and doest not appear when I click on the first option. I can not select either which rows have to be used as class or subject (I am not able to select anything in the items on the red circles). Should I do anything else before this step appart from uploading my text tab-delimited file?\nThanks.\n\nLEfSe960\u00d7720 254 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a8c3d77212481bec6e89fc94cbde8490df7a01e6.png)"
    },
    {
      "role": "system",
      "text": "You need to change the datatype of your txt file. You can do that by clicking on the pencil icon of the file. After that a new window will appear, on top of that window you will find some tabs, choose the datatype tab."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have a Galaxy 19.09 installation I need to upgrade to 20.05.  It is running on Ubuntu 18.04.2 LTS.  I also want to update it to use Python3 (currently running with Python 2.7).  I was generally following the procedure from \u201c@link https://galaxyproject.org/admin/maintenance/\u201d under section \" General Update Procedures\" but I\u2019m worried this may be outdated.  Below I\u2019ve listed my planned steps.  I\u2019m hoping someone more knowledgeable can let me know if I\u2019m following the wrong instructions.\nProcedure:\n\nclone existing prod instance (version 19.09) using tar/gzip\nclone PostgreSQL database using \u201cCREATE DATABASE galaxy_prod2 WITH TEMPLATE galaxy OWNER galaxy;\u201d\nupdate galaxy.yml for cloned installation and change database_connection parameter to use new database called galaxy_prod2.\nNext upgrade to 20.01 so I can step my way to latest version:    \u201cgit fetch origin && git checkout release_20.01 && git pull --ff-only origin release_20.01\u201d\nthen follow instructions on page \u201c@link https://galaxyproject.org/admin/maintenance/\u201d under section \" General Update Procedures\".\nI\u2019m especially unclear whether I need to execute steps #3, #4, and #8.\nassuming that all works, then use git to get latest version 20.05 and follow same instructions.\nupgrade from Python 2.7 to Python 3\n\nI tried upgrading Python first but ran into trouble so I assumed I had to upgrade Galaxy first to latest version.  I followed these steps for the Python upgrade before attempting to upgrade Galaxy.\nI followed these instructions:\n(@link https://docs.galaxyproject.org/en/master/admin/python.html)\n\nI removed .venv from base galaxy install directory\nFollowed Step #3:  set this in .bash_profile and in the current window:  export GALAXY_PYTHON=/usr/bin/python3\ncreated symbolic link to python3 and added at beginning of path\nverified new correct version using \u201cpython --version\u201d <-- now shows 3.6.8\nfollowed step #4 and issued this: \u201crm -rf /path/to/galaxy/database/compiled_templates/\u201d <-- updated for my env\nstarted galaxy:  now when Galaxy started it rebuilt .venv from Python 2.7 instead of 3.6.8.\n\nObviously the Python upgrade did not work.  Something under Galaxy is holding onto 2.7 and I\u2019m not sure what.\nThanks in advance for any help or advice."
    },
    {
      "role": "system",
      "text": "find . -name '*.pyc' -delete : I launched this on the root install dir of galaxy. So it will remove every .pyc file in lib/* . I think it should be launched.\nand I\u2019ve upgraded from 19.05 to 20.05 without any other step, directly! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have about 12 samples I aligned with mm39. All samples had good quality reads and all paired, as trimmomatic gave me empty files for unpaired reads. Strangely, two of them had 0% alignment with the genome. I blasted a few reads and they seem to match mus musculus or other rodents. I\u2019m using HISAT2 on galaxy. Is it a bug?"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry a rerun with the data that didn\u2019t align, just to eliminate server/cluster issues from being a factor.\nAs you do that, double-check that the correct target reference genome is selected on the tool form. The most current version of the mouse genome is GRCm38 (mm10, sourced from UCSC). Most mapping tools, including HISAT2, will only have built-in indexes for mm9 or mm10 available (not the earlier genome builds).\nSometimes the wrong genome is selected. HISAT2 is designed to align reads to the same species they were sourced from. BLAST+ is different, and allows for cross-species mappings to be reported.\nmouse-genomes984\u00d7216 22.2 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f8753b9952e53014f92b5ab050117abc1c9bad6e.png)\nIf that doesn\u2019t work, and the reads are of high quality (tool: FastQC) then something else is going on. Perhaps a sample mixup, or the inputs (forward/reverse) were not entered correctly on the form, or possibly the read content doesn\u2019t meet the minimum mapping criteria set on the HISAT2 tool form.\nExample of the last case: too much QA (trimming) can generate truncated reads that won\u2019t meet the minimum default mapping criteria. You\u2019ll need to investigate then make changes depending on what you uncover about the content.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would appreciate help by someone regarding \u2018flye\u2019 tool in galaxy to perform assembly of Pacbio data in bam format,  step by step guide will be better. i tried it, using SRA accession number from NCBI but it gives error.\nthank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nAssembly tutorials and workflows are available here.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/assembly/)\n\n\nGalaxy Training: Assembly (@link https://training.galaxyproject.org/training-material/topics/assembly/)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\nThe training site can be navigated by topic or searched with keywords like the datatype or tool name. This is probably your best match.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/flye-assembly/tutorial.html)\n\n\nGalaxy Training: Genome assembly using PacBio data (@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/flye-assembly/tutorial.html)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\n\n\n@quote\nassembly of Pacbio data in bam format\nInput Datatype\n\n\nFlye requires input reads in a fastq or fasta format.\nStart up a new empty history, then load up a tool\u2019s form to review what the exact supported input datatypes are (file content/format + \u201cdatatype\u201d metadata).\n\nNCBI SRA\n\nGet the reads from SRA using a tool like Faster Download and Extract Reads in FASTQ.\nAvoid attempting to retrieve reads in BAM format with the tool Download and Extract Reads in BAM. From my experiences, that one rarely works correctly due to time/data limits set at NCBI. The query can either error (red) or output truncated results (putatively \u201csuccessful\u201d, but not actually usable). Confusing, and better to skip using it anyway since very few tools accept read inputs in that format.\n\nRead data in BAM format already?\n\nA tool like SamToFastq can be used to extract the reads.\nIf the BAM was a mapping job result, keep in mind that some reads might be output more than once.\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m trying to upload a large .bam file to my galaxy account using galaxy upload. I did those steps but still getting errors that I\u2019m not able to fix. Please help!\n$ pip install galaxy-upload\n$ python3 -m venv ~/galaxy-upload\n$ . ~/galaxy-upload/bin/activate\n$ pip install galaxy-upload\n(galaxy-upload) (base) TS261@PHS024050 outs % galaxy-upload --url @link https://usegalaxy.org \\ \u2013api-key cae08ba0d4323e26716c58de5cb3c4 --history-name \u2018RNA\u2019 -i \\ \u2013file-name possorted_genome_bam.bam\nUsage: galaxy-upload [OPTIONS] [PATH]\u2026\nTry \u2018galaxy-upload --help\u2019 for help.\nError: Missing option \u2018\u2013api-key\u2019."
    },
    {
      "role": "system",
      "text": "You are showing the same command as your orignal question. Besides the api key the use of --file-name is also not correct. Look at this galaxy-upload \u2014 galaxy-upload 1.0.0 documentation (@link https://galaxy-upload.readthedocs.io/en/latest/)\nYou could also try to upload your file with galaxy itself on the webinterface.\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-short/tutorial.html)\n\n\nGalaxy Training: A short introduction to Galaxy (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-short/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use Diamond alignment tool for short sequences against a protein database. As the title suggests, when I click on the \u201cSelect a reference database\u201d there are only options for the NCBI NR and RefSeq. Underneath is the message \u201cIf your database of interest is not listed, contact your Galaxy admin\u201d.\nSince I do not know how to contact Galaxy admin, I am submitting here. I hope someone can help or at least alert someone if this is something that can be done.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe database is now available.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all.\nAfter having used DESeq2, my results show up as nucleotides (per NCBI: Search NCBI databases - NLM (@link https://www.ncbi.nlm.nih.gov/search/)) instead of genes.\nDo you know what causes this and how I could find out the correct gene IDs?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe content of the gene_id and transcript_id fields of the reference annotation GTF are what Featurecounts uses to generate the counts. Mapped hits at the \u201ctranscript_id\u201d level (aka transcript coordinates) are summarized per \u201cgene_id\u201d in that output. Then, those gene_id level counts are interpreted by DESeq2 for differential expression.\nFor this data, the annotation was predicted using an automated annotation pipeline developed by NCBI. To review the general stats for the annotation, and the pipeline details, go to the assembly page here ASM2155985v1 - Genome - Assembly - NCBI (@link https://www.ncbi.nlm.nih.gov/assembly/GCF_021559855.1/#/qa) and expand the section \u201cComment\u201d\n\nThe annotation was added by the NCBI Prokaryotic Genome Annotation Pipeline (PGAP). Information about PGAP can be found here: NCBI Prokaryotic Genome Annotation Pipeline (@link https://www.ncbi.nlm.nih.gov/genome/annotation_prok/)\n\nExample line from the public GTF\nNZ_CP058222.1\tProtein Homology\tCDS\t1\t1401\t.\t+\t0\tgene_id \u201cHW370_RS00005\u201d; transcript_id \u201cunassigned_transcript_1\u201d; gbkey \u201cCDS\u201d; gene \u201cdnaA\u201d; inference \u201cCOORDINATES: similar to AA sequence:RefSeq:NP_418157.1\u201d; locus_tag \u201cHW370_RS00005\u201d; product \u201cchromosomal replication initiator protein DnaA\u201d; protein_id \u201cWP_000059111.1\u201d; transl_table \u201c11\u201d; exon_number \u201c1\u201d;\nWhat those values represent\n\nNZ_CP058222.1 == chromosome identifier, pairs with the genome fasta on the title line > value (everything before the first whitespace) and the third column in BAMs generated via mapping against the fasta. Those BAMs were not in the shared history but that\u2019s OK \u2013 are the BAMs you input to Featurecounts (maybe in another history, or locally on your computer, or some other online tool)\ngene \u201cdnaA\u201d == the gene the pipeline guessed as the closest valid match. This is what you could lookup, example: @link https://www.ncbi.nlm.nih.gov/gene/?term=dnaA\n\ngene_id \u201cHW370_RS00005\u201d and locus_tag \u201cHW370_RS00005\u201d == both are the same value. This is a distinct name, presumably since more than one locus could be assigned to the same \u201cgene\u201d via the pipeline\u2019s \u201cinference\u201d algorithm. Technically, \u201cgene_id\u201d must be unique in any single assembly\u2019s annotation \u2013 most tools will error or produce weird results if there are any duplications.\n\nYou could transform the GTF to a tabular format with gffread then use Join two Datasets side by side on a specified field to map gene_id/locus_tag \u2192 gene in batch. Tutorial with these and more manipulations: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html).\nYou can also search the tool panel with the keyword \u201cgtf\u201d to find more tools to manipulate or extract (or extract and simplify) any other attribute data you are interested in mapping the gene_id/locus_tag value to (the first column in the DESeq2 output table). Example: you want to map to the \"inference \u201cCOORDINATES: similar to AA sequence:RefSeq:NP_418157.1\"\u201d you would need to strip all the extra content until just the NP value remains to use that key in a search to get the associated protein fasta.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Suppose I have a nested directory structure, something like data > si_n > (si_n.cell, si_n.param) with multiple values of n. Is there a way to upload this folder into a Galaxy collection with the same nested structure?"
    },
    {
      "role": "system",
      "text": "Good suggestion, I\u2019ve created an issue for it (@link https://github.com/galaxyproject/galaxy-upload/issues/11)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nThe plots generated by Deseq2 and EdgeR, is there a way to get the images of them either as SVG or TIFF, or anything aside from a PDF?\nIt\u2019s very difficult when trying to use them for a manuscript etc if the only output is a pdf that you need to screenshot.\nIf anyone has a way around this please let me know."
    },
    {
      "role": "system",
      "text": "If you\u2019re a Mac user you could use Preview to convert PDF to TIFF."
    }
  ],
  [
    {
      "role": "user",
      "text": "I understand how to share histories. But can multiple members of a team share and work on a history (and not just view it)?"
    },
    {
      "role": "system",
      "text": "@quote\nbut you can share the user name and password with your colleagues, so they can access your account,\nImportant Warning \u2013 directly sharing accounts would cause administrative issues at @link http://UseGalaxy.org and @link http://UseGalaxy.eu. Everyone should only use and log into their own single account. If someone already knows your password, update it and keep it private to protect your own data.\nSomeone else using your account could do anything you could do \u2013 up to and including deleting the entire account. This is usually not \u201cun-do-able\u201d.\nThe collaborative working model involves sharing functions. Share your history, others can review and/or import to work with it, then share back. You can generate a link and share that, or directly share with other people also working at the same server (by email address).\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#can-i-create-multiple-galaxy-accounts\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#finding-and-working-with-histories-shared-with-me\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#how-to-set-data-privacy-features\n\n\nAt some public servers you can just keep on adding data to that history with the share link or the history that is directly shared. This is continuous sharing. @link http://UseGalaxy.org is organized like this by default and can be customized.\n\n\nAt some public servers this will mean that you need to re-share to capture new datasets or adjust your account User \u2192 Preferences \u2192 Set Dataset Permissions for New Histories to allow for continuous sharing. @link http://UseGalaxy.eu is organized like this by default, but you can customize.\n\n\nAnd some public servers offer bonus resources for regional affiliations, making multiple account usage unlikely. I believe that @link http://UseGalaxy.org.au is organized this way, and therefore sharing accounts is not restricted (?). @system can correct me or fill in the details.\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I am an undergrad and am in desperate need of any assistance, as my mentor is not familiar with bioinformatics!\nI am using Galaxy to do differential gene expression RNA sequencing and am following Galaxy\u2019s tutorial (Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#mapping)). I have 12 pairs of pair-end samples in FASTQ format of HeLa cells. I did QC and did not do cutadapt as one online source stated that it was not necessary if I would be using STAR to map my sequences. (Do you think that this was a smart decision?)\nRight now, I am trying to use STAR to map my sequences to the Homo_sapiens.GRCh38.109.gtf file, but I keep getting the following error: \u201cFatal INPUT FILE error, no valid exon lines in the GTF file: /jetstream2/scratch/main/jobs/49599174/inputs/dataset_b28934b5-2b19-\u201d I used a file from Ensemble and retried with one from UCSC, but both gave the same error. What should I do for my next step?\nThank you so much in advance for your help! I really appreciate it!! :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user\npopular aligners use soft clipping (ignore unmappable nucleotides at reads\u2019 start and end). You\u2019ll see it in CIGAR string, eg 12S50M means 12 soft clipped nucleotides at alignment start followed by 50 matches. For additional information check specification of SAM format. You can check role of adapters and compare counts from original and trimmed reads. What important: treat all samples in the same way.\nWe do see elevated rate of errors when RNA_STAR is used with gene annotations. Gapped aligners can map reads across splice sites without gene annotations.\nPersonally, I prefer two step approach, mapping and reads counting. Some tools have built-in gene models for popular organisms 1: RNA-Seq reads to counts (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html#counting)\nUsually I import annotations, so I know what is used.\nRNA_STAR provides very limited control over read counting, while featureCounts and htseq-count allow selection of attributes and features.\nIt is hard to say why the job failed without checking the annotation file. The annotation and the reference genome should be for the same version of genome assembly, with identical chromosome/contig names, for example, chr1, Chr1 and 1 might be considered as three different text strings/names.\nMaybe try HiSAT2/featureCounts approach described in the tutorial above on one sample to see if it works for you.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I keep getting the following error when trying to run LDA effect size (lefse):\n\u201cTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 435, in \nfeats = numerical_values(feats,params[\u2018norm_v\u2019])\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 174, in numerical_values\nfeats[k] = [float(val) for val in v]\nValueError: could not convert string to float: MN3207_VT1303\u201d\nAny help would be greatly apperciated!\nBest regards,\nCollin"
    },
    {
      "role": "system",
      "text": "to install the same tool from the main toolshed, if you are the admin of the instance, you can follow the steps present in this tutorial @link https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/#open-the-tool-shed.\nlink to lefte in toolshed link to lefte in toolshed (@link https://toolshed.g2.bx.psu.edu/view/george-weingart/lefse/a31c10fe09c8).\nThen, having never used it, I don\u2019t know if it will work but it is always better to install the tools from the main toolshed."
    }
  ],
  [
    {
      "role": "user",
      "text": "As a new user, the Filter List documentation was confusing/unclear.  The documentation states that the function will:\n\ntake an input list and a text file with identifiers to filter the list with\n\nThis led me to believe that partial identifiers could be used, such as \u201cRep1\u201d matching all data sets in the list with \u201cRep1\u201d in their names.  I found, however, that the filter function will only work if entire file names are used.\nIf the intended behavior is to only match complete file names and not support partial matches, I think the documentation would be more clear if it used \u201cfile names\u201d instead of \u201cidentifiers\u201d.\nIf the intended behavior is to support partial matches, then I think there may be a bug as I was not able to get partial matches using either a plain string or a proper regular expression as identifiers in my text file."
    },
    {
      "role": "system",
      "text": "I understand your confusion and have opened a PR (@link https://github.com/galaxyproject/galaxy/pull/7127) to update the help text. I think changing the text is better than changing the tool name.\nI think it is best to keep to using a text file so that this can be part of a workflow without requiring user intervention."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m using workflows for the first time.  I fed 59 datasets into the workflow.  Each dataset has multiple #hashtags associated with it.  Once I started the workflow, I noticed that the datasets were being split into groups and processed in independent histories.  I did select the \u201csend the results to a new history\u201d option, but anticipated only 1 new history being generated - this has generated >10 new histories.  Is this a result of selecting that option, or is it being caused by something inherent with the #hashtags associated with the datasets.  It does appear to have grouped the datasets based on the tags, as this was not the order that the samples were listed in the original history.\nI\u2019m trying to delete all of these histories on the \u201cuser history\u201d page by selecting them all and clicking \u201cdelete permanently\u201d, but the browser just seems to freeze up and I get an error: \u201cGrid refresh failed\u201d.  Not sure how to clean this mess up x_x\n11%20PM"
    },
    {
      "role": "system",
      "text": "Ok, turns out this is expected behavior. Splitting into \u201cone history per input\u201d was a function put in place before dataset collections/tags were introduced. This works fine for people that have ~2-5 inputs. Not so good for 50+. We might update the workflow execution help text to make this type of \u201csend to new history\u201d output result more clear.\nSo, you\u2019ll need to use dataset collections, or stick with individual dataset inputs and be Ok with all the history outputs. These are all named the same, but there isn\u2019t a clear way to do that better. The new history name is specified by the user. There is no way to know how many inputs before the workflow executes \u2013 and the user entering a new name for each would be tedious anyway.\nI hadn\u2019t involved a workflow like this before (or it was long ago\u2026) so didn\u2019t recognize what was going on. But @system confirmed this is all working correctly.\nHope that helps to explain what is going on!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy users,\nFirst, thanks in advance for your help,\nI am trying to get a de novo assembly from RNA-Seq data using Trintiy. I have just one sample that was sequenced in the Illumina platform with more or less 40 M paired-end reads 150 bp (strand-specific). I have trimmed those reads with Trimmomatic and checked the quality before and after that, and the reads have good quality. I am getting the following error:\nExecution resulted in the following messages:\nFatal error: Exit code 255 ()\nTool generated the following standard error:\nFATAL:   could not open image /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all/mulled-v2-db9b22af087bf11ca8136acc57516d3e3b7cbe14:6b4c1eb9563d366a9f4ff35670f4f430d7306d78-0: failed to retrieve path for /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all/mulled-v2-db9b22af087bf11ca8136acc57516d3e3b7cbe14:6b4c1eb9563d366a9f4ff35670f4f430d7306d78-0: lstat /scratch/xgalaxy/job_24326433/.cvmfsexec/dist/cvmfs/singularity.galaxyproject.org/all: no such file or directory\nGalaxy job runner generated the following standard error:\nFailed to initialize root file catalog (16 - file catalog failure)\nCould you help me with this, please?\nBest wishes,\nJose"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe are looking into reported problems with this version of Trinity \u2192 Trinity de novo assembly of RNA-Seq data(Galaxy Version 2.15.1+galaxy0), and a correction is pending.\nFor now, try this:\n\nAt least one rerun. Why? To eliminate transient server issues.\nTry using a prior version of Trinity. Any of the last three versions work for me Ok at @link http://UseGalaxy.org. Changing the tool version (@link https://training.galaxyproject.org/training-material/faqs/galaxy/tools_change_version.html)\n\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am doing the From Genes to Peaks tutorial and I am getting stuck\n\n\n@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html\n\n\nGalaxy Training: From peaks to genes (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\nWhen I do the Intersect of promoter regions with peaks regions, I get a \u201cAn error occurred setting the metadata for this dataset\u201d message. Inside the attributes, the info says: \u201c/corral4/main/jobs/040/816/40816980/galaxy_40816980.sh: line 123: galaxy-set-metadata: command not found\u201d\nDoes anyone know what\u2019s going on?"
    },
    {
      "role": "system",
      "text": "Update March 8, 2022\nThe tools in this suite are corrected at @link http://UseGalaxy.org. Please rerun any prior failures. The fix was a missing dependency added back at our clusters, so the tool version didn\u2019t change.\nThanks to all who reported it!\n\nThanks for sharing the history. The tool itself is problematic, along with a few related tools. It will take at least a few days to investigate the root cause.\nPlease run the tutorial at either @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au instead for now.\nRelated recent Q&A (someone else ran into the same error today) Get flanks - error setting the metadata (@link https://help.galaxyproject.org/t/get-flanks-error-setting-the-metadata/7587)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have been using @link http://usegalaxy.org (the main browser) since a few months and when I tried to log in today, it shows  \"This account has been marked deleted, contact your local Galaxy administrator to restore the account. Contact: @link mailto:galaxy-bugs@galaxyproject.org\"\nI really need my galaxy account as I have a lot of important data on it. I\u2019m sure this is a mistake. Kindly recover/ restore my account as soon as possible.\nMy email address on Galaxy: @link mailto:chegg0022@gmail.com\nAwaiting your reply."
    },
    {
      "role": "system",
      "text": "This was probably not a mistake. I recommend reading through Accounts deleted from Galaxy Main to enforce posted Terms and Conditions (@link https://help.galaxyproject.org/t/accounts-deleted-from-galaxy-main-to-enforce-posted-terms-and-conditions/1429/3)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m attempting to install a local version of Galaxy on a centos VM.  In order to get admin rights to the instance, it said I have to have a registered account.  I had one under one email, but the local galaxy install will not authenticate it.  It authenticates fine when I login to the main public galaxy site.\nI then registered a new account under a different email address (yeah I know, but it\u2019s broken so what am I supposed to do).  I made that account an admin, then I deleted the original account.  Then I tried to re-create the account, or just login, in the local instance but no luck as it says my email is already registered.  I tried logging in as a registered user on this local instance, tried creating a new account, and also tried to create a user from the admin interface and none of them worked at all.\nHow does this actually work?  When a user authenticates to a 127.0.0.1 loopback based local instance, is it actually checking somehow against your main registration server?  How do I have local users and also have an admin user?  How does any of this work?  If it\u2019s documented clearly somewhere, please advise as I searched the help and faqs and came up empty.\nAt this point I\u2019m just trying to test this thing and see if I can actually get it running on a VM for a user, I\u2019m not a genomics guy I\u2019m a sysadmin.\nFrom what I can tell, once a person registers an email address with you centrally, you can never use that email again on a local user install.  So even though it\u2019s local it phones home to mama to see if the user exists?  It just keeps saying that the account exists already, but won\u2019t actually authenticate to it because the password doesn\u2019t work.   So how do local users even get created and managed?  It requires an email address no matter what.  This is for my local install of galaxy not y\u2019all\u2019s system.\nI\u2019m seeing some suggested topics to browse and will try those but any help or links or suggestions at all will be greatly appreciated.\nThanks -"
    },
    {
      "role": "user",
      "text": "Thank you both for your replies.  I understand now, I had been under the impression that an admin had to be a \u201cregistered user\u201d and that the \u201cregistered user\u201d had to be registered centrally with the @link http://usegalaxy.org site in order to access the tool installs, etc.  Rather, it seems that an admin user just has to be registered locally on that galaxy instance, and added to the config file\u2019s admin list - which completely makes sense."
    }
  ],
  [
    {
      "role": "user",
      "text": "As already discussed in github (@link https://github.com/galaxyproject/galaxy/issues/3144), when sending a galaxy tool error report, the TO and FROM address are the same.\nThough it is possible to set an email_from param in galaxy.yml, it is assigned to error_email_to when the report is sent.\n\n\n@link https://github.com/galaxyproject/galaxy/blob/0fa3ffa01b0d2c2ec542343e064156af94fa50c6/lib/galaxy/tools/errors.py#L243\n\n\n@link https://github.com/galaxyproject/galaxy/blob/0fa3ffa01b0d2c2ec542343e064156af94fa50c6/lib/galaxy/tools/errors.py#L243\n\n        return self._send_report(user, email=email, message=message, **kwd)\n\n\n\n\nclass EmailErrorReporter(ErrorReporter):\n    def _send_report(self, user, email=None, message=None, **kwd):\n        smtp_server = self.app.config.smtp_server\n        assert smtp_server, ValueError(\"Mail is not configured for this Galaxy instance\")\n        to_address = self.app.config.error_email_to\n        assert to_address, ValueError(\"Error reporting has been disabled for this Galaxy instance\")\n\n\n        frm = to_address\n        # Check email a bit\n        email = email or ''\n        email = email.strip()\n        parts = email.split()\n        if len(parts) == 1 and len(email) > 0 and self._can_access_dataset(user):\n            to = to_address + \", \" + email\n        else:\n            to = to_address\n        subject = \"Galaxy tool error report from %s\" % email\n        try:\n\n\n\n\n\n\n\nI wander why such a behavior. Can anyone tell me ?\nIn my case, I have a sender address imposed by the mail server but it is not linked to a real email box. I need to give a different email address as recipient."
    },
    {
      "role": "system",
      "text": "@user\nThe idea is to ensure that a bug report is sent in by the actual person who has control over that registered account.\nNow, bug reports themselves are fairly sparse: the error message and a few clues about what is going wrong. Only the account owner themselves or an administrator on that server would be able to make use of that content (for practical reasons like troubleshooting)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I find the left panel scrollbar of @link https://usegalaxy.org/ was hidden on my Google Chrome and Firefox 71 browser. My PC screen resolution: 1366 x 768\uff0cand I use Windows 7 x64.\nWhen will this problem be fixed\uff1f\n\ngalaxy-srollbar1366\u00d7726 186 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/718f08a5f9f82414046913ee549ca7bbbb605cd7.png)"
    },
    {
      "role": "user",
      "text": "The real reason is that I changed the browser\u2019s default font size to small. When I changed the font in my browser to  Medium(Recommended) , everything was fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "I get the following error when trying to load any multiple sequence alignment on .org\nimage"
    },
    {
      "role": "system",
      "text": "Update Jan 30, 2023\nThis was fixed. If a new problem shows up, please open a new topic and include the link to this one for context. Thanks!\n\n\nHI @user\nWe know of several visualization tools that have this same technical problem.\nPrimary issue ticket (@link https://github.com/galaxyproject/galaxy/issues/14147). When that closes out, expect the fix to show up at @link http://usegalaxy.org and @link http://test.galaxyproject.org soon after.\nAll UseGalaxy.* servers are impacted.\nLet\u2019s keep this topic open until fully resolved with testing in the wild :slight_smile:\ncc @system"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been trying to run the circos tool on galaxy for the past couple of days however my jobs have all been reaching an error. This is not my first time using the application and attempting to rerun previous runs that  have worked in the past results in the same error message."
    },
    {
      "role": "system",
      "text": "Hi @user, you\u2019re using an older version of the Circos Galaxy wrapper which has known issues. Please use a newer version like the one on @link http://UseGalaxy.eu. Try this one: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/circos/circos/0.69.8+galaxy7\nAdditionally, perhaps the admins can update circos on .org? @system"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nRecently, I tried use galaxy bowtie2 tool to mapping chip-seq data. However, some chip-seq data downloaded from SRA has been reported a warning as followed:\n##################################\nWarning: skipping read \u2018Run2_FC2_20100512_Sample1B_1279_21_112_F3/1\u2019 because length (0) <= # seed mismatches (0)\nWarning: skipping read \u2018Run2_FC2_20100512_Sample1B_1279_21_112_F3/1\u2019 because it was < 2 characters long\nWarning: skipping read 'Run2_FC1_2010\n################\nI am curious about this warnings? Is it fine to go on next step (samtools) or need to map the genome again with bowtie2? And how to solve this problem?\nThanks\nBest"
    },
    {
      "role": "system",
      "text": "@quote\nmin length (<30)\nDid you retain the fastq reads over 30 bases or under? Double check the settings used for filtering. Bowtie2 is reporting a read found that is 0 bases long. My guess is that you probably intended to filter for >=30 bases?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI constructed a pipeline like \u2018fastp> RNA STAR> featureCounts> DESeq2\u2019, and exploited the built-in genome of each of RNA STAR and featureCounts.\nThe built-in genome of RNA STAR and featureCounts must be different. Will there be any problem with this use?\n(I don\u2019t know, but I was told that the same genome should be used for mapping and counting)."
    },
    {
      "role": "system",
      "text": "Hi @user\nGencode is a good source for reference annotation for hg38. Be sure to format the data correctly after loading it into Galaxy \u2013 specifically for this source, that means removing the header lines.\nThen, use the built-in hg38 reference genome when mapping your reads.\nFAQs: Galaxy Support (@link https://galaxyproject.org/support/#getting-inputs-right)\n\nDatatypes (@link https://galaxyproject.org/learn/datatypes/#gtf)\nHelp for Differential Expression Analysis (@link https://galaxyproject.org/support/diff-expression/)\n\nTutorials: Galaxy Training! (@link https://training.galaxyproject.org/training-material/)\n\nChoose the topic Transcriptomics\n\nI also added some tags to your post that link to prior Q&A around this type of analysis. You can also try a search with tool names, as that sometimes find more exact examples of troubleshooting/use-cases.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "For the past few days I have been trying to use Cufflinks for my RNA seq analysis but I have been repeatedly getting this error:\nslurmstepd: error: couldn\u2019t chdir to `/srv/pulsar/main/venv/lib/python3.6/site-packages\u2019: No such file or directory: going to /tmp instead\nThis started happening a few days ago and up until then I had no issues running the tool at all.  My inputs are BAM file obtained from HISAT2 alignment and gtf file for mm10 reference annotation.  I noticed that the assembled transcripts gtf file still outputted from Cufflinks despite the error, so I tried inputting these assembled transcripts into Cuffmerge (along with some successfully obtained assembled transcript gtf\u2019s before this error started recurring), but Cuffmerge also resulted in an error:\n[Fri Aug 28 23:06:48 2020] Preparing output location output/\nTraceback (most recent call last):\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 580, in \nsys.exit(main())\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 538, in main\ngtf_input_files = test_input_files(transfrag_list_file)\nFile \u201c/cvmfs/main.galaxyproject.org/deps/_conda/envs/__cufflinks@2.2.1/bin/cuffmerge\u201d, line 268, in test_input_files\ng = open(line,\u201cr\u201d)\nIOError: [Errno 2] No such file or directory: \u2018input_1 input_2 input_3 input_4 input_5\u2019\nI have been able to successfully use this tool without any problems up until now.  Any solutions would be appreciated; I\u2019m still relatively new to Galaxy."
    },
    {
      "role": "system",
      "text": "This was fixed (@link https://github.com/galaxyproject/usegalaxy-playbook/commit/47b9ee1d690f3f077cd2e93975c15af789f554a9) as of September 22, sorry for not updating here. Please let us know if you\u2019re still encountering issues."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage1273\u00d7375 13 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b33675b1e49486af80f382d3002513ddf409b42c.png)"
    },
    {
      "role": "system",
      "text": "HI @user,\nyes, that tool is available in @link http://usegalaxy.eu, but there\u2019s a problem in the tool browser, so it is not easy to find it: @link https://usegalaxy.eu/root?tool_id=repeatmasker_wrapper.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nWhat is the best tool for deduplication for my data?\nlets say i have more than 50% duplication in my reads, how should i remove them.\nI saw some tool but im not sure which one is best for my analysis.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin the case of miRNA, I suggest you go on with the analysis without considering the GC and duplication content for this reasons:\n\nmiRNA sequences usually contain overrepresented motifs that could interfere with the real duplication values (because in many cases they have been originated from duplications).\nThe miRNA GC content is quite variable, and depending of the organism, the miRNAs can be enriched in those nucleotides (Lam et al. (@link https://norgenbiotek.com/sites/default/files/resources/total_rna_purification_kit_effect_of_microrna_gc_content_on_rna_purification_efficiency_application_notes_68.pdf), Mishra et at. (@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2825596/)).\n\nWhat tool did you use to perform the alignment?\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nThis is a complete beginner question and this probably belongs to the RTFM section. However, there is obviously a logic that I do not understand in galaxy xml.\nWhat I want to do (independently of whether or not you can do that with existing tools in galaxy), is to give a list of bed files to my tool, and output a tab like:\n#filename        #nb of lines\nfile1                  4645464\n...                      ......\nfilen                  346354345\n\nHere is my xml file:\n<tool id=\"bedcounts_summary\" name=\"Counts and summarizes line number for each bed file of a list\" version=\"0.1.0\">\n    <requirements>\n    </requirements>\n    <command>                                                                                                                                \n      bash $__tool_directory__/bedcounts_summary.sh #for $key in $bedlist.keys() # $key $bedlist[$key] #end for                              \n      >> $outputtab                                                                                                                          \n    </command>\n    <inputs>\n      <param type=\"data_collection\" format=\"bed\" name=\"bedlist\"/>\n    </inputs>\n    <outputs>\n      <data name=\"outputtab\" format=\"tabular\"/>\n    </outputs>\n    <help><![CDATA[                                                                                                                          \n        TODO: Fill in help.                                                                                                                  \n    ]]></help>\n</tool>\n\nHere is the bash script:\n#!/bin/bash                                                                                                                                  \n\nbedfilename=$1\nbedfile=$2\n\ncountslines=`wc -l < $bedfile`\n\necho -e \"$bedfilename\\t$countslines\"\n\nThis is returning only the first line. I managed to have all lines using a simple echo but that is not the point, obviously.\nIf you can point to my logic/programming error that would be very helpful!\nThanks!"
    },
    {
      "role": "system",
      "text": "It is because you are passing your script two arguments per collection element but your script only takes two arguments.\nYou need to wrap the entire call to your script in the #for block.\nLook at the command in the Info section of the tools output. You will see your mistake."
    }
  ],
  [
    {
      "role": "user",
      "text": "Preparing for an installation of Galaxy:\nI am asking with a simple multicore VM i mind. No cluster.\nHow will Galaxy handle multithreading? Can I define globally the amount of available CPUs somewhere and then the tools will use the availble CPUs. Or what is the strategy?"
    },
    {
      "role": "system",
      "text": "Uhm I am no expert but yes, but it depends how the tools are created but think most tools in the toolshed follow all the official guidelines.\nIn your case if you switch from 8 to 16 cores you only need to adjust config/job_conf.xml"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Wolfgang,\nI also got a error recently when I used MiModD Variant Calling tool in public Galaxy server (@link https://usegalaxy.org/) after aligned with Bowtie2.\nI tried to repeat this step (by \u2018Run Job Again\u2019) in a previous successful analysis case and got the same error. Could you give me any suggestions?\nThanks.\nError code:\n/corral4/main/jobs/051/303/51303573/tool_script.sh: line 10: mimodd: command not found.\nJob Information\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/wolma/mimodd_main/mimodd_varcall/0.1.9\nCommand Line empty\nTool Standard Output empty\nTool Standard Error\n/corral4/main/jobs/051/303/51303573/tool_script.sh: line 10: mimodd: command not found\nTool Exit Code: 127\nJob Messages\n{ \u201ccode_desc\u201d: \u201c\u201d, \u201cdesc\u201d: \u201cFatal error: Exit code 127 ()\u201d, \u201cerror_level\u201d: 3, \u201cexit_code\u201d: 127, \u201ctype\u201d: \u201cexit_code\u201d }"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe have confirmed the problem at the US server. Please try at EU or AU for now.\nConsolidating this this post, where we will post updates: MimodD extract variant sites (@link https://help.galaxyproject.org/t/mimodd-extract-variant-sites/10410)\nThanks for the followup!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am new to Galaxy and I am facing a conda environment problem lately on the @link http://usegalaxy.org Main server.\nI am trying to map my data on the human genome using the RNA STAR tool but whenever I execute my jobs on the @link http://usegalaxy.org server, I have this error:\n\nFailed to activate conda environment! Error was:\n/jetstream/scratch0/main/jobs/41688358/command.sh: line 110: /cvmfs/main.galaxyproject.org/deps/_conda/bin/activate: No such file or directory\n\nand all of my files are empty.\nI tried to do the same mapping step with HISAT2 and TOPHAT tools but I still face the same error.\nHowever, I tried to follow the galaxy tutorial on RNA-seq analysis Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html?utm_source=galaxy-workshop&utm_medium=courseprogram&utm_campaign=galaxy-workshop) and I do not have that problem.\nCan someone tell me if I am doing something wrong or is it problem occurring within the server?\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nCheck these two items. The first may or may not be OK (worth checking), and the second is likely a content problem (mismatched chromosome names). Both might need to be adjusted.\n@quote\nI have paired-end reads and I select multiple datasets.\nMake sure that the pairs are entered in the same order on the tool form, and that none of the ends of any pairs are omitted. You can click into the \u201ci\u201d Job Details page to see what was originally submitted to check (even for failed jobs). If you have many paired end datasets, using dataset collections would help: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=collection)\n@quote\nYes, I am using a built-in reference genome (the human genome (hg38)) without a built-in gene model. As for the gene model, I specify .gtf file (GRCh38) downloaded from the ensemble database.\nThe reference genome was sourced from UCSC when we indexed it on the server.\nAll other inputs must use that same genome/build, including the same chromosome naming formats, or tools will either ignore the mismatched input or trigger a failure.\nThe reference annotation (gene model) was sourced directly by you\u2026 from Ensembl (?). They use a different chromosome name format than UCSC.  Where to get the annotation with UCSC chromosome names is in this prior Q&A:\n@quote\nThehg38reference annotationGTFis now best sourced from UCSC. Not from the Table Browser but from theDownloads area here. There are a few Gene tracks to choose from. You can review what each of those represents then upload the selectedGTFto Galaxy with theUploadtool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\nNothing else pops out. So, please try using the UCSC GTF instead with rerun(s). If any still fail, please send in a bug report from one of the red error datasets, and include a link to this topic in the comments so we can link the two for context. Also, please leave all datasets undeleted (inputs + outputs, successful or failure) for review.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI attempted to input two WGS data sets (two different sets of reads) and a reference genome from NCBI into the BWA-MEM2 tool. The job failed and resulting in the following messages:\nExecution resulted in the following messages:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nLooking to launch executable \u201c/usr/local/bin/bwa-mem2.avx2\u201d, simd = .avx2\nLaunching executable \u201c/usr/local/bin/bwa-mem2.avx2\u201d\n[bwa_index] Pack FASTA\u2026 22.12 sec\n** Entering FMI_search*\ninit ticks = 173525096758\nref seq len = 5422419662\nbinary seq ticks = 119852397722\nAllocation of 40.40 GB for suffix_array failed.\nCurrent Allocation = 45.45 GB\nIf it helps troubleshoot this issue, I have included some information about the job and the parameters used below."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe FAQs explain what you can do in general to continue working at a public Galaxy server. In short: use an indexed genome as the mapping target (bosTau8 instead) + split the query reads into a collection, run some QA, map, then merge the results.\nOr, you can consider setting up a private Galaxy server with scaled up resources.\nDetails:\n\nThe job failed for exceeding memory limits at the public Galaxy server FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-exceeds-memory-allocation-error-messages).\nSpecifically:\n\nThe fastq datasets are too large \u2013 ~25 GB compressed per end. FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fastq-datasets)\n\nThe fasta custom genome is too large, plus has a formatting problem. FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fasta-datasets)\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage1104\u00d7666 24.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dadbaf5bf081fa4f8a686db540479f094a80910c.png)\nI was trying to upload the main annotation GTF file (after downloading it to my computer from the GENCODE website) but I\u2019m getting this error. The upload happens normally for a few seconds but halts right after the 3% mark every time.\nHow to resolve this? I tried different browsers as well but the status displays the same error regardless."
    },
    {
      "role": "system",
      "text": "@user Uploading through the website are limited to files not bigger than 1 GB. For larger files, you can use our FTP service. All the details here Galaxy Europe | FTP Configuration (@link https://galaxyproject.eu/ftp)"
    }
  ],
  [
    {
      "role": "user",
      "text": "The Tetrahymena thermophila isn\u2019t indexed. Is this possible? It can be found at\n@link http://ciliate.org/index.php/home/downloads"
    },
    {
      "role": "system",
      "text": "Hi,\nwhich index are you talking about? A simple fasta index or an index for a specific tool?\nGenerally, the situation with T. thermophila is a bit complicated by the fact that it has a Mic and a Mac genome and you would likely want a different one of those depending on the question you\u2019d like to answer.\nNote however that you can always upload any reference genome in FASTA format to your history, and most tools will be able to generate required indices on the fly from that.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I had an account under the email redacted for couple of years, and I had no issue with logging into my account. But now it is asking me to re-enter the password upon login, which I didn\u2019t remember the password. When I clicked \u201creset password\u201d, it was showing \u201cFailed to produce password reset token. User not found\u201d and did not allow me to reset the password. I\u2019m very confused with this error and hope to get some help finding my account and history back. Thank you for your time."
    },
    {
      "role": "user",
      "text": "Hi. Thank you very much for your information. But I only had one account at the point where it got deleted then I created a second account. I\u2019ve been always using the same server on my laptop. I\u2019m going to email the support email in the registration email. Do you have any other advice? It would be great to find my original account back. Thank you for your time."
    }
  ],
  [
    {
      "role": "user",
      "text": "the jobs shows it\u2019s been waiting to run for two days ,what\u2019s wrong with this tool? please help me !1618550369(1)"
    },
    {
      "role": "system",
      "text": "Hey arronwang,\nGalaxy was currently under maintenance (two days). The cluster accepted jobs, but it had limited resources, and thus many jobs stayed in the queue. Your job should run by now, if this is not the case, then please resubmit your job."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. Why do readings outside of genes appear in an RNAseq experiment in IGV? The readings are supposed to match exons, right?\nThank you"
    },
    {
      "role": "system",
      "text": "At least two possibilities:\n\n\nThese are real RNA molecules that were in your library. poly-a selection in RNA-seq is not 100% efficient, so these could be ncRNAs, or just spurious transcription\n\n\nAlignment error: alignment is also not perfect, some reads may be placed in intergenic regions incorrectly.\n\n\nThis is typically not something to worry about. Downstream, if you assemble transcripts or quantify using an existing transcriptome, these will be ignored."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have been unable to connect to the network using FTP. I am using FileZilla and am getting these messages when I try to connect.\n|Status:      |Resolving address of ftp.usegalaxy.eu|\n|Status:      |Connecting to 132.230.223.103:21\u2026|\n|Status:      |Connection attempt failed with ENETUNREACH - Network unreachable.|\n|Error:        |Could not connect to server|\nI am using @link http://ftp.galaxy.eu for the Host name and using my login credentials. I\u2019ve also tried setting Port to 21.\nI have been able to connect to the @link http://usegalaxy.org network by selecting \u201crequire implicit FTP over TLS\u201d in settings.\nI appreciate any help!"
    },
    {
      "role": "system",
      "text": "The FTP service is still off because the maintenance is still ongoing.\nWe don\u2019t have, at the moment, a time estimation when it will finish.\nCheck regularly the @link http://usegalaxy.eu homepage, we are posting updates there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI am trying to integrate galaxy docker image on my platform and I am having issues with how galaxy docker image resolves GALAXY_URL.\nThe thing is next:\nWhen we make request to the galaxy, request flows through few proxies, and the last one, let\u2019s call it Proxy Z is on another domain, let\u2019s call it @link http://z.proxy.com. Galaxy container is running behind that proxy, thus GALAXY_URL is pointing to @link http://z.proxy.com\nI would like to know if it\u2019s possible to configure GALAXY_URL to point to our organizational public domain, instead of the proxy\u2019s.\nWe are using at the moment latest stable galaxy docker image 20.09 Docker (@link https://hub.docker.com/r/bgruening/galaxy-stable/) running 20.09 galaxy version"
    },
    {
      "role": "system",
      "text": "heya, adding some info to this:\nit looks like GALAXY_URL is built from the request\u2019s HOST header field; if i change my etc/hosts file and navigate to galaxy with the added host, then use the UCSC Get Data tool, the request looks like:\nGET\nscheme: http\nhost: somedomain.bork:8080 <==== my addition to /etc/hosts\nfilename: /tool_runner/data_source_redirect\ntool_id: ucsc_table_direct_archaea1\nAddress: 127.0.0.1:8080\n\n\u2026and the response looks like:\nHTTP/1.1 302 Found\nServer: nginx/1.14.0 (Ubuntu)\nDate: Wed, 04 Jan 2023 21:31:38 GMT\nContent-Type: text/html; charset=UTF-8\nContent-Length: 591\nConnection: keep-alive\nx-frame-options: SAMEORIGIN\nLocation: http://archaea.ucsc.edu/cgi-bin/hgTables?GALAXY_URL=http%3A//somedomain.bork%3A8080/tool_runner&tool_id=ucsc_table_direct_archaea1&sendToGalaxy=1&hgta_compressType=none&hgta_outputType=bed\n\n(note the GALAXY_URL=http%3A//somedomain.bork%3A8080/)\nare you navigating from the desired URL in your browser, but getting the downstream proxy URL when redirected?\nhere\u2019s some more random info in case it\u2019s useful:\nusing the \u2018Get Data \u2192 UCSC Archaea table browser\u2019 follows this link:\n\n$URL/tool_runner/data_source_redirect?tool_id=ucsc_table_direct_archaea1\n\n\u2026which uses this chunk of logic (@link https://github.com/galaxyproject/galaxy/blob/release_20.09/lib/galaxy/webapps/galaxy/controllers/tool_runner.py#L118) to check the galaxy tool:\n\ngalaxy/ucsc_tablebrowser_archaea.xml at release_20.09 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/blob/release_20.09/tools/data_source/ucsc_tablebrowser_archaea.xml)\n\nregards,\n\nLuke\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there! I have reached my disk quota, and therefore am trying to purge my history that is using up all of the storage. However, when I click \u201cpurge\u201d the screen freezes and does not purge the history. Any suggestions on how to fix this?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nYou are working at @link http://UseGalaxy.org, correct? The function is working correctly (just tested it to be certain).\n@quote\nHowever, when I click \u201cpurge\u201d the screen freezes and does not purge the history\nThe screen may fade out in the middle panel under the User > Histories view when large amounts of data are permanently deleted-purged. Allow that process to complete before navigating away.\nYou may also need to then log out and back in again to reset the updated quota usage. The server will eventually catch up but that triggers the quota usage recalculation to run quicker.\nNote: If a history is in a shared state, you may not be able to purge it. Unshare, then purge.\nFAQ: Account quotas (@link https://galaxyproject.org/support/account-quotas/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear admin,\nI cannot access my Galaxy account for the past week, whenever I tried to login to my account, the system states that it fails to submit email and requires me to contact the administrator. I also tried resetting the password but failed as well.\nI further attempted to register a new account using a different email, unfortunately no verification email was sent to me and log-in access was not granted as the system states I have not activated my account.\nAs I am using the Galaxy system for my university assignment, I need to prove that it was a system error with some evidence such as an email reply.\nKindly advise me on this matter. Thank you for your attention."
    },
    {
      "role": "system",
      "text": "@user I\u2019m afraid you need to contact the admins of this particular server. Please have a look here: @link http://galaxy.rhc.ac.ir/static/contact_us.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I am doing an RNA-seq study and I want to group in a single file all the counting data of my six samples obtained with HTseq, to be able to visualize them all together. Is there any tool in Galaxy to do this? Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user, I haven\u2019t run htseq-count in a while, but I think this will work.\nThe Setup\nhtseq-count produces a two column output. The first column is the feature name (from the GFF) and the second is the overlap count.  This is shown for all features, even if there are 0 overlaps.\nWhere to Look\nFor questions where you have multiple datasets, with a common column, and you want to unite them into a single file, start with these two toolboxes in the tool panel (on the left): Text Manipulation and Join, Subtract and Group.\nWhat tio do (I think)\nThe first thing with promise under Text Manipulation is Multi-Join, and I think his will work. Pick one of your htseq-count outputs as the File to Join and then select the rest in the Add additional file list.\nCommon key column is 1\nColumn with values to preserve is 1 and 2\nAdd header line to the output file is yes\nInput files contain a header line is yes (mine did)\nYou can then use the Text Manipulation \u2192 Cut operator to rearrange the columns."
    }
  ],
  [
    {
      "role": "user",
      "text": "I obtained the transcript count file using StringTie and now I am trying to run the count file on DeSeq2 using galaxy server. I am getting the following error.\n\nThe design matrix has the same number of samples and coefficients to fit, so estimation of dispersion is not possible. Treating samples\nas replicates was deprecated in v1.20 and no longer supported since v1.22.\n\nI don\u2019t know how to fix this error, please help!"
    },
    {
      "role": "system",
      "text": "HI @user,\nyou need to provide more than one replicate, otherwise the statistical analysis is not possible.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Jenna,\nI am having the exact same error as posted. I Have an abundance of fast5 Files that return the same result.  Was there an eventual solution to this formatting issue? Would be more than happy to share my history if this helps?\nThanks!\nJesse"
    },
    {
      "role": "system",
      "text": "@quote\nThe original post said there was some formatting issues, could this also be the problem with my data?\nYes, this is likely a problem for you as well if tools cannot read your files.\nIf the data is compressed, then that compression format should be included on the dataset name.\nIf Galaxy did not guess the correct datatype (including the compressed versus uncompressed state), then one of these should work:\n\nUpload the data and set the datatype to match\nConsider loading uncompressed data to avoid any technical mismatches between the utilities Galaxy uses versus your own computer (or wherever the original source data was created).\nConvert to fastq, then Upload the reads to Galaxy for analysis.\n\nKeep in mind that data can be truncated or corrupted when moving it around. This is fatal for a compressed file, and the full original file is what tools will be able to interpret (in Galaxy or otherwise). Utilities from the format developer: GitHub - nanoporetech/ont_fast5_api: Oxford Nanopore Technologies fast5 API software (@link https://github.com/nanoporetech/ont_fast5_api)\nTroubleshooting the above is what the other person had to do, too.\nOnce you have reads, then these tutorials can help.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/single-cell/)\n\n\nGalaxy Training: Single Cell (@link https://training.galaxyproject.org/training-material/topics/single-cell/)\nTraining material and practicals for all kinds of single ...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to follow the MD simulation with GROMACS tutorial (Running molecular dynamics simulations using GROMACS (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/md-simulation-gromacs/tutorial.html)) on a PDB file, 2XQW, which contains 3 protein chains. However, when I try to run the initial GROMACS set up tool, I get an error that the job cannot run:\n\nimage492\u00d7518 30.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/310848a0d42d132e39d583b12722fe1fee813ce4.png)\nThe full error says \u201cThis job was resubmitted to the queue because it encountered a tool detected error condition on its compute resource.\u201d\nIs there a way to follow this tutorial on a PDB file containing more than 1 protein chain?"
    },
    {
      "role": "system",
      "text": "@user thanks for the bug report, I replied by email.\n\nI also wanted to add that I have tried this again with separate PDB files with only a single protein chain and the same error occurs.\n\nThe reason would be that the error is not due to the multiple chains, but unexpected atom naming in the original PDB file."
    }
  ],
  [
    {
      "role": "user",
      "text": "@link http://Usegalaxy.org\nMissing tools for untargeted LCMS interactive tutorial.\nThe list below was copied from the tutorial list of tools for processing demo data.\nI found msnbase_readmsdata but did not have permission to install into Galaxy\nmsnbase_readmsdata\nxcms plot chromatogram\nxcms findChromPeaks (xcmsSet)\nxcms findChromPeaks Merger\nxcms groupChromPeaks (group)\nxcms adjustRtime (retcor)\nxcms fillChromPeaks (fillPeaks)\nCAMERA.annotate\nxcms process history\nI\u2019m requesting that above list be installed into Galaxy or a custom set of the above tools made available to me.\nRob Kleps"
    },
    {
      "role": "system",
      "text": "Hello Rob, you can find the tools here: @link https://usegalaxy.eu/."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, How to a request more space on Galaxy? (apart from deleting files as i go along). Would be great to have around >250gb space or more (happy to pay towards this too). Thanks"
    },
    {
      "role": "system",
      "text": "@quote\nWould be great to have around >250gb space or more (happy to pay towards this too). Thanks\nHi @user\nThis FAQ explains some semi-advanced strategies for managing data on public servers. All of this is GUI based, but Galaxy also has an API if you want to interact that way.\nHow can I reduce quota usage while still retaining prior work (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#how-can-i-reduce-quota-usage-while-still-retaining-prior-work-da)\nAnd this FAQ explains options for setting up a server for larger ongoing work, including academic/scientist friendly grant-based and pay-for-use options with low/GUI-based to no administrative set up on your part. That FAQ also points to all choices: some versions can use your own local/institutional resources, others involve commercial resources, and you could do a mix (process smaller jobs locally + send larger jobs off to regional academic cloud clusters).\nGalaxy choices (@link https://training.galaxyproject.org/training-material/faqs/gtn/galaxy_usage.html)\nIt is fine to keep a single account on any or all of the public free services (for public/sharing purposes, or any other reason), even when working at your own private server. One account on each gives you access to distinct resources: data storage quota, hosted tools, indexed reference data.\nIf you just need an extra 250 GB \u2013 all of the UseGalaxy.* servers offer about that or more as a default for everyone, so that is at least 1 TB of free storage before including the other resources at the remainder of public sites (138 total as of today). Moving data between servers (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#moving-datasets-between-galaxy-servers) is easy and common.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to do some differential expression studies with my dataset\u2019s, i have been feeding just a single replicate followed by settings as blind dispersion method ! yet the process fails each time i select generate SQLite while not selecting it and running the tools generates tabular files !"
    },
    {
      "role": "system",
      "text": "The tutorial themselves include exact tools and associated workflows. Each tutorial starts out with an overview of the analysis concepts, then describes what parts will be specifically covered by the examples, and ends with a list of reference publications. Not every possible path is covered in detail (of course!), but combined, those resources should help you to 1) get oriented and 2) make choices that are best for your own research goals.\nPlease give them a review, it will be worth your time :slight_smile: Plus you can review prior Q&A at this forum. I added the tag \u201ctranscriptomics\u201d to your post. Click on it to find similar Q&A directly, or just do a search to find even more as you progress. Many have links to FAQs and related help that can help if you run into trouble (usage, etc).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have install galaxy locally.\nThe programs/tools  are running well , however the output cannot be displayed in the center.\nThe folllowing is displayed for the fastqc result\n\nimage1834\u00d7905 237 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6e5c5989a09f41972f7cc7a25474ccfdbb8f2f7b.png)"
    },
    {
      "role": "system",
      "text": "Because of security reasons you need to whitelist tools first before you can see the HTML output. You can do that on the admin page (Admin > Manage allowlist ). Here you can see a list of all your tools, select fastqc and add it to the \u201callowlist\u201d. Hope this helps."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nWe would like to establish a local Galaxy server that resembles the current public main Galaxy server (@link https://usegalaxy.org/) as much as possible, and we would like to accomplish this by using the Ansible and other Galaxy team recommended approach.  Unfortunately, we have some confusions about those instructions.  Could someone kindly post a list of key steps that we should follow?\nMany thanks!\nUSC Libraries Bioinfo Service."
    },
    {
      "role": "system",
      "text": "We do publish our playbooks online at e.g. @link https://github.com/galaxyproject/usegalaxy-playbook/ for @link http://usegalaxy.org. However, I am aware that this is quite complex, even for people already familiar with Ansible. The most relevant bits - specifically, the Galaxy config - are all in the galaxyservers group vars (@link https://github.com/galaxyproject/usegalaxy-playbook/blob/master/env/main/group_vars/galaxyservers/vars.yml) as well as the @link https://github.com/galaxyproject/usegalaxy-playbook/blob/master/env/common/stack.yml, @link https://github.com/galaxyproject/usegalaxy-playbook/blob/master/env/common/update.yml, and @link https://github.com/galaxyproject/usegalaxy-playbook/blob/master/env/common/config.yml playbooks. There is a fair amount of extra stuff in those playbooks that most people will not need though, since we deploy Galaxy to CVMFS.\nBeginning with Galaxy 19.05, you will be able to load our shed tool config files directly from CVMFS without needing to also point your Galaxy at our install database (these are both configured in the Galaxy config defined in the group vars linked above), so you can still install tools locally from the Tool Shed without issues.\nRoughly, for the Galaxy setup only:\n\nCreate a group variables file for your Galaxy server and choose the options from galaxy_config_hash that are most relevant to you (but place them in the variable galaxy_config as that is the variable name used by galaxyproject.galaxy)\nSet galaxy_layout and related directory variables as described in the galaxyproject.galaxy (@link https://github.com/galaxyproject/ansible-galaxy/) documentation.\nCreate a simple playbook as shown in the documentation (see also the training materials linked by Jen).\nCreate an Ansible hosts file containing the connection information for your Galaxy server.\nansible-galaxy -p roles install galaxyproject.galaxy\nansible-playbook -i hosts playbook.yml\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "i want to use kallisto. my fragments length are 80 and SD is 8.\nbut it came with the error containing this : This job was terminated because it used more memory than it was allocated.\nPlease click the bug icon to report this problem if you need help.\nwhat should i do ?"
    },
    {
      "role": "system",
      "text": "Ok, that was fast.\nUsing the complete hg38 reference genome will not work. You need to find or create a fasta file for a transcriptome to use for the input option \u201cFASTA reference transcriptome\u201d."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI was trying to run multiple (3 at this time) workflows in galaxy and upload data for another 6 workflow invocations. Everything went fine (not exactly, but not with workflow of question), but then i saw that one of my workflow runs has totally vanished! I am 146% sure it was running (moreover, it had been running for more than one hour). But there are no errors in web interface nor in the log!\ntransgen@transgen-4:~/galaxy$ cat galaxy.log | grep -Pv 'Failed to activate conda environment|VERBOSITY=ERROR|HTTP/1.1\" 200' | grep -Pi 'HTTP/1.1\"|excep|error' --color=always | tail -n 20\ngalaxy.jobs DEBUG 2021-07-15 17:21:03,823 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1237', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:03,975 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:03,984 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1232\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.jobs DEBUG 2021-07-15 17:21:10,706 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1252', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:10,821 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:10,866 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1249\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.jobs DEBUG 2021-07-15 17:21:12,526 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Pausing Job '1254', Execution of this dataset's job is paused because its input datasets are in an error state.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:12,778 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:12,794 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1253\ngalaxy.exceptions.ObjectNotFound: No such object found.\ngalaxy.tools.error_reports DEBUG 2021-07-15 17:21:14,697 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Bug report plugin <galaxy.tools.error_reports.plugins.sentry.SentryPlugin object at 0x7fc87b032220> generated response None\ngalaxy.jobs ERROR 2021-07-15 17:21:14,701 [pN:main.web.1,p:2153100,w:1,m:0,tN:JobHandlerQueue.monitor_thread] Unable to cleanup job 1255\ngalaxy.exceptions.ObjectNotFound: No such object found.\n172.16.2.70 - - [15/Jul/2021:17:22:43 +0300] \"POST /api/workflows/1cd8e2f6b131e891/invocations HTTP/1.1\" 400 - \"http://172.16.2.200:8080/workflows/run?id=1cd8e2f6b131e891\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:17:34:21 +0300] \"POST /api/workflows/1cd8e2f6b131e891/invocations HTTP/1.1\" 400 - \"http://172.16.2.200:8080/workflows/run?id=1cd8e2f6b131e891\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:17:42:24 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:18:01:17 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n172.16.2.70 - - [15/Jul/2021:18:16:02 +0300] \"GET /welcome HTTP/1.1\" 302 271 \"http://172.16.2.200:8080/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"\n\nSo the last error message was at 18:16:02, but the run disappeared at about 19:15."
    },
    {
      "role": "system",
      "text": "Go to User \u2192 Workflow Invocation, it should be there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nWe are upgrading Galaxy from Rel 16 to Rel 20 using the Docker deployment.\nNo matter what we do,  restart the container, update /config/tool_conf.xml with our apps in the order we want them and then  \u201csupervisorctl restart galaxy:\u201d,  Galaxy comes up with the applications,  but not in the order we want them.\nHow can we force it to \u201crespect\u201d the order we give it the apps?\nBelow,  the list of our apps in the order we want them.\nThanks!\nGeorge Weingart PhD\nHuttenhower Lab\nHarvard School of Public Health\n\nScreen Shot 2022-04-25 at 5.21.49 PM963\u00d7785 28 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/169ed471b412e1a82f7eaaa7bb2e16b76df408c0.png)\n\nScreen Shot 2022-04-25 at 5.23.01 PM1254\u00d7794 208 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6ca3197ae1a4d51cec4c6653c76665203359fcb8.png)"
    },
    {
      "role": "system",
      "text": "Hi George\nNot sure whether this works using the Docker deployment:\nHave you tried deleting (or renaming, just to be on the safe side) the \u2018integrated_tool_panel.xml\u2019 file before restarting?\nRegards, Hans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nIs there a tool in usegalaxy that can help me retrieve reads from a fasta file (8Gb) based on a list of read identifier names (~150)? The list of identifer names is in a text file but I can convert to fasta or something else. I\u2019d then run the selected reads through blast for ID.\nThanks for any help."
    },
    {
      "role": "system",
      "text": "Yes, you can find that tool in Galaxy as Filter sequences by ID (if you look for seq_filter_by_id in the tool search bar you will get the same result)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.\nIt seems the bam file ( @link https://zenodo.org/record/60520/files/GIAB-Ashkenazim-Trio-hg19.gz?download=1) in the training session \u2018Calling variants in diploid systems\u2019\n\n\n@link https://training.galaxyproject.org/training-material/topics/variant-analysis/\n\n\nGalaxy Training: Variant Analysis (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\n\nare not able to process for variants calling.\nI also used Samtools to convert this bam file to a sam file, and got the error:\n[W::bam_hdr_read] EOF marker is absent. The input is probably truncated\nAlso, the size of the bam file is extremely small, only 52kb"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe BAM is downsampled so it will run quickly when running through the tutorial.\nThis is a history that has that particular BAM loaded (size is 51.2 KB), and the content is as expected. I also converted Bam-to-Sam in that history, and that worked plus produced the right result. Galaxy | Accessible History | tutorial variants (@link https://usegalaxy.org/u/jen/h/tutorial-variants). When you run Freebayes, input the BAM, no need to convert to SAM. If you want to see the content of the BAM, it is small enough that Galaxy can produce the uncompressed \u201cSAM\u201d view (use the \u201ceye\u201d icon for the dataset).\nMaybe you got a corrupted version? Try pasting just this link into the Upload tool in Galaxy. Leave all the other settings at default. @link https://zenodo.org/record/60520/files/GIAB-Ashkenazim-Trio-hg19.gz\nYou can also make a copy of that history to work with it. I\u2019ll leave it shared for this week.\nTutorial: Calling variants in diploid systems (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html)\nNote: You marked this as being \u201cgalaxy-local\u201d and as if working at @link http://UseGalaxy.org.\n\nIf you are actually working at @link http://UseGalaxy.org, @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au, this will work.\nIf you are trying to run the tutorial in a Local or Docker Galaxy on your own computer, other things may be going wrong, even if the downloaded BAM is the right size. Please explain a bit more about where you sourced Galaxy and what you have done to setup your server. Include details \u2013 is this the first dataset you have tried to load to that server? The first BAM dataset?\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nWe have occurred some problems when using the SortMeRNA with paired end samples. We suspect our RNA seq reads have ribosomal RNA contamination and try to filter them out. However, SortMeRNA reported an error (still green but the output file is 0kb) as shown below. We were wondering which part could be wrong. All parameters are default, and we tried the non-paired option which works perfectly, but the results cannot be used for further analysis like STAR.\nThank you so much for your time!\n\nimage1548\u00d7562 187 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/328121bbbd4a13e0aae27dea211b60d048c0651b.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe discussed this over email but maybe this summary helps :slight_smile:\n\nThe reference needs to be input to the tool (required). It was empty last time I checked, and the stderr in your screenshot has that same error message. Another error I reviewed included a reference but it was in the wrong format.\n\n@link http://UseGalaxy.org does not have any references for this tool natively indexed at this time.\n\nFor a successful run, you\u2019ll need to provide a properly formatted reference yourself from the history, or consider using the tool at @link http://UseGalaxy.eu (has some indexes).\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! If a workflow fails on a certain step, after fixing whatever issue causing it, what\u2019s the best way to resume that step and let the workflow continue? I don\u2019t mean to rerun that step (like using the rerun button on the UI), that will trigger a new invocation. We want to continue the original workflow invocation by restarting from the failed step and continue to the end of workflow, and have the result still associated with the original invocation. Thanks!"
    },
    {
      "role": "system",
      "text": "When you go to re-run the step, it will give you an option to replace the dataset in the workflow and resume all dependent steps."
    }
  ],
  [
    {
      "role": "user",
      "text": "Perhaps this is an easy one, but I haven\u2019t yet found the answer. Can anyone tell me if there is a Galaxy tool that will filter a fasta (specifically a protein fasta) to yield only the longest isoform per gene?\nBTW, I understand that the longest isoform may not be the best, but this is a way of getting a multi-fasta with one entry per gene.\nI have proteomes in fasta format downloaded from Uniprot and also NCBI. Something about the formatting of these fasta files is not letting me use the perl (@link https://www.biostars.org/p/224965/), python (@link https://github.com/hahnlab/CAFE5/blob/master/docs/tutorial/longest_iso.py), and R (@link https://bioinformatics.stackexchange.com/questions/595/how-can-longest-isoforms-per-gene-be-extracted-from-a-fasta-file) suggestions I have found on various help forums. Some of them returned a file with almost nothing in it. Part of this difficulty is likely owing to my naivete for troubleshooting code.\nA solution on Galaxy would be good, and would also be invaluable to others who need this as part of their workflows.\nThanks for any tips!"
    },
    {
      "role": "system",
      "text": "Hi @user\nit is an interesting question. I am not aware about a dedicated tool on Galaxy for this purpose. Files from NCBI and UniProt might use different rules for sequence names, hence, require different processing.\nYou can do it in Galaxy. You need a tabular file with columns for gene name, transcript name, sequence length and protein sequence. The order of columns is irrelevant, and the file can have other columns. usegalaxy.* servers usually have tools for all steps needed, like getting sequence length, FASTA to tabular conversion, splitting text into smaller string.  The processing depends on sequence names. Once you get the tabular file, use Datamash:\ngroup by gene name\nPrint all fields from input file: set to Yes (default No)\nOperation to perform: maximum on column with sequence length\nThis operation selects a longest sequence per gene and print the entire line from the input tabular file.\nTabular file with longest sequences can be converted into FASTQ using Tabular-to-FASTA  converter.\nDevelop a workflow using a small subset of data, check the output, and use it with fill sized dataset.\nHere an example Galaxy | Australia | Accessible History | protein length (@link https://usegalaxy.org.au/u/igor/h/protein-length)\nHope this answers the question.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there any issue with DEseq2 ? I have been doing some RNA seq analysis for DE genes in treated samples with two replicate each for two conditions but it seems to take long time\u2026is it usual? Its my 1st time with the tool\u2026"
    },
    {
      "role": "system",
      "text": "Hi @user\nHas the job been queued for a long time (gray dataset)? That may be possible. The server is very busy. The best strategy is almost always to allow queued jobs to stay queued. If you delete and restart them, they just move back to the end of the queue again, extending the wait.\nNo job should actually be executing (yellow/peach dataset) for over 3 days \u2013 that would exceed the maximum \u201cwalltime\u201d (runtime) on the clusters at Galaxy Main @link https://usegalaxy.org.\nThe runtime for executing jobs varies not only by the number of reads, but the reference genome and parameters used (among other factors). If a job is already executing, allow it to complete unless you already know something is wrong with it (problematic inputs, settings, etc). Certain types of input problems can cause jobs to run out to their maximum memory or runtime usage \u2013 and that can be quick or long (few days, once not queued anymore) \u2013 then they eventually fail (red error result).\nHelp for using DE tools, including links to Tutorials, is here:\n\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\n\nGeneral troubleshooting, including info about how to interpret jobs that fail for resources (memory or walltime):\n\nMy job ended with an error. What can I do? (@link https://galaxyproject.org/support/tool-error/)\n\n@quote\nThanks Already ! Wish you all Good Health in the worst of times !\nThank you! You too.\nLet us know if you need more help. Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am using GATK4 mutect2 for variant calling. I need baserecalibrator for the preprocessing process, but I can\u2019t find it in Galaxy tools and Tool Shed.\nCan you tell me how to use baserecalibrator or a tool that can replace it?\nVariant Calling\nBase (Quality Score) Recalibration\nGATK4\nThanks."
    },
    {
      "role": "system",
      "text": "Hi @user,\ncurrently BaseRecalibrator is not available in the public Galaxy servers. I recommend you use Freebayes (@link https://usegalaxy.eu/root?tool_id=freebayes) for performing the variant calling.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy Community,\nI am trying to educate myself on this NGS thing. Have some questions regarding my data, paired-end RNA seq. So I have mouse tissue, from Knockout and Control animal, 4 biological replicates for each. They were prepared using TruSeq Kits Single Index. I read on the Illumina page (TruSeq Single Indexes (@link https://support-docs.illumina.com/SHARE/AdapterSeq/Content/SHARE/AdapterSeq/TruSeq/SingleIndexes.htm)) that for trimming, I only need these following sequence:\nRead 1\nAGATCGGAAGAGCACACGTCTGAACTCCAGTCA\nRead 2\nAGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\nQuestions:\n\nShould I use those 2 sequences for all my data sets?\nOr do I need only to trim the index sequence, like 6 bp? I know the index sequence for each sample. But it seems inconvenience if I do trimming for all my datasets at once\u2026\nThe aim of my sequencing is to know the differential gene expression from knockout vs. control as well as to identify splice variants. Is trimming still necessary? Because I read in some discussions for that aim, and if I align it with RNA STAR, trimming is not necessary anymore.\n\nI highly appreciate your  help :hugs:\nCheers!"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nThey were prepared using TruSeq Kits Single Index.\nTry using either Trimmomatic or Cutadapt for trimming. Both contain this adaptor already indexed.\n@quote\nIs trimming still necessary?\nThis tutorial has more help about common QA steps. Some of these should be run even if you decide that the reads do not need trimming.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nAnalyses of sequences\n\n\n\n\n\n@quote\nBut it seems inconvenience if I do trimming for all my datasets at once\u2026\nYou can run QA on all of your datasets in batch if using dataset collections and workflows. At a minimum, consider using collections as these help to keep data organized. Workflows are for faster and reproducible processing that you can later layer in (or adapt one of our workflows \u2013 most tutorials have at least one).\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\n\nGalaxy Training: Using Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n@quote\nThe aim of my sequencing is to know the differential gene expression from knockout vs. control as well as to identify splice variants.\nPlease see our tutorials for example workflows that do exactly this. Some cover QA steps, and explain why for each. You should at least run a tool like FastQC to make sure the read data is intact and fully uploaded.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nThere is much Q&A at this forum about the tools you will be using. Search with keywords to find these: tool names, datatypes, error messages. I also added a few more tags to this post."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am running local Galaxy on my dedicated server with version 20.09. I got this problem when I try to run IReport tool with my dataset.\n\" \u2018base64\u2019 is not a text encoding; use codecs.encode() to handle arbitrary codecs\"\nThe weird thing is that I can run IReport in Galaxy from another my local server but with lower version (19.06). And here is the picture from \u201cView Error Logs\u201d sections.\n\nCapture1591\u00d7418 90.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/561329f59542e70af97399abc6b9bd16331b7527.jpeg)\nThanks for your attention to this question.\nI will be glad to have your responses.\nThank you!\nSincerely,\nPham Quang Huy."
    },
    {
      "role": "system",
      "text": "Resolved, see: Error in ireport - base64 is not a text encoding -- Solution: See built in workflow invocation reports instead (@link https://help.galaxyproject.org/t/error-in-ireport-base64-is-not-a-text-encoding/8667)"
    }
  ],
  [
    {
      "role": "user",
      "text": "We have installed a custom galaxy instance and we would like to analyze 20 samples. However, each of these samples is spread over four lanes. For subsequent analysis we would like to merge them first.\nI am aware of several similar questions on this topic and they all point to the Text Manipulation: Concatenate datasets tail-to-head tool. But to do this for a large amount of samples is quite tedious. Is there a better way?\nI am quite proficient in the commandline, but I would really like it if the wetlab people could do this themselves."
    },
    {
      "role": "system",
      "text": "Hi @user,\nan approach would be to build a dataset list by using the Operation on Multiple Datasets option.\nScreenshot from 2021-04-15 10-45-38\nThen, you can use the Collapse collections into a single dataset in order to combine all of them into a single file dataset.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\n@link http://usegalaxy.org is super slow today and I was wondering if there was a problem. My \u201cbamcoverage\u201d and \u201ccomputematrix\u201d do not progress since yesterday.\nBest"
    },
    {
      "role": "system",
      "text": "Update 11/09/2022\nThe small backlog of queued jobs due to the prior issue has now been cleared. Jobs will queue and execute as usual.\nAny jobs that failed should be rerun at least once, starting now, to eliminate technical issues. Persistent failures can be reported in new topics. Remember to include enough information so the community can help: tool/version and error messages at a minimum, with a shared history link ideal.\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n\n\nUpdate 11/8\nThere was a small cluster issue and we are working to correct it. Leaving jobs queued is still recommended for the fastest processing. Our admins are tracking this, and updates as needed will continue to post back to this topic.\n\nHi @user\nThe server is up and processing work but is very busy @link https://status.galaxyproject.org/\nFAQ @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\nIf we have any updates, we\u2019ll post that back here. Meanwhile, please leave your queued jobs queued.\nThanks for reporting the issue!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have the raw R1 and R2 files of a bacterial genome sequence that have been assembled into contigs using spades. How can I calculate the genome coverage / depth? Thanks."
    },
    {
      "role": "system",
      "text": "Dear @user,\nYou calculate the typical genome coverage with paired-end sequencing with the equation:\nC = (N \u00b7 L \u00b7 2) / G\nN = number of reads\nL = read length\nG = genome size\n2 = factor for the paired-end sequencing\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I realized LEfSe provides different results from identical data sets in which only feature names have been modified. Compare the outputs below: the number of significant feature differ by 1 and the LDA scores changed for some features:\n\nGalaxy117-C)_Plot_LEfSe_Results_on_data_1161050\u00d7847 73.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a383fe6613237f0fe474a233b40afd3eb5593c92.png) \nGalaxy120-C)_Plot_LEfSe_Results_on_data_1191050\u00d7817 65.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b5a2c1bc8b75f5332c87fc45ce39cfd730f4b636.png)\nAnyone faced the same issue?"
    },
    {
      "role": "system",
      "text": "Hi @system @system @user\nThe answer is cross-posted here: the way microorganisms are named affect the final results of lefse (@link https://help.galaxyproject.org/t/the-way-microorganisms-are-named-affect-the-final-results-of-lefse/8337)\nIn short, Lefse is picky about identifier formats. Clean those up and these technical problems will likely resolve."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone\nAs for reference genome, when I load hg38, it sends me mm10. Is that a problem?\n\nFor hicBuildmatrix, how to set \"remove self ligation\",when I set this parameters, I get error \"hicBuildMatrix: error: argument --removeSelfLigation: expected one argument\"\n\nAnd how to configure homer in Galaxy?\n\nThank you"
    },
    {
      "role": "system",
      "text": "Hi,\nThanks for reporting this error. I noticed we have a small bug here, the value is always set to true at the moment, so it does not make a difference if you actively set it or not, it is always active. To bypass this error, just uncheck it and it should be fine. Only in the case you want to keep self ligations, you need to wait for the upcoming 3.7 release of HiCExplorer.\nReference genome: That\u2019s an issue for the Galaxy admin of your server. If you use @link http://usegalaxy.eu, please inform me and I will forward it to our admins.\nHomer: If Galaxy wrappers are existing, that\u2019s also an issue for the galaxy admins of your server to install it. If no wrappers are existing, maybe you can ask the developers of Homer to write any.\nBest,\nJoachim"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nif there is a low MAPQ in my reads . someone tells me to Use  samtools view in.bam \"chr1:234-567\"  to explore the reads in the region of the gene.\nhe said : Your read counter might be refusing to count reads that do not align uniquely. this tool will let you count reads in the region, even if your read counter won\u2019t assign them to a gene.\nbut i dont know how do it in galaxy ? im be grateful if someone can help me .\nthis is our conversation link : @link https://www.biostars.org/p/381880/"
    },
    {
      "role": "system",
      "text": "Hi,\ntwo things:\n\nto answer the question you are asking here. If you already know how to inspect your reads with IGV (as you\u2019re mentioning in your linked post on biostars), there isn\u2019t anything samtools view could tell you in addition. If you hover over any read of interest in IGV (depending on your setting you may also have to click on it), you should get a pop-up info window that also has the info about the read\u2019s MAPQ score.\nsamtools view is not directly available through Galaxy as a dedicated tool. To get the functionality you want you could combine the tools Slice Bam (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/samtools_slice_bam/samtools_slice_bam/2.0.1) and BAM-to_SAM (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.1), where the first one allows you to extract the reads mapping to a particular region, but outputs them in binary format, while the second tool would turn that output into the same text format that you\u2019d get with samtools view as suggested to you.\nThere are lots of reasons why specific reads may not be counted by featureCounts and some of them can be controlled through the Options for paired-end reads and the Advanced options of the Galaxy tool. In particular, check these settings:\n\n\n\nExclude chimeric fragments: if set to True, then paired-end reads will only be counted if both mates map to the same chromosome\n\n\nCount multi-mapping reads/fragments: if disabled, then any reads mapping ambiguously to more than one location will not get counted\n\n\nMinimum mapping quality per read: if the aligner gave the mapping of the read a MAPQ score less than the threshold you set here, the read will not get counted.\nIn addition, the last two settings above can be somewhat confusing when you align with STAR because STAR uses a very simplified MAPQ scoring scheme, in which it assigns MAPQ 255 to every uniquely mapping read. Reads that map ambiguously to 2 locations get MAPQ 3, those with 3 or 4 mappings get MAPQ 1, and those with 5+ mappings get assigned MAPQ 0. This means that if you use STAR for alignment and go with the default Minimum mapping quality per read of 12 in featureCounts, you will, effectively, ignore all multimapping reads even if you enable Count multi-mapping reads/fragments.\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Martin,\nThank you so much for your reply!  We have since referenced the info you provided and updated our Galaxy to the 19.01 release.  It is weird that following the upgrade, while the\nfont/color theme of our Galaxy UI has been changed to the same of your public main Galaxy server, the Tool Pane layout/organization remains unchanged.  One of the possible reasons might be that we did not use the Ansible for the tool installation and update.\nSpeaking of the Ansible, we do have a very important question for you:\nWe realize that the Galaxy team is now recommending Ansible for setting up, managing and upgrading Galaxy.  Our question\nis if this Ansible approach is now Galaxy\u2019s team\u2019s official strategy going forward?   In late 2018, we tried the Galaxy Kickstart, and ran into several issues (ftp, postgresql, etc.).   We are unsure if the current Ansible approach has resolved those\nissue.\nWe would appreciate your comments!\nThanks again,\nUSC Libraries Bioinformatics Service"
    },
    {
      "role": "system",
      "text": "@quote\nis if this Ansible approach is now Galaxy\u2019s team\u2019s official strategy going forward?\nYes.\n@quote\nWe are unsure if the current Ansible approach has resolved those issue.\nI do not know what issue you ran into, but be assured that @link http://usegalaxy.org, @link http://usegalaxy.eu, @link http://usegalaxy.org.au, and many other Galaxies are being deployed using Ansible."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI am trying to use FastTree to generate a tree to be visualized on Phandango.\nI have data of 4 E. coli isolates sequences. Three of them are on our own collection (natural strains) and the fourth was a reference genome from NCBI of E. coli K12 MG1665 (This strain was also part of the experiments, thus I wanted to include it on the tree)\nOriginally I had my own annotations with Prokka (from Linux) on the three strains. With the Gff3 files, I ran Roary followed by FastTree (nucleotide) on these three isolates and it worked (used core aligment and newick file from Roary)\nAs I wanted to expand the analysis and include more sequences (for a better understanding of the core genome). I started simple, adding the 4th sequence (MG1665). Running Prokka in Linux for this sequence was no longer an option for me (changed institutions), so I decided to rerun prokka on the four isolates, including MG1665.\nThen I ran Roary on those 4 strains and worked.\nWhen running FastTree (nucleotide) on them, I got the following error: \u201cThis job was terminated because it used more memory than it was allocated\u201d.\nSince they are only 4 isolates, I wondered if this was a memory problem (I run on the galaxy guest, not a cloud server).\nMaybe relevant\u2026 or not:\n\nThe three first strains were annotated from scaffolds, while the MG1665 was annotated from the chromosome sequence.\nThe Newick file from Roary for the 3 strains and 4 strains looks like this (P.S. I shortened the names of the three sequences on the last one)\n\n3 strains: (38.27_PROKKA_09102020.gff:1.843208323,39.62_PROKKA_09102020.gff:0.806511634,9.54_PROKKA_09082020.gff:0.000000005);\n4 strains:\n(39.62.gff:0.417292218,9.54.gff:0.138018002,(38.27.gff:0.253699967,MG1655.gff:0.218665394)1.000:5.990038221);\nFrom the Newick files, I see a big change, yet I don\u2019t know if it is caused by real differences in the sample or by the processing. Trying different possibilities is tricky, as Roary takes around 12h to complete every combination of strains that I am using.\nP.S. I am quite new to WGS analysis, so I apologize if I have done something stupid.\nThank you in advance."
    },
    {
      "role": "user",
      "text": "Hi Cristobal,\nThank you for your message.\nI was first working in @link https://usegalaxy.org/ and I have tried again in @link https://usegalaxy.eu/\nIn the Europe based web FastTree worked, with new analysis and also with the files provided by @link https://usegalaxy.org/.\nThanks for your time and for the service galaxy provide.\nBest, Rebeca."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am very new to galaxy and with no knowledge in Bioinformatics. I\u2019m trying to analyze some scRNA seq data from Cryptococcus neoformans.\nI\u2019ve been trying (unsuccessfully) to use the GTF2GeneList tool to generate the filtered FASTA and transcript gene-map using a GTF file and FASTA file downloaded from the NCBI (@link https://www.ncbi.nlm.nih.gov/assembly/GCF_000149245.1/), for later running Alevin\nEvery time I run the tool the same error appears: Fatal error: Exit code 1 ().\nI\u2019d appreciate if you could tell me what\u2019s that meaning and if you have any suggestions.\nThanks a lot for the help.\nBest wishes,\nDiana T."
    },
    {
      "role": "system",
      "text": "Welcome, @user !\nMaybe it won\u2019t work because you\u2019re using NCBI version:\n\nWhat it does\nGiven an Ensembl GTF file, it will extract all information on chromosomes, coordinates, and attributes provided at the specified feature level. Mitochondrial features can also be flagged.\n\nYou can download from Ensembl Fungi here (@link https://fungi.ensembl.org/info/data/ftp/index.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Admin,\nI would like to add gene information (gene name, strand, transcript IDs, uniprot IDs etc) to the DESEQ2 output file generated by Galaxy (EU).\nPlease check the attached slide picture.\nIs there a way to create it using tools in galaxy or using R ?\n\nGALAXY QUESTION.png960x720 344 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/7fbc53258fcffea866b67823599a355abd5096ca.png)\n.\nThanks in advance.\nJames PC"
    },
    {
      "role": "system",
      "text": "Dear James,\nYou can use @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/deg_annotate/deg_annotate/1.0 for this  purpose. At the moment, it can add gene position, gene symbol and biotype from a GTF file but not uniprot, pathways and GO terms."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI can\u2019t upload my fastq.gz files for rna seq analysis since two days. Altogether, I have 12 files of which only the first two or three make the upload successfullyy, the rest fails.\nCould you please help me?\nBest and thanks in advance,\nB."
    },
    {
      "role": "system",
      "text": "We had an issue for a few days with a misconfigured upload node.\nWe fixed it and now all the upload tasks are going well.\nSorry for the inconvenience."
    }
  ],
  [
    {
      "role": "user",
      "text": "Screenshot 2020-10-13 at 16.36.291686\u00d7264 28.7 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9bf6d1757b12c3da7b45a62b82d59d89e153f6fa.png)\nWhy is this happening??? :frowning:"
    },
    {
      "role": "user",
      "text": "Ahh yes, this morning it is playing ball! Thank you!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Any ideas as to why when I try to click the paired end (as collection) option from the drop down menu in the trimmomatic tool my paired end data collection doesnt appear as an option and only single ended is an option? Thanks in advance"
    },
    {
      "role": "system",
      "text": "Expand a few of your initially uploaded datasets and examine the content.\nSome things to check for:\n\nIs the \u201cdatatype\u201d for the fastq data set as fastqsanger or fastqsanger.gz? Or something else?\n\n\nGalaxy will autodetect fastqsanger for fastq data that is originally uncompressed and will uncompress then assign that datatype to fastq that is originally compressed.\nTo preserve compression, the datatype fastqsanger.gz can be assigned. But you should double check the data really is in that format (fastqsanger) AND is actually compressed.\n\n\nNow click on the \u201ceye\u201d to view the content:\n\n\nDo you see only forward reads?\nOr only reverse reads?\nOr are both forward and reverse reads combined (interleaved/interlaced) into the same single file?\n\nThat review should help to determine the data you have now, and how to get it formatted in the correct way so Galaxy can use it.\nThese FAQs cover the details: @link https://galaxyproject.org/support\n\nHow to format fastq data for tools that require .fastqsanger format? (@link https://galaxyproject.org/support/fastqsanger/)\nUnderstanding compressed fastq data (fastq.gz) (@link https://galaxyproject.org/support/compressed-fastq/)\nReformatting fastq data loaded with NCBI SRA (@link https://galaxyproject.org/support/ncbi-sra-fastq/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\n\nIf you get stuck, we can troubleshoot from here. Please describe the current assigned datatype and the data content itself. Screenshots can sometimes help. If more info is needed after that, we can keep following up until it is right."
    }
  ],
  [
    {
      "role": "user",
      "text": "How can I incorporate an annotated bacterial genome for RNA sequencing analysis using galaxy?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for explaining :slight_smile:\nUse a custom genome fasta and optionally promote that to a custom build to create a \u201cdatabase\u201d metadata that be assigned to datasets. Some tools use the fasta directly and some tools require/use the \u201cdatabase\u201d metadata. Tip: avoid assigning an existing \u201cdatabase\u201d to any data that is associated with a custom genome fasta \u2013 instead create your own custom database key to avoid content conflicts that can be difficult to interpret.\nThe general flow is usually:\n\nUpload the genome fasta to Galaxy\nUpload the genome annotation to Galaxy (GTF is usually best)\nStandardize the format of both\n\nmake sure the \u201cchromosome\u201d IDs are an exact match between the fasta and GTF\nremove any extra content from the fasta > title line\nremove any # header lines from the GTF (and GFF3 would have at least one)\n\n\nCreate the custom build \u201cdatabase\u201d for the standardized fasta\nDone and ready to start an analysis project\n\nThese FAQs explain the \u201chow to\u201d\n\nreference-genomes (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#reference%20genomes)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#adding-a-custom-database-build-dbkey\nworking-with-fasta-datasets (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fasta-datasets)\nworking-with-reference-annotation (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation)\n\nYou can also search this forum with keywords to find prior Q&A about troubleshooting problems. I also added a few tags that do the same. You\u2019ll notice that most of those involve format problems and link back to those FAQs or variations of them. Getting both of these reference files prepped and ready to use, at the very start and before starting the actual analysis, will make life happier :slight_smile:\n@quote\nHowever, I am having difficulties with the featurecount tool downstream.\nWe usually only index reference genomes and not reference annotation since the latter changes so frequently. The problem is likely with the current format of your fasta or GTF or both. The help above usually solves those kinds of errors. The custom genome functions works great for bacterial sized (or smaller) genomes, and sometimes is better than a native index. Why? It is easier to compare the IDs when both datasets are in a history (and not behind an index) and it is something you can do immediately without waiting for the public sites to be updated.\nSo, please give that a try, and if the errors persist, you can post back a shared history link (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history) that contains both reference datasets and the error data (inputs and outputs undeleted) and we can try to help more. That can be public as a reply, or ask for a private message chat to be started up and you can share the history URL in that.\nNote: Larger genomes (mammalian, some plants) do benefit from server-side indexes \u2013 and we would need a link to the data source for those. The associated annotation data would still be uploaded/standardized independently in the working history by you. Any annotation data that can be indexed on the server is probably already available (rare, and only for a few tools).\n\n@quote\nYes, I am using bwa tool and the reference genome that I need to use is not included. I used the fasta format and the auto option for the algorithm for constructing the BWT index (as one of the options suggested). However, I am having difficulties with the featurecount tool downstream.\nThe tool choice could also be a problem \u2013 but not necessarily since you are working with a bacterial genome.\nGeneral usage:\n\n\nBWA is for mapping unspliced reads (WGS, ChIP-seq, others)\n\nHISAT2 and Bowtie2 are for mapping spliced reads (RNA-Seq)\n\nFeaturecounts is for counting up the abundance of transcriptome features from RNA-seq experiments (mostly).\n\nFor examples, including workflows, please see our tutorials here\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to install the mothur suite on my local Galaxy (newest version), but I\u2019m running into troubles. When I attempt to install it, my Galaxy stops working. Then I tried to install a single components through the repository, but it has troubles to install the mothur (1.39) dependency. In the terminal I\u2019m getting this message:\nPackageNotFoundError: Packages missing in current channels:\n\nmothur\n\nWe have searched for the packages in the following channels:\nPlease help me, I do not want to be mothurless any longer."
    },
    {
      "role": "user",
      "text": "If someone has the same issue: I was trying to install it on my mac, while mothur is not available for macOS in Bioconda"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hellow, I see the MimodD read ailgnment tool in Galaxy in this example step 1: @link https://mimodd.readthedocs.io/en/latest/tutorial_example1.html#ex1-step0 . Now i can not find it in same way.\nThank you,\nYang"
    },
    {
      "role": "system",
      "text": "Dear Yang,\nthe linked tutorial assumes a full local install of MiModD. Public severs like @link http://usegalaxy.org and @link http://usegalaxy.eu often choose not to offer MiModD\u2019s own read alignment tool and a couple of others as they would only be redundant with existing alternatives on these well-equipped servers.\nThe recommendation is to use bowtie2 for read alignment before using MiModD tools again for variant calling and linkage analysis.\nMake sure you read @link https://sourceforge.net/p/mimodd/wiki/MiModD%20on%20public%20Galaxy%20servers/ for usage hints of bowtie2 and SnpEff (for variant annotation) in conjunction with MiModD.\nGood luck with your analyses,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to use text reformatting using awk (version 1.1.2) to keep aligned DNA fragments of a certain size range. I want to keep all fragments between size 1 - 120 bp. I\u2019ve used BAM to SAM on aligned files and then text reformatting using awk with the following instruction:\n\u2018 $9 <= 120 && $9 >= 1 || $9 >= -120 && $9 <= -1 \u2018\nHowever, I\u2019m only getting fragments between 100-120bp retained.\nI know I have fragments smaller than 100 bp as I see them in the SAM file and in other size distribution analyses. I tried removing the second portion of the code (negatives) and still get the same result. Am I missing something? Any help would be appreciated.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe consider using Sample, Slice or Filter BAM on flags, fields, and tags using Sambamba with Filter expression template_length > -120 and template_length < 120. No need for BAM to SAM conversion.\nAs for the awk: maybe try something like $9 > -120 && $9 < 120 or ($9 > -120) && ($9 < 120).\nI am somewhat curious how you got mapped fragments of size 1.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m writing a tool to concatenate output csv files from a custom Rmarkdown galaxy analysis tool and am having some issues.  I borrowed the concatenation code from the \u201cconcatenate_multiple_datasets\u201d tool, which works just fine to concatenate files.  The problem is, I really just want to append the concatenated dataset to a header line, but whenever I try to do this it looks like Galaxy decides that we\u2019re just going to concatenate file names to the header.\nHere\u2019s the code, where \u201csome stuff\u201d will eventually be a megalist of comma separated column IDs:\n    <command><![CDATA[\n  echo \"some stuff\" > $out_file1\n  cat\n  #for $file in $input\n      ${file}\n  #end for\n  >> $out_file1\n\n    ]]>\n</command>\n\nAnd here\u2019s the output:\nsome stuff cat /mnt/d/git_repos/Galaxy/galaxy/database/objects/c/f/d/dataset_cfda7470-0c6e-4ff1-8e63-1fd8a9bdffd0.dat /mnt/d/git_repos/Galaxy/galaxy/database/objects/a/3/9/dataset_a397895a-57b5-4963-828f-58414280225c.dat\n\nHere\u2019s what Galaxy says it\u2019s doing on the command line, which looks right to me?:\necho \u201csome stuff\u201d > /mnt/d/git_repos/Galaxy/galaxy/database/objects/4/e/6/dataset_4e657268-daa7-4a2a-9c37-c04d7fd4f221.dat\ncat /mnt/d/git_repos/Galaxy/galaxy/database/objects/c/f/d/dataset_cfda7470-0c6e-4ff1-8e63-1fd8a9bdffd0.dat /mnt/d/git_repos/Galaxy/galaxy/database/objects/a/3/9/dataset_a397895a-57b5-4963-828f-58414280225c.dat >> /mnt/d/git_repos/Galaxy/galaxy/database/objects/4/e/6/dataset_4e657268-daa7-4a2a-9c37-c04d7fd4f221.dat\nIf I leave out the echo \"some stuff\" > $out_file1  line everything just concatenates as normal.  I\u2019m a beginner at Cheetah and python so may have just missed something obvious (at least, I hope this is obvious).  Any advice would be appreciated!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyour immediate issue is a missing & between your shell commands.\nBeyond that I agree with @system. If you write this just to understand how Galaxy tool development works that\u2019s perfectly fine, but otherwise don\u2019t reinvent the wheel :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nfirst of all, I would like to thank the people involved in running and supporting @link http://UseGalaxy.eu, the range and choices of tools available are really great and the platform usually runs very smoothly.\nI don\u2019t know how much work it represents, but I was wondering whether it would be possible to update the transcript quantifier \u201cSalmon\u201d to a more recent version (currently 0.14.0 versus 0.11.2 on @link http://usegalaxy.eu).\nI am referring to this tool:\n\n\n\nGitHub (@link https://github.com/COMBINE-lab/salmon)\n\n\n\n@link https://github.com/COMBINE-lab/salmon\n\ud83d\udc1f \ud83c\udf63 \ud83c\udf71 Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment - COMBINE-lab/salmon\n\n\n\n\n\n\nIt has received a number of fixes/changes/improvements since 0.11.2:\n\n\n\nGitHub (@link https://github.com/COMBINE-lab/salmon/releases)\n\n\n\n@link https://github.com/COMBINE-lab/salmon/releases\n\ud83d\udc1f \ud83c\udf63 \ud83c\udf71 Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment - COMBINE-lab/salmon\n\n\n\n\n\nThanks.\nlabnv40"
    },
    {
      "role": "system",
      "text": "@user after our Galaxy Community Conference we will look at this. Its an important tool!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, wanted to ask for some advice regarding RNAseq data analysis using Salmon.\nI was going to use this tool to align my RNAseq data to human reference genome and then obtain the read counts for downstream DESeq2 analysis. I obtained human reference genome data from ensembl (cDNA) in fa.gz format. However, I see that for the alignment, I need to have alignment files in .bam format. I also seem to need GTF files to map transcripts to genes?\nI am a little bit at a loss on how to proceed with the alignment. At the moment I have trimmed RNAseq data in fastsanger.gz format and that cDNA reference from ensembl, but I understand some intermediary step is needed?\nWould appreciate any advice on how to proceed - most Salmon tutorials I\u2019ve looked at were for count quantification or written for advanced users of bioconda etc., and the threads on this forum cover issues dowstream of the initial alignment steps. So at the moment I\u2019m struggling :frowning:\nWould appreciate your help!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nSalmon works on a reference transcriptome. You don\u2019t need alignments: Salmon takes FASTQ files. Because of alternative splicing, you need a file with transcript to genes map. I had less issues with a tabular transcript-to-genes map file. Here is my old history with Salmon: Galaxy | Australia | Published History | Transcript quantification with Salmon 2018/06/08 (@link https://usegalaxy.org.au/u/igor/h/transcript-quantification-with-salmon-20180608)\nYou may also consider a traditional approach: mapping with HiSAT2 and read counting with featureCounts. GTN provides several detailed tutorials for this approach.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am relativerly new to Galaxy, and i am sorry if this is a very basic question. My main question is what is the intended way to keep permanent record of what has been done to what files in complicated analysis.\nI have run analysis and now is the time to delete the bulky files and make space for another analysis. Is the intended method for keeping record of tools and parameters just the \u201chistory\u201d? My history is relatively messy, because I tried several options of a tool before settling on final analysis. My history also contains intermediate steps of large files i had to delete for space reasons. I thought I could extract workflow from history and cleanup the abandoned branches and keep the workflow as record of the analysis. However, the intermediate deleted files/steps are not part of extracted workflow. Is it possible to extract workflow including deleted intermediate files/steps?"
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease review this FAQ for help about managing data, freeing up quota space, and saving prior analysis work: Account quotas (@link https://galaxyproject.org/support/account-quotas/)\nFor the extracted workflow problems, it sounds like you will need to directly edit the workflow.\nWorkflows are a record of the tools/actions used to create data. Workflows do not contain data themselves. If you deleted intermediate datasets, the tools/actions that created them might not have been extracted, or possibly may just have disconnected tools/actions within the workflow.\nTips:\n\nWorkflows can be created \u201cfrom scratch\u201d or extracted. Sometimes a bit of tuning in the workflow editor is needed when doing either until it runs as expected.\nYou can reconnect tools or add back any tools/actions that are missing from the workflow by editing it.\nIntermediate datasets can be deleted while executing a workflow to save space. This is a per-tool \u201caction\u201d. Tool actions listed in the far right panel of the workflow editor when a tool is selected from the canvas.\nIf you add or adjust workflow steps, you may need to disconnect/reconnect all the \u201cnoodles\u201d between tools again to reset the workflow\u2019s metadata. Reconnect starting from the inputs down through the analysis in the order of execution.\nOnce this workflow is working as expected, try running it again with the original inputs all in one new clean history. That way you can create a \u201chistory archive\u201d and download it, along with the associated workflow, for a complete record of your work. All of your datasets (that were not deleted while running the workflow!) will be inside the archive and can be used any way you want. You can even import that history archive + workflow into your own local Galaxy, to view the data in context, but I wouldn\u2019t try running the workflow locally unless you set up a more complicated local Galaxy, or decide to use a cloud Galaxy.\nIf you are an academic researcher, and need a bit more temporary quota space, most public Galaxy servers will grant that. Each handles requests a bit differently. Let us know if you cannot find where/how to request temporary quota space at the server you are working at (post back the server URL).\nPlease see FAQ above with more details about most of this, plus the tutorials below for how to extract/edit workflows.\n\nGTN tutorials that cover creating or editing a workflow are here. Scroll down past the analysis tutorials, the ones you will be most interested in are listed lower down with the \u201cworkflow\u201d search term.\nhttps://training.galaxyproject.org/training-material/search?query=workflow\n\n\n\n@link https://training.galaxyproject.org/training-material/search?query=workflow\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/search?query=workflow)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was wondering if there is a way to add the \u201cfrogs\u201d tool into galaxy public server please ? Without creating an admin interface.\n@link https://toolshed.g2.bx.psu.edu/repository?repository_id=11a27cab761b5c8c&changeset_revision=834843ebe569"
    },
    {
      "role": "system",
      "text": "Hello @user\nThis tool suite is not available for commercial use, so hosting on the public Galaxy servers is not possible at this time. FROGS: Trainings (@link http://frogs.toulouse.inrae.fr/html/termsofservice.html)\nThe tool authors do keep the wrappers updated and have simplified installation instructions. A Docker Galaxy is probably the quickest way to get a server up and running as much is pre-configured eg utility tools and data."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nWe are attempting to install galaxy following the \u201cGalaxy Installation with Ansible\u201d tutorial (Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)).  Everything is working fine through the galaxy installation.  However, when we try to complete the systemd setup and execute \u201cansible-playbook -vvv galaxy.yml\u201d, we get the following error:\n\n<galaxy-dev.classe.cornell.edu> EXEC /bin/sh -c \u2018rm -f -r /home/galaxy/.ansible/tmp/ansible-tmp-1630271534.663397-13888-233149462200077/ > /dev/null 2>&1 && sleep 0\u2019\nThe full traceback is:\nTraceback (most recent call last):\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/template/init.py\u201d, line 1100, in do_template\nres = j2_concat(rf)\nFile \u201c\u201d, line 24, in root\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/jinja2/runtime.py\u201d, line 903, in _fail_with_undefined_error\nraise self._undefined_exception(self._undefined_message)\njinja2.exceptions.UndefinedError: \u2018__galaxy_user_group\u2019 is undefined\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/plugins/action/template.py\u201d, line 146, in run\nresultant = templar.do_template(template_data, preserve_trailing_newlines=True, escape_backslashes=False)\nFile \u201c/mnt/galaxy/ansible/venv/lib/python3.6/site-packages/ansible/template/init.py\u201d, line 1137, in do_template\nraise AnsibleUndefinedVariable(e)\nansible.errors.AnsibleUndefinedVariable: \u2018__galaxy_user_group\u2019 is undefined\nfatal: [galaxy-dev.classe.cornell.edu]: FAILED! => {\n\u201cchanged\u201d: false,\n\u201cmsg\u201d: \u201cAnsibleUndefinedVariable: \u2018__galaxy_user_group\u2019 is undefined\u201d\n}\n\nAny help would be greatly appreciate.\nMany thanks,\nDevin"
    },
    {
      "role": "user",
      "text": "Thank you, that does appear to be the issue as we do have galaxy_manage_paths disabled.  A workaround was to set __galaxy_user_group in group_vars/galaxyservers.yml."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using a standalone instance of Galaxy.  I have downloaded a data set successfully and it is in my history.  I have another user locally, who is logged in successfully and who shows up in the admin panel as a user.  However, when (as user 1, who is an admin) I click on Edit Attributes and then go to the data set\u2019s Permissions tab, this other user does not appear in any of the drop down boxes when I try to assign permissions to them.  The only user that I see is myself, for both the \u201cmanage permissions\u201d and \u201caccess\u201d fields.\nIs there some sort of magical configuration ritual that I must perform, in order to make this dataset visible to other users?"
    },
    {
      "role": "user",
      "text": "ok wait\nthat panel is talking about roles not users.  That\u2019s counterintuitive\u2026 anyway, I guess I figured it out, you have to create roles first and then assign them, rather than just assigning permissions to user emails directly.  sigh."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have browsed for similar questions, found them, but did not get a working solution from it. Hence a repost, sorry.\nI have troubles logging into FTP to upload files.\n\nI can login to the website of @link http://usegalaxy.eu normally, using either username + password or email + password\nusing Filezilla, login fails using the following: host = @link http://usegalaxy.org, username = email, password = password, port = [empty], quickconnect\noutput:\n\nStatus:       Resolving address of @link http://usegalaxy.org\nStatus:       Connecting to 129.114.60.60:21\u2026\nStatus:       Connection established, waiting for welcome message\u2026\nStatus:       Initializing TLS\u2026\nStatus:       Verifying certificate\u2026\nStatus:       TLS connection established.\nCommand: USER [bleep]\nResponse:  331 Password required for [bleep]\nCommand: PASS *\nResponse:  530 Login incorrect.\nError:         Critical error: Could not connect to server\n\nsimilarly using lftp:\n\nlftp -u [bleep] @link http://usegalaxy.org\nput [file]\nput: [file]: Login failed: 530 Login incorrect.\nCan somebody explain why I can login to the website, but not the FTP? I am not over my data quota. Not sure how to proceed\u2026\nMany thanks for any help you can offer!"
    },
    {
      "role": "system",
      "text": "@user\nGlad you got connected! The FTP directory will be empty until you transfer data. Try with a small file as a test and you\u2019ll figure out how it works. Then once you understand, go ahead and move your larger data :slight_smile:\nMore about FTP transfer to Upload data:\n\nUpload files to the target FTP location following the instructions here: @link https://galaxyproject.org/ftp-upload/\n\nFTP locations are account and server-specific. It is an intermediate data staging area. The FAQ above has the FTP URL for Galaxy Main (@link http://usegalaxy.org) included as an example. Other servers will usually note the FTP URL in the Upload tool (if FTP is enabled on that server).\nGalaxy EU @link https://usegalaxy.eu has server-specific help here: @link https://galaxyproject.eu/ftp/\n\nThe files stay there for 3 days (max) at usegalaxy.* servers. Other public servers may have different criteria \u2013 the home page may state how their site is configured, or you can try contacting them.\nMove the files from your FTP location to a history with the Upload tool within those 3 days. Data will load as a dataset, the same as with other Upload methods.\nOnce an FTP\u2019d file is added to a history as a dataset, or if you don\u2019t move it within 3 days, it will no longer be in that temporary FTP staging location.\nOnly files uploaded with FTP will ever be in your FTP location.\nFiles may start to show up in the FTP location listing before the transfer is completed! So be sure to not move data into a history before FTP is completed successfully, or you\u2019ll need to start over.\nYour FTP location is associated with your account. The same account as used originally to FTP transfer the data.\n\nContact information for all known public Galaxy servers is usually on the server\u2019s home page and/or in their directory listing here: @link https://galaxyproject.org/use/\nGeneral Galaxy questions and most usegalaxy* questions (that do not require privacy) can be asked at this forum. And if you cannot find contact information for other public Galaxy server(s) for some reason (for private or server-specific help), we can try to help locate it, based on the server\u2019s URL (you\u2019ll need to post that info with your question).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to develop a tool on my local galaxy.  Here are the commands and inputs:\n<command>\n    <![CDATA[\n        Rscript '${__tool_directory__}/correlationMatrix.R'\n            -f $expressionData\n            -m $method\n            -c $correlationMatrix              \n\n]]>\n\n<inputs>         \n    <param name=\"expressionData\" type=\"data\" format=\"tabular\" />\n\n    <param name=\"method\" type=\"select\" label=\"select one method\"/>\n            <option value=\"pearson\" selected=\"true\">Pearson(use default) </option>\n            <option value=\"spearman\">Spearman rank </option>\n            <option value=\"kendall\">kendall rank </option>\n     </param>\n\n\n</inputs> \n\nBut now galaxy show en error message: \u201cCurrent on-disk tool is not valid\u201d\nand no method can be selected.\nWhat\u2019s wrong?"
    },
    {
      "role": "system",
      "text": "Have a look at @link https://planemo.readthedocs.io/en/latest/readme.html#quick-start"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nWhat is the normalization method used in Deseq2 in @link http://galaxy.eu exactly ?\nI mean RPKM or TPM ?\nCould some one please explain .\\\nbest"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe normalization method used by Deseq2 is called DESeq2\u2019s median of ratios . This method relies on the calculation of size factors (s), whose objective is to render counts from different samples, which may have been sequenced to different depths, comparable. Herein lies the importance of normalization performed by Deseq2. Deseq2 normalized values are the result of dividing the raw counts by sample\u2019s normalization factor (size factor, s).\nYou can find an extended explanation here  (@link https://hbctraining.github.io/DGE_workshop/lessons/02_DGE_count_normalization.html#2-create-deseq2-object).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI am really new to this and I am trying to RNA Star some trimmed sequences (cutadapt) against an imported genome from NCBI. I imported using \u201cNCBI Accession Download\u201d.\nI have been following the \u201cHands-on transcriptomics tutorial\u201d however, when I multiQC the RNA Star log, the webpage output is over a page of this: \"MultiQC Report N4IgzgLghgTg+lANgSwOYDsC2BTdE4AOiA9hCAFwAEoRpcEAngdhZSAEaxyoxQEAWIADRswUTEWxhWAbRkgAwgAUADACYALAE4A7GrgBGEAF1jIkABMo0MNgjSqc0OnEsqIAKrpkARwCu2IgMlJh8zBbCbFbQsgYGGho6ABxaAKwAdCpmbADGxCQwrCAAxBoAzDrs7EYAviLOrkUAsmHYFpQQxCF+iBDIkpQkOciRltZQsmVqZRoqmdkgeQVFxTo57KnYOSB11CAuOM2t7Z0dxF2h6MFDI+bRE44zZfPmS8SF7sUAZjpQZanbXYNQ7uLyhAjhKidLpgfjvMh3caxFRJJIqDJZV75d4raoojSA+r7RqgrDHKikfjYQqImKObRopI6F65bEfNirL4qbkqHamLHoL5oVigZARdyQLhINBYXD4WgIth9CCINxsADKABUAIIAJSo2pQGBweEo6ryMCkowYiCg7BWlF12CgFmkrwIYAA7sgIDl+HA8n48GA4Lb2IEigA5PyYcMwSjEL6O52unY1GpAA=== {\u201cdecimalPoint_format\u201d: \u201c.\u201d, \u201cnum_datasets_plot_limit\u201d: 50, \u201csample_names_rename\u201d: , \u201cshow_hide_mode\u201d: , \u201cshow_hide_patterns\u201d: , \u201cshow_hide_regex\u201d: \"\nRather than the graphical output that is expected?\nAs I say, I am really new to this so any help would be greatly appreciated.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf this was happening at @link http://usegalaxy.org with MultiQC aggregate results from bioinformatics analyses into a single report (Galaxy Version 1.11+galaxy0), please try to view the HTML result again now. You won\u2019t need to rerun or do anything else special. I just adjusted the rendering (was missed during the prior version update).\nIf you were working at some other public Galaxy server, two choices: 1) post the server URL back here and we can try to help \u2013 or 2) contact that server\u2019s administrators directly and ask. They\u2019ll probably need the full tool name plus version (found at the top of a tool from).\nWhat to do if working at your own server (must be an admin): HTML is not rendered by default for security reasons (and data in that format cannot be Uploaded for similar reasons). But, HTML can be created by certain tools. To have that output rendered correctly, move the tool/version from the \u201cHTML Sanitized\u201d to the \u201cHTML Rendered\u201d list under Admin > Tool Management > Manage Allowlist.\nThanks for reporting the problem, and hope that helps! :rocket:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI got paired-end .fastq files from Genewiz Ampicon EZ service.\nThen I try to de-multiplex the files by the barcodes (in my primers) by the Barcode Splitter tool.\nHowever, that tool returned my .fastqsanger files instead of .fastq format.\nBecause I want to use CRISPResso2 (CRISPResso2 (@link https://crispresso.pinellolab.partners.org/submission)) to analyze my result and CRISPRresso only recognize .fastq format. Does anybody have experience converting .fastqsanger to .fastq?\nI have tried Fastq Groomer tool and Fastq-sanger to Fastq Sequence Converter (@link http://sequenceconversion.bugaco.com/converter/biology/sequences/fastq-sanger_to_fastq.php)\nBut they dont seem to help, the resulting files still cannot be recognized by CRISPResso."
    },
    {
      "role": "system",
      "text": "Yes, your sequences are in fastqsanger format. You should be able to use them with CRISPesso2, since as I mentioned you previously, the test datasets provided by CRISPesso2 developers (e.g.nhej.r1.fastq.gz)  are fastqsanger too, despite having the fastq file extension.\nScreenshot from 2021-04-25 18-24-51\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good evening, I had started a Trimmomatic job on a paired end dataset at about 1:10 UTC. It\u2019s been nearly 3 hours now with the jobs stuck in queue. Are you currently experiencing a lot of traffic?\nAnother question I had was how many jobs I could run concurrently? I was going to queue FastQC as well, but I\u2019m unsure if queuing too many jobs might lead to an error. I apologize if the questions were a little basic, I\u2019ve just started using Galaxy. Thanks for your time!"
    },
    {
      "role": "system",
      "text": "Hi @user\nWere the jobs queued (gray in color) when you wrote in? That means they were staged and will process as resources become available. After 4 days, those likely completed. All public Galaxy servers are very busy, not just @link http://usegalaxy.org/.\nAll compute-intensive jobs are queuing longer than usual due to the high load on the servers and clusters. Jobs queuing for several days would not be unusual, and even then only a subset of jobs may run at a time. Avoid deleting and rerunning \u2013 that will only place the jobs back at the end of the queue again, further extending wait time.\nUsing workflows is a good way to ensure work proceeds at the fastest possible rate. The entire analysis is queued up and will run as resources become available, without intervention.\nYou can also queue up jobs directly. Having many queued jobs is fine. The work will generally execute in the order submitted \u2013 but some tools may route to a different cluster than others and run out of order (unless dependent on some upstream job to run). So yes, there are some limits on the number of concurrent jobs anyone will have \u201cexecuting\u201d (actually running then completing, not just queued) but what tools those are can also be a factor.\nIf you have urgent work, setting up your own Galaxy server is an option, too.\nI added a few tags to your post that will link to other Q&A about all of this, including FAQs and resource links. But if you have a specific question that doesn\u2019t seem to be addressed yet or anything is unclear, we can try to help more/clarify directly here.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI want to use the Inspect AnnData tool to generate a matrix from my data.\nThe annotation file that I give to this tool as input is about 26 GB and every time the output of this tool turns red.\nIs it because of the large amount of data? And if so, is there a way for me to get a matrix from my annotation data?\nThank you in advance for your advice"
    },
    {
      "role": "system",
      "text": "Hi @user\nYou started to ask a question in this thread Problem using Import Anndata tool - #2 by jennaj (@link https://help.galaxyproject.org/t/problem-using-import-anndata-tool/8568/2) but there wasn\u2019t any followup. I\u2019ll close that thread and we can continue here. We can\u2019t help without more details. Many things can contribute to job failures.\nIf you think the data might be corrupted, or too large to process (unlikely if you are working at @link http://UseGalaxy.eu), these are a few items to try:\n\n\nCheck to make sure that this very large file is intact. The Inspect AnnData tool can be used to collect general information as a sanity check.\n\n\nTry converting just parts of the file to a matrix, or other summaries, instead of all of it. The errors (if any) from this might inform more about what might be wrong.\n\n\nAfter running more checks, if you have an error that you don\u2019t understand, you can post that back with information from your checks so people can help you to troubleshoot. This can include job parameters, error messages, and (optionally) a shared history.\n\n\nMore help is in the single cell tutorials here, including more QA/QC checks:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m trying to run the goseq tool on my data.\nHowever, to do this the files need to be converted to genes with true/false for differential expression. The help manuel says to do this using the compute an expression on every row tool. However I do not find this in the left row with tools.\nHow can I access this tool?"
    },
    {
      "role": "system",
      "text": "Resolved: the tool is found in the Text Manipulation tool group at most public Galaxy servers."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am working on rna seq analysis and am trying to add a T/F significant differential gene expression column onto my DeSeq2 results for use in goseq. Compute is failing when it gets to rows with NA listed as the p-value because it\u2019s not converting the strings to floats, but even when I play with the error-handling options it is failing no matter what. I am using c7<0.05 as my function where c7 is the adj. p-value. I have already tried to Skip the Row & Produce an empty column to deal with the errors. TIA"
    },
    {
      "role": "system",
      "text": "@user glad to hear that the GTN was helpful.\n@system just to clarify how the different error handling options are interconnected:\n\nthe most important option with conversion errors like this is Autodetect column types.\nConversion to the types Galaxy has recorded for all columns is otherwise the first thing the tool does, and this doesn\u2019t count yet as trying to compute the expression. So if this step fails, that\u2019s that.\nafter you\u2019ve disabled the above option, you will have to use an extended expression that does the type conversion itself, like float(c7).\nNow, in this case here, that attempt will also fail, but now the failure is part of computing the expression, and can be handled via If an expression cannot be computed for a row\n\n\nFail on references to non-existent columns does not matter in the case here because the column does exist, just its value cannot be converted to a float.\nYou\u2019re right, of course, that if a missing column is the issue, then you need to set this option to No and configure error handling at the same time.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I can\u2019t access @link http://usegalaxy.org page on safari as well as google chrome. It had been working perfectly fine yesterday. Can someone please help me out with the issue?"
    },
    {
      "role": "system",
      "text": "Hi @user @system\nThe database crashed for a few hours last night at @link http://UseGalaxy.org. All is up now. Please try to connect again if you haven\u2019t already.\nApologies for the confusion!\nStatuspage: @link https://galaxyproject.statuspage.io/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am following a Galaxy tutorial \u2192 Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#introduction) \u2013 and have run into an error. While using RNA STAR, the option to \u201cSelect reference genome\u201d drop down arrow is not functioning properly. Can you please advise. Attached is an image illustrating the issue.\nGalaxy_Assignment_Annotated2558\u00d71038 477 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/3207700dcc2afef76b6415e1ca29ba2be5ea4a5f.png)\nThanks,\nZ"
    },
    {
      "role": "system",
      "text": "Update:\nThe most current version of the tool has more reference genomes added at @link http://UseGalaxy.org: RNA STAR Gapped-read mapper for RNA-seq data (Galaxy Version 2.7.5b)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to reset my @link https://usegalaxy.org password but the reset link I am sent  never leads to a page that would allow to change password but always lead to the login page Galaxy (@link https://usegalaxy.org/login/start?redirect=None)\nAny ideas what else I can try?\nthanks a lot"
    },
    {
      "role": "system",
      "text": "Thanks for reporting this, I confirmed this as a bug and we are going to track it in password reset link does not work on usegalaxy.org \u00b7 Issue #15451 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/15451) and fix it asap."
    }
  ],
  [
    {
      "role": "user",
      "text": "Currently using this tutorial (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/mirna-target-finder/tutorial.html#feedback) as I am working with Arabidopsis Thaliana. I would assume the uploaded files of mature_miRNA , star_miRNA , and miRNA_stem-loop are all specific to A.Thaliana so no change is needed there. I ran my uploaded original sequencing up to the mapping and quantification using the tutorial\u2019s reference datasets. However, how could I proceed with differential expression when I don\u2019t have a control sequence to use?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI would assume the uploaded files of mature_miRNA , star_miRNA , and miRNA_stem-loop are all specific to A.Thaliana so no change is needed there.\nYes, the reference data is in full as far as I know.\n@quote\nI ran my uploaded original sequencing up to the mapping and quantification using the tutorial\u2019s reference datasets. However, how could I proceed with differential expression when I don\u2019t have a control sequence to use?\nThe DE analysis could compare the conditions you did generate. Two or more conditions, each with two or more replicates, are the minimum technical requirements. Three replicates per condition is a common minimum scientific requirement."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to upload my own data, but I get the following error:\n\u201cWarning: Waiting for server to resume\u2026\u201d\nIs this normal? How long should I wait? If I want to cancel this upload how do I do it?"
    },
    {
      "role": "system",
      "text": "Hi @user and @system,\nif you are using @link http://usegalaxy.org, it is probably due to heavy load on the server. I recommend you to try a different one, such as @link http://usegalaxy.eu or @link http://usegalaxy.org.au.\nRegards\n@quote\n@system&@systemThe Galaxy Mainhttps://usegalaxy.orgpublic server is very, very busy. Some patience will be needed for all operations. \nThere is also a warning banner at the top of that server specifying certain tools that are running with reduced memory or are temporarily disabled (server resource-related). That banner will update and/or go away as status changes. \nFor those working at other public Galaxy servers, also experiencing delays, it is expected that most are also very busy. But \u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy Team!\nWe try to use RNAStar for analyzing our paired-end sequencing data. After building a list of dataset pairs, the platform returns the following message to us when we try to load RNAStar:\nLoading tool toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.8a+galaxy0 failed: Bad Gateway (502)\nThis does not happen when we try to load the tool in the original list of unpaired .fastq files. Can you provide any help to solve this problem?\nBest regards,\nMax"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis problem seems to be related to the storage issue. We hope it will be fixed this week. Sorry for the inconvenience.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am concatentating some RNA-seq datasets and have been noticing the stated gb size is off. This only seems to be occurring with forward strands, as my reverse strand concatenated file is the appropriate size. It was saying 21gb when it should have been 24gb (I double checked this on @link http://Galaxy.org) I made no errors when entering the data. I ran it a second time and while it is green it is saying 0 bytes for the size, and there does not appear to be any data.\nI am wondering if maybe the tool is experiencing a bug?"
    },
    {
      "role": "system",
      "text": "Hi @user\na few comments; I recommend you to use this tool for concatenating the FASTQ files Concatenate (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/0.1.1). It is the UNIX version, and is supposed to be faster; the failure can be due to a memory issue. Regarding the different datasets files, the size provided by Galaxy is an estimation. I suggest you to count the lines of both files (the one in @link http://usegalaxy.org and in @link http://usegalaxy.eu) by using the Count lines (@link https://usegalaxy.eu/root?tool_id=wc_gnu) tool.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have tried the gubbins analysis for 314 genomes but after >5 days, no results are available.\nThe initial fastq files are from ion torrent, and then I ran the Snippy, the Snippy-core, and the Snippy-clean_full_aln, all of them gave green results. But with Gubbins, error.\nI tried with a subset and the error persists.\nDoes anyone know how to proceed?"
    },
    {
      "role": "system",
      "text": "Hi @user and @system,\nwe have increased the number of cores up to 6 and the tool has been updated to version 3.2.1 (@link https://github.com/galaxyproject/tools-iuc/pull/4745). I hope it will solve the problems. Please, try to run Gubbins again from Monday.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "As you can see in the image, the Kmer distribution field is empty: \"No options available\"\nWhat am I missing?\nIs there another tool I must use first to generate a Kmer distribution?\n \n\n (@link https://i.stack.imgur.com/u7RpL.png)\n"
    },
    {
      "role": "system",
      "text": "I installed the DBs, please check tomorrow again. Thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am leaning from the \u2018\u2018Exome sequencing data analysis\u2019\u2019 tutorial in order the develop the skill to analyze my genomics data against the reference genome for identifying variants on the specific genes of my samples.\nI followed the tutorial to use the \u2018\u2018SnpSift Annotate\u2019\u2019 to give variant ID to the vcf file of the \u2018\u2018father\u2019\u2019.\nHowever, the dbSNP vcf file (dbSNP_138.hg19.vcf) offered in the tutorial does not contain ID information.\nSo I could run SnpSift Annotate, but the output vcf still does not have ID.\nI also downloaded the dbSNP vcf file from the NCBI database. I could not run the SnpSift Annotate of the \u2018\u2018father vcf\u2019\u2019 against the NCBI dbSNP vcf, I guess it is because the chromosomes in the NCBI dbSNP vcf file are not named as \u2018\u2018chr#\u2019\u2019 but only \u2018\u2019#\u2019\u2019.\nDo I understand the error correctly? Any suggestions how I can solve this?\nMany thanks.\nSusan"
    },
    {
      "role": "system",
      "text": "Hi Susan,\nso you\u2019re trying a tutorial that\u2019s currently broken in several ways, and I\u2019m feeling somewhat guilty because I was planning to fix it last week but didn\u2019t get round to it.\nHowever, by diagnosing correctly the first problem, and even trying to work around it, you\u2019re demonstrating that you have a sound understanding of the topic.\nIndeed, the dbsnp file used in the tutorial is not what it\u2019s supposed to be. I would have thought that the different chromosome names between your vcf and the NCBI file could be handled by Snpsift, but it\u2019s possible that they pose a problem. One other possibility I could think of is that the genome versions differ. Did you download the dbsnp hg19 or the hg38 version?\nAnyway, you don\u2019t really need to run this step at all when using GEMINI next because that tool knows about the dbsnp ids anyway. However, if you\u2019re working on @link http://usegalaxy.org, you will soon find out that GEMINI is not currently available there. You can find it on @link http://usegalaxy.eu, but at a newer and heavily reworked version than what\u2019s described in the tutorial.\nSo overall this experience will not be as comfortable as it should be, and I\u2019m sorry for that.\nIf this is not super urgent, you may be better off waiting 1 or 2 weeks longer for my fixes.\nBest,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello to everybody!!\nI am running the trinotate workflow and I noticed that the \u201cSignalP\u201d has a new versions available.\nSignalP is in v3 instead of v5.\nHope you can update this tool.\nThanks a lot in advance!"
    },
    {
      "role": "system",
      "text": "Hi Agustin,\nI think you could ask the v3 wrapper developer for upgrading it\n\n\nGitHub (@link https://github.com/peterjc)\n\n\n\npeterjc - Overview (@link https://github.com/peterjc)\npeterjc has 168 repositories available. Follow their code on GitHub.\n\n\n\n\n\n\n\n\n@link https://github.com/peterjc/pico_galaxy/tree/master/tools/protein_analysis\n\n\npico_galaxy/tools/protein_analysis at master \u00b7 peterjc/pico_galaxy (@link https://github.com/peterjc/pico_galaxy/tree/master/tools/protein_analysis)\n@link https://github.com/peterjc/pico_galaxy/tree/master/tools/protein_analysis\nGalaxy tools and wrappers for sequence analysis. Contribute to peterjc/pico_galaxy development by creating an account on GitHub.\n\n\n\n\n\n\nor try yourself making a pull request :slight_smile:\nCiao"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been using LDA on several data sets. For some it seems to be successful and for other very similar data sets (all that has changed is my grouping variable of interest) the analysis does not work beyond the Effect Size calculator. I have pasted the error message below. Please let me know what the next best steps are.\nError Message:\n/galaxy_venv/local/lib/python2.7/site-packages/rpy2/rinterface/init.py:185: RRuntimeWarning: Error in (function (file = \u201c\u201d, n = NULL, text = NULL, prompt = \u201c?\u201d, keep.source = getOption(\u201ckeep.source\u201d),  :\n:1:47: unexpected numeric constant\n1: effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\"1\n^\nwarnings.warn(x, RRuntimeWarning)\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/run_lefse.py\u201d, line 89, in \nif params[\u2018rank_tec\u2019] == \u2018lda\u2019: lda_res,lda_res_th = test_lda_r(cls,feats,class_sl,params[\u2018n_boots\u2019],params[\u2018f_boots\u2019],params[\u2018lda_abs_th\u2019],0.0000000001,params[\u2018nlogs\u2019])\nFile \u201c/export/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/lefse.py\u201d, line 199, in test_lda_r\nrobjects.r(\u2018effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\u2019+p[0]+\u2019\"]) - mean(LD[sub_d[,\u201cclass\u201d]==\"\u2019+p[1]+\u2019\"]))\u2019)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/init.py\u201d, line 358, in call\np = _rparse(text=StrSexpVector((string,)))\nrpy2.rinterface.RRuntimeError: Error in (function (file = \u201c\u201d, n = NULL, text = NULL, prompt = \u201c?\u201d, keep.source = getOption(\u201ckeep.source\u201d),  :\n:1:47: unexpected numeric constant\n1: effect.size <- abs(mean(LD[sub_d[,\u201cclass\u201d]==\"\"1\n^\nThank you for any and all help!"
    },
    {
      "role": "system",
      "text": "Could someone of you provide test data or a link to some public history for reproducing the issue.\nFor my reference, linked issue here: @link https://github.com/galaxyproject/tools-iuc/issues/2816"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI just stood up a new Galaxy server (21.05). I have installed FASTQC 0.73 and allowlisted it but the HTML is still being blocked I think as the output is still blank. When I uncheck it from the allowlist I do see the sanitized html output and the jobs are completing fine.\nI have also tied older versions of fastqc.\nNot seeing anything in logs\u2026\nAnyone have any ideas?\nThanks,\nMorgan"
    },
    {
      "role": "user",
      "text": "Or more likely when using nginx make sure you follow the wiki:\n@link https://docs.galaxyproject.org/en/master/admin/nginx.html#sending-files-with-nginx"
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried 3 different ftp clients (FileZilla, CyberDuck, and CoreFTP) and all of them are unable to open a connection on @link http://usegalaxy.org using my username and password.  I am able to sign in with my username and password on the website, but the server rejects it through one of the ftp clients.  I get these errors in FileZilla:\n\n\n\n\nStatus:\nResolving address of @link http://usegalaxy.org\n\n\n\n\nStatus:\nConnecting to 129.114.60.60:21\u2026\n\n\nStatus:\nConnection attempt failed with ECONNREFUSED - Connection refused by server, trying next address.\n\n\nStatus:\nConnecting to 129.114.60.56:21\u2026\n\n\nStatus:\nConnection attempt failed with ECONNREFUSED - Connection refused by server.\n\n\nError:\nCould not connect to server\n\n\n\nI seems as if my account is not allowed to access the ftp server on galaxy, since I don\u2019t have the \u201cchoose ftp file\u201d button when I click \u201cupload data\u201d.   Any suggestions on how to get this to work so I can upload my 64 GB files?"
    },
    {
      "role": "system",
      "text": "@link http://usegalaxy.org does not support FTP uploads anymore, please see details here: The usegalaxy.org FTP service will be decommissioned on August 12, 2022 (@link https://help.galaxyproject.org/t/the-usegalaxy-org-ftp-service-will-be-decommissioned-on-august-12-2022/8318)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI had the same problem while using the program.\nHave you solved the problem?\nWhere should I inquire about this matter?\nCould you please tell me how to fix it?\nThank you"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nThe file I uploaded is an Excel file.\nThe input data is expected to be in tabular format.\nIf the data is in an Excel file right now, you can output a tabular (plain text) dataset from Excel on your computer, then upload just the tabular dataset to Galaxy."
    }
  ],
  [
    {
      "role": "user",
      "text": "UPDATE: I originally posted before Christmas but my posts now has updates:\n\nreflecting the use of the original Bioblend API vs the OO BioBlend API).\nwith details about manual upload to Galaxy GUI\n\nI am trying to use BioBlend to upload a file (locally stored on my computer). I have tried doing so in 2 ways:\n(1) Using the original Bioblend API\n(2) Using the OO BioBlend API\nIn both cases, I obtain the same error (at least it seems). Here is my code:\nUsing original BioBlend API (bioblend.galaxy)\nimport os\nfrom bioblend.galaxy import GalaxyInstance\nfrom bioblend.galaxy.histories import HistoryClient\nfrom bioblend.galaxy.tools import ToolClient\n\nos.chdir(\"/home/username/Documents/projects/galaxy\")\n\n# Set up GalaxyInstance ####\ngi = GalaxyInstance(\"https://usegalaxy.org\", \"my-instance-number\")\n\n# Make a new history ####\nhistoryClient = HistoryClient(gi)\nhi = historyClient.create_history(\"tmp2-100119\")\n\n# Upload datasets ####\ntoolClient = ToolClient(gi)\ncounts = toolClient.upload_file(\"example.txt\", hi[\"id\"])\ncounts2 = toolClient.upload_file(\"counted.txt\",hi[\"id\"]) # not working\n\nUsing OO BioBlend API (bioblend.galaxy.objects)\nimport os\nfrom bioblend.galaxy.objects import GalaxyInstance\n\n os.chdir(\"/home/username/Documents/projects/galaxy\")\n\n# Set up GalaxyInstance ####\ngi = GalaxyInstance(\"https://usegalaxy.org\", \"my-instance-number\")\n\n# Make a new history ####\nhi = gi.histories.create(\"tmp-100119\")\n\n# Upload datasets ####\ncounts = hi.upload_file(\"example.txt\")\ncounts2 = hi.upload_file(\"counted.txt\", file_type=\"tabular\") # not working\n\nIn both code version, I have the same problem: I can upload a simple file called space-delimited example.txt which looks like this below. On the Galaxy GUI, I can see this file and it says \u201cformat: txt\u201d and \u201cuploaded txt file\u201d.\n1 2 3\nA B C\nD E F\n\nHowever, I cannot upload my counted.txt file. This file is a tab delimited file, which has 51,827 lines. It represents a matrix of counts I want to perform DE analysis on. It looks like so (first few lines):\nGeneID\tS1\tS2\tS3\tS4\tS5\tS6\tS7\tS8\tS9\ngene1    0\t 1\t 0\t 2\t 0\t 1\t 0\t13      34\ngene2\t 4\t 0\t 0\t 3\t 0\t 5\t 0\t 1      13\ngene2\t13\t44\t94\t30      45\t34\t46\t40\t44\ngene3\t 2\t 0\t 0\t 0\t 0\t 0\t 0\t 5       6\n\nThe error I get in my terminal is as follows (the upload_file commands takes a min or so before the error appears). There are slightly different for both codes but they both in a ConnectionError: (\u2018Connection aborted.\u2019, RemoteDisconnected(\u2018Remote end closed connection without response\u2019,)) error.\nTraceback (most recent call last):\n\n  File \"<ipython-input-14-345175af4c4e>\", line 1, in <module>\ncounts2 = hi.upload_file(\"counted.txt\", file_type=\"tabular\") # not working\n\nFile \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/objects/wrappers.py\", line 1000, in upload_file\nout_dict = self.gi.gi.tools.upload_file(path, self.id, **kwargs)\n\nFile \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/tools/__init__.py\", line 162, in upload_file\nreturn self._tool_post(payload, files_attached=True)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/tools/__init__.py\", line 232, in _tool_post\nreturn self._post(payload, files_attached=files_attached)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxy/client.py\", line 152, in _post\nfiles_attached=files_attached)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/bioblend/galaxyclient.py\", line 137, in make_post_request\ntimeout=self.timeout)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/api.py\", line 116, in post\nreturn request('post', url, data=data, json=json, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/api.py\", line 60, in request\nreturn session.request(method=method, url=url, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/sessions.py\", line 533, in request\nresp = self.send(prep, **send_kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/sessions.py\", line 646, in send\nr = adapter.send(request, **kwargs)\n\n File \"/home/username/anaconda3/envs/myenv-3.6/lib/python3.6/site-packages/requests/adapters.py\", line 498, in send\nraise ConnectionError(err, request=request)\n\nConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n\nI have managed to upload this counted.txt file to the Galaxy GUI successfully. It says \u201cformat: tabular\u201d and \u201cuploaded tabular file\u201d. I have even manage to run some analyses on it. This file is 2.1 MB in size.\nI am very lost as to what is causing this error. Any help would be deeply appreciated."
    },
    {
      "role": "system",
      "text": "This has been later reported on @link https://github.com/galaxyproject/bioblend/issues/271 and solved in the BioBlend master branch."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have been use galaxy for the last couple of weeks in order to analyze RNA-seq. Today I logged in to my account and found out my History has been deleted and the datasets are are accessible only through User>datasets. I need those files but cannot figure out how to get them back all at once.\nThanks for the help."
    },
    {
      "role": "system",
      "text": "Hi @user\nIt appears that you have been using two accounts at @link http://UseGalaxy.org. One does not contain any data at all (datasets or histories) and the other contains both. Each account was activated but just the account with data was logged into since account creation/activation (in Jan 2021).\nIt is very important to have/create/use just one account at each public Galaxy server.\nI am going to send you a direct message with more details so you can 1) find your most recent work and 2) we can clear up the multiple account issues together before those become administratively problematic. If something else is going on, we can also resolve that in the private direct message.\nThe two accounts may have been created during a class.\n\nAre you certain that you were logged into your own Galaxy account while working? One account was logged into and activated on 2021-01-15, but was not logged into when any data was created. If you are not logged into your Galaxy account while working, then it is the same as using no registered account. Data is not retained once you close your browser window, and it is lost.\n\nImportant: It isn\u2019t possible to have datasets available (User > Datasets) when there are no active histories to contain them (User > Histories) when logged in .\nIf you have not closed your browser window yet, it may be still possible to log into Galaxy and have the work saved into your own account. Try that first before anything else is done. Then write back. You can write back in the direct message to keep your email/credentials private. This topic is public \u2013 please do not post back your email address or any other private information here. Use the direct message instead. And even in that direct message, do not post back your password. Administrators will never ask for or need it.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am following along with the de-novo transcript analysis tutorial here (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html).  I have been using collections to organize the data as I go (1 collection for Megakaryocyte data and 1 for G1E data).\nThe collections were not a problem until I needed to run featureCounts() on the alignment files from all 4 samples (the 2 MegaK replicates with the 2 G1E replicates).  It does not appear that featureCounts() will allow you to select multiple collections for analysis (ie. it only allows me to choose either the MegaK collection, or the G1E collection, but not both).  I also cannot seem to figure out how to \u2018unpack\u2019 the collections into individual files so that I can run all four files together.\nAny help on either 1) running two collections together or 2) \u2018unpacking\u2019 the collections so that I can submit the individual files together,  would be greatly appreciated."
    },
    {
      "role": "user",
      "text": "I seem to have answered my own question.  I will leave this here incase anyone else has the same question in the future.\nTo run multiple collections together I used the Merge Collections function to merge my two collections into one, then selected the merged collection as the input to featureCounts.\nTo split collections into individual files (or subgroups of files) I used the Filter List (from a text file) command.  I found that I needed to have the exact file names in order for the filtering to work, partial names were not matched.  To separate a collection out into individual files I called Filter List multiple times using a single file name each time.  I found the easiest way to make the text files was by clicking the upload button and then selecting the Paste/Fetch Data button in the popup display and then pasting in the file names I wanted to extract."
    }
  ],
  [
    {
      "role": "user",
      "text": "Trackster won\u2019t let me add multiple datasets, it keeps getting stuck when I click \u2018add new\u2019 in \u2018loading histories\u2019. Help!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nYes, Trackster is a bit problematic right now.  Specifically, add more datasets to an existing visualization does not work (at least for all cases I have tested). The same is true whether the visualization is based on a built-in database or a custom genome build\u2019s database. We are working to fully characterize the problem and get it fixed quickly. A development ticket exists (too cryptic to post, and we are not quite sure if it will fully resolve the issue) \u2013 so I\u2019ll post the end-user oriented issue ticket back once completed, likely early Monday. More testing is needed first. But we wanted to let you know now that we\u2019re aware of the problem, plan to fix it with priority, and appreciate that you took the time to create a public post about the issue. It will help both you and others.\nUntil Trackster issues are resolved, alternative genome browsers are listed in an expanded dataset when the database (genome) is assigned. Common options linked are below. Depending on any particular dataset\u2019s metadata (datatype, database), other visualization options may display as linked resources.\n\n\nUCSC Main \u2013 Only some genomes are available. If a link does not show up, it is not at UCSC.\n\nIGV \u2013 Are a few versions, with links presented as appropriate. The publically hosted version has many genomes available. A local version is easy to download/install, has a GUI interface, and can have even more pre-built indexes added in. IGV also has the option to create your own custom genome index in a local IGV, matching a custom genome build created in Galaxy, to view multiple datasets together with the genome fasta backbone. If you just want to browse a single dataset from a novel genome, a scaffold index will be created on the fly.\n\nMore details for using IGV are in this prior Q&A. You can also click on the tags I added to your post to find more related Q&A in the content of different use cases.@quote\nThe assembly is done, but moving the data into any 3rd party external application can require some reformatting or the creation of additional files that the other application is expecting \u2013 or they can require none \u2013 it depends on the external application. \nIn this case (IGV), a fasta index is required for new genomes. This would be true even if you decided to load the fasta directly into IGV as a new genome (instead of loading it directly from Galaxy). The \u201cGalaxy-to-Local IGV\u201d data web-based l\u2026\nThanks for reporting the issue!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have a matrix and I want the header values of its columns to move to the end of the matrix the size of a column because the header of each column is placed on top of the previous column and the last column is left without a header.\nPlease tell me what tool to use?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis tutorial covers many common data manipulations: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\nThe manipulation will probably go something like this:\n\nIsolate the header line from the matrix\nAdd a new column to the start of that header line\nPut the header back on the matrix\n\nNote: If are you viewing the dataset in Galaxy with the :eye: and noticed this as a potential problem, consider using the Cut tool first, to isolate the problematic column and double check that the header is actually missing. Sometimes the matrix header or the data values are so long (or short!) that they skew the view but the data content itself is Ok."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to run featureCounts for zebrafish, however it returns the error:\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file.\nThe specified gene identifier attribute is \u2018gene_id\u2019\nAn example of attributes included in your GTF annotation is\u2019 ID = id1; Parent = rna0; Dbxref = GeneID: 192301, Genbank: NM_173235.3, ZFIN: ZDB-GENE-020419-25; gbkey = mRNA; gene = rpl24; product = ribosomal protein L24; transcript_id = NM_173235.3 \u2019\nSomething similar occurs when running htseq-count.\nDoes anyone know how to solve?"
    },
    {
      "role": "system",
      "text": "Welcome @user\n@quote\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file.The specified gene identifier attribute is \u2018gene_id\u2019An example of attributes included in your GTF annotation is\u2019 ID = id1; Parent = rna0; Dbxref = GeneID: 192301, Genbank: NM_173235.3, ZFIN: ZDB-GENE-020419-25; gbkey = mRNA; gene = rpl24; product = ribosomal protein L24; transcript_id = NM_173235.3 \u2019\nYour annotation appears to be in GFF3 format, not GTF.  There is no gene_id attribute in the 9th column of GFF3 data.\nBoth Featurecounts and HTseq-count expect GTF input, as explains on the tool forms and as reported in errors.\nPlease review the FAQs below if you are not sure about the datatype difference. The tags added to your post also point to similar help from prior Q&A.\n\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\n\nThanks!\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having a problem with the \u2018Funannotate predict annotation\u2019 tool:\n\n[May 25 11:25 AM]: OS: Rocky Linux 8.6, 36 cores, ~ 103 GB RAM. Python: 3.8.15\n[May 25 11:25 AM]: Running funannotate v1.8.15\n[May 25 11:25 AM]: ERROR: sordariomycetes_odb10 busco database is not found, install with funannotate setup -b sordariomycetes_odb10\nMaybe the busco databases got clobbered during and update?"
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease try using the prior version of the Funannotate database (not a prior tool version).\nThe tool form\u2019s menu lists the 2022 database first right now because of issues discovered (last week) with the 2023 database. There isn\u2019t an issue ticket to link back, or I can\u2019t find it, but you can ask for updates later on if it still seems to be a problem after a week or so.\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to install Galaxy so that it sends jobs to slurm. I have used the tool ParallelCluster to create my cluster, which has slurm installed in /opt/slurm. I have also downloaded slurm-drmaa, as the docs suggest. I installed it with the following commands: ./configure  && make, sudo make install.\nI downloaded Galaxy by using git clone on my main directory.\nI then changed my Galaxy config files to interface with slurm. My current galaxy/config/job_conf.yml file is set up like this:\n<job_conf>\n<plugins workers=\"10\">\n    <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\">\n        <param id=\"drmaa_library_path\">/usr/local/lib/libdrmaa.so</param>\n    </plugin>\n</plugins>\n<handlers default=\"handlers\">\n    <handler id=\"main\" tags=\"handlers\">\n        <plugin id=\"slurm\"/>\n    </handler>\n</handlers>\n<destinations default=\"slurm_part\">\n    <destination id=\"slurm_part\" runner=\"slurm\">\n</destination>\n    <destination id=\"slurm_part_12\" runner=\"slurm\">\n        <param id=\"nativeSpecification\">--ntasks=12</param>\n    </destination>\n</destinations>\n<tools>\n    <tool id=\"flexbar\" destination=\"slurm_part_12\"/>\n    <tool id=\"tophat2\" destination=\"slurm_part_12\"/>\n    <tool id=\"bowtie2\" destination=\"slurm_part_12\"/>\n    <tool id=\"bwa_wrapper\" destination=\"slurm_part_12\"/>\n</tools>\n</job_conf>\n\nNow, when I run  sh run.sh, I get this error:\ngalaxy.jobs INFO 2019-07-22 20:34:02,874 [p:29606,w:0,m:0] [MainThread] Handler 'main' will load specified runner plugins: slurm\ngalaxy.jobs.runners DEBUG 2019-07-22 20:34:02,874 [p:29606,w:0,m:0] [MainThread] Loading SlurmRunner with params: {'drmaa_library_path': '/usr/local/lib/libdrmaa.so'}\ngalaxy.jobs.runners.state_handler_factory DEBUG 2019-07-22 20:34:02,875 [p:29606,w:0,m:0] [MainThread] Loaded 'failure' state handler from module galaxy.jobs.runners.state_han               dlers.resubmit\ngalaxy.jobs.runners.drmaa INFO 2019-07-22 20:34:02,875 [p:29606,w:0,m:0] [MainThread] Overriding DRMAA_LIBRARY_PATH due to runner plugin parameter: /usr/local/lib/libdrmaa.so\n    \nTraceback (most recent call last):\n      File \"lib/galaxy/webapps/galaxy/buildapp.py\", line 49, in app_factory\n        app = galaxy.app.UniverseApplication(global_conf=global_conf, **kwargs)\n      File \"lib/galaxy/app.py\", line 204, in __init__\n        self.job_manager = manager.JobManager(self)\n      File \"lib/galaxy/jobs/manager.py\", line 26, in __init__\n        self.job_handler = handler.JobHandler(app)\n      File \"lib/galaxy/jobs/handler.py\", line 51, in __init__\n        self.dispatcher = DefaultJobDispatcher(app)\n      File \"lib/galaxy/jobs/handler.py\", line 945, in __init__\n        self.job_runners = self.app.job_config.get_job_runner_plugins(self.app.config.server_name)\n      File \"lib/galaxy/jobs/__init__.py\", line 599, in get_job_runner_plugins\n        rval[id] = runner_class(self.app, runner['workers'], **runner.get('kwds', {}))\n      File \"lib/galaxy/jobs/runners/drmaa.py\", line 68, in __init__\n        (exc.__class__.__name__, str(exc)))\n    ImportError: The Python drmaa package is required to use this feature, please install it or correct the following error:\n    ImportError: No module named drmaa\n\nI followed the instructions on the DRMAA Python github page, and installed the package using pip install drmaa, but I don\u2019t know how to configure it. Can anyone help?"
    },
    {
      "role": "system",
      "text": "I was able to solve this by activating the .venv and just pip installing drmaa in case anyone else runs across this."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m a beginner user here and went through some patchy starts with aligning my RNA-seq reads through RNA STAR. I know you\u2019re not supposed to delete/re-run jobs a bunch of times, but I didn\u2019t realize that until I had done so a few times\u2026 I\u2019ve finally been letting the jobs queue, but rather than the yellow color indicating a running job, my first three datasets seem to have a pink/orange color and I\u2019m not sure what that means. Additionally, they\u2019ve been that way for almost 24 hours, so not sure if that\u2019s normal either.\nHave I just messed things up for myself by deleting and re-running jobs? Any help would be appreciated. Thanks!\nKanishk"
    },
    {
      "role": "user",
      "text": "Thanks! I think was being overly cautious/slightly color blind\u2026 that color was most likely yellow because those jobs have since finished. I think I just need to be patient!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I did chip seq analysis in Galaxy but when  was trying to load Narrow peaks in my IGB desktop it does not work\n\npeaks349\u00d7611 24.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/ddfedcda2d18a50dcc8648889cb720d95697d48a.png)\nCan anyone help me?"
    },
    {
      "role": "system",
      "text": "Hi @user\nI am not familiar with IGB. I tried it today but download of 3 Gb of reference data was too slow at home network.\nI assume IGB has similar functionality to other browsers. Can you check status of the uploaded track? On some browsers, like JBrowse, custom track data is not visible by default, users have \u2018to tick a box\u2019 to see the data. Maybe click on name of the custom (uploaded) track at the top left corner or check Available data - configure section.\nHave you checked a position where the data is available?\nIt seems your track has been connected to the browser, so it might be IGB use/configuration question.\nMaybe try it on other browsers, such as UCSC Main.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I need help with SAM files. I have a SAM file but i dont have idea how to view the result to finds snp, coverage, etc. is there any program to view and interpret easier? thanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin that case, I suggest you have a look at these two trainings:\n\nGenome Annotation with Maker (@link https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/annotation-with-maker/tutorial.html)\nVariant calling in diploid systems (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html)\n\nhowever, I found that the genome of P. lunatus have been annotated already: annotation (@link https://data.jgi.doe.gov/refine-download/phytozome?organism=Plunatus&expanded=Phytozome-563). Perhaps it can be useful for you.\nLet me know if you have any additional questions as you begin the analysis.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI am very new to galaxy and it is my first problem :frowning: I have gz archive with .fastq, but when it unpacking  it still .fastq (not .fastq.gz) And when I upload my file it appears to the right and it is red. Galaxy writes \u201cfile not found\u201d or next error:\n\nInternal Server Error\n\nGalaxy was unable to successfully complete your request\nAn error occurred.\nThis may be an intermittent problem due to load or other unpredictable factors, reloading the page may address the problem.\nI suppose I need only format .fastq.gz but how create it?\nAny help would be appreciated\nThank you"
    },
    {
      "role": "system",
      "text": "Yes, all are understood by Galaxy.\nException: there are rare circumstances when the specific compression program used to generate the gz format is not understood by Galaxy. The solution is to try to load uncompressed reads instead. You can then re-compress the reads in Galaxy or use them uncompressed.\nThese two tutorials cover general information about how fastq read data is handled and labeled in Galaxy. That training site can also be searched or navigated to find analysis domain protocols with more examples of read data plus workflows.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\n\n\nGalaxy Training: NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nAnalyses of sequences\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I built a table with Merge, Paste and Cut, but now the bold header line is just 0, 1, 2 as the column names. How can I change the column names (or earlier, and how to keep them names)?\nThank you in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe bold header line: do you mean text in white font on blue background? This is \u2018metadata\u2019, hence names depend on datatype. If your file tabular, you\u2019ll see 1, 2, 3. If you deal with other tabular formats, for example, BED, you\u2019ll see other values. If you change the datatype, the header might change.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am running VGP workflow on Galaxy but everytime i perform VGP HIFI phased assembly or VGP hybrid scaffolding, i end up with problem in busco \"Fatal error: Exit code 1 () \" or it get stuck with no or corrupted results. Everything else seems workig fine in the workflow\nAny advice how to solve it ?\nRegards"
    },
    {
      "role": "user",
      "text": "Hello @jennj\nVGP workflow was successful when i moved to @link http://usegalaxy.eu, busco went fine.\nThank you"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have a problem with the heatmap2 tool. My problems is in the final data page showing the results. It seems that the labelling is to big, thus resulting in overlapping gene names (see link below).\n@link https://usegalaxy.eu/datasets/11ac94870d0bb33a4397b2e602d9e2ab/display?to_ext=pdf\nis there a possibility to sove this problem?\nThanks in advance.\nCS"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe cannot see your graph from that link but the display options are known to be limited for text scaling with this version of the tool. About 30-32 total labels are the maximum before the text begins to overlap. Sometimes longer names don\u2019t display well either using those default settings.\nThis is a sample matrix dataset that displays well for reference/testing: @link https://raw.githubusercontent.com/galaxyproject/tools-iuc/master/tools/ggplot2/test-data/mtcars.txt\nChoices:\n\nChoose a different wrapped tool \u2013 search the tool panel with the keyword \u201cheatmap\u201d to review and test others out. You could compare your data with the example file above and see how both graph. None of these will shrink the font size but some do display in a nicer font. The versions under \u201cVisualize\u201d will have the same default resolution limits as the tool you are using now \u2013 but you could review those two and decide for yourself which is the best (if any).\nCreate your own custom graph. The EU server hosts both Rstudio and Jupyter interactive environments, where you can move data into and out of an IE from a Galaxy history with custom commands: gx_get() and gx_put(). The GTN training site has example usage, search with the IE name. Searching with the keyword \u201cheatmap\u201d also finds examples (but less than 30 labels \u2013 or no labels, sometimes just a legend): @link https://training.galaxyproject.org/\n\nMaybe reconsider if more than ~30 labels in a tiny font are informative or not? This wasn\u2019t my idea but it is a reasonable comment I read, originally didn\u2019t like (!!), thought about more, and then decided wasn\u2019t as off-putting as it seemed at first. Much depends on how you plan to use the graphic of course. Source (plus includes some very clear/helpful R tips): r - How to scale the size of heat map and row names font size? - Bioinformatics Stack Exchange (@link https://bioinformatics.stackexchange.com/questions/4742/how-to-scale-the-size-of-heat-map-and-row-names-font-size)\n\n\nHope that helps! Others are welcome to add a better reply :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "The error, parameter \u2018sp_name\u2019: an invalid option (\u2018selectkbest__k\u2019) was selected (valid options: memory,Note:,Parameter,steps), occurred in the workflow \u201cAge prediction from RNASeq (from training material)\u201d which was imported from the published workflow repository in Galaxy Europe. The other two hyperparameters, \u201celasticnet__normalize\u201d and \u201celasticnet__alpha\u201d in this step also met similar errors.\n\nerror578\u00d7596 58.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/40f0a5b8a12812e3eae870a880da693b8e9dbc1c.jpeg)\n\npublished workflow1544\u00d7168 23.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/276b6884bf7df215e70fbd995415eb17d020f253.jpeg)\nTo solve this problem, I tried to create a new workflow according to the tutorial in @link https://training.galaxyproject.org/training-material/topics/statistics/tutorials/age-prediction-with-ml/tutorial.html#figure-4 by the latest version of tools. However, it still does not work. This error is weird and I hope for some help very much. Thanks a lot! :pray: :pray:"
    },
    {
      "role": "system",
      "text": "Hi @user,\nCan you try to use the latest tool version (the latest version on Galaxy Europe of search cv, hyperparameter optimization tools is: Galaxy Version 1.0.8.4) in the workflow you are using? Everything else remains the same as explained in the age prediction tutorial. There was an issue with the previous version and we fixed it a couple of weeks ago. Let us know if you face further issues.\nBest,\nAnup"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m currently trying to run a sort on some bed files. The instructions for the sort are:\nsort -k1,1 -k2,2n -k3,3n\n\nhowever, I\u2019m having difficulty translating this to the sort tool in Galaxy. Is the K1, k2, k3 referring to the columns? and if so, what do 1, 2n and 3n refer to? Any help would be appreciated."
    },
    {
      "role": "system",
      "text": "Hi @user\nYou have this right:\n\nsort\n\nUse the Sort tool in Galaxy\n\n-k1,1\n\nMeans to start sorting at the first column and stop sorting this way at the first column (otherwise the sort would apply from whatever the \u201cstart\u201d was until the end of the line). Limiting to the first column is important because the \u201csorting type\u201d to apply differs by column. In this case, the sorting is \u201calphabetical\u201d. But see the tool form \u2013 alphabetical for any letters in the first column (a-b) will be what humans expect but any numbers after may not be. Example: \u201c11\u201d comes before \u201c2\u201d because the first character of \u201c11\u201d (1) is smaller than the first character of \u201c2\u201d (2).\n\n-k2,2n\n\nSort the second column with a \u201cnumerical\u201d sort (\u201cn\u201d) \u2013 that means smallest to largest number. For this case, \u201c2\u201d would be smaller than \u201c11\u201d and listed first.\n\n-k3,3n\n\nSort the third column, same rules as the second column (numerical aka \u201cn\u201d).\n\nIn practical terms, these methods will \u201ccoordinate sort\u201d the first three columns of a bed file. Most bioinformatics tools are designed to interpret chromosome positions this way for bed datasets. The rest of the line isn\u2019t considered, just passed through the tool associated with whatever was originally included per line after the \u201cchrom-start-stop\u201d data.\nTry different sort conditions on your data a few different ways, you\u2019ll notice the difference. The tool form also has a few examples that should help to clear up why the command-line was written that way."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to filter a VCF dataset of depth (DP), however I do not want to filter on region.  If I leave this section blank the tool does not run when submitted.  What do I need to put in the region filter to ignore this parameter please?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nGiven the extra information \u2013 the job is not even submitting, correct? We made a change earlier today to fix a problem that is contributing to the odd behavior. It isn\u2019t on the server yet but expect it in the next few days.\nMeanwhile, this is the workaround:\n\nLoad up the tool form\nSubmit \u2013 this will abort and redirect to the top of the form\nClick into the blue box for \u201cregions\u201d, type anything, then remove it\nSubmit \u2013 this one will actually launch the job\n\nThis gets around the problem. Several tools are impacted, and only at @link http://usegalaxy.org. The problematic \u201cfree text\u201d will always be highlighted in a light blue box, and may be nested in advanced settings. If you run into this again with another tool, and can\u2019t find where the \u201cblue box\u201d is, post back with the full tool name and version and we can help.\nScreenshot:\n\nvcffilter-bug1292\u00d7226 22.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e1191a90b579e2448798fed804274d731f8b8351.png)\nThanks for reporting the bug :beetle:!\nCross-posted in Matrix chat here: You're invited to talk on Matrix (@link https://matrix.to/#/!IEcbSOGXlqPpSmwmPp:gitter.im/%24_wOghSMyQiOmfYdrNSMPOGFKGPwvBA0DWaSzriCCJ-s?via=gitter.im&via=matrix.org&via=hacknw.org)\nand Gitter chat here: galaxyproject/Lobby - Gitter (@link https://gitter.im/galaxyproject/Lobby?at=622a472e9a09ab24f39a7693)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody,\nI\u2019m asking if it is possible to add private conda channel to use in Galaxy. I use conda on the same server and can add a private channel with conda config --add channels file//<path_to_my_channel>.\nResulting in this :\n$ conda info\n     active environment : None\n       user config file : /home/XXX/.condarc\n populated config files : /home/XXX/.condarc\n          conda version : 4.5.12\n    conda-build version : 3.17.6\n         python version : 3.6.6.final.0\n       base environment : /home/XXX/miniconda3  (writable)\n           channel URLs : file://tmp/conCAPSID/linux-64\n                          file://tmp/conCAPSID/noarch\n                          https://conda.anaconda.org/conda-forge/linux-64\n                          https://conda.anaconda.org/conda-forge/noarch\n                          https://repo.anaconda.com/pkgs/main/linux-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/free/linux-64\n                          https://repo.anaconda.com/pkgs/free/noarch\n                          https://repo.anaconda.com/pkgs/r/linux-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n                          https://repo.anaconda.com/pkgs/pro/linux-64\n                          https://repo.anaconda.com/pkgs/pro/noarch\n          package cache : /home/XXX/miniconda3/pkgs\n                          /home/XXX/.conda/pkgs\n       envs directories : /home/XXX/miniconda3/envs\n                          /home/XXX/.conda/envs\n               platform : linux-64\n             user-agent : conda/4.5.12 requests/2.18.4 CPython/3.6.6 Linux/4.4.0-116-generic ubuntu/16.04 glibc/2.23\n                UID:GID : 658240:200198\n             netrc file : None\n           offline mode : False\n\nIs it possible to do the same under galaxy ?\nThanks in advance :slight_smile:"
    },
    {
      "role": "system",
      "text": "You can add custom conda channels for Galaxy to use using the conda_ensure_channels setting (@link https://github.com/galaxyproject/galaxy/blob/1887e9f54a1c1fde6962ae7a47f956337854c71d/config/galaxy.yml.sample#L243)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to run Blastn for some data. I have tried several times in different days with different input files and some different file formats. However, I keep getting the following error message: \u201cRemote job server indicated a problem running or monitoring this job.\u201d Could you please help me figure out how to run Blastn successfully?\nThank you !"
    },
    {
      "role": "user",
      "text": "Blastn works again ! Thank you ! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a bedgraph file that, when uploaded to @link https://usegalaxy.org, shows a \u201cdisplay at UCSC [main]\u201d link. That link is not present on my local galaxy instance.\nWhere, and how do I set that up?\nThank you"
    },
    {
      "role": "system",
      "text": "Make sure you uncommented this line in your config: @link https://github.com/galaxyproject/galaxy/blob/dev/config/galaxy.yml.sample#L643"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am working with collections of 2000-3000 files and trying to run MAFFT and/or dist.seqs on them but the submission keeps failing. I have done that with similar smaller collections (~200 files) and it worked well, but with the larger collections the submission takes forever to process and eventually ends with a \u201cjob submission fail\u201d error. Is there a limit to the number of files allowed for job submission ? If not, is there a way to make this work without having to divide the collections into smaller ones and run them one by one ?\nThanks!\nJulian"
    },
    {
      "role": "system",
      "text": "Can you please try to catch me on @link https://gitter.im/bgruening then we can debug this together.\nThanks,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nDoes DESeq2\u2019s \u2018Output normalized counts table\u2019 mean typical normalization like TPM/RPKM/FPKM? Or is some other form of normalization computed internally in DESeq2?"
    },
    {
      "role": "system",
      "text": "Hi Spring,\nBased on the Rscript, the function being used is counts(), which is part of the DESeq2 package. The function includes a parameter, normalized, which \u201cindicat[es] whether or not to divide the counts by the size factors or normalization factors before returning (normalization factors always preempt size factors)\u201d\nYou can find the description here: counts function - RDocumentation (@link https://www.rdocumentation.org/packages/DESeq2/versions/1.12.3/topics/counts)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to display the HTML output of my MultiQC report in the workflow invocation report. How can I do that? Using the templating in the Markdown\nhistory_dataset_display(output=multiqc_label)\n\nwon\u2019t work, it just outputs the raw html/css."
    },
    {
      "role": "system",
      "text": "Hi @user\nUsing the workflow report (aka Page) editor,\n\nchoose from the left panel Insert Object \u2192 History \u2192 Dataset\n\nThen pick the dataset the pop-up menu. This populates the Markdown with the correct target identifier.\nFor MultiQC, this would be the \u201cWebpage\u201d output (HTML datatype)\n\nThis is a simple test example at @link http://UseGalaxy.org that you could import and review. Galaxy | Accessible Page | Demo GHelp 9115 (@link https://usegalaxy.org/u/jen-galaxyproject/p/how-to-display-multiqc-report-in-workflow-invocation-report-93059a0ee0d0682d)\nIf you are working at your own Galaxy server where you are admin, the tool that created the HTML output might need to be added to Admin \u2192 Tool Management \u2192 Manage Allowlist to render it anywhere in the application, including in Pages. More details are in our admin docs (@link https://docs.galaxyproject.org/en/master/search.html?q=allowlist&check_keywords=yes&area=default).\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "@link https://galaxyproject.org/authnz/cloud/demo/\nIn step 10 python get_object.py is run.  Where is this script located?  I tried to find it in galaxy github repositories as well, but it is not available.  Has the name been changed?\npython get_object.py [API KEY]\nUltimately my goal is for Galaxy to be able to import sequences residing within my S3 bucket(s).\n\nI have installed a local instance of Galaxy within my AWS account.  It is functioning well, with our bioinformatics group running various tools.\nWithin one of my AWS S3 buckets I am storing my sequences and would like Galaxy to import these sequences (> 1000 RNA-Seq files).\nAlthough just a demo, I found the following two documents which appeared to fit my Galaxy-S3 bucket requirements:  @link https://galaxyproject.org/authnz/cloud/aws/ and @link https://galaxyproject.org/authnz/cloud/demo/\n\nI was able to define an AWS role and also generate API keys described, but was not able to run the python script get_object.py as provided per step 10.  The script was not locatable, but this is understandable since the document is simply a demo.\nOf note, I did find an object store config file which seems to have the capability to accept a S3 secret key and access key, but using a role I think is a better solution.\n"
    },
    {
      "role": "system",
      "text": "The demo is running off a pre-baked branch of Galaxy (@link https://github.com/VJalili/galaxy/tree/aws_demo_role), where a number of configurations are already made. The get_object.py script is also available from that branch (@link https://github.com/VJalili/galaxy/blob/aws_demo_role/get_object.py), and it is used to POST a request to the cloud storage API (@link https://docs.galaxyproject.org/en/latest/lib/galaxy.webapps.galaxy.api.html?highlight=cloud#galaxy.webapps.galaxy.api.cloud.CloudController.get) to get a demo dataset stored in a S3 bucket into the demo history.\nTo use this functionality on your own Galaxy instance (not the pre-baked demo branch), you may need to follow the cloud storage documentation (@link https://galaxyproject.org/cloud/storage/); in general:\n\n\nSetup Galaxy to let users login with their Google account (@link https://galaxyproject.org/authnz/config/oidc/idps/google/);\n\n\nLogin with your Google account, and get your authnz_id by sending a GET request to the /authnz/ controller;\n\n\nSetup an AWS IAM Role and define it Galaxy (@link https://galaxyproject.org/authnz/cloud/aws/);\n\n\nSend a POST request to the /api/cloud/storage/get API with a following payload (docs (@link https://galaxyproject.org/cloud/storage/#get-data-from-cloud)):\n{\n    \"history_id\": \"...\",\n    \"authz_id\": \"...\",\n    \"bucket\": \"...\",\n    \"objects\": [\n        \"your S3 object 1\",\n        \"your S3 object 2\",\n        ...\n    ]\n}\n\n\n\nPlease note that this flow might be broken on your branch (due to some recent changes), the issue is known and we\u2019re working on it (@link https://github.com/galaxyproject/galaxy/pull/9801).\n\nI did find an object store config file which seems to have the capability to accept a S3 secret key and access key, but using a role I think is a better solution.\n\nThere are a number of important distinctions between the objectstore-based approach and the cloud storage  API:\n\nObjectStore is an admin-level setting and the buckets you define will be used to store data belonging to all the users of that instance of Galaxy (albeit users will not see your configuration and buckets), while the API-based approach is user-specific, where the configuration of one user is not visible to the other user, and each user can have their own buckets.\nYou can configure ObjectStore to store all the data added to Galaxy (e.g., upload or generated) on S3. However, you cannot configure it to upload data stored on your S3 bucket onto one of your histories. In other words, ObjectStore cannot read your bucket containing RNA-seq data and upload them onto a history. For this we developed the cloud storage API.\n\nThe ObjectStore documentation is available from: @link https://galaxyproject.org/admin/objectstore/"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use edgeR to perform a differential expression analysis. This is a paired analysis (samples are in pairs - their healthy vs their diseased tissues).\nGalaxy Tool ID: toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.20.7.2\nGalaxy Tool Version: 3.20.7.2\nTool Version: R version 3.4.1 (2017-06-30) \u2013 \u201cSingle Candle\u201d, edgeR version 3.20.7, limma version 3.34.9, scales version 0.5.0, rjson version 0.2.15, getopt version 1.20.0\nI have 2 input files. The first is input-counts.txt and looks like so:\nGeneID  M1  M4  M7 M10 M13  N1  N4  N7 N10 N13\ngene1    0   0   0   0   4   0   0   1   0   0\ngene2  589 602 646 403 390 204 357 511 266 387\ngene3    5   5   7   4   8   0   2  13   2   5\ngene5    1   1   0   0   0   0   0   0   0   0\ngene6    0   0   0   0   0   0   0   0   0   0\ngene7    0   0   0   0   0   0   0   0   0   0\netc\n\nMy factor file looks as such:\n   Sample Patient Tissue\n1      M1       1      M\n2      M4       4      M\n3      M7       7      M\n4     M10      10      M\n5     M13      13      M\n6      N1       1      N\n7      N4       4      N\n8      N7       7      N\n9     N10      10      N\n10    N13      13      N\n\nThe aim of my analysis is  to detect genes differentially\nexpressed between affected (M) and normal skin (N), adjusting for any differences between the patients.\nIn the edgeR window in Galaxy, in \u201cContrast\u201d of interest\", I have written: M-N.\nHowever, edgeR doesn\u2019t run and I get the following error:\nFatal error: Exit code 1 () Error in makeContrasts(contrasts = contrastData, levels = design) : The levels must by syntactically valid names in R, see help(make.names). Non-valid names:\n\nI am confused as to what to do. I made this file in R. I have tried changing the levels() in R and I have tried putting \u201cTissue\u201d in the second column but I still get the error each time.\nAny help would be deeply appreciated."
    },
    {
      "role": "user",
      "text": "Seems I have found the problem. Despite Galaxy saying \u201cNOTE: Please only use letters, numbers or underscores\u201d when inputing factors manually, it seems you can\u2019t use numbers for Groups when you use a factor file instead of manual input.\n\nScreenshot from 2019-02-01 14-47-24.png901\u00d7415 25.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/51467ead337fab8a9d13cb98a28af92866c9e9ea.png)\nIndeed, when I changed my file to the following, it worked. I added an X in front of the Patient numbers.\n\n\n\n\nSample\nTissue\nPatient\n\n\n\n\nM1\nM\nX1\n\n\nM4\nM\nX4\n\n\nM7\nM\nX7\n\n\nM10\nM\nX10\n\n\nM13\nM\nX13\n\n\nN1\nN\nX1\n\n\nN4\nN\nX4\n\n\nN7\nN\nX7\n\n\nN10\nN\nX10\n\n\nN13\nN\nX13\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi There,\nCan anybody help me with renaming the output dataset in Nanoplot - I have managed to successfully change the output name for all the other tools in my workflow but i cant seem to get it to work with Nanoplot.\nThanks in advance,\nJesse\nNanoplot"
    },
    {
      "role": "system",
      "text": "That was fixed in Record input datasets and collections at full parameter path by mvdbeek \u00b7 Pull Request #15978 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/pull/15978) and is available on Galaxy release 23.1, which as of today is only live on @link http://usegalaxy.org"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy Community, I\u2019m needing your help.\nWhen running snippy (toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy/4.6.0+galaxy0) using a list of dataset pairs, ie paired reads grouped in a paired collection, the tool fails in some of the samples with the following message in the last line: --tmpdir \u2018/var/lib/condor/execute/dir_3665157\u2019 is not a directory.\nThe weird thing is that when I run it for the second time using the same samples in the same paired collection, some samples fail but they are different to the ones that failed in the previous run. That is no say apparently it has nothing to do with the samples as they fail randomly.\nAdditionally, I tried running the paired fastq files separately one by one, and snippy finishes successfully.\nDo you have any clue about what could be going on?\nThanks!!\nSol"
    },
    {
      "role": "system",
      "text": "Hi @user,\nfound it. It is called \u2018Replace elements in collection?\u2019. Set it to Yes during re-run, and if the job completes, the output replaces the failed output in collection.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to validate Star-fusion for detecting gene fusions in RNASeq data and seem to have some problems in implementing the package through the @link http://usegalaxy.org server.  The results with very clean Tophat2 fusion validated reads were nothing like they were supposed to be.  I posted a query on a Star fusion bulletin board and got a response from  the folks at Broad institute who developed the package.  They looked at what I did and got the correct answers.  Something is different in the program implementation and I would like to know how to do it correctly\nHere is what I did:\nDownloaded the CTAT file GRCh37_v19_CTAT_lib_Feb092018.source_data.tar.gz  at Index of /Trinity/CTAT_RESOURCE_LIB (@link https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/) and undbundled the individual files.\nUploaded to the Galaxy server @link http://usegalaxy.org/\nref_annot.cdna.allvsall.outfmt6.toGenes.sorted\nref_annot.gtf\nref_genome.fa\nUploaded from NCBI by fastq-dump\nSRR6796340    Anaplastic Large T-Cell Lymphoma     Expect NPM1-ALK chr5->chr2\nSRR6796341    Anaplastic Large T-Cell Lymphoma-    technical replicate\nSRR6796358    Diffuse Histiocytic Lymphoma              Expect NPM1-ALK chr5->chr2\nSRR6796374    Large Cell Immunoblastic Lymphoma Expect NPM1-ALK chr5->chr2\nSRR6796384    Bladder Transitional Cell Carcinoma   Expect FGFR3-TACC3 chr4->chr4 high counts\nSRR6796352    Chronic Myeloid Leukemia                 Expect BCR-ABL1 chr9->chr22\nI ran RNA Star first then Star-fusion or Star-fusion in control of the RNA Star aligner program.  The run parameters are below.\n\n\n\n\nRNA STAR\n\n\n\n\n\n\n\n\n\nDataset Information\n\n\n\n\n\n\n\nNumber:\n87\n\n\nName:\nRNA STAR on data 51, data 53, and data 73: chimeric junctions\n\n\nCreated:\nSat Dec 29 19:01:27 2018 (UTC)\n\n\nFilesize:\n4.9 GB\n\n\nDbkey:\n?\n\n\nFormat:\ninterval\n\n\nJob Information\n\n\n\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.6.0b-1\n\n\nGalaxy Tool Version:\n2.6.0b-1\n\n\nTool Version:\n\n\n\nTool Standard Output:\nstdout\n\n\nTool Standard Error:\nstderr\n\n\nTool Exit Code:\n0\n\n\nHistory Content API ID:\nbbd44e69cb8906b5b3d043534aebc71b\n\n\nJob API ID:\nbbd44e69cb8906b5f73bf7040b268a06\n\n\nHistory API ID:\nde36650321118931\n\n\nUUID:\n8fe466e2-9dee-48ca-91b4-b395c6757088\n\n\nTool Parameters\n\n\n\n\n\n\n\nInput Parameter\nValue\n\n\nSingle-end or paired-end reads\npaired\n\n\nRNA-Seq FASTQ/FASTA file, forward reads\n73: SRR6796352 (fastq-dump)\n\n\nRNA-Seq FASTQ/FASTA file, reverse reads\n73: SRR6796352 (fastq-dump)\n\n\nCustom or built-in reference genome\nhistory\n\n\nSelect a reference genome\n53: ref_genome.fa\n\n\nGene model (gff3,gtf) file for splice junctions\n51: ref_annot.gtf\n\n\nLength of the genomic sequence around annotated junctions\n100\n\n\nCount number of reads per gene\nTRUE\n\n\nWould you like to set output parameters (formatting and filtering)?\nno\n\n\nOther parameters (seed, alignment, limits and chimeric alignment)\nstar_fusion\n\n\nJob Resource Parameters\nno\n\n\nInheritance Chain\n\n\n\n\n\n\n\nSTAR-Fusion\n\n\n\n\n\n\n\nDataset Information\n\n\n\n\n\n\n\nNumber:\n91\n\n\nName:\nSTAR-Fusion on data 48, data 51, and others: fusion_candidates.final\n\n\nCreated:\nSat Dec 29 21:47:32 2018 (UTC)\n\n\nFilesize:\n17.9 KB\n\n\nDbkey:\n?\n\n\nFormat:\ntabular\n\n\nJob Information\n\n\n\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/iuc/star_fusion/star_fusion/0.5.4-3\n\n\nGalaxy Tool Version:\n0.5.4-3\n\n\nTool Version:\nsoftware version: STAR-Fusion_v0.5.4\n\n\nTool Standard Output:\nstdout\n\n\nTool Standard Error:\nstderr\n\n\nTool Exit Code:\n0\n\n\nHistory Content API ID:\nbbd44e69cb8906b569580af0b3b0f6c0\n\n\nJob API ID:\nbbd44e69cb8906b539bb05ac571e27ea\n\n\nHistory API ID:\nde36650321118931\n\n\nUUID:\n8426b56c-6591-4871-94eb-1e7f258e39ad\n\n\nTool Parameters\n\n\n\n\n\n\n\nInput Parameter\nValue\n\n\nUse output from earlier STAR run or let STAR Fusion control running STAR\nuse_chimeric\n\n\nChimeric junction file from STAR (with STAR-Fusion settings)\n87: RNA STAR on data 51, data 53, and data 73: chimeric junctions\n\n\nSource for sequence to search\nhistory\n\n\nSelect the reference genome (FASTA file)\n53: ref_genome.fa\n\n\nGene model (gff3,gtf) file for splice junctions and fusion gene detection\n51: ref_annot.gtf\n\n\nResult of BLAST\u00b1blastn of the reference fasta sequence with itself\n48: ref_annot.cdna.allvsall.outfmt6.toGenes.sorted\n\n\nSettings to use\ndefault\n\n\nJob Resource Parameters\nno\n\n\n\nEither way the results were the same.  None of the fusions listed were the correct ones and at most one fusion partner on the correct chromosome distant from the expected break point.  What mystifies is that the people at the Broad Institute got the expected results what one would expect.  When running RNA Star for linking to fusion, the Star fusion instance on Galaxy on the server it generates 4  output files, including one with the chimeric fusion junctions which I input into the Star fusion run.  Is this the correct one to use?  I noticed that the Github Star fusion tutorial instructions indicate downloading a mutation resource and then using something out of this zip folder.  Sounds like the versions of Star-fusions are differently set up.  This might be close to where the error is being generated.\nIf you can figure out what I did wrong in setting this up I would greatly appreciate suggestions on how to fix it."
    },
    {
      "role": "system",
      "text": "Update\nThis was the immediate problem:\n@quote\n|Input Parameter|Value||Single-end or paired-end reads|paired||RNA-Seq FASTQ/FASTA file, forward reads|73: SRR6796352 (fastq-dump)||RNA-Seq FASTQ/FASTA file, reverse reads|73: SRR6796352 (fastq-dump)|\nThe same single dataset was entered for two different inputs. This would fail the tool now in the most current version of Galaxy. My guess is that the previous output @user had was corrupted (or empty or truncated) and that is what lead to scientific problems with downstream tools.\n\n@system If you are having a problem now, it is very unlikely to be exactly the same as the original problem in this question.\nTo troubleshoot your problem:\n\nMake sure that you are using the most current version of both tools.\nVerify your inputs (content + format). You can search at this forum or the FAQs at the GTN (@link https://training.galaxyproject.org/training-material/faqs/galaxy/) with keywords like a datatype (fasta, gtf, fastqsanger) or tool name to review common issues/solutions.\nThis FAQ covers troubleshooting in general: @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n\nReview tutorials\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nIf that doesn\u2019t help to solve your problem, please create a new topic and explain more about what is going wrong in your situation now. Please include enough context so we can help. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having an issue with the FASTQC tool. It appears to run to completion (turns green as if done), but when I expand the box to show view/download options, etc., it says there are 0 bytes of data, and there isn\u2019t anything to view. However, this is only happening for some of the FASTA sequences I\u2019ve tried. For some of them it provides data, but some it does not. Any ideas? Also, when viewing the Job Information for the ones that show 0 bytes of data, the Tool Standard Output and Tool Standard Error features say \u201cEmpty\u201d\u2026 I can open the fasta sequences on my computer and see there are sequences, so I KNOW there is data there, the tool just isn\u2019t actually running?"
    },
    {
      "role": "system",
      "text": "Hello,\nThe @link http://UseGalaxy.org server has some technical issues that started about 30 minutes ago.\nWe are activating tracking this. The scope is not clear yet. An important part of our infrastructure is unstable at the source.\nStatus\n\n@link https://status.galaxyproject.org/\nUnresolved incident: @link http://usegalaxy.org network instability.\nUpdate: * Resolved - The network appears to have stabilized and Galaxy is accessible.*\n\nUpdates will post to this thread and the status link above\nQueued jobs should not be impacted. Failed or odd results will likely need to be rerun but that isn\u2019t possible yet.\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, All.  When attempting to update to v22.01 using ansible, we seem to be running into a version conflict between the versions of cwltool and pyparsing requested in galaxy/server/lib/galaxy/dependencies/pinned-requirements.txt.  The exact error is:\nThe conflict is caused by:\nThe user requested pyparsing==3.0.7\ncwltool 3.1.20211107152837 depends on pyparsing!=3.0.2\ncwltool 3.1.20211107152837 depends on pyparsing<3; python_version <= \"3.6\"\nAny suggestions for resolving this would be greatly appreciated.\nMany thanks,\nDevin"
    },
    {
      "role": "system",
      "text": "Hi Devin,\nwhat Python version are you using? You need at least Python 3.7 for Galaxy 22.01."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I  am a beginner in run Galaxy. Starting from this morning, my galaxy stopped to run, the jobs in the history say \u201cthis job is waiting to run\u201d and this status has last for a couple of hours. Anyone can tell me what I can do? I appreciate your help."
    },
    {
      "role": "system",
      "text": "Hi @user\n@link http://Usegalaxy.org has a banner up explaining how the server resources will be allocated over the upcoming week (and why).\nJobs are expected to queue even when there is not an event going on. If the jobs do not move from queued (gray) to execute (yellow) after 72 hours, then a rerun may help. Just be aware that this removes the original job from the queue, and places the new job back at the end of the queue. If a server is busy, and many are, a rerun will result in even longer delays \u2013 or possibly never running if you keep deleting/rerunning before the jobs have a chance to be deployed to an executing stage.\nFAQ: How Jobs Execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nI also added a tag to your post that points to similar Q&A about the subject. In short, job queue times at public servers depend on a few factors: how busy the server is, how many concurrent jobs you have running (queued + executing), what kinds of tools are you running (some tools require more resources, and those cluster nodes are the busiest), \u2026 and special factors noted in banners (events, downtime, known server issues).\nFor right now, I\u2019d suggest leaving the job as it is, to give it some time to move through the queue. Or you can rerun as @system suggests, as it sounds like it hasn\u2019t been that long since you started it up and there was a small issue at @link http://usegalaxy.org earlier today \u2013 although that resulted in job failures that required a rerun (resource allocation issues) \u2013 not jobs stalling in queued status (what it seems your situation is).\nThe choice is yours on how to proceed :slight_smile:\nps: For anyone participating in the GCC2021 training event, special queues will be set up to prioritize your tutorial work once the event starts. These particular queues will only be available to registered students and will have resources appropriate for training-sized data only (larger work will fail for resource reasons). Your event instructions will describe how to join these queues, and you can contact your instructors in Slack should you need more help. Non-event end-users will have their jobs sent the regular queues the servers support, and some may have extended queue times and related resource allocation changes \u2013 if those are in place, the server will note so in their banner."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! When using bcftools isec I came across the following error:\n( {% snippet  faqs/galaxy/histories_sharing.md %}) .\n\nCaptura de tela 2023-01-31 0954201011\u00d7537 57.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/13be2bcfba10c24a3cc33870be0898e173b7a3ca.jpeg)\nI need to overlap two VCF files to find the different mutated loci, variants and detect if there are differences in zygosity between the samples. If anyone knows what I did wrong regarding the input on bcftools or has any suggestions on other tools I can use to perform this type of analysis, the help would be very much appreciated. Thank you in advance and I apologize for the basic question, I am a medical student still learning how to use Galaxy for research."
    },
    {
      "role": "system",
      "text": "Hi @user\nthe message points to conflicting settings. It seems you\u2019ve changed Complement to Yes and added something into Nfiles box, resulting in the job error.\nFor de novo mutations try Complement set to Yes. However, you need to put the \u201cmutant\u201d sample first, and I am not sure how this can be done with the tool. You might consider VCF-VCFintersect.\nFor changes in zygosity, such as heterozygosity in parental sample and homozygosity in the offspring sample consider using multi-sample VCF and filter it.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello all,\nHow do I add the Admin Tab to my Galaxy account? I would like to install a tool from the Tool Box but I believe I require admin status?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hello,\nThe admin tab is enabled for any email address listed in the comma-separated configuration option in galaxy.yml titled admin_users. By default, it is:\nadmin_users: None\n\nIn my local galaxy, I have set the value to\nadmin_users: myemail@example.com,anotheremail@example.com\n\nYou may need to restart your galaxy process after changing this value."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have set up Galaxy for the first time\non a remote virtual desktop at AWS\nIt seems to be working.\nI am accessing it via Chrome on Windows.\nI have several problems:\n\nThe desktop size it, is there a way to stretch it to the whole window?\nCopy paste from Windows to the virtual desktop doesn\u2019t work.\nI don\u2019t think file transfer from Windows to the virtual desktop is possible.\nIt seems to be laggy.\n\nI wonder if there is a better client than Chrome (for windows), which could solve these problems?\nWhat do other people use? I assume thousands of people use Galaxy at AWS, how do you guys access it? Is there a client for UNIX that works well and solves the above problems?\n\n64367515_10216887876042613_6251879186332385280_o.jpg1835\u00d71080 64.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/53456aebb523f736180ef41b6ec916b57f3bad8b.jpeg)"
    },
    {
      "role": "system",
      "text": "It seems you have set up a genomic virtual lab image - your instance runs here: @link http://18.205.16.67/ and your Galaxy, which is a part of GVL image, is at @link http://18.205.16.67/galaxy.\nThere is no need for you to connect to the desktop of that server using VNC."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run RNAStar since yesterday and it won\u2019t start the job. I performed RNAStar analysis on the same data set before so this should not be the problem. There is no Error message, it just stays in cue and won\u2019t run. Does anyone have an idea on how to solve this?\nThis is only a problem on the .org server, the european server is working. It would be easier to load my data to the .org server though, so it would be great if it was running here too.\nThank you :slight_smile:"
    },
    {
      "role": "system",
      "text": "hi @user\nThe cluster that runs RNA Star may just be busy, so please leave the job queued.\nMore feedback soon & thanks for reporting the issue.\nUpdate: Your jobs are confirmed to be queued and the server/cluster is fine. One of your jobs is now executing and the others will execute as resources become available.\nFAQ: Datasets and how jobs execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nSimilar Q&A to yours can be found at this forum by clicking on the tag added: \u201cqueued-gray-datasets\u201d"
    }
  ],
  [
    {
      "role": "user",
      "text": "Why is this so? I mean why it has been reduced or disabled? Can we have a idea of the issue?\nThank you."
    },
    {
      "role": "system",
      "text": "Most of the prior impacted tools are now running again with full resources now.\nThe exception is RNA STAR \u2013 the banner will update as the issues with that tool are resolved. STAR jobs run at @link http://usegalaxy.org jobs are expected to fail until resolved.\nDevelopment ticket: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/266\nGalaxy Help topic with more details: RNA STAR Reference Genome (@link https://help.galaxyproject.org/t/rna-star-reference-genome/3156)\nClosing this topic to keep the RNA-STAR updates consolidated into the same topic/ticket.\nThanks for your patience!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello to everybody!!\nI ran the snippy tool a few times and everytime I had an error.\nIn the github of snippy I found there is a new version (4.6) and the author suggest this update for many improvements respect previous version.\nThanks a lot!"
    },
    {
      "role": "system",
      "text": "Hi @user,\na PR has been opened in order to update it Update Snippy v.4.6 (@link https://github.com/galaxyproject/tools-iuc/pull/3795). Probably it will be available the next week.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi . Im looking into a package in usegalaxy that can show my  RNAseq data is stranded or unstranded."
    },
    {
      "role": "system",
      "text": "Hi @user,\nin order to know if your RNAseq datasets are stranded or unstranded, you need to align it against the reference genome; you can use RNA STAR. Then by using the BAM/SAM Mapping Stats tool, you can infer it by comparing the number of reads which map to positive and negative strand."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello All,\nI have been trying to use featurecounts to count by RNA-seq reads.  When I use a downloaded GTF from UCSC - I get the results for transcript IDs not gene IDs, so I tried to use the in-built genome as I did for the HISAT2, however despite everything being set to hg19 - no reads are being assigned and I get 0 counts for all genes.  Can anyone advise as to what I am doing wrong please?\nThanks in advance :slight_smile:"
    },
    {
      "role": "system",
      "text": "hi @user\nwhat UCSC annotation was used? Maybe try hg19.refGene.gtf.gz. It should produce gene counts with the default featureCounts settings. Usually it is very reliable with single end data.\nDo you have paired-end data (trimmed)? Have you used \u2018strandness\u2019 setting for HiSAT2 and featureCounts? What do you see in the summary file? Do you see majority of reads in Unassigned_Read_Type category? If yes, featureCounts manual gives the following description:\nreads that have an unexpected read type (eg. being a single\nend read included in a paired end dataset) and also cannot be counted with confidence\n(eg. due to stranded counting). Such reads are typically generated from a read trimming\nprogram.\nMake sure you use correct strand settings in HiSAT2 and featureCounts. To determine strandness, map one sample with unstranded settings and run Infer Experiment. Enable Count fragments instead of reads in Options for paired reads (featureCounts).\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using the tool \u2018MEGAN: meganize a DIAMOND file\u2019 to meganize two DIAMOND (.daa) files that I have uploaded on the server. One of the files is 40 GB and the other one is 47.9 GB. The job was submitted on 9th November 2022 and it is 15th November 2022 today. It has been running for a week. Is there anyway to find out how much longer it will run for? Or can we speed up the process?\nThanks a lot.\n-Ishraq"
    },
    {
      "role": "system",
      "text": "Hi @user\nJobs process on the public services as resources become available. There is no way to speed this up. The most effective strategies involve getting jobs in the queue as soon as possible \u2013 and the best way to achieve that is to use workflows to execute tools in a series without manual intervention.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\n\nGalaxy Training: Using Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\nThe job was probably originally queued for a while (gray), then it started executing (yellow). Never interrupt a queued or executing job unless you already know that there is a content problem and it needs a rerun anyway, as any rerun starts the whole process over again. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\nSometimes the EU team has more information about specific cluster \u201cbusiness\u201d. Feel free to join the chat here usegalaxy-eu/Lobby - Gitter (@link https://gitter.im/usegalaxy-eu/Lobby)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am analyzing RNA-seq data and I got stucked in the DEseq because I get the following error:\nError in DESeqDataSet(se, design = design, ignoreRank) :\nsome values in assay are negative\nCalls: get_deseq_dataset \u2026 DESeqDataSetFromHTSeqCount -> DESeqDataSetFromMatrix -> DESeqDataSet\nDoes anyone know how to solve it?\nLaura"
    },
    {
      "role": "user",
      "text": "Hi Alessandro,\nI forward you the answer of a Galaxy team member:\n\u201cI looked at the history and found one minor issue: the option to specify if count files have headers (\u201cFiles have header?\u201d) is not set even though they have headers. Whenever there are headers in all counts file, please set this field to \u201cYes\u201d. Otherwise, just remove the header row in all the counts file and leave this field as \u201cNo\u201d.\u201d\nSo you have to click \u201cYes\u201d on files have header?when you do the Deseq"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there, can I add any plugin to galaxy user interface? Thanks in advance."
    },
    {
      "role": "system",
      "text": "It is unclear what you mean by a plugin. Two basic options of extending interface are @link https://docs.galaxyproject.org/en/latest/admin/special_topics/webhooks.html and visualizations \u2013 for the latter check the Viz section of the training material (@link https://training.galaxyproject.org/training-material/topics/dev/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am using Circos and trying to make tile 2dplot. My ideogram is mm10. Everytime I execute the job I can just visualise ideogram with no 2D plot."
    },
    {
      "role": "system",
      "text": "Hi @user\nThere is likely a format/naming problem with the data provided for the \u201c2D Data Tracks\u201d input.\nThe tutorial here provides many examples with sample data, and should help to find the problem: Visualisation with Circos (@link https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html)\nThe Circos tool form also has some tips:\n\n\u201cNo data or plot crashes? Check that your chromosome naming scheme matches between your Karyotype file and your datasets.\u201d\n@link http://circos.ca/documentation/\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi guys,\nI encountered an issue with my RNAseq data analysis while running Trinity and my main question is whether my data is too big or if there is a problem within the Galaxy server. I submitted the bug report 3 days ago but still no answer.\nThese are the details:\n\nI run the Trinity tool (tool ID: toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2)\non Galaxy Europe server\nFor the input RNAseq data, I used a collection of 16 paired datasets (each dataset is a pair of forward and reverse sequences previously trimmed with the Trimmomatic tool, also In Galaxy). Total of approx 42 GB of data\n\nJob API ID: 11ac94870d0bb33a197fd4c8059a6f81\nAfter submitting the starting analysis 2 jobs were created (Gene to transcript map &  Assembled transcripts)\nAll the time both jobs had a yellow flag and after 4 weeks of running the analysis I received this error with my job becoming red flagged:\n\"An error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2 \"\ntool error:\nAn error occurred with this dataset:\nCluster could not complete job\n\nTool Standard Output and Eror fields are empty.\nThese are my Tool parameters:\nAre you pooling sequence datasets? (Yes). Paired or Single-end data? (paired_collection). FASTA/FASTQ dataset collection with R1/R2 pair (257: Trimmed pairs_collection). Strand specific data  (true). Strand-specific library type (Forward-Reverse). Jaccard Clip options (False). Run in silico normalization of reads (True). Minimum Contig Length (300). Use the genome guided mode? (no). Minimum count for K-mers to be assembled (3)\nCan you please help?\nBest,\nLada"
    },
    {
      "role": "user",
      "text": "Hi @system,\nthank you for keeping in touch and helping me out. :slight_smile:\nyes i see it in my history at the moment and it seems like its running.Thanks!\nIn the meantime, I managed to do Trinity job and some of the downstream applications with only 10 samples (the one that had an issue and that you resubmitted is with 16 samples) so I suppose the tools are working just fine. I expect it to be running for a couple of weeks I guess.\nBtw, do you know if there is a time frame for the job to be executed on Galaxy after which it gets canceled or it will run until it\u2019s finished (assuming there is no other errors) ?\nBest,\nLada"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m trying to replicate the results of the scRNAseq dataset from the following publication \u201cDissecting the multicellular ecosystem of metastatic melanoma by single-cell RNA-seq\u201d from this link @link https://science-sciencemag-org.ezp-prod1.hul.harvard.edu/content/352/6282/189. They have provided a series matrix at GEO \u201cGEO Accession viewer (@link https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72056)\u201d. I would like advice from the forum on the best Galaxy tools to format this file for analysis.\nI thought this file looked like a a processed dataset (like what we would get after RNA-Star Solo pre-processing), so I started out by putting this directly into the RACEID Initial Processing to make a Count Matrix (RDS). However this didn\u2019t work. When I tried to plug this directly into the RACEID clustering directly, I also got an error.\nWhat are the proper steps and tools to process a series matrix into a matrix I can put it into RACEID for analysis? Thanks for your help!"
    },
    {
      "role": "system",
      "text": "Hello,\nPlease try with ScanPy instead. RaceID is great on smaller datasets but struggles a bit on larger ones."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am running Unicycler and getting the message when I try to run a job where I got 9 paired files for my pair ended reads and 3 files with Pacbio data.\n\u201cThe server could not complete the request. Please contact the Galaxy Team if this error persists. Received 9 inputs for \u2018paired_unpaired|fastq_input1\u2019 and 3 inputs for \u2018long\u2019, these should be of equal length\u201d"
    },
    {
      "role": "user",
      "text": "The answer was available in the SARS-CoV-2 genome assembly tutorial. The Unicycler tool does not have a way to merge datasets so that needs to be done beforehand with the \u201cCollapse collection\u201d tool. Otherwise the Unicycler tool will try to run a batch job where the first dataset from the forward, reverse and long dataset produce assembly 1, the 2nd dataset in each produce assembly 2 and so on. I should have realized this when looking at the text below the collection selection box in the Unicycler tool interface which clearly states that \u201cThis is a batch mode input field\u201d but which I for some reason did not think of when looking at the problem. I posted this question as I am sure other people will make the same mistake from time to time.\nTo merge reads from several samples into a combined final assembly, we need to pass the data to Unicycler tool in partially merged form. The forward and reverse reads of paired-end data should be kept separate, and so should short and long reads. However, the tool has no option to combine data from individual samples, so we need to merge the forward, reverse, and the long reads data, respectively, across samples. Conveniently for us, the outputs of the earlier Samtools fastx tool runs have already returned the data structured into three corresponding collections for us."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI selected \u201cInteractive Jupyter Notebook\u201d from Tools on the left side bar to include it to my Galaxy interactive tools,but it isn\u2019t added to my history.The job is constantly running and taking so long.\nI did it many times but each time is the same.This screenshot of my interactive environment shows this problem.\nerror-galaxy1687\u00d7563 47 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/ee8f48c03de5afcf658d4f0d8b4b946e1de9a17d.png)\nWhat is the problem ?\nHow to solve it?"
    },
    {
      "role": "system",
      "text": "@user you need to click on the \u201cJupyter Interactive Tool\u201d link :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All! So, I need some help getting off the ground :sweat_smile:\nI sent in a plate of 96 samples for RAD-seq: Half were my samples, half were a colleague\u2019s. However, the company returned a single file of all sequences produced in fastqsanger format, including both mine and my colleague\u2019s samples intermixed together.\nDoes anyone know how to filter this so I that only analyze my own samples?\nI tried \u201cFilter Fasta\u201d already, but this failed."
    },
    {
      "role": "user",
      "text": "Hi jennaj,\nYes, the sequence reads all came back as an interleaved fastq file. They provided a sample sheet so I know which samples are mine and which are not, so my problem is purely a logistical one in figuring out how I select for my own samples!\nHowever, having talked to a few people, it seems my only option is the demultiplex all of the sequences first and then select which sequences I want to use in downstream analyses. If you have another idea, though, I\u2019d love to hear it!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m adding apache as a proxy server in galaxy 20.05. But I have a problem with the wsgi socket permissions. It\u2019s created as:\n\n0 srwxr-x\u2014 1 galaxy galaxy 0 Aug 10 14:15 /opt/galaxy/var/uwsgi.sock\n\nI made apache member of the group galaxy but it still missed the write permission for group.\nI don\u2019t find a way in the galay.yml to tune the ownership/permissions, so, what is the proper way to configure the owership of the socket?\nBest,"
    },
    {
      "role": "system",
      "text": "Try chmod-socket: 664 in the uwsgi section of galaxy.yml to make it group writable.\nYou might also need to explicitly set the Apache Group (@link https://httpd.apache.org/docs/2.4/mod/mod_unixd.html#group) directive to the galaxy group and restart Apache."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have a very confusing problem with some of the galaxy outputs. I run the same tool with different samples, and for two of the ten samples, galaxy displays red boxes, with the message \u201cUnable to finish the job\u201d as you can see in the image below.\nSurprisingly, the output files are well produced. There are no errors in the tool_stderr file either.\nAnd when I run the tool from the command line, the tool runs fine\u2026\nHow can I figure out what is wrong ? Have you ever had this problem?\nThank\u2019s for your help\n\nerror_galaxy1499\u00d7636 86.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/19420964e01019875321251bd58d5802bb17a4c2.png)"
    },
    {
      "role": "user",
      "text": "Hi,\nThank you for your reply.\nI found the problem. In fact, this is because Galaxy expected a tabular output file, when in fact the file was separated by spaces. Thus, the first line with many spaces was too big and exceeded the size limit.\nit occured with only few samples"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m running a paired-end de Novo assembly with Trinity. Inputs are 2 fastq (7 gb each) for forward and reverse ends, with 31 million reads of same long.\nThe Trinity step in my history started running a few more than 48 hs ago. Is this too long? May I clean an rerun?\nHistory link:\n@link https://usegalaxy.eu/u/rivero.alexander/h/srr59\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Trinity uses a lot of resources and usually runs for very a long time. So, nothing unusual here."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was trying to download all my large collections to clear out spaces for my next analysis. But all download failed no matter when using Chrome or curl command, my browser said incomplete files and curl just closed transfer with nothing downloaded (and I tried using APIs as suggested in the tutorial but still same). The message shows as below:\n\nimage1402\u00d7120 9.56 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d8315272a2dc56317b8d415545b54cedf8fbe897.png)\n\nimage1200\u00d7118 43.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/ef7f27082a4aaddf1011cbb5730c786907f693d7.png)\nCan someone help me out? I thought this could be network problem but I tried different places and different time point, still not working."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe collection download functionality at @link http://UseGalaxy.org (and only that server) is undergoing some updates. Details: Download collection broken fails with \"file incomplete\" \u00b7 Issue #15390 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/15390)\nFor now, exporting a history is the most reliable method."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!,\nI was running a transcriptome assembly with Trinity on @link http://usegalaxy.org with paired .fastq files that had been trimmed with Trimmomatic on my local computer (and further QC checked with FastQC).  The files contain 178 million paired reads.\nOvernight, the run was halted with the following Error:\nTool Error: Remote job server indicated a problem running or monitoring this job.\nAs far as I can tell, there are no other error identifiers, so I am unsure why the the job was halted/\nCan you offer any advice?  thanks!"
    },
    {
      "role": "user",
      "text": "Thanks,\nI may have identified the problem,  Looking back at my FastQC reports that were run on my computer, they fq files were in an illumina 1.5 encoding format, and I did not realize that this is not compatible with usegalaxy programs (need Illumina 1.8+).\nThe original 2 paired files I uploaded via HTTP were .fq.gz (15.4 GB and 14.7 GB), which galaxy uncompressed during the first Trinity run attempts (and hide from history view).   Since all 4 files (2 compressed and 2 uncompressed) remain on my galaxy allocation, I am right now using FASTQ Groomer on the hidden uncompressed files to convert them to a PHRED +33 format and will attempt to use these newly generated files with Trinity.   Hopefully that works!\nThanks for your help!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi I am looking to use the latest of the sus scrofa genome in HISAT2 for BAM alignment but I believe someone from the galaxy team is needed to add the 11.1 version.\n@link https://useast.ensembl.org/Sus_scrofa/Info/Index"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis genome is now available in @link http://usegalaxy.eu.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nOver the past couple of days I have been running a workflow for basic data analysis of samples which looks like this:\nTrimmomatic \u2192 SPAdes \u2192 MLST & ABRIcate\nThe past 6 samples that I ran through this workflow have been perfectly fine however with my most recent sample the Trimmomatic stage completes but the next step of SPAdes assembly does not activatev and sits on \u2018waiting to run\u2019. Is this a known issue or has anyone else run into this problem? I\u2019ve tried to run the trimmed reads into SPAdes serparately too but the same issue occurs.\nThanks for any help!"
    },
    {
      "role": "system",
      "text": "Consolidating into Spades genome assembly waiting to run for a long time. - #3 by jennaj (@link https://help.galaxyproject.org/t/spades-genome-assembly-waiting-to-run-for-a-long-time/7113/3)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have run into a problem when trying to count features from a genomic alignment.\nI have used RNA-star to align single end FASTQ data from the rabbit. The mapping goes well and delivers a good unique mapping of 74%. To then count features (using featurecounts) I use the same genome imported via UCSC table browser as ncbi-refseq-genome-all in GTF format. So far so good.\nAt first inspection everything looks great, genes I expect highly represented are indeed so. However, one key gene I expected to be highly represented was missing from the counts and was given as being zero, something that seemed very odd.\nI converted the aligned BAM from the RNA-star output to a bigwig for UCSC import and could clearly see this gene should certainly be present in terms of transcript counts. I also examined my GTF file and could also confirm that all the exon locations and gene annotations were present as expected for this gene and all matched the gene_id locations for the observed BigWig output under UCSC. I proceeded to inspect other genes that according to the featurecount table were zero. Many, were indeed zero, however, I came across many entries that were clearly not zero according to the aligned BigWig. Since I previously came across this issue (by chance) when performing a mouse RNA-seq experiment (but at the time just disregarded it a a \u2018glitch\u2019), I am now more concerned. I have inspected all the files and inputs I am using but as for myself, can find no good explanation for this output mismatch.\nAny ideas?\nBest regards,\nPhil"
    },
    {
      "role": "system",
      "text": "@quote\nTo then count features (using featurecounts) I use the same genome imported via UCSC table browser as ncbi-refseq-genome-all in GTF format.\nHi @user\nRetrieving GTFs from the UCSC Table Browser can introduce two problems. UCSC explains about why not to get GTFs using the TB on their site but sometimes that is missed.\n\nThe gene_id and transcript_id values will be the same value (scientific problem wherever it is used, and probably explains the odd results)\nThe output can be truncated due to output size limits (technical problem once in Galaxy)\n\nYou didn\u2019t state which genome you are using, but check in their Downloads area for GTF annotation. If your genome doesn\u2019t have this already available, write into UCSC\u2019s help and they can assist you to get a proper file.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_working_with_reference_annotation.html)\n\n\nGalaxy Training: Working with GFF GFT GTF2 GFF3 reference annotation (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_working_with_reference_annotation.html)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "im working with pseudogenes and i think i need this option (outFilterMultimapNmax) but i cant find it in star or feature counts . does galaxy even has this option ?"
    },
    {
      "role": "system",
      "text": "The option is available for STAR - just a bit hard to find (hidden under advanced advanced options :slight_smile: ):\n\n\nWould you like to set output parameters (formatting and filtering)?: Yes\n\n\nWould you like to set additional output parameters (formatting and filtering)?: Yes\n\nMaximum number of alignments to output a read\u2019s alignment results, plus 1\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nSo yesterday I was able to use stringtie to get gene and transcript counts from my aligned BAM files with no issues. Today I tried to run new samples and I was able to get assembled transcripts but no counts. I figured this was an issue with my new files. So I took the files that worked perfectly yesterday and ran stringtie on them again (same settings, files, etc.) and now these are also not outputting any counts. I\u2019m honestly stumped on this one and not sure if it is just an issue with the site at this point. Any advice would greatly help my sanity. Thank you."
    },
    {
      "role": "system",
      "text": "Hi Stephen,\nI am sorry for the confusing explanation. With the default settings StringTie only produces a gene annotation file. It can produce count and abundance files, as additional outputs. For example, you can enable output of abundance estimation files, as in your reply. To get counts, try gene annotation in your history, and change Output files for differential expression? to DESeq2/EdgeR/limma-voom.\nFor counting I use dedicated tools, such as featureCounts.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Fastqsanger.gz is not working in the trim galore or trimmomatic flexible trimming tool as a paired end collected. Ive tried to use a D interlacer to get it to work but the drop down list doesnt allow me to select my  SRA data.  Is there something i could do to fix this?"
    },
    {
      "role": "system",
      "text": "Hello @user\nItems to check:\n\n\nFirst, be sure that you are using the option on the tool forms to look for collections of input reads. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#selecting-a-dataset-collection-as-input\n\n\nIf the form is looking for a collection but doesn\u2019t find your specific collection, next check that the individual files in your collection were assigned a datatype that matches the input field on the tool form.\n\n\n@quote\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:Create a new empty historyLoad up the tool formReview the input datatypes listed in the tool form\u2019s input areaNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.\n\n\nWhy the Fastq De-interlacer tool would help is not clear. Is the existing collection a list of datasets? Or a list of dataset pairs? If the reads are already split into pairs, then you won\u2019t need that tool.\n\n\nThis tool is a great way to get reads from SRA into Galaxy that are sorted into collections: Faster Download and Extract Reads in FASTQ format from NCBI SRA\n\n\nThese tutorials can help more.\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\n\n\nGalaxy Training: NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nAnalyses of sequences\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)\n\n\nGalaxy Training: Using dataset collections (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/processing-many-samples-at-once/tutorial.html)\n\n\nGalaxy Training: Multisample Analysis (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/processing-many-samples-at-once/tutorial.html)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n\nIf you are still stuck after reviewing, and please create a shared history link and post that back in your reply. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history. Leaving all datasets undeleted, including your tests, will help to get the best feedback from our community.\nLet\u2019s start there :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use TXSScan MacSyFinder to identify secretion systems in my bacterial genome. I upload the genome in the requested GenBank format. Upon selecting the sequence and running the analyses, results populate my \u201chistory.\u201d However, when I look at the data, seems like there is nothing there. It provides me with 5 outputs for a single analysis:\nMacsyView output\nsummary output\nreport output\noutput\nhmmer results\nThey all seem to have not data in them. The only one that has a download option is the hmmer results, but when I download the file there three files for each thing it searched for:\n_.search.hmm.out\n_.search.hmm.err\n_res_hmm_extract\nI dont know how to open these files or what to do. As this bacterium doesn\u2019t have certain secretion systems, there should be no file; however, I am pretty sure this is not really data, as there are these files for every secretion system protein that the program was searching for. So not sure what to do or why it doesn\u2019t give me any data. This bacterium should at least have one secretion system.\nAny help would be great. Thanks in advance."
    },
    {
      "role": "system",
      "text": "This tool is specific to the public Galaxy server at @link https://galaxy.pasteur.fr/. There could be a data input problem or some server-side issue leading to unexpected results.\nThe tool hasn\u2019t been published to the wider Galaxy community through the Tool Shed (@link https://usegalaxy.org/toolshed), which means there is little help we can offer at this forum unless their admins/support staff find your post and reply later.\nIt is probably quicker to contact this server\u2019s support team directly. Find the contact email on the homepage of the server, second box down, center pane. It is also listed here: @link https://galaxyproject.org/use/galaxy-pasteur/\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI recently installed Galaxy on Ubuntu.\nNo problem with the installation per se.\nBut I realize that most of the modules are not available at least in the main package.\nIs there a way to download them and to install them aftewards?\nFor instance the section \u201cMultiple alignement\u201d is simply missing and the clustal W module is not available.\nThank you for your help.\nC\u00e9cile"
    },
    {
      "role": "system",
      "text": "You can install tools of your choice into Galaxy you maintain. This tutorial is a good start @link https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I have readspergene.out.tab as an output after running star. Is it possible to create a count matrix using that file for input into DESeq2 using galaxy, instead of running feature count as per the galaxy DESeq2 tutorial?"
    },
    {
      "role": "user",
      "text": "Hi David,\nThank you for your reply! I managed to convert my read counts per gene by extracting the second column :slight_smile: If that helps anyone else facing the same problem as I did!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried to perform mapping with BWA-MEM using fastq files to reference genome, which absolutely worked yesterday. But since this morning I kept getting error saying:\n[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\n[31mFATAL: e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nWould you be able to assist me on this? I also tried using the fastq files of the ones successfully mapped yesterday, but it also returned as error. I do not understand why it kept getting error.\nThank you so much!"
    },
    {
      "role": "system",
      "text": "Hello @system and @user\nThe problem has been resolved. Thanks for reporting it!\n@quote\nHello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one : \n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800 \nSome help ? \nThanks a lot\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nNew Galaxy user, so just getting used to it.\nI was trying to upload a FASTQ file of 300Mb, uploaded it using the FTP and everything went smoothly but, the file has been charging for two hours now to the server. It stills with the clock and in grey.\nIn your experience, it takes so long for large files to be uploaded? should I re-load the charging?\nThank you very much for your help.\nBest,\nEva Sandoval."
    },
    {
      "role": "system",
      "text": "Welcome, @user\nFew questions:\n\nDid you check that the file was completely loaded during the FTP step before you attempted to move it into the history using the Upload tool? Whatever method you used (ex: client such as Filezilla or lftp line-command) produces statistics or a message confirming a complete/successful transfer.\nAre you working at @link https://usegalaxy.org (just to double-check)\n\n300 MB is not that large of a file. And the Upload process to move the dataset into a history should happen quickly.\nThe server is a bit slow right now, so this could be a problem on our end. More feedback soon. But please confirm the questions if you have time, just to make sure we are working on solving the same problem.\nUpdate: We addressed the slower response times at @link http://usegalaxy.org. Your job for the data transfer FTP > Upload > History should have completed by now."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to download different datasets from my history, but am running into errors when trying to unzip them.\nFirst, I can download a zipped collection of my html report outputs from fastp but when I try to unzip them with Windows, I get an unspecified error. I have tried unzipping them with WinRar but it says the files are corrupted. This also happens when trying to download html reports from FastQC. Note this also happens when I try to download the html file for a single sample, as it still zips the report.\nI am using Chrome, but have also tried this on two different computers with the same result.\nAny clues on what is going on here? This problem only popped up in about the past week. I had no trouble unzipping my files before.\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nOk \u2013 then that was another user who reported the same problem in a very similar way to the @link http://UseGalaxy.org mailing list.\nThis does turn out to be an actual bug from what I can tell after reviewing this more. I\u2019ve opened an issue ticket here for expanded review by our developers. Updates will post in that ticket. Bug: FastQC webpage output fails complete download from disc icon \u00b7 Issue #14365 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/14365)\nI\u2019m going to mark this reply as the \u201csolution\u201d to make it easier for others to find the Q&A, but also add the tag \u201cserver open issue\u201d for accuracy as this functionality is still pending a bug remedy.\n\nThe workaround until the fix is made (tool or Galaxy itself) is the command-line method.\n@quote\nA complete command string would be like this one. It happens to be an actual (small, test data) FastQC webpage dataset if anyone wants to test it. I\u2019ll leave it undeleted for now.curl -o out.html --insecure 'https://usegalaxy.org/api/datasets/f9cad7b01a472135987ad5307248c882/display?'"
    }
  ],
  [
    {
      "role": "user",
      "text": "[33mWARNING:e[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\ne[31mFATAL:  e[0m Couldn\u2019t determine user account information: user: unknown userid 819800\nI keep getting this error when I try to run any function.  Any advice?"
    },
    {
      "role": "system",
      "text": "Hi @system and @user\nThe problem has been resolved. Thanks for reporting it!\n@quote\nHello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one : \n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800 \nSome help ? \nThanks a lot\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Another quality control question.\nI trimmed my datasets of interest with Trim Galore! After this, I ran FastQC to check if the removal of adapter and primer sequences was successful. While, based on the Adapter Content plot, the adapters were removed successfully during trimming, I still had some overrepresented sequences in some datasets:\n\nimage956\u00d7164 10.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f9455004384495861dc154ffefb1bbc0b6bc6581.png)\nHowever, since there were no hits to indicate what these were, I copied them into a FASTA file and ran them using the NCBI BLAST + blastn tool against the locally installed BLAST databases.\nThe output of this was the following:\n\nimage946\u00d7741 23.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/ce7a3dcc7dd0ae0895c01f6be9bd4eead144aa46.png)\nThe reads matched whole genome sequencing contigs across multiple species (predominantly, mammalians). What I am wondering now is how to interpret this - are these overrepresented sequences mapping to some potentially conserved sequences across species and are overall false positive (in terms of not being a contaminating adapter, primer, etc.) or if it\u2019s something else?\nIf they match to innocent sequences, can I ignore the overrepresented sequences when running downstream analyses?\nThe reads I am investigating are short (~76 bp), so I would assume they have higher chances of matching something broadly than longer reads. They are also from RNAseq experiments (I read that sequences flagged as overrepresented can sometimes indicate highly expressed genes).\nWould appreciate your opinions and advice.\nThanks!"
    },
    {
      "role": "system",
      "text": "@quote\nIf they match to innocent sequences, can I ignore the overrepresented sequences when running downstream analyses?\nOne thing that I usually try is to review regions like that in the UCSC Genome Browser @link https://genome.ucsc.edu/. (I\u2019ve also just googled them, and tried at NCBI, etc). Sometimes you won\u2019t find out but can always check to see if they at least fall out during read mapping against a genome/transcriptome later on or not. Public WGS data is much \u201cnoisier\u201d and sometimes the age of the sequence submissions and sequencing protocols are clues, too.\nI tend to try to BLAT map the \u201cmystery sequence\u201d to a well annotated model organism (sometimes a few, if I don\u2019t get hits with the first), then click into the hit and view it in the genome alignment view. Consider scrolling down and turning on more tracks to suit what seems relevant to make a decision.\nI usually start with the Comparative Genomics \u2192 Conservation track plus everything in the Variation and Repeats section along with the existing defaults since all those combined provide basic annotation markers for the hit region (or regions \u2013 which can also be informative).\nThe rest is a judgement call :slight_smile: Happy hunting!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have a problem to display data on UCSC Cell Browser. I 'm trying to run Atlas-Seurat-CellBrowser workflow (GitHub - ebi-gene-expression-group/container-galaxy-sc-tertiary: Galaxy container for single cell RNA-Seq tertiary analysis tools (@link https://github.com/ebi-gene-expression-group/container-galaxy-sc-tertiary))  on my own local Galaxy (v19.05) and get html format data for UCSC Cell Browser. When I click the \u201cView Data\u201d icon of the result, UCSC Cell browser starts but the result cannot be displayed even though I click \u201cOpen\u201d button. I also uploaded the result on Human Cell Atlas Galaxy (@link https://humancellatlas.usegalaxy.eu) and found again it wasn\u2019t displayed.\nI tested on other versions local Galaxy and figure out t works on Galaxy version 18.05 and 19.01 but doesn\u2019t work on 19.05 and 19.09. Also I tried other clients, Mac/Windows, Safari/Firefox/Chrome only to get same behaviour.\nAnyone have the same problem? What might be wrong?\nThank you.\nYukie"
    },
    {
      "role": "system",
      "text": "@system @user\nData in HTML format cannot be uploaded at public Galaxy servers intact.\nAny data in HTML format that was created at any particular Galaxy server has a specific flag that allows it to be displayed at that server when using the \u201cview data\u201d icon (\u201ceye\u201d).\nBoth are for security reasons at public Galaxy servers, including @link https://humancellatlas.usegalaxy.eu.\nI\u2019m not sure about all the details at @link https://cells.ucsc.edu/. You could contact them and ask a question, or you can try to figure out the usage first. You might just need to set the data to be in a \u201cshared\u201d state in Galaxy. Or, maybe download the data into a local file and try to load it that way. They do offer a version that can be set up locally \u2013 similar to how you set up Galaxy locally \u2013 and I would expect that permission configurations could be customized by the administrator (you).\nRelated:\n\n\nHTML data is blocked by default (Upload and display). How any particular private Galaxy server handles HTML format is configurable by the server administrators. Decisions depend on if the server is publically accessible, has moderated access/account creation methods, etc.\nData needs to be publically accessible (on a server hosted publically) and have the privacy preferences set as \u201cshared\u201d to move data from Galaxy to a different public web service. Plus, any service in the EU will be following GDPR compliance strictly.\nThat service may be another Galaxy server or it may be a third-party application.\nData in a shared state is still somewhat \u201cprivate\u201d for many use-cases \u2013 but this is not the same as encrypted/secure data transfer. Data have distinct URLs. When those data URLs are in a shared state, someone would have to correctly \u201cguess\u201d the data URLs with an open sharing access set in order to gain access.\nAny work that needs to be completely secure/protected should not be created or uploaded to public web services \u2013 Galaxy or otherwise.\nThere are many ways to use Galaxy for sensitive data with security/privacy requirements (example: personally identifiable human biological data). These deployments are usually behind a firewall and are not publicly exposed.\n\nPractical help for end-users:\n\nData Privacy in Galaxy (@link https://galaxyproject.org/learn/privacy-features/)\n\nAdministrative topics:\n\n@link https://docs.galaxyproject.org/\nsearch the site with keywords such as \u201cprivacy\u201d or review admin topics like this one Proxying Galaxy with Apache \u2014 Galaxy Project 20.09 documentation (@link https://docs.galaxyproject.org/en/master/admin/apache.html#)\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "i am unable to download the FastaQC quality report"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nI just tried to do this and it worked Ok.\n\nServer: @link http://UseGalaxy.eu\n\nResults from using tool/version: FastQC Read Quality reports (Galaxy Version 0.73+galaxy0)\nMethod: Downloading datasets (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_download_datasets.html)\n\n\nHow to troubleshoot:\n\nTry again now, maybe the server had some temporary issue.\nIf that fails, check to make sure that the output actually contains content and is not in an error state (red)\nIf that all looks Ok, please post back a shared history link and the EU moderators here can help more. Sharing your History (@link https://training.galaxyproject.org/training-material/faqs/galaxy/histories_sharing.html)\n\nIf the reports are in a collection, try downloading individual files. Collection download had some issues that I think are still pending a fix (testing that now). ping @system or @system\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI would like to know, how to identify Cre-recombinase is expressed or not in my scRNAseq data? I have fastq files, Please help to guide in this. Thanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can use RNA STAR for mapping the Cre-recombinase sequence (@link https://www.ebi.ac.uk/ena/browser/view/X03453) against your dataset. You can examine if the Cre-recombinase mapped to the sequences in the log file generated by that tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone\nI got an error with Mothur command \u201ccluster\u201d\n\u201cThis job was terminated because it used more memory than it was allocated\u201d\nThe preceded command \u201cDist.seqs\u201d was used with cutoff \u201c0.20\u201d and the output file is ~170Gb\nI know that I can use the alternative command \u201ccluster.split\u201d which uses less memory, but for some reason I have to use cluster command.\nBTW I am using @link http://usegalaxy.org\nany advices?\nI appreciate your help in advance\nthank you"
    },
    {
      "role": "system",
      "text": "Hi @user\nWe discussed this directly via email, correct?\nSummary for others:\n\n\nThe input dataset is so large that it exceeds computing resources at @link http://UseGalaxy.org and probably would exceed the resources at any public Galaxy server.\n\n\nComputing resources are distinct from data storage (quota space) associated with accounts.\n\n\nThe 16s tutorial here explains why splitting clusters is a reasonable choice for many: 16S Microbial Analysis with mothur (extended) (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html#otu-clustering)\n\n\nIf that method does not suit your needs or the needs of anyone running any analysis that is too large for public servers, then moving to a private Galaxy server where you can control the resources is one way forward.\n\n\n\nWebinar: Use Galaxy on the web, the cloud, and your laptop too >> Webinar: Use Galaxy on the web, the cloud, and your laptop too (@link https://galaxyproject.org/events/2020-12-webinar-where/)\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources (@link https://galaxyproject.org/use) (scroll down for the platform choices matrix)\nThe GVL version of Cloudman is one choice and AWS offers grants. AWS Programs for Research and Education (@link https://aws.amazon.com/grants/)\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been dealing with the shotgun data and want to use the MetaPhlAna for my analysis.\nI am wondering at the option of \" Cached database with clade-specific marker genes\u201d where i have to select MetaPhlAn2 clade-specific marker genes. but my page is showing \u201cNo option is available\u201d.\nWhy it is showing as no option to me or what to do for the step? Please help\nimage1920\u00d71080 188 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/02d6457217db82914a62ff66fe8cdf8af4e17d68.png)"
    },
    {
      "role": "user",
      "text": "Actually not. But i have use another webpage of galaxy like galaxy australia and europe where they have in built"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello - complete n00b here so hopefully its something very quick.\nI am running a local instance of Galaxy 22.05 on Ubuntu.\nI\u2019ve managed to set up the instance, become admin and install some tools from the toolshed (RNA-star, edgeR, featurecounts) without any issues (they are all running fine).\nHowever, when I try to install \u2018Dexseq\u2019, I seem to run into an issue where it is stuck on \u2018Installing tool dependencies\u2019. I don\u2019t see any obvious error message but this is the output from Terminal (just keeps repeating this with new values of 127.0.0.1:X.\nuvicorn.access INFO 2023-03-02 07:34:46,163 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:54370 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nSuvicorn.access INFO 2023-03-02 07:34:51,184 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:36840 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:34:56,201 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:36842 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:01,217 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:37582 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:06,235 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:37596 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:11,255 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:56610 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:16,273 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:56622 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\nuvicorn.access INFO 2023-03-02 07:35:21,287 [pN:main.1,p:5583,tN:MainThread] 127.0.0.1:58406 - \"GET /api/tool_shed_repositories/?uninstalled=False HTTP/1.1\" 200\n\n\nI have re-installed and restarted Galaxy several times and its a no-go.\nAny help appreciated !"
    },
    {
      "role": "system",
      "text": "Hello @user\nIf you still need help, please start here: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/admin/)\nFull docs are here, including dependency handling topics: Releases \u2014 Galaxy Project 23.0.1.dev0 documentation (@link https://docs.galaxyproject.org/en/master/releases/index.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m trying to download the entire history from Galaxy through FTP. I first exported the history to the FTP remote site, and then tried to connect to @link http://galaxy.eu through WinSCP using the parameters shown in screenshot #1. But I had problems downloading the file because of permission issues (screenshot #2). Could you help me?\n\nScreenshot 2022-08-15 151815994\u00d7658 26.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/121453e1e9af338815b782f56e25b6ba1298f1d3.png)\n\nimage1912\u00d7685 73.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dd35823032080c1fd32876f54722440866f9e077.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe current FTP configuration doesn\u2019t allow downloading files from Galaxy by using the FTP protocol.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nim trying to use Slurm for our local galaxy, but somehow all jobs immediately fail when i run them. When i go back to singularity, everything works again. I can also run jobs on slurm with sbatch or so, but when starting a job in the Galaxy UI, they become yellow and then red with this error: \u201cThis job failed for reasons that could not be determined.\u201d I\u2019ll attach all logs that might be useful. I would be very grateful if someone could have a look. I feel like it is a problem because im running slurm on the same machine as the galaxy.\nIs slurm needed or recommended if i want to run jobs to run with more than one core? I thought slurm to be most efficient because i could use it with tpv so i dont have to find out how many resources each tool would need.\n------------slurm.log--------------------------\n#log while running job\n[2023-07-04T16:46:05.716] Launching batch job 24 for UID 999\n[2023-07-04T16:46:10.257] [24.batch] sending REQUEST_COMPLETE_BATCH_SCRIPT, error:0 status:256\n[2023-07-04T16:46:10.259] [24.batch] done with job\n------------slurmctld.log--------------------------\n#Log from job run\n[2023-07-04T16:46:05.504] _slurm_rpc_submit_batch_job: JobId=24 InitPrio=4294901754 usec=545\n[2023-07-04T16:46:05.713] sched: Allocate JobId=24 NodeList=galaxyworkstation #CPUs=1 Partition=debug\n[2023-07-04T16:46:10.258] _job_complete: JobId=24 WEXITSTATUS 1\n[2023-07-04T16:46:10.258] _job_complete: JobId=24 done\n#Log when starting slurm\n[2023-07-04T17:04:57.311] error: Node galaxyworkstation appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.\n[2023-07-04T17:04:57.698] Terminate signal (SIGINT or SIGTERM) received\n[2023-07-04T17:04:57.795] Saving all slurm state\n[2023-07-04T17:04:58.153] error: Configured MailProg is invalid\n[2023-07-04T17:04:58.154] slurmctld version 21.08.5 started on cluster cluster\n[2023-07-04T17:04:58.157] No memory enforcing mechanism configured.\n[2023-07-04T17:04:58.185] Recovered state of 1 nodes\n[2023-07-04T17:04:58.185] Recovered information about 0 jobs\n[2023-07-04T17:04:58.185] select/cons_res: part_data_create_array: select/cons_res: preparing for 1 partitions\n[2023-07-04T17:04:58.186] Recovered state of 0 reservations\n[2023-07-04T17:04:58.186] read_slurm_conf: backup_controller not specified\n[2023-07-04T17:04:58.186] select/cons_res: select_p_reconfigure: select/cons_res: reconfigure\n[2023-07-04T17:04:58.186] select/cons_res: part_data_create_array: select/cons_res: preparing for 1 partitions\n[2023-07-04T17:04:58.186] Running as primary controller\n[2023-07-04T17:04:58.186] No parameter for mcs plugin, default values set\n[2023-07-04T17:04:58.186] mcs: MCSParameters = (null). ondemand set.\n[2023-07-04T17:05:03.191] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2\n------------slurm.conf--------------------------\n\nThis file is maintained by Ansible - ALL MODIFICATIONS WILL BE REVERTED\n\nDefault, define SlurmctldHost or ControlMachine to override\nControlMachine=localhost\nConfiguration options\nAuthType=auth/munge\nClusterName=cluster\nCryptoType=crypto/munge\nProctrackType=proctrack/pgid\nSelectType=select/cons_res\nSelectTypeParameters=CR_CPU_Memory\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nSlurmctldPidFile=/run/slurmctld.pid\nSlurmctldPort=6817\nSlurmdLogFile=/var/log/slurm/slurmd.log\nSlurmdParameters=config_overrides\nSlurmdPidFile=/run/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/lib/slurm/slurmctld\nSlurmUser=slurm\nStateSaveLocation=/var/lib/slurm/slurmctld\nNodes\nNodeName=galaxyworkstation CPUs=32\nPartitions\nPartitionName=debug Default=YES Nodes=galaxyworkstation State=UP\n------------scontrol show job--------------------------\n#scontrol show job\n#scontrol show job 18\nJobId=18 JobName=g67_secure_hash_message_digest_niklas_petzold_tum_de\nUserId=galaxy_ans(999) GroupId=galaxy_ans(999) MCS_label=N/A\nPriority=4294901759 Nice=0 Account=(null) QOS=(null)\nJobState=FAILED Reason=NonZeroExitCode Dependency=(null)\nRequeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0\nRunTime=00:00:04 TimeLimit=UNLIMITED TimeMin=N/A\nSubmitTime=2023-06-30T16:07:07 EligibleTime=2023-06-30T16:07:07\nAccrueTime=2023-06-30T16:07:07\nStartTime=2023-06-30T16:07:08 EndTime=2023-06-30T16:07:12 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-06-30T16:07:08 Scheduler=Main\nPartition=debug AllocNode:Sid=galaxyworkstation:22799\nReqNodeList=(null) ExcNodeList=(null)\nNodeList=galaxyworkstation\nBatchHost=galaxyworkstation\nNumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0::\nTRES=cpu=1,mem=1M,node=1,billing=1\nSocks/Node=* NtasksPerN:B:S:C=0:0:: CoreSpec=*\nMinCPUsNode=1 MinMemoryNode=1M MinTmpDiskNode=0\nFeatures=(null) DelayBoot=00:00:00\nOverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\nCommand=(null)\nWorkDir=/media/Galaxy2022/data/jobs/000/67\nStdErr=/media/Galaxy2022/data/jobs/000/67/galaxy_67.e\nStdIn=/dev/null\nStdOut=/media/Galaxy2022/data/jobs/000/67/galaxy_67.o\nPower=\n-------------------groupvars-------------------\n#Job_config\nGalaxy Job Configuration\ngalaxy_job_config:\nrunners:\nlocal_runner:\nload: galaxy.jobs.runners.local:LocalJobRunner\nworkers: 4\nslurm:\nload: galaxy.jobs.runners.slurm:SlurmJobRunner\ndrmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1\nhandling:\nassign: [\u2018db-skip-locked\u2019]\nexecution:\n#default: local_env\n#default: singularity\ndefault: slurm\nenvironments:\nlocal_env:\nrunner: local_runner\ntmp_dir: true\nslurm:\nrunner: slurm\nsingularity_enabled: true\nenv:\n- name: LC_ALL\nvalue: C\n- name: SINGULARITY_CACHEDIR\nvalue: /tmp/singularity\n- name: APPTAINER_TMPDIR\nvalue: /tmp\nsingularity:\nrunner: local_runner\nsingularity_enabled: true\nenv:\n# Ensuring a consistent collation environment is good for reproducibility.\n- name: LC_ALL\nvalue: C\n# The cache directory holds the docker containers that get converted\n- name: APPTAINER_CACHEDIR\nvalue: /tmp/singularity\n# Apptainer uses a temporary directory to build the squashfs filesystem\n- name: APPTAINER_TMPDIR\nvalue: /tmp\ntools:\n- class: local # these special tools that aren\u2019t parameterized for remote execution - expression tools, upload, etc\nenvironment: local_env\n#Slurm configuration\nSlurm\nslurm_roles: [\u2018controller\u2019, \u2018exec\u2019] # Which roles should the machine play? exec are execution hosts.\nslurm_nodes:\n\nname: galaxyworkstation # Name of our host\nCPUs: 32         # Here you would need to figure out how many cores your machine has. For this training we will use 2 but in real life, look at htop or similar.\nSockets: 2\nCoresPerSocket: 8\nThreadsPerCore: 2\nslurm_config:\nSlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2\nSelectType: select/cons_res\nSelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node\n\n------------journalctl while running job--------------------------\n#Journalctl when running job\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools INFO 2023-07-04 16:46:04,383 [pN:main.2,p:13025,tN:WSGI_0] Validated and populated state for tool request (15.737 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,398 [pN:main.2,p:13025,tN:WSGI_0] Handled output named out_file1 for tool secure_hash_message_digest (2.066 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,414 [pN:main.2,p:13025,tN:WSGI_0] Added output datasets to history (15.295 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.actions INFO 2023-07-04 16:46:04,417 [pN:main.2,p:13025,tN:WSGI_0] Setup for job Job[unflushed,tool_id=secure_hash_message_digest] complete, ready to be enqueued (2.917 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.execute DEBUG 2023-07-04 16:46:04,417 [pN:main.2,p:13025,tN:WSGI_0] Tool secure_hash_message_digest created job None (29.101 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.web_stack.handlers INFO 2023-07-04 16:46:04,464 [pN:main.2,p:13025,tN:WSGI_0] (Job[id=71,tool_id=secure_hash_message_digest]) Handler \u2018default\u2019 assigned using \u2018db-skip-locked\u2019 assignment method\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: galaxy.tools.execute DEBUG 2023-07-04 16:46:04,481 [pN:main.2,p:13025,tN:WSGI_0] Created 1 job(s) for tool secure_hash_message_digest request (97.784 ms)\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:04,499 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cPOST /api/tools HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,539 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:45:27.511849 HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,607 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&limit=1000&q=update_time-ge&qv=2023-07-04T14:45:30.441Z&details=ac07c226041e4298,efb7f17bc8d9ab0f,d166293c8b6f10d0 HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:04,674 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/users/dba0b16b7fc2d9dd HTTP/1.0\u201d 200\nJul 04 16:46:04 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:04,741 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&order=hid&offset=0&limit=100&q=deleted&qv=false&q=visible&qv=true HTTP/1.0\u201d 200\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler DEBUG 2023-07-04 16:46:05,052 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.mapper DEBUG 2023-07-04 16:46:05,130 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: slurm\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler DEBUG 2023-07-04 16:46:05,154 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to slurm runner\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,218 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: slurm)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,227 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /media/Galaxy2022/data/jobs/000/71\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners DEBUG 2023-07-04 16:46:05,260 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] Job [71] queued (104.998 ms)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.handler INFO 2023-07-04 16:46:05,266 [pN:handler_0,p:11617,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs DEBUG 2023-07-04 16:46:05,392 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Job wrapper for Job [71] prepared (107.937 ms)\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver] found description [None]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [ExplicitSingularityContainerResolver] found description [None]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2023-07-04 16:46:05,393 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Image name for tool secure_hash_message_digest: python:3.7\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.tool_util.deps.containers INFO 2023-07-04 16:46:05,395 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Checking with container resolver [CachedMulledSingularityContainerResolver[cache_directory=/srv/galaxy_ans/var/container_cache/singularity/mulled]] found description [ContainerDescription[identifier=/srv/galaxy_ans/var/container_cache/singularity/mulled/python:3.7\u20131,type=singularity]]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.command_factory INFO 2023-07-04 16:46:05,427 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] Built script [/media/Galaxy2022/data/jobs/000/71/tool_script.sh] for tool command [python \u2018/srv/galaxy_ans/server/tools/filters/secure_hash_message_digest.py\u2019 --input \u2018/media/Galaxy2022/data/datasets/f/7/f/dataset_f7fbfb00-7fd5-49dd-985a-c246018f4e3f.dat\u2019 --output \u2018/media/Galaxy2022/data/jobs/000/71/outputs/galaxy_dataset_0f3d2303-4e23-4ec1-a255-9b77d77ecee1.dat\u2019 --algorithm \u201cmd5\u201d]\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners DEBUG 2023-07-04 16:46:05,493 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) command is: mkdir -p working outputs configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: if [ -d _working ]; then\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]:     rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: else\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]:     cp -R working _working; cp -R outputs _outputs; cp -R configs _configs\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: fi\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: cd working; SINGULARITYENV_GALAXY_SLOTS=$GALAXY_SLOTS SINGULARITYENV_GALAXY_MEMORY_MB=$GALAXY_MEMORY_MB SINGULARITYENV_GALAXY_MEMORY_MB_PER_SLOT=$GALAXY_MEMORY_MB_PER_SLOT SINGULARITYENV_HOME=$HOME SINGULARITYENV__GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR SINGULARITYENV__GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR SINGULARITYENV_TMPDIR=$TMPDIR SINGULARITYENV_TMP=$TMP SINGULARITYENV_TEMP=$TEMP singularity -s exec --cleanenv -B /srv/galaxy_ans/server:/srv/galaxy_ans/server -B /srv/galaxy_ans/server/tools/filters:/srv/galaxy_ans/server/tools/filters -B /media/Galaxy2022/data/jobs/000/71:/media/Galaxy2022/data/jobs/000/71 -B /media/Galaxy2022/data/jobs/000/71/outputs:/media/Galaxy2022/data/jobs/000/71/outputs -B /media/Galaxy2022/data/jobs/000/71/configs:/media/Galaxy2022/data/jobs/000/71/configs -B \u201c$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR\u201d -B \u201c$TMPDIR:$TMPDIR\u201d -B \u201c$TMP:$TMP\u201d -B \u201c$TEMP:$TEMP\u201d -B \u201c$_GALAXY_JOB_HOME_DIR:$_GALAXY_JOB_HOME_DIR\u201d -B /media/Galaxy2022/data/jobs/000/71/working:/media/Galaxy2022/data/jobs/000/71/working -B /media/Galaxy2022/data/datasets:/media/Galaxy2022/data/datasets -B /srv/galaxy_ans/var/tool_data:/srv/galaxy_ans/var/tool_data -B /srv/galaxy_ans/var/tool_data:/srv/galaxy_ans/var/tool_data --home $HOME:$HOME /srv/galaxy_ans/var/container_cache/singularity/mulled/python:3.7\u20131 /bin/bash /media/Galaxy2022/data/jobs/000/71/tool_script.sh > \u2018\u2026/outputs/tool_stdout\u2019 2> \u2018\u2026/outputs/tool_stderr\u2019; return_code=$?; echo $return_code > /media/Galaxy2022/data/jobs/000/71/galaxy_71.ec; cd \u2018/media/Galaxy2022/data/jobs/000/71\u2019;\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: [ \u201c$GALAXY_VIRTUAL_ENV\u201d = \u201cNone\u201d ] && GALAXY_VIRTUAL_ENV=\u201c$_GALAXY_VIRTUAL_ENV\u201d; _galaxy_setup_environment True; python metadata/set.py; sh -c \u201cexit $return_code\u201d\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:05,501 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) submitting file /media/Galaxy2022/data/jobs/000/71/galaxy_71.sh\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa INFO 2023-07-04 16:46:05,505 [pN:handler_0,p:11617,tN:SlurmRunner.work_thread-2] (71) queued as 24\nJul 04 16:46:05 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:05,801 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) state change: job is running\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,796 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:46:04.427855 HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,846 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&limit=1000&q=update_time-ge&qv=2023-07-04T14:46:04.555Z&details=ac07c226041e4298,efb7f17bc8d9ab0f,d166293c8b6f10d0 HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:07,908 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/users/dba0b16b7fc2d9dd HTTP/1.0\u201d 200\nJul 04 16:46:07 galaxyworkstation galaxyctl[13017]: uvicorn.access INFO 2023-07-04 16:46:07,979 [pN:main.1,p:13017,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/histories/7ccd42b23e9772e4/contents?v=dev&order=hid&offset=0&limit=100&q=deleted&qv=false&q=visible&qv=true HTTP/1.0\u201d 200\nJul 04 16:46:08 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:08,353 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /api/entry_points?running=true HTTP/1.0\u201d 200\nJul 04 16:46:10 galaxyworkstation galaxyctl[13025]: uvicorn.access INFO 2023-07-04 16:46:10,938 [pN:main.2,p:13025,tN:MainThread] 141.39.152.83:0 - \u201cGET /history/current_history_json?since=2023-07-04T14:46:05.984307 HTTP/1.0\u201d 200\nJul 04 16:46:11 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.drmaa DEBUG 2023-07-04 16:46:11,110 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) state change: job finished, but failed\nJul 04 16:46:11 galaxyworkstation galaxyctl[11617]: galaxy.jobs.runners.slurm WARNING 2023-07-04 16:46:11,125 [pN:handler_0,p:11617,tN:SlurmRunner.monitor_thread] (71/24) Job failed due to unknown reasons, job state in SLURM was: FAILED"
    },
    {
      "role": "user",
      "text": "After adding the cgroup patch and updating all packages, as described by Nate Coraor, it suddendly worked. I put the update tasks into a new playbook, which looked like this:\n\n\nhosts: galaxyworkstation\nbecome: true\nbecome_user: root\npre_tasks:\n\n\nname: Update repos\napt:\nupdate_cache: yes\ncache_valid_time: 900\ntags:\n\nupgrades\n\n\n\nname: Upgrade packages\napt:\nupgrade: yes\nautoremove: yes\nnotify:\n\nreboot\ntags:\nupgrades\n\n\n\nname: Install other packages\npackage:\nname:\n- acl\n- bc\n- sudo\n- make\n- build-essential\n- git\n- nano\n- vim-nox\n- vim-pathogen\n- emacs-nox\n- virtualenv\n- python3-pip\n- jq\n- htop\n- zlib1g-dev\n- libbz2-dev # planemo \u2192 samtools\n- liblzma-dev # planemo \u2192 samtools\n- tree\n- byobu\n- screen\n- cockpit # web console\n- moreutils # for gat-cli\n- silversearcher-ag # For @hexylena\nstate: latest\ntags:\n- packages\n\n\nname: Ensure unnecessary stuff is purged\npackage:\nname:\n- python\n- fail2ban\n- snapd\n- emacs-lucid\n- emacs-gtk\nstate: absent\n\n\nhandlers:\n\nname: update-grub\ncommand: /usr/sbin/update-grub\nname: reboot\nreboot:\n\n\n\nI hope this might help anyone at some point :slight_smile: Thanks for the help!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, How do i get more disk space. I have exceeded my allocated disk space for my analysis."
    },
    {
      "role": "system",
      "text": "The first step you can follow is to try to free up some space from your account, and here are a few good practices for data management in Galaxy (@link https://galaxyproject.org/support/account-quotas/) and a tutorial explaining how to delete data from Galaxy (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/download-delete-data/tutorial.html#deleting-data-from-galaxy).\nAfter that, if you still need more space, you can apply for a quota extension (@link https://docs.google.com/forms/d/e/1FAIpQLSf9w2MOS6KOlu9XdhRSDqWnCDkzoVBqHJ3zH_My4p8D8ZgkIQ/viewform) for a limited time period."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI recently installed Galaxy release_21.09 on Ubuntu 21.04. as described in the \u201cGet Galaxy\u201d tutorial and am now following the \u201cAdding custom tools tutorial\u201d (Adding custom tools to Galaxy - Galaxy Community Hub (@link https://galaxyproject.org/admin/tools/add-tool-tutorial/)). The first problem is that at Step 4 no tool_config.xml exists in the /config folder. The only files that exist are tool_config.xml.sample and tool_config.xml.main. After adding the section to either of these files, the tool does not show up in galaxy after >sh run.sh.\nI also tried creating a new file tool_config.xml and copying tool_config.xml.[sample/main] to tool_config.xml and also to edit the galaxy.yml #tool_config_file accordingly. The tool never shows up.\nI also ran >sh run.sh --daemon to check the log, but it does not contain any errors regarding tools.\nDoes anyone know what I am doing wrong or could it be that the tutorial I am using is not up to date?\nBest,\nLeander"
    },
    {
      "role": "system",
      "text": "I can\u2019t fully debug it now but you could try to stop galaxy and remove the file galaxy/config/integrated_tool_panel.xml and restart it. On top of my head you need to rename tool_config.xml.sample to tool_config.xml.\nIt is also possible that you need to clear the cache of your browser."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nWhen I use Galaxy every time getting same error, was thinking it\u2019s something temporary, but unfortunately it\u2019s not.\nCould you please help me and recommend something?\n(admin redacted for data privacy)\n\nerror1589\u00d7935 172 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/3ac8c67588f5fa93dd8a2e4d9c2132f6a745c545.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nIt looks like you clicked on the pencil icon :memo: of a dataset to reach the \u201cEdit Attributes\u201d functions. The server is unable to load that data, possibly because of a metadata problem or some server side problem. You could try making a copy of the history in your own account there to see if that resets the metadata. If not, you\u2019ll need to contact an administrator.\nThe administrators of the @link https://galaxyproject.org/use/repeatexplorer/ public Galaxy server do not follow this forum, and it is difficult for others to help more, especially for any actual technical server-side problems. Instead, please try contacting them directly via email.\nQuote from there website:\n@quote\nIf you need help with RepeatExplorer or you want to report a problem and our wiki is not able to give you answers, please contactserver administrator."
    }
  ],
  [
    {
      "role": "user",
      "text": "hello\ni was redirected to this support page while tryin to log in into galaxy galaxian @link https://bioinf-galaxian.erasmusmc.nl/galaxy/\nI have an accoutn for galaxy but i not recognozed on the galazxian\nthanks\nirene"
    },
    {
      "role": "system",
      "text": "Every Galaxy instance has independent user database. User account from @link http://usegalaxy.org will not work on @link https://bioinf-galaxian.erasmusmc.nl/galaxy/. You have to create instance specific account."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Support,\nI ran the ChIP-Seq data analysis tutorial.\nI got error in Step 6: Plot the signal between samples (In attached picture).\n\nConcatenate two datasets into one dataset -Got Output\nSortBED  -Got Output\nMergeBED -Got Error\n\nI was still able to perform\n4. computeMatrix (tried with output of Concatenate)\n5. plotHeatmap\nI have attached pictures of the steps and output pictures.\nCould anybody try the tutorial and give advice on my issue?\nThanks in advance.\nJP\n\nI have detailed the issue in the attached picture.\nTutorial Source: @link https://galaxyproject.github.io/training-material/topics/chip-seq/tutorials/formation_of_super-structures_on_xi/tutorial.html\n\nimage.png935\u00d7665 261 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/80437272c46f13ac6890ad80be06d5916172d50e.png)\nimage\nimage\nimage"
    },
    {
      "role": "system",
      "text": "the \u201c29. SortBed\u2026\u201d dataset appears to not be sorted properly; this seems to be verified by the fragment of error message I can make out:\n\u201cError: Sorted input specified, but\u2026\u201d\nthen it lists the second entry in \u201c29.\u201d \u2013 whose Start is sooner than the preceding data-point (ie, not in ascending order)\nIs it possible you accidentally sorted by the wrong value? The tutorial simply says \u201cSort the following bed,bedgraph,gff,vcf file\u201d so it expects the default sort parameter \u201cSort by: chromosome, then by start position (asc)\u201d"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI tried using the ribogalaxy bowtie tool to remove ncRNA however, after the job was run I couldn\u2019t find any new files other than the mapping statistics for my data.\nThe mapping statistics said that 98% of the reads had not aligned (mean 2% were ncRNA) but then there was no new file in my history.\nWould anyone be able to help me?\nThanks!\nJanhavi"
    },
    {
      "role": "system",
      "text": "Cross-posted here: Contact RiboGalaxy Support (@link https://help.galaxyproject.org/t/contact-ribogalaxy-support/8288)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everybody,\nI have been getting this error message multiple times now since I queued a lot of jobs in the Trim Raw Reads tool. I get the same message when trying to purge my deleted data sets: \nserver error1915\u00d71060 145 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/76e56652b62ab4eb6a56f66501403f38c6afb49a.png)\nShould I just wait and see if the problem persists or can I reset somehow?\nThank you for your help.\nBest,\nSven"
    },
    {
      "role": "system",
      "text": "@quote\nSo I guess I should contact the people running that server then?\nYes, the administrators of that server would be the people for you to contact.\n@link https://galaxyproject.org/use/cafu/\nThey can always reach out to the Galaxy Admin working group if they need help beyond what the docs cover. Gitter would be a good place to start for them: galaxyproject/admins - Gitter (@link https://gitter.im/galaxyproject/admins)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am doing a de novo assembly with Trinity. It was running for 14 days and then ended with no error (the job box is green), but both the assembly and the gene to transcripts map are empty (0 bytes).\nThe only \u201cerror\u201d I see in the output is this:\nSequence: CCAATATTAAGCCACGATTAAGGAis smaller than 25 base pairs, skipping\nrepeating for a bunch of sequences, and nothing else.\nThe input files are paired-end, RF stranded, Trimmomatic output files, not interleaved. I used them for assembly with Trinity in the same history almost a year ago, and everything worked perfectly.\nDoes anyone have an idea as to what could be causing this?\nThanks!"
    },
    {
      "role": "system",
      "text": "@quote\nThe only difference in this run is that I put all of my samples for one assembly instead of separating them into two different assemblies like the last time. That comes up to around of 500M reads, so it could be, as you say, that the job was too large.\nHi @user\nThanks for explaining. Combining more reads into a single assembly can definitely impact runtime and performance.\n500M reads in the same assembly is about 10 times the size of datasets the authors used for benchmarking statistics: Trinity Memory Usage (@link https://trinityrnaseq.github.io/performance/mem.html). Assembly can be impacted for scientific content reasons (depth, coverage, redundancy) \u2013 but I don\u2019t think that is the root issue right now. The issue is simply volume versus resources.\nWhat I would suggest is to consider downsampling the reads before assembly. One tool choice is Sektk. That way you can include all samples together but not run up against computational issues. There is much discussion about assembly strategies at the different bioinformatics forums, including here: @link https://groups.google.com/g/trinityrnaseq-users.\n@quote\nCould you please also tell me how to start a private chat to share my history?\nI\u2019ll start up a direct message. But \u2026 I don\u2019t think that reviewing your history right now will change the advice. It sounds like the reads are in the correct technical format, can pass through QA steps, and will assemble when in a smaller batch.\nAt this forum, a new user\u2019s trust level will increase with time and activity, and the option to start up a direct message will become available. The goal is to try to keep initial questions and general discussion/troubleshooting public. This helps us to build up the knowledge base and helps you to reach more potential helpers :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "What does this error mean?\nncol(countData) == nrow(colData) is not TRUE"
    },
    {
      "role": "system",
      "text": "Hi - Thanks for the extra information!\nThe problem is with the format of the count input datasets. Each sample needs to be run through a supported count tool and entered individually under Factor/Factor levels. Details are on the DEseq2 tool form.\nSince your data has the counts in a matrix, that won\u2019t work with DEseq2 as currently wrapped for Galaxy.\nOptions:\nTo use DESeq2, I would suggest generating the counts within Galaxy. Or, you could run one of your samples in Galaxy, compare those results to your per-sample counts generated outside of Galaxy (individual counts, before merged into a matrix), and make changes to sync up the formatting as needed.\nConsider alternative tools. Limma/EdgeR do support count matrix input. How to use each is described on their tools forms with example data input content/format.\nLimma DE (with matrix input) is included in this provisional Galaxy Training Network (GTN) Tutorial: @link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html\nNote:  At this time, that tutorial still under development. Once the tutorial finalized, it will be listed here: @link https://galaxyproject.github.io/training-material/topics/transcriptomics/. Feedback to the GTN about it would be welcomed to help us improve it \u2013 find the feedback form at the end of the tutorial. If you want to use all the tools in the tutorial, those might not be all at Galaxy Main @link https://usegalaxy.org server yet \u2013 but I\u2019m fairly certain the tools are at Galaxy EU @link https://usegalaxy.eu server (the server used for development of the tutorial) \u2013 and if not for some reason, that could be part of your feedback."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am using Windows10 and trying to upload some bam.files to Galaxy Australia FTP using fileZilla.\nI have been following the steps according to the tutorial video from this site: Galaxy FTP Upload - Galaxy Community Hub (@link https://galaxyproject.org/ftp-upload/)\nHowever, I always end up getting the following error messages:\nError:|Connection timed out after 20 seconds of inactivity|\nError:|Could not connect to server|\nAnyone know how to fix it ?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi, upload by ftp is not available at Galaxy Australia for some time. Files in GB range can be uploaded using web interface. Alternatively, upload files using URLs. Australian users can import data from CloudStor."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run the pipeline from this Clustering 3K PBMCs with Scanpy (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-scanpy-pbmc3k/tutorial.html) on my own data from 10x genomics but when I try to run quality control as well as plotinf functions on the annotated data matrix I get the error that the variable names are not unique. Is there anyway to make them unique in galaxy rather than using Python?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas Pavan suggested you, you can use Manipulate Annadata tool (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/anndata_manipulate/anndata_manipulate/0.7.5+galaxy1) with \u201cFunction to manipulate the object\u201d \u2192 \u201cMakes the var index unique by appending \u20181\u2019, \u20182\u2019, etc\u201d.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI trimmed my paired datasets for adapters using Cutadapt. However, the output of the Cutadapt is not visible if I want to run HISAT2 subsequently. RNA STAR can easily use detect Cutadapt datasets for further processing. I can circumvent this problem by generating a new list of dataset pairs, but I assume there must be a faster way, because, when building a new list of dataset pairs, I also need to edit the pair name in order to keep a better overview further down the pipeline. Am I overlooking something?"
    },
    {
      "role": "system",
      "text": "Update 4/22/21\nRe-tested with the prior test, plus created a new extracted workflow and reran that. Everything is working.\nI had forgotten that Cutadapt will split a paired-end collection into two collections: one contains the forward reads, and one contains the reverse reads. Maybe this is the problem you had? And this example usage will help you to sort out the problems you were having? The end result from HISAT2 is a single collection.\nThe data is a bit large in the history, but you don\u2019t need to actually import it \u2013 use the \u201cview\u201d function, all the details are available.\nThe workflow is very simple and doesn\u2019t consume any extra quota space, so importing and perhaps using it or reviewing it in the editor will help even more. You\u2019ll see that a single collection of paired-end reads was input to Cutadapt, then resulting collections of the forward and reverse reads were input to HISAT2, producing one final collection of mapped BAMs (one per pair).\nHistory: @link https://usegalaxy.org/u/jen-galaxyproject/h/copy-of-test-cutadapt-listpaired--hisat2\nWorkflow: Galaxy | Accessible Workflow | Workflow constructed from history 'Copy of 'test cutadapt listpaired' + hisat2' (@link https://usegalaxy.org/u/jen-galaxyproject/w/workflow-constructed-from-history-copy-of-test-cutadapt-listpaired--hisat2)\n\nHi @user\n@quote\nHowever, the output of the Cutadapt is not visible if I want to run HISAT2 subsequently. RNA STAR can easily use detect Cutadapt datasets for further processing.\nThe data is in a paired-end dataset collection with the datasets inside it assigned the datatype fastqsanger.gz, correct? The output from CutAdapt?\nHISAT2 should be able to recognize that dataset collection and process it directly.\nThe collection should be in the active history, with this option set on the HISAT2 tool form. Note: The screenshot is from loading the tool with a new, empty history that does not contain inputs of the proper datatype. It is a good way of discovering what the appropriate/recognized inputs are. When you run this, the input should be populated with one or more paired collections assigned one of those specified datatypes. Compressed will be recognized and uncompressed automatically as part of the pre-processing.\n\nhisat2-paired-collection-input1914\u00d7286 25.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9f944ed01d5a212f5756e838d8e756161b00720a.png)\nIf you do not get a result like that, something is going wrong, and we can follow up on that. Rebuilding the collection or unhiding collection elements should not be needed (although they both are valid workarounds for this problem!). Let\u2019s try to fix it at the source. Cutadapt had some issues with Workflows in the past \u2013 but am pretty sure those are resolved now (most current Galaxy release + most current tool version).\nFew questions:\n\n\nWhere are you using Galaxy? URL if a public server. And are you using the most current tool versions available there? We can follow up from there \u2013 an example might be good to review (directly, not publically) \u2013 if needed, we\u2019ll explain how.\n\n\nIf you own Galaxy (local, cloud), are you upgraded to the latest Galaxy version? Are the tools updated from the ToolShed? Has this worked before or it is a new problem? If using a Workflow, did this stop working after changes (not necessarily these steps)? You might even want to check to see if some sample of this works at one or any of the UseGalaxy.* servers (as a comparison).\n\n\nLet\u2019s start there. This was an issue before, so seems odd to have it come up again. I\u2019ll also review/rerun our prior test cases at @link http://UseGalaxy.org and will write back with that result.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, my name is Julia. Im new to Galaxy NGS and want to edit my Fastq files: I would like to crop bases from the beginning of the read and from the end of the read. As far as Ive seen, there are only fastq tools, which allow me to crop fastq files depending on their quality scores.\nI want to crop fastq files independently from the quality scores.\nThanks for your help in advance!\nJulia"
    },
    {
      "role": "system",
      "text": "You can use the \" Trim sequences\" tool to trim from the beginning and end of the read. So e.g. if you have 100 bp reads, and set \u201cFirst based to keep\u201d to 10 and \u201cLast base to keep\u201d to 85\" you will end up with 75 bp reads - from the 10th base to the 85th base.\nI hope this helps."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI run a private Galaxy instance and a toolshed. I recently upgraded from 19_05 to 19_09 and also moved to python3. Since this upgrade, my tools from the local toolshed disappeared from the \u2018Tools\u2019 pane. All tools from my toolshed were contained with a separate target section within the Tools pane, and that section header has disappeared from the user interface too. Within the Galaxy admin interface the tools are still shown as installed. If I uninstall and the install a tool from my local toolshed (and re-enter the target section), then the tools return to the Tools pane. However, when I try to run these tools I get an error:\nInstance <ToolShedRepository at 0x7f4b5c113610> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: @link http://sqlalche.me/e/bhk3)\nThere are some tickets on the galaxy github repo about this error message, but they have all been resolved through commits that I believe I have pulled in (@link https://github.com/galaxyproject/galaxy/pull/8933) or the posted work around does not resolve the issue (@link https://github.com/galaxyproject/galaxy/issues/9074).\nAny help getting past this error would be appreciated.\nthanks,\n-Will"
    },
    {
      "role": "system",
      "text": "There is an important section in the 19.09 release notes about tool configuration files: @link https://docs.galaxyproject.org/en/master/releases/19.09_announce.html#tool-configuration-file-handling-changes\nAre your tools from local toolshed perhaps loaded from a separate configuration file? That could be one of the causes I can think of (and you\u2019d need to change Galaxy\u2019s configuration as described in the above linked document)\nAs of the is not bound to a Session error, we believe we have fixed that, but it took plenty of PRs over time, please make sure you are running the latest commit of  the release_19.09 branch."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m doing some basic de novo transcriptome analysis and have found that while I\u2019m assembling transcripts in Stringtie, my downstream jobs such as running stringtiemerge, gffcompare, and featurecounts all finished even though I still had some original Stringtie jobs going\u2026\nAnd then DESeq2 fails to run. This doesn\u2019t seem right, or even possible. Does anyone have any insight on this?"
    },
    {
      "role": "system",
      "text": "Related Q&A Stringtie not running (@link https://help.galaxyproject.org/t/stringtie-not-running/7487)\nIt seems that the problem was some setting in the workflow used."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to import a Dataset from User History\nIn the Datasets dropdown list, from History item is visible but from User History does not show up for a non-admin user.\nFor an admin of the Galaxy instance,  from User History is present.\nThere is no error in the galaxy log.\nIn galaxy.yml, the library_import_dir and user_library_import_dir have been configured (they point to the same directory).\nA subdirectory has been manually created under user_library_import_dir for the non-admin user with rwx permissions.\nI\u2019ve noticed that in $GALAXY_ROOT/client/src/components/LibraryFolder/TopToolbar/FolderTopBar.vue\nuser_library_import_dir is set to false (function data()). Is this related to my problem?\nThanks a lot,\nJin"
    },
    {
      "role": "user",
      "text": "Actually this is a bug in v20.09 which is described in @link https://github.com/galaxyproject/galaxy/issues/10982\nand fixed in later commits."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have searched through all the threads on this topic and I have yet to find a reasonable explanation for why I continue to encounter this error. I have already run Cufflinks on my samples, but when I attempt to run Cuffmerge (cuffmerge -p 8 -g /scratch/sbag/Tikshana/BANANA/RAW/Mac.gff3 -s /scratch/sbag/Tikshana/BANANA/RAW/Mac_genome.fa /scratch/sbag/Tikshana/BANANA/cuff/assemblies_Macbal.txt -o /scratch/sbag/Tikshana/BANANA/cuff/Macbal_merge), I get the following error:\nGFF Error: duplicate/invalid \u2018transcript\u2019 feature ID=Ma03_t01040.3\n[FAILED]\nError: could not execute cuffcompare\ni could not understant how to resolve can you please explain me clearly."
    },
    {
      "role": "user",
      "text": "Thank you all for your kind help. I simply want to suggest the future that if this sort of error will occur just check whether your input file is correct or not go through each and every location and check the path as well as your file name. Just keep everything simple do not complex anything. This is what I resolved these days. A small mistake in your command may hamper your time and experiment.\nIn short my bam file in samtools was not created properly so I run all command again and checked my path that resolved my issue/error. If you face the same just go through it make it SIMPLE"
    }
  ],
  [
    {
      "role": "user",
      "text": "Helllo\nI\u2019m trying to run fastQC on my fastq.gz file. The FastQC job itself seemed successful,and there was no error report. All dependencies were done. But there was no image in the webpage report like this:\n\nScreenshot from 2020-10-15 09-43-581280\u00d71024 293 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/fce6236b8db15442ebee7346de6a72f9ae18b980.png) \nScreenshot from 2020-10-15 09-44-091280\u00d71024 168 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d75aeccba82771f566104f5b1ca9a4eaa97be4ba.png) \nScreenshot from 2020-10-15 09-44-141280\u00d71024 156 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/97eda29ed69a6069bccefb6ee32e3d268ac59611.png) \nScreenshot from 2020-10-15 09-44-191280\u00d71024 156 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/701bf067fb726a770ef50cc64d9bb115c78e14ed.png) \nScreenshot from 2020-10-15 09-44-251280\u00d71024 160 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/440bf9f31b7fae66e586fc49efaf1bc628499cda.png)\nWhat can i do to fix it?\nthank you"
    },
    {
      "role": "system",
      "text": "You need to whitelist the fastqc tool first. This is because of security.\nGo to the admin page, in the menu on the left you will see \u201cmanage whitelist\u201d. On this page you can white list the fastqc tool. And you will see images after that."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m having some troubles understanding what I\u2019ve done wrong.\nI want to assembly a transcriptome from different samples retrieved from the NCBI. Since I had several forward and reverse reads I used the concatenate tool to obtain one dataset for all the forward reads and one for all the reverse reads. At this point I\u2019ve built a collection (with the build dataset pair function, named:  All sequences-cleaned ) and I\u2019ve performed trimmomatic and FASTQC (both ran well).\nAfter that I ran Trinity with these parameters:\n|Are you pooling sequence datasets?|No|\n|Paired or Single-end data?|unmerged_paired_collection|\n|FASTA/FASTQ dataset collection with R1/R2 pair| 142: All sequences-cleaned\nThis gave me this error:\nExecution resulted in the following messages:\nFatal error: Exit code 2 ()\n\nDetected Common Potential Problems\nThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\nCan someone help me to fix this problem?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nDo you mean total size of gzipped FASTQ files is 50 GB? It might be too big, but you need a feedback from the server admin. I assume, in silico normalization of reads was enabled. Maybe consider using khmer: Normalize By Median on merged files before Trinity.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI used to run the galaxy-stable container (on Ubuntu) with an export folder mapped to an external drive. Now, I have moved to a different computer, attached the external drive and tried to start the container mapping the same folder as the export folder. However, the container does not start and the postgresql database does not initialize (spawn error, \u201ccould not connect to database\u201d). Can anyone help?\nThanks!"
    },
    {
      "role": "system",
      "text": "(Have also replied in the Galaxy Gitter - bgruening/docker-galaxy-stable - Gitter (@link https://gitter.im/bgruening/docker-galaxy-stable?at=61e742e5e1a1264f0a62b945))\nHaving a quick read of the problem, I\u2019m going to assume you\u2019re moving from one Ubuntu machine to another one. My gut feeling is that there might be some underlying differences between the machines and the way the external drive is mounting.\nCheck that you can read/write into other folders on your external drive on the new machine. If not, it sounds like an I/O error and you\u2019ll need to have a dig into the mounting side of things.\nOtherwise, see if you can read/write into the external drive, log into the container and make sure you can run some basic bash commands. If you can do that, make sure PostGreSQL fires up and you can create new databases.\n(Sorry I can\u2019t dig into the problem any more right now)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m learning single cell RNA data preprocessing\nWhen I try to extract UMI-tools, I get the message \u201cPlease provide a value for this option\u201d in the Reads in FASTQ section, and it does not open any selectable options for me.\nI have attached the photo of this error to you\n\nScreenshot (30)1366\u00d7768 129 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/032806ca2a87e289d9d8f993501e3dedc681a7ab.png)"
    },
    {
      "role": "system",
      "text": "You have a dot character in a uploaded file name, this may be the cause. You can click the pencil (edit) icon and change the datatype to fastq. But try not to put a dot in a filename unless it is before the file extension."
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to convert a 23andme tsv file to vcf using bcftools convert to vcf. Building a lesson for UG class.\nI get this error: Fatal error: Exit code 255 ()\n\u2013tsv2vcf requires the --samples option\npython: : Unknown error 59090864\nCan someone specify proper options to get the bcf conversion to work?"
    },
    {
      "role": "system",
      "text": "To use the Galaxy wrapped BCFtools convert to vfc tool/function with tsv input, both the reference genome and sample information need to be specified at runtime. This is noted on the tool form \u2013 highlighted in the screenshot attached below.\nSample names can be entered directly on the form or supplied as a \u201clist\u201d dataset file in a tabular format. Include/exclude are both possible. For help with what a sample content represents and the proper formatting, please see: @link http://samtools.github.io/bcftools/bcftools.html#convert (is linked from the bottom of the tool form).\nbcftools-sample-input\nIf you run into problems even with those entered, double check both of the below are true:\n\n\nThe reference genome fasta has the same exact chromosome identifiers on the \u201c>\u201d title lines as the VCF includes for mapping positions. No extra whitespace or description content + consistently wrapped at 40-80 bases. The tool NormalizeFasta can help to reformat fasta data correctly in most cases and the final formatting is the same as that of a custom genome.\n\n\nThe sample names are exactly the same between the form entry/list and the VCF content. This is a file that you create/upload to Galaxy or by using the Upload tool\u2019s \u201cpaste\u201d function (use the gear icon option to \u201cconvert spaces to tabs\u201d to ensure proper tabular formatting).\n\n\nSome help links:\n\nFormat help for Tabular/BED/Interval Datasets (@link https://galaxyproject.org/support/tabular/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\nPreparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/)\nMismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\n\nHope this works out!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Does anyone know if it\u2019s possible to do a local install of Galaxy on an older iMac? Can only update to OS X 10.13 and have about 150 GB of free space. Can I use an external drive for data? I have lots of space available there\u2026Thanks!!!"
    },
    {
      "role": "system",
      "text": "It may be possible, but you\u2019re likely to encounter numerous issues along the way, and it\u2019s questionable whether the end result will be worth the effort.\nThe one real limiting factor in the end would probably be RAM. I assume your system does not have more than 4GB memory? Then you may be able to get Galaxy running, but most likely you won\u2019t have enough free memory left to do any kind of interesting analyses with it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nWhen trying to use the get method in a Jupyterlab  I see this:\n(base) root@b049c3385e8d:/storage/galaxy/jobs_directory/001/1474/working/jupyter# get -i 2730\n/opt/conda/lib/python3.8/site-packages/bioblend/galaxy/histories/__init__.py:120: FutureWarning: The history_id parameter is deprecated, use the show_history() method to view details of a history for which you know the ID.\n  warnings.warn(\n/import/2730\n\nAnd no file is uploaded.\nWhat would be the places to check to solve this?"
    },
    {
      "role": "user",
      "text": "Yes, so the problem was related to collection datasets. I realized I had to unhide to get the id of each of the items in the collection, as get does not work on a collection.\nRealized by looking closer at the tutorial  JupyterLab in Galaxy (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/jupyterlab/tutorial.html) where it does indeed inform about unhiding hidden datasets."
    }
  ],
  [
    {
      "role": "user",
      "text": "Why have fastqc and trimmomatic been deprecated and what should I use as a replacement?"
    },
    {
      "role": "system",
      "text": "It has been fixed, thanks for reporting it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nCan anyone of you help me config my galaxy server. I gave my server 16 virtual cores in HyperV and found some informations that i have to change some params in galaxy.yml file(galaxy/config). Whats the best way to do my galaxy faster. Now i\u2019ve got\nbuffer-size: 16384\nprocesses:3\nthreads:15\noffload-threads:2\nThanks in advance\nMaciej"
    },
    {
      "role": "system",
      "text": "Yes, in that case, you need to change the job_conf.xml file to give specific resources to your tools. Like BWA should get 6 cores and 16GB of memory for example."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I\u2019m a beginner in Galaxy. I have tried several times, in different days, to run a job with ANNOVAR on biomina, but the job was failed. This message appeared: Job cannot be completed due to a cluster error, please retry it later: Unknown error: 271.\nCan anyone help me?\nTnx!"
    },
    {
      "role": "system",
      "text": "@quote\nbiominavm-galaxy.biomina.be\nThis error can mean that there is an input problem or an actual cluster problem.\nEach public Galaxy server is independently administered. If contact information is not on the home page of the server (is this case it isn\u2019t), that can usually be found here on the server\u2019s page: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\nFor the server @link http://biominavm-galaxy.biomina.be/galaxy/, see: Biomina - Galaxy Community Hub (@link https://galaxyproject.org/use/biomina/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy helper,\nI am trying load some data into galaxy from NCBI however, when I run the SRR numbers most of them only come in as 1 file for single end read but not for the paired end. My data is paired. Here are some examples of how my data is unpacked\n\nScreenshot 2022-01-28 at 18.57.45586\u00d7962 56.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/73ba754ca076f5656e78c8912f9675e61b37e07c.png)\nwhen I go into the single end data it looks like this\nScreenshot 2022-01-28 at 18.57.53\nI am new to galaxy but I assume my FASTQ file should come in like this\n\nScreenshot 2022-01-28 at 19.28.162266\u00d71366 446 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6e2c1985b2a7ff44deea1edc812d9e42d98aeb61.png)\nOnly one of them shows it like this.\nThank you in advance"
    },
    {
      "role": "system",
      "text": "Hi @user\nData source at NCBI: Run Browser : Browse : Sequence Read Archive : NCBI/NLM/NIH (@link https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR8368419)\n\nThe sequencing protocol is described in the Metadata tab (link outs there provide more details)\nThe read types are broken down in the Reads tab\n\nGalaxy Training Network (GTN) tutorials: @link https://training.galaxyproject.org/\n\nTutorials can be searched with keywords or browsed in topic categories.\nExample @link https://training.galaxyproject.org/training-material/search?query=single-cell\n\nExample @link https://training.galaxyproject.org/training-material/search?query=10X\n\n\nThere is a training event in March that you might want to consider joining. Single-cell analysis will be covered.\n\nAll events: Galaxy Event Horizon - Galaxy Community Hub (@link https://galaxyproject.org/events/)\n\nFrom that page find the GTN Smorgasbord2 (Tapas) event: GTN Sm\u00f6rg\u00e5sbord 2: 14-18 March | Gallantries (@link https://gallantries.github.io/posts/2021/12/14/smorgasbord2-tapas/)\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxians. I have obtained Illumina paired-end reads of microbiome metagenomes from a human sample and would now like to remove human (contaminant) sequences from them. I have tried installing Bowtie2 on Anaconda but didn\u2019t get very far as I am new to Anaconda and Bowtie2. I found some instructions at this link (@link https://www.metagenomics.wiki/tools/short-read/remove-host-sequences) for doing this but sadly, I do not know how to implement it. Is it possible to add this function to Bowtie2 in Galaxy? Or is there a tool in Galaxy that can perform a similar function?\nThank you in advance for your help."
    },
    {
      "role": "system",
      "text": "There is Removal of human reads from SARS-CoV-2 sequencing data (@link https://galaxyproject.github.io/training-material/topics/sequence-analysis/tutorials/human-reads-removal/tutorial.html). It\u2019s demonstarting things with SARS-CoV-2 sequencing reads, but try to work through it and it should be rather obvious how to apply this to your data, I hope."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nWhen I implement my workflow, it simply will not run, despite giving the following message:\nSuccessfully invoked workflow  RNAseq_test_singleend .\nYou can check the status of queued jobs and view the resulting data by refreshing the History pane, if this has not already happened automatically.\nNothing appears in the History or any other History (Sharing results to new History = NO). Simply nothing happens.\nThis has been an issue before however there was no user end fix:  @link https://biostar.usegalaxy.org/p/21100/index.html\nHere is my workflow: @link https://usegalaxy.org/u/t_little/w/rnaseqtestsingleend\nAnd my history: @link https://usegalaxy.org/u/t_little/h/rings\nHope both of these are accessible and that the issue can be fixed!\nTi."
    },
    {
      "role": "system",
      "text": "Hello,\nThanks for the description of the issue, the timing, and sharing the example data.\nWorkflow execution was transiently disrupted, please try executing your workflows now. Testing the above workflow with the shared data worked on Main for me.\nLet us know if the problem persists.\nThanks for using Galaxy!\nMo"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have sequenced on an Illumina platform a virus grown in eggs. I want to assemble the virus genome, but the vast majority of the reads in the fastq file are not of the virus. I guess only less than 1% of the reads belong to the virus.\nWhat tool should I use to filter all of the non-target DNA? I assume I can\u2019t start the assembly before that."
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease see this GTN tutorial for an example. That particular protocol uses Bowtie2 to remove reads that align to the non-target genome but other mapping tools are discussed.\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/assembly-with-preprocessing/tutorial.html\n\n\nGalaxy Training: Unicycler assembly of SARS-CoV-2 genome with preprocessin... (@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/assembly-with-preprocessing/tutorial.html)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!  Recently, I was trying to create a gxformat2 Galaxy workflow by hand, and import the workflow file to usegalaxy.org via the import via \u201cArchived Workflow URL\u201d method .  When the file was well-formed, everything went fine.  However, if the file had a syntax error, the import would fail with the following error message:\n\nimage813\u00d7175 6.16 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/37be1a6487d461ed064ae8b4746c29b7b89e33cb.png)\nAlas, this message doesn\u2019t contain enough information to help me correct the problem(s).\nIs there a way to get more details about the errors in a gxformat2 file at import time?\nThanks in advance for the help,\nSteve"
    },
    {
      "role": "system",
      "text": "At this point you\u2019d be best off running gxwf-lint (part of gxformat2 pip install gxformat2) on your hand-written files (or planemo workflow_lint, which should run equivalent checks)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have tried to run RNA STAR with Mus_musculus.GRCm39.108.gtf.gz as input for \u201cGene model (gff3,gtf) file for splice junctions\u201d and it gives error.\nInterestingly I can run RNA STAR with gencode.vM10.annotation.gtf.gz as input for \u201cGene model (gff3,gtf) file for splice junctions\u201d successfully. However, tools like Annotate ID gives an error and Gene IDs can not be converted to Gene Symbols.\nPlease can someone help me with this. Thanks."
    },
    {
      "role": "system",
      "text": "@user\nwhat reference genome do you use for mapping? Gencode.vM10 is for GRCm38 - see GENCODE - Mouse Release M10 (@link https://www.gencodegenes.org/mouse/release_M10.html), while Mus_musculus.GRCm39.108.gtf.gz is for GRCm39 assembly.\nPrecomputed mm39 index is not available on @link http://usegalaxy.org or @link http://usegalaxy.org.au. Do you use Galaxy Europe?\nSTAR jobs can fail for several reasons, and without looking into the input files and job settings I can only guess. Other common issues are different chromosome names in reference genome and annotation. Genomes in Galaxy use \u201cUCSC style\u201d chromosome names (chr1, chr2 etc), while some annotations use 1, 2, etc. These are different text strings. If you use a custom mm39 genome in the history, STAR jobs might fail because of insufficient memory.\nI assume you mean AnnotateMyID. I tested it on a count table made using an old gencode annotation vM4. The annotation is for mm10/GRCm38. In the count table ENSEMBL genes names contain versions (.1, .2 etc), while example of input file in the tool description does not have versions. I stripped versions in the count table by replacing dots with tabular character, basically, split the 1st column into two. This makes the gene names compatible with AnnotateMyIDs. No such issue if you use non-default settings, such as gene_name for gene identifier, just change input type in AnnotateMyID to Symbol.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to set up Galaxy proxying (on our 19.01 instance) with Apache.\nAccording to @link https://docs.galaxyproject.org/en/latest/admin/apache.html, I set up configuration as is :\n\nin httpd.conf :\nProxyPass /galaxydev unix:///tmp/galaxy_dev.sock|uwsgi://\n\nRewriteEngine On\nRewriteRule ^/galaxydev/$ /galaxydev [R,L]\nRewriteRule ^/galaxydev/static/style/(.) /home/galaxydev/galaxy/static/june_2007_style/blue/$1 [L]\nRewriteRule ^/galaxydev/static/(.) /home/galaxydev/galaxy/static/$1 [L]\nRewriteRule ^/galaxydev/favicon.ico /home/galaxydev/galaxy/static/favicon.ico [L]\nRewriteRule ^/galaxydev/robots.txt /home/galaxydev/galaxy/static/robots.txt [L]\n\nin galaxy.yml :\nuwsgi :\nsocket: /tmp/galaxy_dev.sock    ###syntax  unix:///tmp/galaxy_dev.sock is not recognized ???\nmount: /galaxydev=galaxy.webapps.galaxy.buildapp:uwsgi_app()\nmanage-script-name: true\n\ngalaxy :\ncookie_path: /galaxydev\nAccessing \u201cgalaxy_host\u201d/galaxydev with a browser fails. Apache error_log returns this error :\n[Tue Apr 09 11:35:57.850743 2019] [:error] [pid 19307:tid 47544504309504] [client 195.221.174.148:63192] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 11:36:06.537558 2019] [:error] [pid 19211:tid 47544525321984] [client 195.221.174.148:63195] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:13.854039 2019] [:error] [pid 19307:tid 47544514815744] [client 195.221.174.148:63669] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:14.718221 2019] [:error] [pid 19307:tid 47544519018240] [client 195.221.174.148:63670] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:44:14.953882 2019] [:error] [pid 19307:tid 47544523220736] [client 195.221.174.148:63671] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 12:46:12.715991 2019] [:error] [pid 19307:tid 47544529524480] [client 195.221.174.148:63704] AH10097: error parsing URL //: Invalid host/port\n[Tue Apr 09 16:52:12.155767 2019] [:error] [pid 19307:tid 47544542131968] [client 195.221.174.167:59696] AH10097: error parsing URL //: Invalid host/port\nI am stuck with the \u201cURL //: Invalid host/port\u201d error message.\nWould you have any hint ?\nRegards."
    },
    {
      "role": "user",
      "text": "This seems to be related to an apache mod_proxy_uwsgi bug.\nFor those stuck in same situation, I used this workaround :\n\nconfigure uwsgi proxying on a TCP socket (ie uWSGI on a TCP socket)\ncheck apache2/mod_proxy_uwsgi.c and apply following fix if needed : @link https://github.com/unbit/uwsgi/commit/6ce856c9fdb98833f9142f84c92b75f546b32dc2#diff-85cd090aa23e09feb1f28c92ae5aded6\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve been using Filezilla for FTP transfers to the Main and I tried today and no matter what configuration of ports, account information, etc. I attempted, I could not connect to the server. I know that galaxy refuse SFTP requests so it wasn\u2019t surprising that this didn\u2019t work, but I also tried modifying Filezilla\u2019s network configuration and that didn\u2019t work either. I tried Cyberduck and I tried reinstalling the client and neither worked. I\u2019m on MacOS 11.2.2 and my firewalls are turned off. I just updated to 11.2.2 a couple days ago and am wondering if this might have something to do with this issue. Regardless, I tried everything I could find online to fix the problem and came up empty handed, so I figured I\u2019d bring this up here. Any help is appreciated.\nNick"
    },
    {
      "role": "system",
      "text": "A Galaxy team member restore the FTP connection after a system scheduled maintenance. Now is working."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Team,\nI would like to bring to your notice that the command RNA STAR is not running and the status says \u201cThis job is waiting to run\u201d for almost 18 hours by now. How can I fix this?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nFor the jobs at @link http://usegalaxy.org:\n\n\nWhen working at any public Galaxy server, you are using a shared resource. That means that some of your jobs will run, then some of other people\u2019s jobs will run, then more of yours, repeat. During the training last week, all servers had extra limits on concurrent jobs plus the usegalaxy.* servers overall were very busy. This week everything is back to normal but still busy, and processing a large number of jobs will still take time.\n\n\nFor the RNA STAR jobs that already failed, that is due to an input problem. The reference GTF used is labeled with Ensembl chromosome names. The built-in rn5 index is sourced from UCSC and uses a different chromosome naming format/label. The mismatch triggers a failure.\n\n\nTry using a GTF from UCSC instead. The Q&A below discusses where to get a human reference annotation in GTF format, but the URL to the rat annotation is similar (replace \u201chg38\u201d with \u201crn5\u201d). The same is true for every genome with an annotation track that is hosted by UCSC and indexed in Galaxy.\n@quote\nThehg38reference annotationGTFis now best sourced from UCSC. Not from the Table Browser but from theDownloads area here. There are a few Gene tracks to choose from. You can review what each of those represents then upload the selectedGTFto Galaxy with theUploadtool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\nThe solution is\n\nget the correct GTF into Galaxy\nrerun all of your jobs using that GTF instead\n\nThose new jobs will execute as resources become available. The jobs already queued up will also fail, so be sure to purge those to clear them from your queue.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Our local galaxy production server is facing issue of downloading file larger than 1 GB.\nGalaxy version is 20.01\nwe have tried downloading through wget | curl options and tried to change nginx parameters.\nPlease assist."
    },
    {
      "role": "system",
      "text": "Did you also used the  try x-accel-redirect option?\nAlso take a look at a solution I have posted earlier:@quote\nIf I download a file, fasta or zip of lets say 3GB the downloaded file is always max 1GB. Where do I need to look to solve this? \nI tried to add proxy_max_temp_file_size 0; to my nginx.conf but that is not working. \nIn my galaxy.log I get the following error: \nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n\nEDIT: \nI am still searching for a solution (and a cause) so here an update. My problem is not solved yet. \nAt the moment I have two galaxy servers running, both a different s\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI uploaded files in Galaxy with SRA accesion list. It created 2 files: Single-end (fastq-dump) and Paired-end (fastq-dump). My workflow accepted Single-end (fastq-dump) however Galaxy does not recognize the fastq-dump for paired-end as a collection. I assumed that is because of  the batch mode file. Now, i am trying to separate the forward and the reverse reads and create two separated collection. I cannot find any function that would help me manipulate the fastq-dump. (Fastq splitter was a disappointment). Any hints where they might hide?"
    },
    {
      "role": "system",
      "text": "The \u201ctype\u201d of the dataset collection created will matter for the data to show up on an input field, as well as the \u201cdatatype\u201d of the data inside the collection, and the \u201ccontent\u201d of that data inside each dataset (this part doesn\u2019t have a distinct metadata attribute \u2013 could be forward fastq reads, reverse fastq reads, or interlaced forward/reverse fastq reads). Collections are built up from those to create four options.\nMany options \u2026 but maybe the explaination below will help.\nHISAT2\n\nFour different content inputs variations are possible\nThree can be given as one of three different dataset types: individual files, multiple individual files, or a dataset collection that fits the overall entry type.\nOne can be only given as a dataset collection (item 3 below).\nThat translates to 10 different choices, just for the fastq input.\n\nScreenshots, then I\u2019ll explain what each can contain:\nThe four input \u201ccontent\u201d types for this tool (many will only have the first three but expect that to evolve over time as interlaced inputs are added to more tools):\n\ninput-content.png1200\u00d7422 24.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/37e2820946644e235811cb840f78ef371595a343.png)\nThe tool form changes dynamically based on the choice made.\nThe three dataset types available for the first two and the last from the list above are in the screenshots below. The declared \u201ccollection\u201d will only accept a paired-end collection (so, you won\u2019t see the options below for it - a collection is already expected).\none-indiv-dataset-input\nmultiple-indiv-dataset-input\none-collection-input\nWhere:\n\n\nSingle-end: Single end sequences in each original individual dataset. Those can be entered as individual datasets, or selected as multiple datasets, or be put in a \u201clist\u201d type of dataset collection.\n\n\nPaired-end: Paired-end sequences, forward in an original individual dataset and reverse in an original individual dataset. Those can be entered as individual datasets, or selected as multiple datasets, or be put into two distinct \u201clist\u201d type of dataset collections. One list for the forward, one list for the reverse.\n\n\nPaired-end Dataset Collection: Paired-end sequences, forward and reverse added to a dataset collection that contains one pair of F/R reads or a combined \u201clist of pairs\u201d  if you have more than one pair. The collection is created from data that has the forward reads in one or more datasets originally, each with a matching reverse read dataset).\n\n\nPaired-end data from single interleaved dataset: Paired-end sequences, forward and reverse interleaved (interlaced) in an individual dataset (this is what you had/have, before \u201cde-interlacing\u201d).  Those can be entered as individual datasets, or selected as multiple datasets, or be put in a \u201clist\u201d type of dataset collection.\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "@system @system\nCircos job executed error free before. On re-run gives an error\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /usr/local/tools/_conda/envs/mulled-v1-3cf5c7d734898be0f8d76d0fbf3face5f99ca4b3a9b294235f9271dd27115368/bin/\u2026/lib/Circos/Colors.pm line 348,  line 424.\nPlease suggest."
    },
    {
      "role": "system",
      "text": "@quote\nCircos job executed error free before. On re-run gives an error\nHi @user\nWould you please try at @link http://UseGalaxy.org instead for now? Use the most current version of the tool Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy10).\nI can replicate the problem at the @link http://UseGalaxy.eu server across several tool versions, and we are still looking into why.\nMoving this to a new topic for followup"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run goseq on an RNA-seq dataset from A. thaliana. I need to import a GO category file, and I have tried with two different files, but it keeps failing.\nMost recently I downloaded the GO file from TAIR functional annotation data | Zenodo (@link https://zenodo.org/record/7843882#.ZEaz1nbMI2w) but it is still giving me the following error:\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-conda-linux-gnu (64-bit)\nRunning under: Rocky Linux 8.6 (Green Obsidian)\nMatrix products: default\nBLAS/LAPACK: /usr/local/tools/_conda/envs/mulled-v1-ac9928cd0c80e707f8c859eeebc088f04f34d841be6e3b5ac8f7a331c\nHere is the link to my history with the files I used, two different GO files and the failed jobs if anyone can help- Galaxy | Europe (@link https://usegalaxy.eu/u/ms.bliss/h/314-goseq-trial)\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\ncheck examples of expected input files at the bottom of goseq job setup page. All files have two columns and no header. Maybe consider adjusting your files to the expected format using tools in Text Manipulation section (remove first lines, cut columns). Maybe convert GZipped file with GO categories into plain text tabular by clicking at Edit Attributes (pencil icon) > Convert tab in the middle window) > Select an appropriate option in pull-down menu.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run StringTie on 6 files in batch mode, but upon clicking \u201cExecute,\u201d nothing happens. Trying this one file at a time also isn\u2019t working. No pending jobs show up in my history. I have also tried using different browser to no avail.\nNot sure what could be going wrong since there are no jobs to check errors\u2026"
    },
    {
      "role": "user",
      "text": "Issue resolved. Apparently I had to designate an output transcript prefix name under Advanced Options. I\u2019ve never had to do this before and didn\u2019t think to expand the Advanced Options menu before\u2026\nOh well, hopefully if someone else has the same issue, this will help!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Have 24 fq.gz files (2 GB each) that I would like to upload to usegalaxy. eu. Using filezilla, I get the following error:\nStatus: Resolving address of @link http://galaxy.uni-freiburg.de\nStatus: Connecting to 132.230.68.85:21\u2026\nError:   Connection timed out after 20 seconds of inactivity\nError:   Could not connect to server\nStatus: Disconnected from server\nAny idea what goes wrong?"
    },
    {
      "role": "system",
      "text": "Hi Henk,\nplease, follow the instructions at Galaxy Europe | FTP service instructions (@link https://galaxyproject.eu/ftp/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I want to use the Paste tool to place a column next to other columns in my data\nBut the number of rows in my two files are not equal\nAnd my file is so big that I can\u2019t see how exactly the unequal number of rows in two files will harm my data.\nIt is very important to me that the columns are placed together in such a way that each row of the columns of my file is exactly in front of its own row of the column of my second file.\nplease guide me?"
    },
    {
      "role": "system",
      "text": "hi @user,\nbased on the description you are after Join tool. Galaxy Europe probably has several versions of Join tool, like Join two datasets, Join two files and some other. Usually these tools allow \u2018fill up\u2019 of a missing data in non-matching rows.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I am trying to identified the regions that are differently enriched (H3K4me3) between 2 groups using Diffbind, but keep getting following error. I\u2019d appreciate any help!\nError in wilcox.test.default(toplot[[i]], toplot[[j]], paired = FALSE) :\nnot enough \u2018y\u2019 observations\nCalls: dba.plotBox \u2026 pv.plotBoxplot \u2192 pvalMethod \u2192 wilcox.test.default\nWarning messages:\n1: Unable to perform PCA. Make sure there aren\u2019t fewer sites than there are samples.\n2: In bxp(list(stats = c(4.28039961112653, 4.28039961112653, 5.75150066843621,  :\nsome notches went outside hinges (\u2018box\u2019): maybe set notch=FALSE"
    },
    {
      "role": "system",
      "text": "Yes. However, be careful about the pooling option in macs2 (see this post (@link https://www.biostars.org/p/246898/)). MACS2 pooling is just combining the replicates into one entity and not taking the difference of the replicates into account. So it is better to perform a robust peak analysis as described in the post.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have installed galaxy in local machine and downloaded Human Genome Resources from:\n@link https://www.ncbi.nlm.nih.gov/projects/genome/guide/human/index.shtml.\nHow use this data in tools Data menu without uploading it every time to local machine?"
    },
    {
      "role": "system",
      "text": "There are several possibilities:\nVia data tables / data managers\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/reference-genomes/slides.html\n\n\n\nGalaxy Training: Reference Genomes in Galaxy (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/reference-genomes/slides.html)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\nTools need to support it.\nAnother way is to import the data into a library (one can do this also via symlink) which can then be used in multiple histories. This way works for all tools."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have FASTA file which begins with carat(>) and not @. Hence, FastQC gives following error:\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/046/646/46646055/_job_tmp -Xmx28g -Xms256m\nFailed to process AK1_clean_fa\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: ID line didn\u2019t start with \u2018@\u2019\nat uk.ac.babraham.FastQC.Sequenc\nHow do I fix my FASTA file to begin with @? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nFasta files start with a > on the title line for each sequence. These data do not have quality scores (what most of the FastQC modules profile with statistics). Datatypes - Galaxy Community Hub (@link https://galaxyproject.org/learn/datatypes/#fasta)\nThe tool Fasta Statistics can generate statistics and do some basic QA.\nThe FAQs here might help \u2192 @link https://training.galaxyproject.org/training-material/faqs/galaxy/#datatypes. That entire page can be browser searched with keywords like datatypes. You can also search this forum.\nThis quote from one of those FAQs describes one way to convert fasta to fastqsanger:\n\nIf your data is FASTA, but you want to use tools that require FASTQ input, then use the tool Combine FASTA and QUAL. This tool will create \u201cplaceholder\u201d quality scores that fit your data. On the output, click on the pencil icon galaxy-pencil to reach the Edit Attributes form. In the center panel, click on the \u201cDatatype\u201d tab, enter the datatype fastqsanger, and save.\n\nWhether any of that is needed depends on the content of the original data, what your analysis goals are, and the tool(s) that are rejecting the fasta as a valid input. FastQC results would not be very meaningful for a fasta file converted to fastq except to generate a few basic statistics that don\u2019t involve quality scores.\nAnd, this recent Q&A is how to check what datatypes a tool is expecting as valid input: fastq unavailable -- Tool does not recognize inputs? How to check why - #2 by jennaj (@link https://help.galaxyproject.org/t/fastq-unavailable-tool-does-not-recognize-inputs-how-to-check-why/8925/2)\nIf you want to explain more about your goals and target tools, we can help more :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI have two sets of sequence data for the same genome (Illumina and PacBio) and I want to combine them via Unicycler (has worked for 5 genomes so far) but somehow it does not work for my current dataset. The error message is:\nFatal error: Exit code 139 ()\n/data/jwd04/main/054/067/54067810/tool_script.sh: line 44: 3800151 Segmentation fault      (core dumped) unicycler -t \u201c${GALAXY_SLOTS:-4}\u201d -o ./ --verbosity 3 -1 \u2018fq1.fastq.gz\u2019 -2 \u2018fq2.fastq.gz\u2019 -l lr.fasta --mode \u2018normal\u2019 --min_fasta_length \u2018100\u2019 --linear_seqs \u20180\u2019 --min_kmer_frac \u20180.2\u2019 --max_kmer_frac \u20180.95\u2019 --kmer_count \u201810\u2019 --depth_filter \u20180.25\u2019 --start_gene_id \u201890.0\u2019 --start_gene_cov \u201895.0\u2019 --min_component_size \u20181000\u2019 --min_dead_end_size \u20181000\u2019 --scores \u20183,-6,-5,-2\u2019 --keep 1\ncan you please help me understand what the problem is? As mentioned above it has worked this way for other genomes from the same batches of sequencing. Thank you in advance!\nBest regards\nManuel"
    },
    {
      "role": "system",
      "text": "Hi @user\nsimilar error was discussed on the forum, eg@quote\nDear All, \nI\u2019ve recently encountered some issues with Unicycler assembly. I\u2019ve tried to perform hybrid assembly with use of \n\n\ntrimmed Illumina reads R1 (0.17 Gb) + R2 (0.16 Gb); format: fastqsanger.gz \n\n\nnanopore reads (2.3 Gb); format: fasqsanger \n\n\nUnicycler readily deals with individual assembly of either Illumina or Nanopore reads. However, it fails to generate hybrid assembly. Any suggestions? \nmaybe is it about available working memory @ Galaxy server? \nthanks in advance, \nPiotr \nPS here \u2026\n\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey all!\nI have run gubbins on some snippy-core output (clean_full_allignment) but I get a failed job. The standard error is:\n\n/bin/sh: line 1:   477 Illegal instruction     (core dumped) raxmlHPC-PTHREADS-AVX2 -T 6 -safe -m GTRGAMMA -f d -p 1 -s /corral4/main/jobs/046/905/46905264/working/foo.aln.phylip -n foo.iteration_1 > /dev/null 2>&1\nFailed while building the tree.\n\nI\u2019m  confused by the message since I don\u2019t know if the problem is with the data or the tool. I understand that it crashes after the first iteration to perform the tree but everything before went ok.\nI have tried running the same job in @link http://galaxy.org and .eu, in the first case I get this message, in the latter I get a job that has been running for the last 3 days.\nAny help would be appreciated :slightly_smiling_face:"
    },
    {
      "role": "system",
      "text": "Hi @user\nAllow the job to complete at @link http://UseGalaxy.eu \u2192 that specific server has some cluster nodes that can process larger and longer running analysis. If that also fails, send in a bug report to the @link http://UseGalaxy.eu team. Maybe they can adjust the resources or suggest a better way to organize your data.\nThe jobs at @link http://UseGalaxy.org are too large with the current inputs and parameters. I reviewed the bug reports and the direct message (back through August). It is best to allow Galaxy decide where to send jobs. And, you could try with built-in index for K-12 instead."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been using MultiCovBed occasionally. Lately it simply doesn\u2019t work regardless of the files loaded (ERROR: Unrecognized parameter: -f , although I used the default).\nWhile several weeks ago the versions used were:\n|Galaxy Tool Version:|2.27.0.0|\n|Tool Version:|bedtools v2.27.0|\nNow it states:\n|Galaxy Tool Version:|2.27.1|\n|Tool Version:|bedtools v2.16.1|\nSeems like an updated version of Galaxy Tool (2.27.1 vs 2.27.0) uses a downgraded version of bedtools (2.27.0 vs 2.16.1)\nHow can I overcome this and run the tool without an error just as before?\nThanks!"
    },
    {
      "role": "system",
      "text": "Update: We found the missing tool dependencies and are correcting. If interested, this is the ticket: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/186\nOnce fixed/tested the ticket will close out and the newer version of all Bedtools tools can be used if you want. Some do have bug fixes.\nThanks again for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I can not see my history, it is like if had desapear i was log in with my admin-redacted-email-for-privacy account. And now the history look completly empty. Can somebody help me with this?"
    },
    {
      "role": "system",
      "text": "Hi,\n@user it seems you have deleted your history permanently.\n\ngrafik1260\u00d7445 46.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e036880ea0d6378c332c0bff1c217978fb7c210f.png)\nYou can see this list if you go to Users --> Histories --> Advanced Search --> Deleted.\nYou can still see this deleted history, but your data is gone and you need to redo this. However, you should be able to see all steps and parameters.\nSorry,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone. This might be a very basic question, but I am only starting to analyze my transcriptomic data for the first time\nMy sample sequencing was run on 4 lanes, so I have 4 FASTA files per sample. How do I merge them together, to get a complete summary of each sample sequencing?\nThank you for your help"
    },
    {
      "role": "system",
      "text": "Hi @user\nA single sample run on multiple lanes can be combined with the Concatenate tool. This stacks the reads top to bottom.\nFor QA or summary information like quality metrics, the protocol varies by the analysis domain.\nTutorials: @link https://training.galaxyproject.org/\nStart here: NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am using Transit Gumbel tool to determine gene essentiality from my transposon libraries using the tutorial titled \u201cEssential genes detection with Transposon Insertion sequencing\u201d.\nHowever, the Gene ID (\u201corf\u201d column) that was generated does not exactly match the gene ID of my organism. The gene ID generated was listed as multiples of 5 (D7S_RS00010, D7S_RS00015, D7S_RS00020, etc).\n\nAny ideas on what have gone wrong?\nI suspected that maybe I uploaded the wrong \u201ctype\u201d of file from NCBI, instead of the gene annotation file. How do you upload gene annotation file (in the gff3 format) from NCBI to your history?\n\nThank you all!\nNatalia"
    },
    {
      "role": "system",
      "text": "@quote\nHow do you upload gene annotation file (in the gff3 format) from NCBI to your history?\nCapture the link from the genome page at NCBI and paste that URL into the Upload tool in Galaxy. Use all default settings.\nHere is an example of where to find the file for a human build:\n\nncbi-genome-gff3-reference-annotation1258\u00d7726 87.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/064b5a834f7ed9c34d7734ac503eebecde533fa9.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nScreen Shot826\u00d7309 15 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/650af1df040052bb79ebf3a23d596d42f2b1de44.png)\nHello!\nThe tool \u201cspot_detection_2d\u201d seems missing on @link http://usegalaxy.eu. I could not find it in the tool list. A workflow that includes this tool cannot be executed either.\nCould someone help me to fix this problem? Thanks a lot!\nGreetings."
    },
    {
      "role": "system",
      "text": "From where do you have this workflow? The tool ID has changed to ip_spot_detection_2d some time ago. Maybe that is an old workflow or important from an older Galaxy instance? Technically its not the same tool, as the ID is different.\nHere is the tool on EU: Galaxy | Europe (@link https://usegalaxy.eu/root?tool_id=ip_spot_detection_2d)\nOr do I mix something up and a different tool is missing?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using a private Galaxy server hosted by our organization which can then be used by other people in the organization. I was following this tutorial - Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#quality-control) to run RNA-seq analysis. However, I faced this error when using the Join two Datasets side by side on a specified field (Galaxy Version 2.1.3) tool. It throws this error -\nTraceback (most recent call last):\nFile \u201c/data/tools/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nI tried several things, and referred to this article - Specify column to group by -- Join two datasets -- Fixed - #6 by jennaj (@link https://help.galaxyproject.org/t/specify-column-to-group-by-join-two-datasets-fixed/3027/6), but it didn\u2019t help me. I tried installing an earlier version of this tool but it gave the same error. Any help or suggestions are highly appreciated.\nThank you!"
    },
    {
      "role": "system",
      "text": "@quote\nModuleNotFoundError: No module named \u2018galaxy\u2019\nHi @user\nThis is a configuration problem that the administrator of the service needs to fix.  In short, the place where the job was executed did not have access to the database where the server is running.\nIf one job fails with this reason, it might have been a small hiccup and a rerun might fix it. If many jobs are failing with this reason, it is a larger problem.\nThis was due to a dependency problem in most prior Q&A at this forum. Example: Specify column to group by -- Join two datasets -- Fixed (@link https://help.galaxyproject.org/t/specify-column-to-group-by-join-two-datasets-fixed/3027)\nShould your admins need help, the Galax admin chat is here: galaxyproject/admins - Gitter (@link https://gitter.im/galaxyproject/admins)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I saw on some topics that I\u2019m not the only one who have an error message, here by using DESEQ2 or featureCounts, or HISAT2\u2026 The message is the next one :\n?[33mWARNING:?[0m Could not lookup the current user\u2019s information: user: unknown userid 819800\nSome help ?\nThanks a lot"
    },
    {
      "role": "system",
      "text": "Hi @user and @system\nIssues should be resolved again. Thanks for reporting the problem!\nRelated Q&A Job error \"unknown userid\" at UseGalaxy.org June 15-16, 2022 -- Resolved, try a rerun - #4 by jennaj (@link https://help.galaxyproject.org/t/job-error-unknown-userid-at-usegalaxy-org-june-15-16-2022-resolved-try-a-rerun/8202/4)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I encountered an error while running SPAdes. Here is the standard output/error of the tool\ncode 17: slurm_submit_batch_job error: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits)\nGalaxy tool ID as below toolshed.g2.bx.psu.edu/repos/nml/spades/spades/3.15.3+galaxy2\nI already try re-running the job once again and still got the same result. Any solutions for this?\nMany thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nFirst, try one more rerun now. This is a server-side cluster error and those are usually transient but tend to clump together temporally.\nIf a rerun persistently fails, we will need more information to help.\n\nThe server are you working at will matter. Please post the URL\nYou could also post back a share link (@link https://training.galaxyproject.org/training-material/faqs/galaxy/histories_sharing.html) to the working history. If you don\u2019t want to post that publically, please state so and a moderator will start up a direct private message.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "My project involves genotyping individuals at targeted sites in the human SNP databases.  The original DNA has been enriched in the targeted SNPs  following capture hybridization.  I have bam files of the aligned sequences resulting from the capture hybridization and a list of approximately 250 targeted SNP  sites.  I would like the output file(s) to list the two alleles from each site, the allele frequency and the coverage.  I am hoping for suggestions as to how to retrieve this information from the bam files using Galaxy?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nBAM files contain reads aligned to reference genome. You need to call variant first, for example Free Bayes is a good caller. You may want to work on alignments before calling variants, for example, re-align indels to left. VCF format contain information about variants including depth of coverage. Once you get list of variants, IDs from dbSNP can be added to the identified variants using tools like SnpSift Annotate SNPs from dbSnp. Have a look at GTN tutorials in\n\n\n@link https://training.galaxyproject.org/training-material/topics/variant-analysis/\n\n\nGalaxy Training: Variant Analysis (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html\n\n\nGalaxy Training: Calling variants in diploid systems (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\n\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Reference data: no option available\nContact the administrator of this Galaxy instance if you miss reference data (\u2013config-dir)\n\nScreenshot_20230517_182122_Chrome1080\u00d72400 245 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/ca4fdd6b31630ddc713406d44e2d6719e1d3fa15.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nFor immediate use, try using the custom genome feature.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/reference_genomes_custom_genomes.html)\n\n\nGalaxy Training: How to use Custom Reference Genomes? (@link https://training.galaxyproject.org/training-material/faqs/galaxy/reference_genomes_custom_genomes.html)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\nIf you need to assign a database metadata to datasets, your custom genome can be promoted to a custom build.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_add_custom_build.html)\n\n\nGalaxy Training: Adding a custom database/build (dbkey) (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_add_custom_build.html)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n\n\nFor use later on, please post back details about the genome and we can consider adding it to the native reference indexes. We\u2019ll need enough information to locate the data, and the genome should be from a public source. Not all genomes can be added but it doesn\u2019t hurt to ask :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI wrote a custom tool for image segmentation on our compute cluster\u2019s local instance of galaxy. I want the users to be able to see an image from the galaxy gui. As it is, the segmented images are saved and users need to navigate to the directory. I\u2019m wondering if it is possible to view an image through galaxy? If so, please point me in the right direction.\nThanks!\nJosh"
    },
    {
      "role": "system",
      "text": "Yes it is possible but you probably need to change the underlying script and the galaxy wrapper itself. Guessing that you want to keep the functionality of writing the image to a designated folder on the server I can point you toward one option (there may be more). You can change the script in a way that it also writes the images to a folder in the work directory, this directory is the folder where galaxy is executing the tool. In your script it is a relative path like output/images/image.jpg. In the tool wrapper you will have to add something like:\n<collection name=\"images\" type=\"list\" label=\"images\">\n      <discover_datasets pattern=\"__name_and_ext__\" directory=\"output/images\" format=\"jpg\"/>\n</collection>\n\nFor more information you can check the following links:\n@link https://planemo.readthedocs.io/en/latest/index.html\n@link https://docs.galaxyproject.org/en/latest/dev/schema.html\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/dev/)\n\n\nGalaxy Training: Development in Galaxy (@link https://training.galaxyproject.org/training-material/topics/dev/)\nGalaxy is an open-source project. Everyone can contribute...\n\n\n\n\n\nHope this helps to point you in the right direction."
    }
  ],
  [
    {
      "role": "user",
      "text": "Could you help me to convert GFF3 to GTF to use it in featureCounts?  I can not figure out how to use gffread.  Thanks."
    },
    {
      "role": "user",
      "text": "Thanks! It works great.\nIt did not work at the beginning, I just found out about the issue. It did not work with a compressed file, which NCBI provides. Unzip it, and then everything works fine. Thanks.\nSatomi"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI use local my Galaxy and installed plink tool. When I upload test-data, plink.bed, plink.bim and plink.fam,  plink.bed doesn\u2019t show in plink tool  pbed input.  The datatype is binary and  I cannot change datatype to pbed because the option pbed does not appear in datatype pull-down list. I can see pbed in datatype list in admin menu. How can I use it?\nThanks,\nYukie"
    },
    {
      "role": "system",
      "text": "Hi @user\npbed is a composite datatype (i.e. a datasets consists of multiple files). You need to use the Composite tab in the upload dialog."
    }
  ],
  [
    {
      "role": "user",
      "text": "What is the difference between the two?\nhow can i convert my je-demultiplexed files that are fastqsanger to fastq?"
    },
    {
      "role": "system",
      "text": "Hi @user\nfastqsanger is datatype for FASTQ files with Phred33 quality score encoding used in these days practically by all sequencing technologies. fastq datatype in Galaxy is used for FASTQ data with either unknown quality encoding or for old and obsolete illumina Phred64 encoding. Very few tools in Galaxy can handle fastq datatype. If the data was generated recently, a proper datatype is fastqsanger or fastqsanger.gz. Galaxy support gz compressed data.\nHope this answers the question.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to retrieve a data set from EBI SRA for a couple of days now and keep having issues. I go to the \u201cGet Data\u201d tab in galaxy and select EBI SRA. I\u2019m then taken to the site and can search for the data set. But when I click on the file I want under the FASTQ Galaxy column I get a black screen and the following error:\nThis page isn\u2019t working\n@link http://usegalaxy.org  is currently unable to handle this request.\nHTTP ERROR 500\nI\u2019ve tried refreshing the page but then I get a different error that looks more like an error from galaxy itself (see screenshot). Is the website currently experience maintenance? I am the only with this problem and maybe it\u2019s an issue with my internet? Any insight on how to fix this problem would be much appreciated!\n\nScreen Shot 2020-10-05 at 11.23.26 AM1316\u00d790 5.99 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4e926320a99ac7ba3d3b760d53c288a50a31db8f.png)"
    },
    {
      "role": "system",
      "text": "Thanks again, this should be working again."
    }
  ],
  [
    {
      "role": "user",
      "text": "We are trying to upload BAM(hg19) and fastq.gz files with FTP, but the upload stops after about 70G of the files have been uploaded. Our file sizes are about 95-140G in size.\nHow can we get these files uploaded to useGalaxy?\nTthanks"
    },
    {
      "role": "system",
      "text": "Hello \u2013 The maximum dataset size for Upload is usually 50 GB but needs to be a bit smaller for BAM datasets (around 25-35 GB). Datasets produced by tools can be larger.\nThis is true when working at the public Galaxy servers like @link https://usegalaxy.org. You would run into quota problems (max 250 GB total per account) and likely tool problems (exceed resources) working with such large data.\nIf you are running your own Galaxy, larger data can be loaded by administrators into data libraries using other methods instead of FTP, please see: @link https://docs.galaxyproject.org/en/master/admin/useful_scripts.html?highlight=data%20libraries\nGalaxy choices:\n\n@link https://galaxyproject.org/choices/\n@link https://galaxyproject.org/use/\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi @system - I\u2019ve been reading through these threads as I have had the same problem - thank you for your wisdom so far! Please could I just confirm what you mean by \u201cAvoid the gff3 version\u201d?\nBecause when I click on GENCODE\u2019s human GTF files I see they are all gff3 files. I am confused which one I need to choose (I have included a screengrab below):\n\nFor context I am running RNA STAR on Galaxy for Illumina paired-end reads of human pancreatic cancer cells that I want to align to the human hg38 genome\n\n\nimage1537\u00d7494 49.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/63f79627f324d096ab6e4491968a787c7ff80b7f.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe column with the \u201cDownload\u201d links include both formats. Copy the link for the GTF, paste that into the Upload tool, and leave all \u201cauto\u201d settings at default.\nThe datatype will be detected as \u201cGFF\u201d.\nTwo additional steps are need to format this data correctly:\n\nRemove the # header lines,  @link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation\n\nThen redetect the datatype,  @link https://training.galaxyproject.org/training-material/faqs/galaxy/#detecting-the-datatype-file-format\n\n\nThis should result in the datatype set as \u201cGFT\u201d and the tools you are using will understand how to use this annotation with any (most?) other data that use or incorporate data also associated with the hg38 genome assembly.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I create several test accounts on the Galaxy server thinking I could easily remove them later. But now I\u2019m not finding a way to delete these accounts from the Galaxy UI.  Tried searching this list and docs but not finding anything.\nDo I have to do this through the PostgreSQL database and remove from the galaxy_user table?"
    },
    {
      "role": "system",
      "text": "Hi @user\nOne way to do this is to log into each account in the UI and delete them under User > Preferences > Delete account.  The accounts will still be in the database but won\u2019t be accessible in the UI for regular non-admin users.\nIf you enabled GDPR compliance, the registered email addresses/usernames will be changed (once deleted) to anonymous placeholder keys. The passwords are already encrypted (for any account state). Database logs can also be adjusted.\nMore details here: GDPR Compliance \u2014 Galaxy Project 20.09 documentation (@link https://docs.galaxyproject.org/en/master/admin/special_topics/gdpr_compliance.html)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI have a problem with Join two datasets:\nFirst I can cut successfully one column and six columns of interest. If I want to join them again to a new table file, I only get the header and the values from the first but from the second \u201csix columns\u201d only the header and no values.\nCould you please help me.\nThanks in advance"
    },
    {
      "role": "system",
      "text": "Welcome, @user !\nMaybe your inputs have no common fields to join the files, which is how the Join two Datasets tool works (@link https://usegalaxy.eu/root?tool_id=join1):\nThis tool joins lines of two datasets on a common field\nCan you confirm this?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was hoping to get some advice - I am trying to create a custom genome with combine human and rat builds to use as a reference genome for alignment with bulk RNA sequencing data. I am trying to use the UCSC database but I am not sure which settings (group, track and output format mostly) that would be required for this application. Any advice or point toward a tutorial that might help would be extremely helpful!\nThanks in advance,\nKiera"
    },
    {
      "role": "system",
      "text": "Hi @user\nHuman or rat individually will both be too large to use as a custom reference genome. Combined will also be too large.\nYou could create your own indexed reference with this data if you are running your own Galaxy server. UCSC would be a good source. Find the fasta\u2019s in their Downloads area, as this data is too large to use the Table Browser query function.\nGeneral instructions for creating reference indexes in Galaxy are here:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\n\nGalaxy Training: Galaxy Server administration (@link https://training.galaxyproject.org/training-material/topics/admin/)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there, I\u2019d like to calculate TPM from my HISAT2 output files (.bam).  I\u2019ve used Stringtie and provided a reference gtf file and have successfully gotten the TPM calculations.  However, I realized that because Stringtie also predicts new transcripts, I\u2019ve gotten TPM values for the predicted transcripts as well.\nIs there another tool I could use that doesn\u2019t include the predicted transcripts, or could I scale up the TPM values of the known transcripts after removing the TPMs of the predicted transcripts?  I\u2019m aware of Salmon/Sailfish/Kallisto but would prefer to use the HISAT2 files which were aligned to a genome rather than transcriptome for consistency.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user Stringtie also has Use Reference transcripts only? option, and the default setting is No.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hay,\nI am trying to extact genomic DNA out of a FASTA file and a flanked VCF file.\nI have a VCF file and a FASTA file and I want to exctact the SNP\u2019s with 200 bp flanks.\nFirst I uploaded my Fasta (this one is TAB delimited) and my VCF file. I transformed my VCF file to a pgSnp file and then get flanks with \u201cGet flanks returns flanking region/s for every gene\u201d. Then I used Extract Genomic DNA for getting te sequences. But the output I get is totally empty and says \u201cUnable to fetch the sequence\u201d.\nI hope someone is able to help me.\nKind regards,\nIlse"
    },
    {
      "role": "system",
      "text": "Hi @user,\nExtract Genomic DNA expects two inputs. If the formatting or content is off for either, you will not get valid results.\n\n\nQuery: Genomic coordinates in bed or gtf format.\n\nThe tool states interval format as a valid input, which is a less strict format datatype version than bed. Even so, the data must have bed column format/ordering for at least the first 3-6 columns (six columns if including strand).\nThe tool also states gff format as a valid input, which is a less strict format datatype version than gtf. This means that the 9th column (attributes) does need to include the stricter minimum values (gene_id and transcript_id). Do not use a gff3 input or expect errors.\nUsing VCF to pgSnp is fine. The first four columns of data are in interval format, and if you use Cut to restrict to the first four columns the data will then be in bed format. But using Get flanks will also restrict the output to be in interval format with bed column ordering.\n\n\n\nTarget: A locally-cached index on the server or a Custom genome in fasta format.\n\nTo use a \u201clocally-cached\u201d genome, that genome must be assigned as the \u201cdatabase\u201d metadata to the query input.\nTo use a use Custom genome, you might need to run NormalizeFasta on your fasta to remove the description line content (data on the \u201c>\u201d title line after the first whitespace) and wrap the bases to a consistent length (80 is good). The tool will only be able to interpret data that is actually in fasta format, not tabular. Transform with Tabular-to-Fasta, if you need to, first.\n\n\n\nAlso, check your data for chromosome/identifier naming mismatches. Between the two inputs, the \u201cchromosome\u201d names must be formatted exactly the same and the overall content based on the same reference genome/transcriptome version/build. This means that the identifiers in the first column of your interval/bed dataset must exactly match what is on the \u201c>\u201d title line of the fasta dataset.\nThe FAQs below have more details:\nFAQs: @link https://galaxyproject.org/support/\n\nFormat help for Tabular/BED/Interval Datasets (@link https://galaxyproject.org/support/tabular/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\nPreparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/)\nMismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\n\nHope that helps, but if not, share some more details. You could copy/paste the first few lines of both inputs and/or post back screenshots in a reply. Make sure to expand the datasets to show the currently assigned datatypes or state exactly what those are for each."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to recover a fasta file from a blast search run in galaxy but blastdbcmd doesn\u2019t recognize the database.\nI made the blast database with makeblastdb in galaxy and I have tried it with and without parse_seq_ids - it is not recognized as a database either way."
    },
    {
      "role": "system",
      "text": "Hi @user\nThis is an older tool wrapper that was not designed to work with BLAST+ results associated with mapping against a custom blast database. Only natively indexed genomes can be parsed, and which are available vary by Galaxy server. You could set up your own local Galaxy, index the genome, and use the tool but I am guessing that is not your goal.\nParsing this data from custom/created databases without having it specifically indexed on the server certainly seems like useful functionality. There is also likely a workaround to achieve similar results without using this exact tool.\nMore feedback soon \u2013 I\u2019m going to review all options (not those already available at Galaxy Main @link https://usegalaxy.org), consult with our developers, then will write back with the best way forward. Thanks for your patience!\nUpdate: the tool now works on results from makeblastdb as run in Galaxy and is quite forgiving about which \u201cindex\u201d identifier is used."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI just created a galaxy account on @link http://galaxy.org, but i\u2019m in Europe and i would like to shut this one done and open one on @link http://galaxy.eu, as it is said that one can\u2019t have 2 accounts.  When using the current one, i couldn\u2019t  find the zebrafish genome on the built-in genome list for example but my colleagues on the eu version have it. How can i do that or is it considered different services and therefore ok to create a new one on the .eu ?"
    },
    {
      "role": "system",
      "text": "Hi @user\nEvery user can now delete accounts themselves under User > Preferences.\nFor other account administrative help when working at @link http://usegalaxy.org, you can also send an email to @link mailto:galaxy-bugs@lists.galaxyproject.org. The email should be sent from the same email address as the account is registered under.\nIf working at another public Galaxy server, the administrative contact information is usually on the server\u2019s home page and/or at their directory listing here @link https://galaxyproject.org/use/. If you cannot find it, ask and we can try to help here. Please post the server\u2019s base URL for context.\nGeneral usage questions should be asked at this forum. We will usually need the public Galaxy\u2019s URL or a description of the private server (local, docker, cloud) you are working at for context eg: where sourced, version of Galaxy. The full name/version of the tool (if a tool question) also helps and will aid in community assistance.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I just wondered why featureCounts is listed under RNAseq analysis, I thought it has been superseded. Maybe I\u2019m wrong. Can someone explain? Thanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nTools in the left tool panel are organized into \u201csections\u201d related to analysis domains, functionality, common operation on specific datatypes, and/or by \u201ctool suites\u201d. That is just a default organization \u2013 and there are ways to customize.\nThe tool featureCounts Measure gene expression in RNA-Seq experiments from SAM or BAM files  is nested under the section \u201cRNA-seq\u201d since that is the primary analysis domain the tool is used for.\nSome tips:\n\n\nOverview in the release notes from last September related to the latest \u201ctool panel views\u201d updates: September 2021 Galaxy Release (v 21.09) \u2014 Galaxy Project 22.01.1.dev0 documentation (@link https://docs.galaxyproject.org/en/master/releases/21.09_announce_user.html#tool-panel)\n\n\nTry using the tool search at the top of the panel to find tools. Here you can use keywords to limit the results, and toggle the display to include the section labels or not.\n\n\nYou can also organize the list of tool by EDAM Ontologies, or your own list of favorites. Once you load up a tool, you can click on the star in the upper right corner of the tool form to \u201cadd to your own personal favorites\u201d then restrict the tool panel view just your own favorites.\n\n\nTry hovering over the \u201cstar\u201d and \u201clist\u201d icons to see how each is labeled, then try those functions out to learn how they work and to discover if any are helpful for you. :slight_smile: \nadd-tool-to-favorites1616\u00d7258 44.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7434cbe8ba030972abdcf54111abbedc5b333b00.png)\n\n\n\nIf this doesn\u2019t address your question, would you please you explain with a bit more detail? I\u2019m not sure what \u201cI thought it has been superseded\u201d refers to.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying out deseq2 and edgeR for differential analysis on @link http://usegalaxy.org, however, on both occasions, I only get plots (which look very ok) but no actual result files or tables of DEGs as output (shows \u2018this list is empty\u2019 instead). Please help, what could be wrong?"
    },
    {
      "role": "user",
      "text": "Thank you for your mail Jennifer. Everything seems to work fine now, surprisingly the tools worked as expected after I repeated the analysis today. I think the problem has been fixed already.\nRegards,"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nWhen plotting my LEfSe results, I am getting some blanks where there should be feature names.\nI have checked my .txt file, and there are no blanks that I can see that would be contributing to this. Has anyone ever encountered a similar problem?\nThanks in advance!\nScreen Shot 2020-06-23 at 1.12.48 PM638\u00d7759 68.1 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/89ffcad78b91a1626f7f4c152a6bf61f4826b8fb.png)"
    },
    {
      "role": "user",
      "text": "Hi @system - You were correct, thank you! Adjusting the text settings so that the class text size was smaller (7 pt) worked.\nBest,\nElizabeth"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nWhen I attempt to trim reads via galaxy vs. command line, I get different results with a near 2000 BP difference.\nThese are the commands I used:\ncommand line:\njava -jar /software/trimmomatic/0.39/trimmomatic-0.39.jar PE ERR048396_1.fastq ERR048396_2.fastq trial2.trimmed.paired.R1.fastq trial2.trimmed.unpaired.R1.fastq trial2.trimmed.paired.R2.fastq trial2.trimmed.unpaired.R2.fastq ILLUMINACLIP:TruSeq3-PE-2.fa:2:40:15:8:True LEADING:15 TRAILING:15 SLIDINGWINDOW:4:20 MINLEN:35\n\nThis is what I set my galaxy options to, which should be the same as what I have set my command line to do.\n\nGalaxy1| 690x4801642\u00d71144 90.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/eb8dff9785c7667f1be8abf506c21779b8cdfba4.jpeg)\n\nGalaxy21640\u00d71010 147 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c77047a40a727853a6a607657e40314a508700b6.jpeg)\n\nGalaxy31670\u00d71004 150 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5066bbcdc6b1e4be83a7fd8e43824309d5ffbfeb.jpeg)\nI did download these sequences from galaxy and moved them to a remote server, prior to doing that, however, I made sure to fastqc both sets of data to ensure I have the same data in each.\nAny idea where/if I\u2019ve done something wrong? Any help would be greatly appreciated"
    },
    {
      "role": "system",
      "text": "Hi @user\ndo you use the same version of Trimmomatic?\nI believe, Trimmomatic applies rules in the specified order, at least in Galaxy. It might be the case on command line, but I have not tested it. You have different order of trimming rules on command line and Galaxy. Maybe swap \u2018rules\u2019 on command line and check if you get different output.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nThe log2 fold change is calculated as log2 (treat)/(control). In the factor level section, is \u20181: Factor level\u2019 a denominator and \u20182: Factor level\u2019 is a numerator?\nGalaxy\u2019s DESeq2 both states \u201cSpecify a factor level, typical values \u200b\u200bcould be \u2018tumor\u2019,\u2018normal\u2019,\u2018treated\u2019 or \u2018control\u2019\u201d, which is a bit confusing.\nIf the order is changed, the opposite result will come out, so I think everyone should pay attention to this part."
    },
    {
      "role": "system",
      "text": "Hi,\nin Galaxy, factor level 1 is considered the treated condition (numerator), and factor level 2 is considered the control condition. Indeed it can be a bit confusing; thanks for your feedback, it will be reported in order to clarify it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have a question regarding the \u201cCLIP-Seq data analysis from pre-processing to motif detection\u201d training. I understand the meaning behind switching the R2 mates with the R1 mates in \u201cumi-tools extract\u201d and \u201cSTAR\u201d steps. However, I do not get why in Van Nostrand et al. 2016 they do not switch R2 with R1 as well. Even in the processing pipeline associated with the libraries I am interested in (@link https://www.encodeproject.org/documents/739ca190-8d43-4a68-90ce-1a0ddfffc6fd/@@download/attachment/eCLIP_analysisSOP_v2.2.pdf) they do not switch R1 and R2 as well,m although is very similar to the training pipeline. I hope I have been clear enough.\nThanks in advance, also I want to congratulate for the amazing job you are doing!"
    },
    {
      "role": "system",
      "text": "Dear @user,\nI have not read the paper. Assuming is never a good idea in data analysis because it leads to errors. For me, it is a bit unlikely the UMI is at the 3\u2019 end in single end fashion because the sequencing error is the highest at the 3\u2019 end.\nMaybe contact the authors and clarify the read library design. I would probably also try to locate the adapter sequence and thus get an idea, where the UMI might be.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI\u2019m trying to install a tool from the Toolshed but I got this error:\n\nRepository installation is not possible due to an invalid Galaxy URL: None. You may need to enable third-party cookies in your browser.\n\nThe cookies are OK (tested on Firefox and Chrome).\nI\u2019ve found this similar topic:\n@link https://biostar.usegalaxy.org/p/8548/index.html\nSo I checked the admin_users parameter in galaxy.yml file. It was empty.\n-> First of all it\u2019s weird that I could connect as admin mode (with @link mailto:admin@galaxy.org login) if this field is empty.\n-> Then I added my admin login and restarted Galaxy, without success.\nI tested with several tools (mirkwood from Test Tool Shed, bamtools from Main Tool Shed), always with the last revision.\nI\u2019m using Galaxy with docker, tested on bgruening/galaxy-stable:18.05 and bgruening/galaxy-stable:19.05.\nFinally, I tested with Install new tools (Beta) mode from Tools management. This time it worked. It\u2019s a bit weird that the only way to install a tool is the beta mode.\nSo I don\u2019t really know if it\u2019s worth bothering to try to solve this problem or if anyway the old, classical way to install tools is going to be replaced by the currently beta mode\u2026\nThank you for reading me."
    },
    {
      "role": "system",
      "text": "Hello @user\nDependency resolution with conda is now the default and recommended method for tool installation. Start here: @link https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html && @link https://docs.galaxyproject.org/en/latest/admin/conda_faq.html\nThe versions of Galaxy noted are over a year old \u2013 use the latest versions of Galaxy releases for the best functionality. You\u2019ll find that the dependency resolution functions are no longer in beta and can be customized. Using default settings is generally best unless you have specific reasons to install differently. The links above cover the options.\nIf you need help with a Docker version of Galaxy, this Gitter chat is a direct way to communicate with the image maintainers: @link https://gitter.im/bgruening/docker-galaxy-stable\nThe Galaxy Test ToolShed @link https://testtoolshed.g2.bx.psu.edu/ is a testing sandbox \u2013 meaning, tools are not in a stable/release form. Obtain tools from the Galaxy Main ToolShed @link https://toolshed.g2.bx.psu.edu/ instead.\nIf a tool is only available in the Test ToolShed, and not in the Main ToolShed, it may still be useable, but installation may not work \u201cautomatically\u201d and require some advanced tuning. Explore the development repository for the tool (is usually linked to the tool\u2019s ToolShed page, but not required, especially at the Test ToolShed), review open known issues, and open a new issue there describing problems as needed.\nThe tool wrapper authors are the best resource for troubleshooting installation issues (if using managed dependencies and the most current version of Galaxy itself). An exception can be tool wrappers authored by the IUC or Devteam \u2013 you can ask if there are known issues (if you can\u2019t find one yet at the development repository) or for installation help (after reviewing the development docs linked above) at this Gitter chat: @link https://gitter.im/galaxy-iuc/iuc\nPlease be aware that some tools may no longer be actively maintained, and some may use older or custom dependency resolution methods. ToolSheds are community resources \u2013 anyone can publish a tool there. How to tell if a tool is maintained/functional or requires a specific installation method? 1) The tool wrapper\u2019s publication date, how many times downloaded, and other notes on the tool\u2019s ToolShed page and 2) commit/update dates at the linked development repository.\nThe Test and Main ToolSheds were just upgraded, so there were recently windows where tool installation was expected to not function correctly, no matter which ToolShed was the source. Example Q&A about that at this Gitter chat: @link https://gitter.im/galaxyproject/dev?at=5f59fa68a5788a3c29e651a5\nCurrent and historical status/notes about downtime are at the Galaxy status page. It includes all usegalaxy.* servers and core resources like ToolSheds. @link https://galaxyproject.statuspage.io/\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I am having the same problem in which my account is reported as using 100% of available space. I have deleted and the permanently deleted many data sets and yet there is no more available space. I have logged out and back in and waited days and yet the account doesn\u2019t seem to be updating, is there any way my account can be updated by a member of the galaxy team?\nMany thanks!\nAlysha"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis updated FAQ covers all the ways to locate and manage data. Double-check and if you are still having problems, write back.\n@link https://galaxyproject.org/support/ >>  The account usage quota seems incorrect (@link https://galaxyproject.org/support/account-quotas/)\n\nShould you still have problems after that review, where you are working will probably matter for more help.\nYour email address at this forum is not the same as at @link http://usegalaxy.org. Is that really where you are working?\nIf yes working at @link http://usegalaxy.org, post back confirming then I\u2019ll send you a direct message where you can share your registered account email privately, and I\u2019ll try resetting the quota administratively. You should not share your password: no administrator will ever need OR ask for it. Note: there are no known problems with this function, so there is almost certainly un-purged data in your account. But we can follow up. The server did just go through some updates.\nIf not, please clarify \u2013 the public server URL or some other configuration (local, docker, cloud: explain details such as where sourced and what version of Galaxy).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "hello,\ni am using bowtie 2 and i didn\u2019t find my reference genome Caenorhabditis elegans ce11.\nthank you"
    },
    {
      "role": "system",
      "text": "Hi @user\nThat reference genome is indexed at @link http://UseGalaxy.eu for Bowtie2. Please try there.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I upload SRR15324080 data in @link http://plants.usegalaxy.eu. I am using Trimmomatic flexible read trimming tool for Illumina NGS data tool.\nthe data is paired end sequencing (merged in one file). So, I select paired end (as collection) in the tool but I do not see the name of my data in Select FASTQ dataset collection with R1/R2 pair. How can I set the default for this sample? Can anyone help me?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe quickest way to import fastq data into Galaxy from NCBI with the pairs split is with the tool Faster Download and Extract Reads in FASTQ format from NCBI SRA. The output creates collections for you and can be used as input to Trimmomatic.\nOr you can go back to the original interleaved reads and split them up \u2013 then run Trimmomatic on them. That particular tool might not fail on interleaved inputs but is also isn\u2019t designed to handle them.\nI added some tags to your post to help with how to split up the interleaved reads. And here is an FAQ and a tutorial that explain directly what interleaved fastq data is and how to manipulate it. Some tools accept that format, but most do not. But if you are trying to build up a workflow or plan to do a similar analysis again, using the tool that splits the read data at retrieval is simpler (and \u201cfaster\u201d).\nFAQ:\n\n\nReformatting fastq data loaded with NCBI SRA (@link https://galaxyproject.org/support/ncbi-sra-fastq/) >> NCBI SRA Fastq (@link https://galaxyproject.org/support/ncbi-sra-fastq/#interlaced-forward-and-reverse-reads)\n\n\nTutorial:\n\n\n@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html\n\n\nGalaxy Training: NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI have sequenced a bacterial genome with a MinION and I intend to assemble it with miniasm. For this, I first intend to generate the PAF file with the sequence overlaps using minimap2. In the minimap2 tool in Galaxy (I use Galaxy Europe),  I understand that I have to set the configuration of the alignment parameters to the preset option \u201cOxford Nanopore all-vs-all overlap mapping\u2026\u201d, and run the tool using the same reads (my fastq file) as the reference (to build the index) and as dataset (where it says \u201cSelect a fastq dataset\u201d). However, the tool does not allow me to provide any fastq file where it says \u201cSelect a fastq dataset\u201d. I have verified that my file is recognized as fastq, and I have uploaded the same file twice with two different names, but it does not work. It seems that the tool does not allow to upload a fastq file in the field labeled as \u201cSelect fastq dataset\u201d. Any clues why?"
    },
    {
      "role": "system",
      "text": "Maybe you need to use the datatype fastqsanger instead of fastq.\n\n\n@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/minimap2/minimap2.xml\n\n\n@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/minimap2/minimap2.xml\n<?xml version=\"1.0\"?>\n<tool id=\"minimap2\" name=\"Map with minimap2\" version=\"@TOOL_VERSION@+galaxy@VERSION_SUFFIX@\" profile=\"20.01\">\n    <description>A fast pairwise aligner for genomic and spliced nucleotide sequences</description>\n    <xrefs>\n        <xref type=\"bio.tools\">minimap2</xref>\n    </xrefs>\n    <macros>\n        <import>macros.xml</import>\n    </macros>\n    <expand macro=\"edam_ontology\"/>\n    <expand macro=\"requirements\"/>\n    <stdio>\n        <exit_code range=\"1:\" level=\"fatal\" />\n        <regex match=\"\\[ERROR\\]\" source=\"stderr\" level=\"fatal\" />\n    </stdio>\n    <version_command>minimap2 --version</version_command>\n    <command>\n<![CDATA[\n    #if $reference_source.reference_source_selector == 'history':\n        ln -f -s '$reference_source.ref_file' reference.fa &&\n\n\n\n\n  This file has been truncated. show original (@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/minimap2/minimap2.xml)\n\n\n\n\n\nIt looks like the tool accepts fastqsanger, fastqsanger.gz and fasta."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage874\u00d7861 243 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b58b5d47e98d6d660736b85ce18e4b9e21841314.jpeg)\nHi,\nI have done some Nucmer alignment with a set of genome sequence retrieved from UCSC genome browser against my genome assembly. And the plot looks like pic above. But the contig or scalfold names are blurred just because of the limited space on X-axis. Is there any way to view or read which contig or scalfold are best maches from the plot or the alignment files?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe option Plot Size option can be adjusted \u2013 the default is \u201csmall\u201d, but \u201clarge\u201d would be better given the number of sequences. There are other plot variables that you can adjust if needed (example: Plot a particular reference or query sequence?).\nYou can also hide the tool and history panel in the Galaxy application to give the plot more space, or open your browser window larger, or download the plot and open it that way.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am working with RNA seq analysis, and using Fastp but the output of Fastp after trimming still not good. Any solution.\n\nFastp-before1363\u00d71019 37.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b8a9181fb668918c735fe82afb9d99bb4c96f118.gif)\n\nFastp-after1341\u00d71029 32.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/568d41f56c03a8cc0693f0195be39a4d02f92c59.gif)"
    },
    {
      "role": "system",
      "text": "Hi @user,\naccording the FASTQC report, this peak is caused by a region of  ambigous base-calls in the GSM1949048_Pt2_D726_2.gz dataset, probably due to sequencing errors.\n\nScreenshot from 2021-06-22 14-18-21929\u00d7688 15.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b5e44fd344cbf6f1fc851a2ba5256a72cb6f03df.png)\nIn order to remove those reads you can reduce the N base limit parameter in fastp.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI got a local instance of galaxy on my computer, and wanted to run a differential expression analysis for some samples of human RNA-seq data. I\u2019ve managed to download the fastq reads for the samples and I\u2019ve downloaded and indexed the human genome using HISAT2 index builder. But when I tried to align the reads from just one of the six files, it wasn\u2019t able to align them in almost 60 hours of running. So I decided that I\u2019d need to do this on the cloud. However, I don\u2019t want to have to reinstall the tools, and re-download all the files and re-index the genome. I was wondering if there was any way to take whatever I have on my computer, and put it onto the cloud as is, so that I can pick up where I left off.\nThank You"
    },
    {
      "role": "system",
      "text": "It\u2019s not possible to transfer a local setup to CloudMan/GVL and instead the tools would needs to be installed again. The GVL does come with a number of tools pre-installed, so perhaps the tools are already available, or maybe only a couple of them would need to be installed. It also comes with a bunch of reference data so hopefully that would not need to be setup.\nAn alternative you could perhaps try is to use the GalaxyCloudRunner: @link https://galaxycloudrunner.readthedocs.io/en/latest/\nThis is still under development (you would likely be the first user), but in principle, it allows you to connect additional cloud machines to your local Galaxy instance. The remote instance will generally be automatically setup for the necessary tools at runtime while it\u2019ll transfer the data automatically."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am unable to create a custom gene categories file to use in goseq. Can some please guide me as to how we create a custom gene category file to use in goseq. I am doing RNA seq analysis and the reference genome is of the microbe Desulfovibrio alaskensis G20 (NCBI Reference Sequence: NC_007519.1). Thank you for any help."
    },
    {
      "role": "system",
      "text": "Consolidating with Go-seq Not listed Gene categories (@link https://help.galaxyproject.org/t/go-seq-not-listed-gene-categories/10092)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nFirst of all, I would like to thank everyone involved in running and supporting the Galaxy servers (I use @link http://UseGalaxy.eu). I think the range and choices of tools are great, the platform runs very reliably and fast, and everything is well documented.\nI have been trying to use Salmon for transcript quantification in RNA-seq. There has been a major update to the tool (@link https://combine-lab.github.io/salmon/). The current version is 1.2.1, while @link http://usegalaxy.eu runs 0.14.1.2. I am not sure how much work it is, but would it be possible to update to the current version?\nThank you so much!\nSebastian"
    },
    {
      "role": "system",
      "text": "Hi, thanks for the praise!\nAs of Salmon, the tool itself is hosted here (@link https://github.com/bgruening/galaxytools/tree/master/tools/salmon) which is a github repository and the best way to reach its authors would probably be to either create an @link https://github.com/bgruening/galaxytools/issues or contact the IUC on @link https://gitter.im/galaxy-iuc/iuc. They know everything about tools there is to know."
    }
  ],
  [
    {
      "role": "user",
      "text": "i am using a local install of galaxy and looking to add a reference genome. i used the \u201ccreate dbkey and reference\u201d data manager, and imported h38 genome. the fasta file exists on the server, but I can not see it listed as a reference genome in histat2 or any other tool. it does show up as a data table-- any idea what i need to do to make it show up as a reference genome?"
    },
    {
      "role": "system",
      "text": "Hello,\nSome tools, such as hisat2, need an index built against that genome which is referenced in the tool\u2019s tool_data_table_conf.xml.sample file. Looking at that file for @link https://github.com/galaxyproject/tools-iuc/blob/master/tools/hisat2/tool_data_table_conf.xml.sample, you\u2019ll notice that it uses a table named hisat2_indexes. A data manager for that data table can be viewed at and installed from @link https://toolshed.g2.bx.psu.edu/view/iuc/data_manager_hisat2_index_builder/\nThere is also the option of using the galaxy reference data rsync server, detailed in this document (@link https://galaxyproject.org/admin/reference-data-repo/)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, I am looking to map my pair-ended trimmed reads to my reference genome using Bowtie2. I input the R1 and R2 fastq files and the reference genome FASTA file. It gives me the below error and I am not sure how to fix this.\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 Error, fewer reads in file specified with -2 than in file specified with -1\nthan in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1Error, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nError, fewer reads in file specified with -2 than in file specified with -1\nterminate called after throwing an instance of \u2018int\u2019\nterminate called recursively\nterminate called recursively\n(ERR): bowtie2-align died with signal 6 (ABRT)\nThe R1 and R2 reads are definitely same size and I also have enough space left on my galaxy for analysis. Any help much appreciated on how I can fix this please?"
    },
    {
      "role": "system",
      "text": "Hi @user and @system\nWhen I\u2019ve seen this error, it was due to not having intact pairs present in the input fastq files.\nThat isn\u2019t a requirement always but ensuring intact pairs usually solves it. There is probably a specific option that is the trigger \u2013 but I don\u2019t know for certain which :upside_down_face:\nSo, if filtering by length isn\u2019t enough (minimum seed length), try this to get rid of reads in either read dataset that no longer have a mate present in the other read dataset. In short, ensure that intact pairs are input to the tool. Some QA tools sort the outputs by paired/not state but others will just remove reads from one end.\n@quote\nFastQ Interlacerfollowed byFastQ Deinterlacer"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nWe recently installed the Galaxy Server to our mainframe. However, we are having difficulty in using RNA STAR as there is no reference genome available (see attached picture). How can we rectify this?\n\nScreenshot 2022-06-13 at 17.10.202052\u00d71616 302 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9ac0b619ac8075cf51693b5be23a7c0e7842f99a.png)\nMany thanks,\nChris"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe slides that @system shared are part of a larger tutorial suite. Admin topics are here:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\n\nGalaxy Training: Galaxy Server administration (@link https://training.galaxyproject.org/training-material/topics/admin/)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\nReview the section \u201cData Management & Reference Data\u201d. In short, you have several choices for the method. These include:\n\nCreate your own indexes using Data Managers. These are like other tools Galaxy, but only available for local server admins. You\u2019d need to install the tools, then access them under the \u201cAdmin\u201d tab, then either run the tools in a specific order directly or use our batch processing utility.\nLink in or copy of the indexes available at public Galaxy servers from our data server. This would be configured by a local admin. Human genome builds are included.\n\nThe how-to is covered in the tutorials and some include videos. I also added a few tags to your post that cover prior Q&A about this topic (troubleshooting, etc). If you get stuck or have problems, feel free to ask more questions.\n@quote\nWe just simply want to add the reference genome (human) with built-in index and without the built-in gene model\nEach genome build is associated with\n\nthe genome fasta and several technical indexes. Some optional, some required. Example: items like the Samtools index would be needed to interpret BAM outputs produced by mapping tools.\nand tool specific indexes (including RNA-Star for most).\n\nHaving a built-in indexes is different from using a Custom genome/build where just the fasta is add to the working history. Why? Galaxy creates those technical and tool specific indexes at runtime as single-use temporary files. Custom genomes/builds are fine for smaller genomes (or any \u201c-ome\u201d) but larger genomes would require much more time/resources to create those indexes every time the tool is run. Convenient for some use cases but not practical for others.\nFor human, definitely use a built-in indexes. And if you chose to use the pre-computed indexes we host, you won\u2019t need to scale up your local resources to create the indexes with Data Managers. You will still need to have sufficient resources to execute a tool. For `RNA-star, I think the minimum working memory would be about 60 GB \u2013 but you can check with your own data. Run the tool at a public server with a representative dataset query and against a genome build target, click into the Job Details ( :information_source: icon within the result dataset), and review the resources used in the lower part of the report.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI am working with bunch of datasets and one of them wont map to hg38 reference genome. I attached the fastq formats which i think there is a problem with that. I tried every thing else( changing references, or mapping options)\nit seems i should do some thing to fix the fastq formats.\nAlso, It is miRNA-seq dataset.\nThanks\n\n88.474\u00d7536 6.27 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f17d548189ac4bbacb5a0f46db0ff7797396ce50.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\ndo you get an error for the mapping step, or reads do not map to the genome? If the job failed, check the error message. Run FastQC on file with reads. If the reads are all unmapped, check several sequences through UCSC Blat search, to make sure the reads have a mach to the human genome. Alternatively, run Blast search against the human genome for several sequences.\nThe reads are longer than miRNA and presumably contain adapter sequences. You can trim reads before mapping. I assume all other samples have similar untrimmed sequences.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am getting this error message when trying to run \"Map with BWA-MEM \" with a fastq file. Could somebody help me?. Thanks in advanced.\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method:\n{\n\u201chistory_id\u201d: \u201c9bbf4c9f6a067ff4\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2\u201d,\n\u201ctool_version\u201d: \u201c0.7.17.2\u201d,\n\u201cinputs\u201d: {\n\u201creference_source|reference_source_selector\u201d: \u201ccached\u201d,\n\u201creference_source|ref_file\u201d: \u201chg19\u201d,\n\u201cfastq_input|fastq_input_selector\u201d: \u201csingle\u201d,\n\u201cfastq_input|fastq_input1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33ac5a1f754c20927fd\u201d,\n\u201chid\u201d: 9,\n\u201cname\u201d: \u201cFilter by quality on data 1\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201crg|rg_selector\u201d: \u201cdo_not_set\u201d,\n\u201canalysis_type|analysis_type_selector\u201d: \u201cillumina\u201d,\n\u201coutput_sort\u201d: \u201ccoordinate\u201d\n}\n}"
    },
    {
      "role": "system",
      "text": "We fixed the server a few minutes ago. Now it\u2019s working properly"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to access the RNA category of the toolshed at @link https://toolshed.g2.bx.psu.edu/. When I click RNA nothing happens and it seems to be waiting for a response from @link https://toolshed.g2.bx.psu.edu that never comes. The page is also very slow to load so this could be related. Can anyone advise me in this regard? Thank you so much!!"
    },
    {
      "role": "system",
      "text": "Hello @user\nThere was another problem that we just resolved. This exact query worked for me. Please also try now and let us know if it doesn\u2019t work.\nThanks for your patience! Jen"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I am using cuffdiff in the galaxy. Between my cuffdiff output, there are no isoforms differential expression file and I get 17 files in my output instead of 23. how can I get isoforms differential expression file?\nplease guide me.\nthank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nUpdated transcriptomic analysis methods are strongly recommended, and many tools for transcriptomics analysis are covered in the Galaxy tutorials here: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/). You can also review the RNA-seq or RNA Analysis tool groups.\nIf you get stuck with a non-deprecated tool, review this troubleshooting FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html) first, then ask questions here with the context from your troubleshooting and a shared history (@link https://training.galaxyproject.org/training-material/faqs/galaxy/histories_sharing.html).\n@quote\nBetween my cuffdiff output, there are no isoforms differential expression file and I get 17 files in my output instead of 23\nCuffdiff and many other of the \u201cCuff\u201d tools are deprecated on most public Galaxy servers. Check to see if this, or any of the tools that you may have used upstream, is marked deprecated where you are working. Deprecated tools may directly fail, or they might just produce odd results.\nEven if the \u201cCuff\u201d tool is not marked as deprecated on a particular server, it may still produce odd results or have limited/modified results for technical reasons. These tools won\u2019t be updated. That said, common problems that people used to run into usually involved the reference annotation. Upstream from that, the BAM sometimes had problems. Both were usually about missing attributes or format issues.\nThis link (@link https://galaxyproject.org/search/?q=biostars%20cuffdiff#gsc.tab=0&gsc.q=biostars%20cuffdiff&gsc.page=1) points to a search of our prior forum with the keywords \u201cbiostars cuffdiff\u201d. That may not fix the problem you are having (missing expected outputs), but if you really want to use the old tool, checking the usage/inputs for prior known issues is where to start.\nTo do your own search, go to the  Pan-Galactic Google Search (@link https://galaxyproject.org/search/?q=) and add in the search term (names other \u201cCuff\u201d tools used upstream? or the mapping tool?) along with the keyword \u201cbiostars\u201d to limit the results. This forum doesn\u2019t include that original discussion since it started being used a few years after those tools were deprecated."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have tried to download gemini database through the gemini data manager, and encountered an error related\u2026\n\nimage1321\u00d7101 18.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/56c7819ae583c40dc19ad28b46cc64de9f169d7f.png)\n\nimage1321\u00d7268 44.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/27a8e54dc839bcdcb60ca1ab4e5b4845207733f7.png)\nAfter everything has been downloaded\u2026 it seems like python script failed\u2026\nThere were no options to change besides some optional one which are part of database information (and failed as well when checked them, with the same error as well)\nDoes anyone knows how to help me?"
    },
    {
      "role": "system",
      "text": "one question though: can you provide a bit of details on how you\u2019ve been installing the DM\u2019s dependencies and on which system?\nSince gemini requires python2 and conda doesn\u2019t have a python2-build of pyyaml > 5.3.1, you should only see a Warning, but no error w.r.t. the Loader.\nCan you check which versions of gemini and pyyaml you have ended up with?\n@user ok, we\u2019ve found otu what he issue with the data manager was and released a new version on the toolshed, which should fix things.\nPlease update to that latest version and try again."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am running galaxy in a container (using image: GitHub - bgruening/docker-galaxy-stable: Docker Images tracking the stable Galaxy releases. (@link https://github.com/bgruening/docker-galaxy-stable)). I was wondering if there is a way to browse files inside of the running container rather than on my local file system (i.e. my laptop) while doing the file upload.\nThank you for your time and help,\nAnna\n\nimage914\u00d7570 30.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c58b274d5769347d2ec5efcd32bcba5e1b19e27b.png)"
    },
    {
      "role": "system",
      "text": "I am not sure what would be the best solution but I see you dont have an answer yet. My best guess now would be to check out data libraries. Here is some info:\n\n\n\n@link https://galaxyproject.org/data-libraries/\n\n\nGalaxy Data Libraries (@link https://galaxyproject.org/data-libraries/)\nAll about Galaxy and its community.\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/data-library/tutorial.html)\n\n\nGalaxy Training: Data Libraries (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/data-library/tutorial.html)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Im trying to add annotations to a vcf file.\nI had paired-end reads of 3 individuals, I aligned each pair to the human hg19 genome, then I generated the VCF file using FreeBayes to call for variants of the 3 sets of data, then i wanted to annotate these variants using ANNOVAR Annotate VCF.\nthe output was:\n\u201c0 lines\nformattabulardatabasehg19\nWARNING to old ANNOVAR users: this program no longer does line-to-line conversion for multi-sample VCF files. If you want to include all variants in output, use \u2018-format vcf4old\u2019 instead.\nNOTICE: Finished reading 2497 lines from VCF file\nNOTICE: A total\u201d\nI did this previously on a VCF file with a single set of data (1 sample) and it worked fine. I looked up the help forum, and then found an advice to split the samples and then run ANNOVAR tool. I split the samples using VCFselectsamples tool and then when I ran the Annovar on these single-sample files I obtained a file with no data (0 lines & 0bytes) for each.\nI have to point out that I re-ran the annovar tool step I used before (which worked fine) and it gave me 0 lines as well!! so does this mean ANNOVAR is down? I was also looking for alternative tools, but they all look very complicated and Im still a beginner.\nSo could someone support me with this issue? how can I get the task of annotating VCF done? I mean it should be straight forward but I spent 2 whole days reading forums and trying things and still getting no success.\nThanks in advance,\nYahia"
    },
    {
      "role": "system",
      "text": "Hello @user\nYes, Annovar at @link https://usegalaxy.org is currently non-functional. The tool will likely be deprecated: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/284\nAlternative VCF annotation tools can be found in the tool grouping \u201cVariant Calling\u201d. SnpEff is one example.\nGTN Tutorials may help with usage and creating a workflow. Start with those under the topic \u201cVariant Analysis\u201d.\n@quote\nGalaxy Training Network Tutorials: SomeGTNtutorials are appropriate forGalaxy Mainand some are not. Where you can run each is noted per tutorial \u2013 click on theGalaxy instancesgear iconto review thePublic Galaxyserver choices. If a tutorial is supported by a pre-configuredGalaxy Dockertraining image, instructions for how to get it will be listed below the tutorial listings, per category.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am a new user of galaxy. I want to searchThermo RAW files using maxquant on galaxy. But, it runs into an error. One of my colleaguge thinks it might be because of insufficent memory. I appreciate any idea how to solve it? Thank you for your help,\nDataset Error Report\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/1.6.17.0+galaxy4.\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 134 ()\nTool generated the following standard error:\nUnhandled Exception: System.Exception: Exception during execution of external process: 2988633\nat QueueingSystem.WorkDispatcher.ProcessSingleRunExternalProcess(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.DoWork(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.Work(Object threadIndex)\nat System.Threading.Thread.ThreadMain_ParameterizedThreadStart(Object parameter)\nat System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)\n\u2014 End of stack trace from previous location where exception was thrown \u2014\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n/usr/local/bin/maxquant: line 4: 2986039 Aborted                 (core dumped) dotnet $DIR/MaxQuantCmd.exe $@\nDetected Common Potential Problems\nThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.\nThe tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices."
    },
    {
      "role": "system",
      "text": "Hi @user\nThese parts of an error message are from Galaxy\u2019s automatic input checks:\n@quote\nDetected Common Potential ProblemsThe tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices.The tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\nTo review the inputs sent to the job in a top level summary, examine the \u201cDataset Information\u201d summary page (:information_source: icon). Were any of the input datasets included more than once? Do all inputs contain content? More checks are listed here @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages.\nTutorials with example data and usage help:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/proteomics/)\n\n\nGalaxy Training: Proteomics (@link https://training.galaxyproject.org/training-material/topics/proteomics/)\nTraining material for proteomics workflows in Galaxy\n\n\n\n\n\n\nIf you can\u2019t find the problem, please share a few more details. The content of the \u201cDataset Information\u201d  page can be screenshot or copy-pasted back. Or sometimes better, post back a share link to the history."
    }
  ],
  [
    {
      "role": "user",
      "text": "In the FeatureCounts->advanced setting-> GFF feature type filter, the default is exon, but if change to intron, the values are the same, and if leave it blank, it still the same. So it looks like this function do not work. I need to get the counts for the whole gene (GRO-seq), including UTR, exon, and intron, how can I do that?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nwhich annotation file are you using? I checked the wrapper, and it seems to work fine.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m relatively new to all of this and managed to successfully launch my GVL server on cloudman, woo!\nI\u2019m having issues uploading data now though. At first I tried uploading BAM files via local upload, which I realize now was a bit dumb. I\u2019m now trying to setup and FTP connection to my cloudman server, but having a very difficult time.\nI read the documentation on setting up an FTP server, but for someone not in the know like myself, I\u2019m finding it difficult. I\u2019ve used FileZilla in the past for a separate EC2 connection on AWS. Is it possible for me to connect to my server via FileZilla? I have my pem key stored on FileZilla and everything, but I\u2019m getting errors connecting.\nDoes anyone have any guide for dummies or a video detailing the process?"
    },
    {
      "role": "user",
      "text": "Apologies, I did. I got a very helpful response for accessing the admin panel, and replied with the issue I was having on uploading data.\nSorry for the cross post, and thank you for the help with figuring this out!"
    }
  ],
  [
    {
      "role": "user",
      "text": "error\nDoes anyone know why I might be getting this error?\nI\u2019m trying to use HISAT2 on forward and reverse reads of files.\nI would think that it wouldn\u2019t matter how many reads were in the forward vs. reverse files."
    },
    {
      "role": "system",
      "text": "Dont know much about exceptions but in general you always need to have the same number of forward and reverse reads. You can double check the number of reads with fastqc. The main question is why are the number of reads not equal? Did you do any pre-steps like quality trimming? In that case make sure you use a tool that is designed for paired-end data."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m trying to download some files directly from Ensembl FTP:\nUpload Data (Download from web or upload from disk) > Choose remote files > ENSEMBL FTP server\nUnfortunately, the only message I receive is:\n\n\u201cProblem listing file source path FileSourcePath(file_source=, path=\u2018/\u2019)\u201d.\n\nCould someone please help me to solve the problem? Or maybe suggest another way to do this?\nBest,\nMagdalena"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you provide more details about which file are you trying to download? I checked it with a few of them, and it seems to work fine.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear galaxy project members,\nI have been having a lot of problems running unicycler since at least a month now. I have reported every different bug that came across and I have exchanged a few emails with people from the galaxy support team, they told me that it was due to an upgrade of the hosted tool. But trying after that I keep on having problems. Unicycler just keeps failing.\nCould you please help me on this? I really need the results.\nThank you very much!"
    },
    {
      "role": "system",
      "text": "Update, please see run Unicycler always got \" unicycler: command not found\" -- RESOLVED at UseGalaxy.org 20230210 (@link https://help.galaxyproject.org/t/run-unicycler-always-got-unicycler-command-not-found-resolved-at-usegalaxy-org-20230210/9297)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have been trying to run SnpEff in Galaxy, but have been facing a number of issues that I am not sure how to resolve.\nBackground: I am analysis different strains of Mycobacterium tuberculosis. I have created a vcf file, and would now like to see the effects of my variants, hence I am using SnpEff.\nProblems:\n\nWhen I use the SnpEff download tool to download the SnpEff database for Mycobacterium_tuberculosis_h37rv, the tool runs successfully but outputs a file with only 2 lines. (SnpEff version number and Mycobacterium tuberculosis H37Rv).\n\nI tried using the SnpEff build tool as well with the Genbank annotation file, however it results in the same 2 line output.\n\nBecause of this problem, I directly downloaded the SnpEff database from SourceForge, and uploaded the file as a Snpeffdb to galaxy. This file however, shows as \u2018unavailable\u2019 in SnpEff eff. The issue might be with the metadata, which I am not able to edit manually and autodetect does not work.\n\nAs you can see I have tried all the approaches I can think of and nothing seems to work. I even used the @link http://galaxy.eu version. Please help, I am new to NGS analysis and will appreciate any guidance.\n@system @system"
    },
    {
      "role": "system",
      "text": "Hmm, from your description (which is very detailed BTW - thanks for that!) there does not seem to be anything wrong here. Those two lines of output are how Galaxy presents the newly downloaded genome to you. After all, SnpEff genome files are kept in a binary format and there is not really a more helpful way to display that data.\nWhat you should try next is to go to the SnpEff eff tool, select Downloaded snpEff database in your history under Genome source, and the version you downloaded via SnpEff download should be offered as a choice.\nBuilding the genome file yourself should also work, but is not recommended if there\u2019s an exisitng genome for download. Such genomes should be offered if you select Custom snpEff database in your history as the Genome source.\nUploading an externally built SnpEff genome to a Galaxy history is not currently supported.\nLet us know if you keep facing issues and best wishes,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to use Velvet for educational purpose.\nI was able to search and use that tool until yesterday, but it suddenly disappeared.\nCould you check this problem ?\nThanks !"
    },
    {
      "role": "system",
      "text": "@quote\nEU for scheduled maintenance, should be back soon\n@link http://usegalaxy.eu is back up and does host a few velvet tools, but for limited usage (training-sized datasets). Details about server status earlier today are in this Q&A: is galaxy Australia down? (@link https://help.galaxyproject.org/t/is-galaxy-australia-down/804/3?u=jennaj)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI want to delete a test user. But i get the following error:\n\nUser deletion must be configured on this instance in order to allow user self-deletion. Please contact an administrator for assistance.\n\nBut i can not find any facilities neither to delete user, nor to \u201callow user self-deletion\u201d in the admin panel, moreover, google also told me nothing about this!\nHow can i delete some user?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe feature was introduced in the 20.09 release. Is your server upgraded to at least that or a later release? The 20.01 release is the most current.\n*September 2020 Galaxy Release (v 20.09) \u2014 Galaxy Project 21.01 documentation (@link https://docs.galaxyproject.org/en/master/releases/20.09_announce_user.html#you-can-self-delete-your-account) << where the feature and the PR for that change are in the release notes.\nThere is also a galaxy.yml option that allows admins to delete accounts (Admin > User Management > Users). Search for the user by email and manage accounts in the browser. The default is false \u2013 change to true and remove the hash to enable.\n   # Allow administrators to delete accounts.\n   #allow_user_deletion: false\n\nThere are admin command-line options as well. The utility gxadmin is helpful and functions can be modified/added. Useful for batch admin actions when the browser is way too tedious due to volume, or queries that are not wrapped in the browser for practical reasons.\n*GitHub - galaxyproject/gxadmin: Handy command line utility for Galaxy administrators (@link https://github.com/galaxyproject/gxadmin)\n*Searching the Galaxy (@link https://galaxyproject.org/search/?q=gxadmin#gsc.tab=0&gsc.q=gxadmin&gsc.page=1)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have high-quality draft genomes and I\u2019m having issues running them on GrowthPred to calculate predicted ideal growth temperature. I\u2019ve uploaded the following files (see below) and in all instances I\u2019ve received the error \u201cERROR: sequence length not multiple of 3:\u201d (see screenshot attached).\nI\u2019ve uploaded:\nfull genome fasta\nfeature only genome fasta (should only include the CDS regions)\nGBK file obtained from ANTISMASH that should only include the CDS regions\nAny advice?\nThank you!\n\nScreen Shot 2022-06-28 at 9.41.56 AM932\u00d7178 6.02 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cd5477bc2fb5a009ee3438d6a659de8d1e25656c.png)"
    },
    {
      "role": "system",
      "text": "I dont know this tool and could also not find it on galaxy (maybe I dont look good enough, your tag says you use local). But the error is clear, did you manually checked if the length of the CDS regions is indeed not a multiple of 3? If not, your input is not correct. So the advise would be to double check the lengths and maybe run the tool on a very small dataset where you are sure the CDS input is a multiple of 3."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have a question about conda used in galaxy. In 2020, anaconda\u2019s TOS changed and organizations larger than 200 people will have to pay for the use of the anaconda defaults repository.\n\n\n\nAnaconda (@link https://www.anaconda.com/blog/anaconda-commercial-edition-faq)\n\n\n\nAnaconda | Anaconda Commercial Edition FAQ (@link https://www.anaconda.com/blog/anaconda-commercial-edition-faq)\nAnaconda Commercial Edition FAQ\n\n\n\n\n\n\nGalaxy uses conda for dependency resolver and it is very useful. I think many of tools can be installed with conda-forge , bioconda , and iuc repository, but sometimes need to use the defaults repository. Do public and local Galaxy server admins belonging to a large for-profit organization charge for the use of the anaconda defaults repository? Has this ever been discussed in Galaxy community?\nBest,\nYukie"
    },
    {
      "role": "system",
      "text": "Hi,\nif you follow the best-practices and use only packages from Bioconda and conda-forge this should not be a problem as most if not all packages are on both channels. In addition, Anaconda is so generous and supporting both community projects with free CDN hosting and unlimited space.\nI hope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m new to Galaxy, so I hope this is the correct way to post a question. I\u2019m attempting to assemble a bacterial genome using a pair of Illumina reads and a PacBio read with ABySS and with Unicycler. I\u2019ve tried doing this in several different ways according to the information included in the tool descriptions, but both assemblers fail to run and give a tool error. What are the correct file types and suffixes for each read in each assembler? (by suffixes, I mean the ending of \u201c1\u201d and \u201c2\u201d for paired short reads, for example).\nThank you for your help, I look forward to learning more about Galaxy."
    },
    {
      "role": "system",
      "text": "Unicycler is functional. ABySS has been removed.\nGTN Tutorial for assembly (including Unicycler):\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/assembly/\n\n\n\nGalaxy Training: Assembly (@link https://training.galaxyproject.org/training-material/topics/assembly/)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\n@quote\nUpdateThe assembler ABySS has been technically problematic atUseGalaxy.orgfor some time now.The tool has been removed fromUseGalaxy.org.An alternative usegalaxy.* server for this tool isUseGalaxy.eu"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello I am helping a friend running Jbrowse and it seems like Jbrowse (ie Jbrowse genome browser, toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1) is broken on @link http://usegalaxy.org. This is the result I get running it on @link http://usegalaxy.org while it runs fine on @link http://usegalaxy.eu using the same dataset.\n\nJbrowse1355\u00d7663 15.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/98dd6e845083638b0467f6921c25ad18b73f34a6.png)\nBest regards\nTomas"
    },
    {
      "role": "system",
      "text": "Thanks for the ping!  Should be good to go now, refresh and let me know if it\u2019s not better."
    }
  ],
  [
    {
      "role": "user",
      "text": "The C. elegans genome version available for the RNA STAR tool is very old, WS220 (Oct 2010 release). Many changes have been incorporated since then and the latest version (as of now) is WS284 (May 2022 release: Wormbase release WS284 - WormBase : Nematode Information Resource (@link https://wormbase.org/about/wormbase_release_WS284)). Can the Galaxy Team please make this genome version available for RNA STAR analysis? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe installation of the latest C. elegans genome has been requested to the Galaxy admins. Since it could take a few days, meanwhile I recommend you upload the genome reference to your history and use it from them.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am running Featurecounts for Desulfovibrio alaskensis RNA seq data. I am stuck with the following error.\nERROR: failed to find the gene identifier attribute in the 9th column of the provided GTF file. The specified gene identifier attribute is \u2018gene_id\u2019\nCould anyone help me with this?"
    },
    {
      "role": "system",
      "text": "Hi @user\nCheck your reference annotation (GTF) dataset. Does it have that attribute in the  9th column? For all rows? No headers?\nIf not, you\u2019ll need to source and format a GTF that works with this tool (and many others).\nFAQ: Datatypes - Galaxy Community Hub (@link https://galaxyproject.org/learn/datatypes/#gtf)\nYou can also click on the tags I added for prior Q&A about this with context.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am following the instruction of 16S Microbial Analysis with mothur (extended) and using the example data. Everything went well until the step of using Unique.seqs. I have attached the parameter that I used and the error messages. Can anyone please let me know what did I do wrong? Thank you!\n\nCapture1604\u00d7879 118 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7a04380243dc76559cf129f46a1fee504a615faa.png)"
    },
    {
      "role": "system",
      "text": "Status resolved: Troublshooting Unique.seqs: Status: resolved, please rerun prior failures (@link https://help.galaxyproject.org/t/troublshooting-unique-seqs-status-resolved-please-rerun-prior-failures/9503)\n\n\nWelcome, @user\nThis tool has a small bug right now. Please see the solution on this topic for the current options.\n@quote\nJennaj, \nI am running into problems with the Unique.seq tool in Galaxy.  I use Galaxy online to teach a course and we use the 16S Metagenomics (Mouse) dataset as the training module. I have been using this approach for a couple of years now, but we have run into a Fatal Error when running the first step of the Unique.seq tool.  We have tried to run Unique.seqs from multiple accounts (students and mine), but always using Galaxy online. I have tried it from our computers at work (faster internet) \u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to perform a scRNA sequence experiment with the Galaxy tutorial Generating a single cell matrix using Alevin (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/droplet-quantification-preprocessing/tutorial.html) and keep hanging up when I get to the SalmonKallistoMtxTo10x step.  The failure happens both with the data set in the tutorial and with a smaller data set directly from 10x genomics.  Details on data set and run parameters below.  Any suggestion on how to get past this blockage would be helpful.  The error output files are empty.\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/droplet-quantification-preprocessing/tutorial.html\n\n\nGalaxy Training: Generating a single cell matrix using Alevin (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/droplet-quantification-preprocessing/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n1k PBMCs from a Healthy Donor (v3 chemistry)\nSingle Cell Gene Expression Dataset by Cell Ranger 3.0.0\nPeripheral blood mononuclear cells (PBMCs) from a healthy donor (the same cells were used to generate pbmc_1k_v2, pbmc_10k_v3). PBMCs are primary cells with relatively small amounts of RNA (~1pg RNA/cell).\n\u2022\t1,222 cells detected\n\u2022\tSequenced on Illumina NovaSeq with approximately 54,000 reads per cell\n\u2022\t28bp read1 (16bp Chromium barcode and 12bp UMI), 91bp read2 (transcript), and 8bp I7 sample barcode\n\u2022\trun with --expect-cells=1000\n\nGTF2GeneList extracts a complete annotation table or subsets thereof from an Ensembl GTF using rtracklayer (Galaxy Version 1.42.1+galaxy6)\nEnsembl GTF file\nFeature type for which to derive annotation\ntranscript\nField to place first in output table\ntranscript_id\nSuppress header line in output?\nYes\ntranscript_id,gene_id\nAppend version to transcript identifiers?\nYes\nFlag mitochondrial features?\nNo\nFilter a FASTA-format cDNA file to match annotations?\nYes\nAnnotation field to match with sequences.\ntranscript_id\nRename galaxy-pencil the annotation table to Map\nRename galaxy-pencil the uncompressed filtered FASTA file to Filtered FASTA\n\nAlevin Quantification and analysis of 3\u2019 tagged-end single-cell sequencing data (Galaxy Version 1.3.0+galaxy2)\nTool Parameters\nInput Parameter\tValue\nSelect a reference transcriptome from your history or use a built-in index?\thistory\nTranscripts fasta file\t10 Filtered FASTA uncompressed (Hidden)\nKmer length\t31\nPerfect Hash\tFalse\nSingle or paired-end reads?\tpaired\n4 pbmc_1k_v3_S1_L001_R1_001.fastq.gz\n5 pbmc_1k_v3_S1_L001_R2_001.fastq.gz\nRelative orientation of reads within a pair\tMates are oriented toward each other (I = inward)\nSpecify the strandedness of the reads\tread comes from the reverse strand (SR)\nprotocol\t10x chromium v3 Single Cell protocol\nTranscript to gene map file\t9 Map\nRetrieve all output files\tTrue\noptional\t\nWhitelist file\t\nnoDedup\tFalse\ndumpBfh\tFalse\ndumpFeatures\tTrue\ndumpUmiGraph\tFalse\ndumpMtx\tTrue\nforceCells\tNot available.\nexpectCells\tNot available.\nnumCellBootstraps\tNot available.\nminScoreFraction\tNot available.\nkeepCBFraction\t1.0\nlowRegionMinNumBarcodes\tNot available.\nmaxNumBarcodes\tNot available.\nfreqThreshold\t3\n\nSalmonKallistoMtxTo10x Transforms .mtx matrix and associated labels into a format compatible with tools expecting old-style 10X data (Galaxy Version 0.0.1+galaxy5)\nTool Parameters\nInput Parameter\tValue\n.mtx-format matrix\t11 quants_mat.mtx\nTab-delimited genes file\t13 quants_mat_cols.txt\nTab-delimited barcodes file\t14 quants_mat_rows.txt\nPrefix to prepend to cell names / barcodes\tEmpty"
    },
    {
      "role": "system",
      "text": "Hi @user  and @system\n@quote\nOSError: [Errno 30] Read-only file system: \u2018.//matrix.mtx\u2019\nThis tool SalmonKallistoMtxTo10x needs some tool developer then administrative changes to work at @link http://UseGalaxy.org. I suspect Alevin has this problem as well (not confirmed yet).\nPlease try running this tutorial, or any workflows based on it, at the @link http://UseGalaxy.eu server instead for now.\nIf you need to move existing data between the servers, the how-to is in this FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#moving-datasets-between-galaxy-servers).\nPs: @system , Thanks for submitting the bug reports. I replied to some of those already re: mixed up inputs. That is a bit common with this tutorial since it repeats groups of steps with slightly different criteria a few times, which is arguably confusing but unavoidable when doing the work step-by-step and not with a workflow/collection yet. You could consider adding in dataset #tags (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_add_tag.html) to help keep track of the different runs (example screenshot in that tutorial with tags included (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/droplet-quantification-preprocessing/tutorial.html#adding-batch-metadata)). Later on when using a workflow and collections, tags or not, that will be less likely to happen, and is one reason why both of those functions are popular and worth learning about (search the GTN tutorials with keywords for help with those: \u201ccollection\u201d and/or \u201cworkflow\u201d).\nHope that helps, and apologies for the confusing tool trouble on top of a complicated tutorial!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey,\nI found that I could not download the MultiQC HTML in general. it gave me errors like the following:\nSnip20201012_21432\u00d7692 22.3 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/da603abd4e4bcd24235415f3247f7cc9b0e28e4d.png)\nSnip20201012_61382\u00d7852 147 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/180112fafaf2ce7d9ac66a183707ce6ef050c229.png)\nDoes anyone know how to troubleshoot?\nKira"
    },
    {
      "role": "system",
      "text": "Please see: UseGalaxy.org scheduled maintenance downtime October 13, 2020 -- status and updates (@link https://help.galaxyproject.org/t/usegalaxy-org-scheduled-maintenance-downtime-october-13-2020-status-and-updates/4605)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I wrote five tools to process genetic data. All them were verified to work correctly out of galaxy and also as isolated tools inside galaxy. Also, the intermediate results, produced by a tool and used as an input of the next one, are ok. In short, everything works well when using the tools in isolation, but not when they are laid in sequence in a workflow. To illustrate, I show here the xml files of the first two tools.\nTool1: reads a paired collection of genetic data where the genes are identified by Gene Symbol and changes the identification to UniProt Id.\nCurrent xml file is as below:\n<tool id=\"gs2up\" name=\"GS2UP: Gene Symbol to UniProt\" version=\"0.1.0\">\n  <description>-</description>\n  <command>\n    <![CDATA[ perl $__tool_directory__/GS2UP.pl $gstcga $gstcga.name $uptcga ]]>\n  </command>\n  <inputs>\n    <param type=\"data\" name=\"gstcga\" format=\"tabular\" />\n  </inputs>\n  <outputs>\n    <data format=\"tabular\" name=\"uptcga\" label=\"up_$(gstcga.name)\" />\n  </outputs>\n</tool>\n\nTool 2: reads a paired collection of genetic data, where the genes are identified by UniProt Id and for each dataset generates three output datasets. That is to say, this tool uses a paired collection as input and generates a triplet list.\nThe xml file is as below:\n<tool id=\"urgel\" name=\"URGEL\" version=\"0.1.0\">\n  <description>-</description>\n  <command>\n    <![CDATA[perl $__tool_directory__/URGEL.pl $tcga.forward $tcga.reverse $triplet.thrsh $triplet.diffrn $triplet.upreg ]]>\n  </command>\n  <inputs>\n    <param name=\"tcga\" type=\"data_collection\" collection_type=\"paired\" format=\"txt\" />\n  </inputs>\n  <outputs>\n    <collection name=\"triplet\" type=\"list\" label=\"ur_$(tcga.name)\" >\n      <data name=\"thrsh\" format=\"txt\" />\n      <data name=\"diffrn\" format=\"txt\" />\n      <data name=\"upreg\" format=\"txt\" />\n    </collection>\n  </outputs>\n</tool>\n\nAs a newbie to Galaxy, I may be incurring an error. However, I can\u2019t find it alone and would appreciate any help."
    },
    {
      "role": "user",
      "text": "I solved it by myself. In \u201cEdit Workflow\u201d screen, there is a button \u201cInputs\u201d to define types of input data.\nThanks a lot."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I am running a galaxy in my institute. Lately my galaxy has a lot of hiccups. Installing tools doesn\u2019t work. These will keep cycling on the \u201cnew\u201d status. Internally I see the error below.\nWhen I restart Galaxy the issue is fixed, only for the issue to reoccur the next day.\nWhat I can grok from the error is that it has something to do with a connection that explicitly needs to call a rollback. This is not something that can be fixed by an end-user.\nDoes this issue have any known cause? I work with PostgreSQL as a backend database, so nothing too fancy. The Galaxy version is 21.09.\nThanks!\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.tool_shed.galaxy_install.repository_dependencies.repository_dependency_manager DEBUG 2021-11-26 16:03:06,145 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Building repository dependency relationships...\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.tool_util.toolbox.base DEBUG 2021-11-26 16:03:06,145 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Appending to tool panel section: ARGalaxy Pipelines\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: galaxy.web.framework.decorators ERROR 2021-11-26 16:03:06,156 [pN:main.web.1,p:372188,w:1,m:0,tN:uWSGIWorker1Core2] Uncaught exception in exposed API method:\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: Traceback (most recent call last):\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/web/framework/decorators.py\", line 312, in decorator\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     rval = func(self, trans, *args, **kwargs)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/webapps/galaxy/api/tool_shed_repositories.py\", line 161, in install_repository_revision\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     payload)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 714, in install\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     install_options\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 808, in __initiate_and_install_repositories\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return self.install_repositories(tsr_ids, decoded_kwd, reinstalling=False, install_options=install_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 827, in install_repositories\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     tool_panel_section_keys=tool_panel_section_keys)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/galaxy_install/install_manager.py\", line 1102, in order_components_for_installation\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repo_info_dicts)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 265, in get_prior_import_or_install_required_dict\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repository, repository_dependencies = get_repository_and_repository_dependencies_from_repo_info_dict(app, repo_info_dict)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 299, in get_repository_and_repository_dependencies_from_repo_info_dict\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     repository = get_repository_for_dependency_relationship(app, tool_shed, repository_name, repository_owner, changeset_revision)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tool_shed/util/repository_util.py\", line 380, in get_repository_for_dependency_relationship\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     app.tool_shed_repository_cache.rebuild()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/server/lib/galaxy/tools/cache.py\", line 304, in rebuild\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     defer(ToolShedRepository.metadata), joinedload('tool_dependencies')\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 2709, in all\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return self._iter().all()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 2847, in _iter\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     execution_options={\"_sa_orm_load_options\": self.load_options},\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/orm/session.py\", line 1689, in execute\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     result = conn._execute_20(statement, params or {}, execution_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1582, in _execute_20\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     return meth(self, args_10style, kwargs_10style, execution_options)\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\", line 324, in _execute_on_connection\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     self, multiparams, params, execution_options\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1461, in _execute_clauseelement\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     cache_hit=cache_hit,\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1668, in _execute_context\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     conn = self._revalidate_connection()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 560, in _revalidate_connection\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     self._invalid_transaction()\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:   File \"/galaxy/venv/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 540, in _invalid_transaction\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]:     code=\"8s2b\",\nNov 26 16:03:06 my.galaxy.tld uwsgi[372188]: sqlalchemy.exc.PendingRollbackError: Can't reconnect until invalid transaction is rolled back. (Background on this error at: http://sqlalche.me/e/14/8s2b)\n"
    },
    {
      "role": "user",
      "text": "Hi @system ,\nThank you for your response. I found the error. The database connection was unstable due to a configuration issue. The postgresql log had entries complaining about broken or aborted connections. The cause was found and now the issue has been resolved.\nThanks again for looking into this!\nBest regards,\nRuben"
    }
  ],
  [
    {
      "role": "user",
      "text": "The annotation I am trying to use is the Zm-B73-REFERENCE-NAM-5.0_Zm00001eb.1.gff3.gz file linked here: Index of /Zm-B73-REFERENCE-NAM-5.0 (@link https://download.maizegdb.org/Zm-B73-REFERENCE-NAM-5.0/). I suspect this gff3 file is the source of the problems and needs to be altered somehow to work with featurecounts\nI have paired-end RNA-seq reads from maize that I am trying to convert to counts using featurecounts. I basically followed the steps outlined in this tutorial, but with a few differences (e.g. my RNA library is forward-stranded and has longer reads): Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#annotation-of-the-deseq2-results)\nThe creation of a collection of 2 paired ends, quality control, and initial mapping with RNA-star seemed to work, but then when I tried to do featurecounts and got an error: \u201cfailed to find the gene identifier attribute in the 9th column of the provided GTF file. The specified gene identifier attribute is \u2018gene_id\u2019. An example of attributes included in your GTF annotation is \u2018Parent=Zm00001eb000010_T001;Name=Zm00001eb000010_T001.exon.1;ensembl_end_phase=0;ensembl_phase=0;exon_id=Zm00001eb000010_T001.exon.1;rank=1\u2019.\u201d\nThen I looked into my file and saw that it was using \u201cgene_id\u201d as an identifier, so I tried GFF gene identifier: gene_id, which is used in the 9th column of my file; however, I still got \u201cfailed to find the gene identifier attribute in the 9th column of the provided GTF file\u201d error.\nThen I tried leaving the \u201cGFF feature type filter\u201d and \u201cGFF gene identifier\u201d fields empty, but I got an error saying \u201cno features were loaded in format GTF. The annotation format can be specified by the \u2018-F\u2019 option, and the required feature type can be specified by the \u2018-t\u2019 option\u201d\nWhat I tried next: I learned that featurecounts requires a genome to be specified, so I created a custom genome by uploading the fasta file Zm-B73-REFERENCE-NAM-5.0.fa.gzfrom the page linked above. I ran NormalizeFasta as suggested to create a uniform line length of 80 and created a custom genome, maize_v5. I went back in my history and tagged the gff3 file, the reads, and the RNA Star outputs as being associated with maize_v5 and ran it again, but the same errors occurred.\nWhat should I do next to troubleshoot this problem?\nThanks!"
    },
    {
      "role": "user",
      "text": "Update: It seems to have worked! I was able to get some kind of reasonable output by just setting GFF feature type filter to \u201cgene\u201d and GFF gene identifier to \u201cID.\u201d It didn\u2019t work before when I used \u201cexon\u201d and \u201cID,\u201d because the exon columns don\u2019t have an \u201cID\u201d assigned to them, only a \u201cParent.\u201d Since what I\u2019m interested in is just the gene counts, I\u2019m happy with this result. Thanks for your hint to focus very carefully on the feature type and gene identifier terms."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI was running RNA STAR and got this error: |Job Start Time|2022-09-15 23:24:13|\n\nimage886\u00d7113 3.22 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cbd735ee11e73821218380284ae8337c32eee7cc.png)\n\nFATAL:   could not open image /cvmfs/singularity.galaxyproject.org/all/mulled-v2-69a6f67cb46e41b4c393f71634a9956d5e31f3e9:8047ce78ee60f6a1f4390a36cee78a99371e4f59-0: failed to retrieve path for /cvmfs/singularity.galaxyproject.org/all/mulled-v2-69a6f67cb46e41b4c393f71634a9956d5e31f3e9:8047ce78ee60f6a1f4390a36cee78a99371e4f59-0: lstat /cvmfs/singularity.galaxyproject.org: transport endpoint is not connected\n\nWhich is an error that I see has been popping up from last month. I am wondering if there is an issue with the cvmfs dependency once again.\nPlease let me know if there is a different issue that I can try on my end. Otherwise, I would like to know when I should try re-running the workflow.\nAll the best,\nChristine"
    },
    {
      "role": "user",
      "text": "Hello,\nI ran the same workflow with the same datasets this morning: |Job Start Time|\t2022-09-16 11:38:29\nand STAR was able to successfully finish mapping! It did not run into the same cvmfs error.\nI don\u2019t know how the error resolved itself, but I wanted to let you all know.\nThanks!\n-Christine"
    }
  ],
  [
    {
      "role": "user",
      "text": "Update: The server is back up! Please rerun any failed jobs.\nThe @link http://UseGalaxy.org server is offline right now, and no one can log in. We expect the unscheduled maintenance event to be resolved within a day. Updates will post back to this topic.\nOnce the server is up, please rerun any jobs that failed.\n\n\nHello everyone,\nKindly note that I am facing issues with jobs from yesterday. I have several jobs on queue from yesterday for Nanoplot pipeline. I have run this pipeline before as well but this had never happened\nKindly help me out and let me know how I can resolve this issue.\nThanks and Regards,\nSagar Upadhyay"
    },
    {
      "role": "system",
      "text": "Thanks @user for reporting the problems. Hopefully you have been able to rerun by now. If you run into other issues, please start up a new topic with your question. Thanks for reporting the issues!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI started a local galaxy on a local server. I was able to insert the local reference genomes for mapping in Bowtie2 and BWA-MEM along with doing indexes for it. Mapping works great and without problems.\nI wanted to do variant calling for created BAM files (FreeBayes or VarScan) and I have a problem because this local reference genome that I used for mapping does not appear in these tools. If I make a variant calling on the file hg19.fasta from history, then the numbering unfortunately does not match.\nHow to make these reference genomes (from Bowtie2 or BWA) available in other tools?\nThanks in advance for your help"
    },
    {
      "role": "system",
      "text": "So I assume you\u2019ve been using the corresponding data_managers to build the Bowtie2 and BWA-MEM indices?\nWhat freebayes and varscan need is a genome and its samtools .fai index. There is a dedicated data manager for this type of index, too, and it\u2019s called @link https://toolshed.g2.bx.psu.edu/view/devteam/data_manager_sam_fasta_index_builder/f8418a1bf7d0. Run that on your ref genomes and they should appear in freebayes, varscan, and quite some other tools.\nBest,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI installed the Bowtie2 index builder/data manager from the toolshed (Galaxy | Tool Shed (@link https://toolshed.g2.bx.psu.edu/repository?repository_id=f80c0d648facd3a1)) however whenever I open it, I get the \u201cRequested version unavailable\u201d error at the top, and cannot select any FASTA files as input. I see that the dependencies are resolved for the data manager in the \u201cManage Dependencies\u201d tab of the admin panel (only dep. is bowtie2). I am using index builder v2.3.5.1+galaxy1. Any ideas why this may be? I actually have the same issue with another data manager as well on my system, so basically both of my data managers have that error.\nThanks!"
    },
    {
      "role": "system",
      "text": "@user you can ignore this for the moment. It\u2019s a Galaxy bug that we need to fix. But harmless."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI have an expression matrix and I want to move some of its rows with the help of Galaxy tools\nWhat tool should I use?"
    },
    {
      "role": "system",
      "text": "Hi @user\nDatamash, Reverse, and Transpose are commonly used tools to manipulate a tabular matrix.\nOr, try a Filter or Select tool (there are a few versions available). You can \u201cstack\u201d results back into one file with Concatenate.\nIf you are familiar with regular expressions, tools like Sed and Search in textfiles (grep) are more options.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet)\n\n\nGalaxy Training: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nthe function annotate my IDs failed because of a server problem with following message:\n\nob submission failed\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method\n\u2026\nCan anyone help me please?\nThanks in advance"
    },
    {
      "role": "system",
      "text": "We\u2019ve been having issues with job submission on @link http://usegalaxy.eu for several hours.\nHopefully, things are back to normal now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I wanted to know if it is possible to transfer several items in one history to a new history in another account?\nI understand that it is possible to copy and move items from one history to another.\nBut I wanted to know if it is possible to transfer between two accounts\u061f"
    },
    {
      "role": "system",
      "text": "Hi @user\nJust in case: all public Galaxy servers usually do not support multiple registrations. It is OK to have accounts on different Galaxy servers, but only one account per server.\nYou can copy several datasets into a new history and share it either using a link or directly with another user, if you know the registration email. To share the history, go into History menu (an icon at the top right corner of the history panel (Show history option) > Share or Publish.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey guys,\nFirst time posting here. I\u2019m running an RNA-seq analysis but I am stuck at the Limma-voom step.\nHere\u2019s the error I am getting:\nError in eval(ej, envir = levelsenv) : object \u2018Control_Old\u2019 not found\nCalls: makeContrasts \u2192 eval \u2192 eval\nExecution halted\nI just genuinely can\u2019t understand how it isn\u2019t found. I double checked the spelling and everything."
    },
    {
      "role": "system",
      "text": "@quote\nError in eval(ej, envir = levelsenv)\nHi @system\nThis error comes up when the tool cannot build a data structure properly during processing. This is an R error, so R formatting requirements need to be met.\nThe tool form has examples/detailed help for each of these:\n\nAt least two groups are input\nEach group contains at least two count inputs\nThe same number of count files were input per group\nLabels for groups are all \u201coneword\u201d or \u201cone_word\u201d. Alphanumeric characters plus (optionally) underscores.\nIf you are inputting a matrix instead of individual count files, the same labeling rules would apply.\nSame for contrast inputs \u2013 full group labels separated by a dash \u201c-\u201d.\nWhitespace or any other characters will cause interpretation problems.\n\nIf you can\u2019t find the problem, please post back either a shared history link or a screenshot of the job details page for one of the errored outputs (specifically, the portion of the log where the tool version and inputs/parameters are listed out). How to do both is explained here @link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors\nLet\u2019s start there :slight_smile:\nRelated prior Q&A:  edgeR error in galaxy (@link https://help.galaxyproject.org/t/edger-error-in-galaxy/8058)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am not able to find Bismark aligner for WGBS data on @link http://usegalaxy.org. Can you please tell how I can install it or how to access it?\nThanks!!\nAbishek"
    },
    {
      "role": "system",
      "text": "@quote\nBismark\nHi @user\nPlease use the tool suite at @link https://usegalaxy.eu"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Colleagues\nI would like to ask you an advice for best commercial server that can be used to analyze RNA seq. data. This is due to long time queue waiting for my analysis that I am conducting.\nThanks in advance for any advice\nAdnan"
    },
    {
      "role": "system",
      "text": "Hello @user\nAWS is one of the most popular choices. It requires very little set-up configuration (an AWS account) and all of the Galaxy administration is done through a web interface. AWS also offers grants for academic research projects.\nAll Galaxy choices:\n\n@link https://galaxyproject.org/choices/\n@link https://galaxyproject.org/use/\n\u2026and a bit more detail about how AWS and other options are used for teaching \u2013 may be informative when making your own decision for research work: @link https://galaxyproject.org/teach/computing-platforms/\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Could Galaxy/HicExplorer includes samtools for bam files merge? Or is there a tool for merging bam files in HicExplorer? Hi-C datasets always contain multiple runs. Thanks"
    },
    {
      "role": "system",
      "text": "Hi Chen,\nare you referring to @link http://hicexplorer.usegalaxy.eu? If so you can use this tool: Galaxy | Europe (@link https://hicexplorer.usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/sam_merge/sam_merge2/)\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Wondering if anyone could give a bit of advice on my situation. I have rnqseq ngs data for 6 different human cell lines. Each has a normal and a stressed condition. I have featurecounts for each of those that are the summation of the replicates, so 12 summary count files. I have successfully compared each matching line stressed-normal but was wondering if there was a simple way to visualize possible significant genes when comparing each line to another (eg: instead of just Line1 stressed vs Line1 normal, I could easily plot all of the top 20 or so genes of significance by logFC and visualize overlaps that may exist between Line1 normal and any of the other lines, Line1 normal to Line3 normal. One large multi-colored plot type? Instead of running limma on each possible comparison (66 possible pairwise comparisons, which would be a crazy amount of data to then manually try to find overlaps within).\nWe are looking for some very early preliminary hints as to comparisons between these lines with regard to up and down regulated similarities. Would obviously dig into individual comparisons in a different way after but was hoping to some sort of visualization plot method without having to run each comparison individually and then seeking out common top genes that overlap. Hope that makes sense. Tried to solve this for myself prior to asking here."
    },
    {
      "role": "system",
      "text": "Hi @user\nare you after a heatmap style plot or interactive representation?\nPlease check limma-voom Report (dataset #16) in this history: Galaxy | Australia | Accessible History | rna-seq-2 (@link https://usegalaxy.org.au/u/igor/h/rna-seq-2)\nThe input was six conditions, two replicates each, but only two contrasts were used (two comparisons). Scroll down and check heatmap output (pdf) and Glimma interactive MDPlot. Click on any DE gene on the interactive plot to see expression in other conditions.\nAre you after something like this?\nThe original tutorial: 2: RNA-seq counts to genes (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html)\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear,\nI was running Cufflinks with my BAM files. I have done it with success for the first time. However, when I try to do the second file with the same setting it comes out a wronging alert:\n\u201cRemote job server indicated a problem running or monitoring this job.\u201d\nI have searched the similar error in Galaxy Help. If I understood it right, the file formats and size or the server problems may cause this error. The second time I apply a 6Gb BAM file, which is slightly larger than my first time BAM file. Is there anything I should notice but I missed or it could be the server error?\nThanks for your reading! It would be nice to have your response!\nSincerely,\nSzu Shuo"
    },
    {
      "role": "system",
      "text": "Hi @user\nAll of the older Tuxedo tools are in a deprecated state but some still work. This includes Tophat/Tophat2, all Cuff* tools, and cummeRbund. Tophat for Illumia, in particular, should definitely be avoided. Cuffdiff does have a known issue with the SQLlite output option (the input for cummeRbund) and that will not be fixed. There are probably other issues with that tool and/or other deprecated tools.\nWe strongly recommend updating to use current/supported (eg: maintained) differential expression tools/methods. Tutorial links below, plus an FAQ that covers the various usage issues that can come up and be addressed \u2013 many will apply to the tools/methods you are using now \u2013 as well as other tools/workflows. It is a list of summarized help for the most common usage errors.\nThat said, you could try at least one more rerun. Sometimes the particular error you got comes up due to transient cluster issues. Even if you reran already, try one more time unless you discover that there was an input problem first.\nIf it fails again, then check your inputs more carefully. If those are correct, you probably will need to move to use the current tools/methods for scientifically correct results, even if a deprecated tool does not actually error. We will not be addressing fixes for any deprecated tools and can offer only limited support if you decide to use them anyway for some reason.\nThe FAQ that includes both DE troubleshooting tips plus the current DE tutorial links: @link https://galaxyproject.org/support >> FAQ >> Extended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\nNote: If you are using HISAT2 for mapping or decide to use it going forward, a specific setting to format the output for Cufflinks is required. The same BAM result cannot be used with both Cufflinks/Cuffdiff and Stringtie. Find the setting on the tool form under Advanced Options > Spliced alignment options > Transcriptome assembly reporting. Again, it is strongly recommended to NOT use Tophat or Tophat2 \u2013 both can produce unreliable scientific results or metadata problems that can be difficult to detect, even if the mapping job did not actually error for technical reasons. For metadata problems, sometimes you\u2019ll need to expand the mapping dataset to see the warning in a green, putatively \u201csuccessful\u201d job. A link is given to \u201credetect\u201d the metadata. Sometimes it works, sometimes not. And even if it works, there could still be content issues.\nMore troubleshooting help, summarized with more tips/links for common usage issues and solutions:\n\nPost here at the Galaxy Help forum: Troubleshooting resources for errors or unexpected results (@link https://help.galaxyproject.org/t/troubleshooting-resources-for-errors-or-unexpected-results/42)\n\nSupport FAQ. Includes help for this error and is linked both in error reports (bug icon in error datasets) and in the post above. My job ended with an error. What can I do? (@link https://galaxyproject.org/support/tool-error/)\n\n\nHope this advice helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone\n    How can I get more space for hic data analysis temporary?\n\n   Thank you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nhere you can find all the information about it: Account quotas (@link https://galaxyproject.org/support/account-quotas/).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\ni am following the tutorial at Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html) to setup a galaxy instance using ansible.\nI am at the step afer setting up an ansible-vault where galaxy should be deployed and dependencies installed etc., unfortunately the tutorial does not take into account a situation where you are located behind a http(s) proxy as far as i can tell.\nFor the errors I got with git and pip I was able to set the proxy settings globally, but now for the nodejs install with nodeenv I am at a point where i cant seem to solve it.\nIs there some way to specify proxy information in the ansible playbook or a work around to install it (though i guess the proxy will produce more problems down the line)?\nHere is the last few outputs of ansbile and the error message for nodeenv:\nTASK [galaxyproject.galaxy : Ensure galaxy_node_version is set] ******************************************************************************************************************************\nincluded: /home/galaxy/galaxy_install_tutorial/galaxy/roles/galaxyproject.galaxy/tasks/_inc_node_version.yml for x\n\nTASK [galaxyproject.galaxy : Collect Galaxy Node.js version file] ****************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Set Galaxy Node.js version fact] ********************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Check whether nodeenv is available] *****************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Create Galaxy virtualenv] ***************************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Ensure pip is the desired release] ******************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Ensure setuptools is latest working release (<58)] **************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Install nodeenv if it doesn't exist] ****************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Report preferred Node.js version] *******************************************************************************************************************************\nok: [x] => \n  galaxy_node_version: 16.13.2\n\nTASK [galaxyproject.galaxy : Check if node is installed] *************************************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Collect installed node version] *********************************************************************************************************************************\nskipping: [x]\n\nTASK [galaxyproject.galaxy : Remove node_modules directory when upgrading node] **************************************************************************************************************\nok: [x]\n\nTASK [galaxyproject.galaxy : Install or upgrade node] ****************************************************************************************************************************************\nfatal: [x]: FAILED! => changed=true \n  cmd:\n  - nodeenv\n  - -n\n  - 16.13.2\n  - -p\n  delta: '0:04:20.971769'\n  end: '2023-01-23 12:56:49.502045'\n  msg: non-zero return code\n  rc: 1\n  start: '2023-01-23 12:52:28.530276'\n  stderr: |2-\n     * Install prebuilt node (16.13.2) .\n    Traceback (most recent call last):\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1354, in do_open\n        h.request(req.get_method(), req.selector, req.data, headers,\n      File \"/usr/lib/python3.8/http/client.py\", line 1256, in request\n        self._send_request(method, url, body, headers, encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1302, in _send_request\n        self.endheaders(body, encode_chunked=encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1251, in endheaders\n        self._send_output(message_body, encode_chunked=encode_chunked)\n      File \"/usr/lib/python3.8/http/client.py\", line 1011, in _send_output\n        self.send(msg)\n      File \"/usr/lib/python3.8/http/client.py\", line 951, in send\n        self.connect()\n      File \"/usr/lib/python3.8/http/client.py\", line 1418, in connect\n        super().connect()\n      File \"/usr/lib/python3.8/http/client.py\", line 922, in connect\n        self.sock = self._create_connection(\n      File \"/usr/lib/python3.8/socket.py\", line 808, in create_connection\n        raise err\n      File \"/usr/lib/python3.8/socket.py\", line 796, in create_connection\n        sock.connect(sa)\n    OSError: [Errno 101] Network is unreachable\n  \n    During handling of the above exception, another exception occurred:\n  \n    Traceback (most recent call last):\n      File \"/srv/galaxy/venv/bin/nodeenv\", line 8, in <module>\n        sys.exit(main())\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 1079, in main\n        create_environment(env_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 956, in create_environment\n        install_node(env_dir, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 721, in install_node\n        install_node_wrapped(env_dir, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 743, in install_node_wrapped\n        download_node_src(node_url, src_dir, opt)\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 570, in download_node_src\n        dl_contents = io.BytesIO(urlopen(node_url).read())\n      File \"/srv/galaxy/venv/lib/python3.8/site-packages/nodeenv.py\", line 606, in urlopen\n        return urllib2.urlopen(req)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 222, in urlopen\n        return opener.open(url, data, timeout)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 525, in open\n        response = self._open(req, data)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 542, in _open\n        result = self._call_chain(self.handle_open, protocol, protocol +\n      File \"/usr/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n        result = func(*args)\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1397, in https_open\n        return self.do_open(http.client.HTTPSConnection, req,\n      File \"/usr/lib/python3.8/urllib/request.py\", line 1357, in do_open\n        raise URLError(err)\n    urllib.error.URLError: <urlopen error [Errno 101] Network is unreachable>\n  stderr_lines: <omitted>\n  stdout: ''\n  stdout_lines: <omitted>\n\nPLAY RECAP ***********************************************************************************************************************************************************************************\nx : ok=78   changed=8    unreachable=0    failed=1    skipped=43   rescued=0    ignored=0\n"
    },
    {
      "role": "user",
      "text": "I managed to \u201cfix\u201d the error by running the playbook as root user after getting access from my admins.\nPreviously I was executing it as sudo allowed \u201cnormal\u201d user."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Community\nI have recently doing LefSe analysis ,now i am facing a problem.\n\nQQ\u622a\u56fe20181203093754.png1878\u00d7871 75.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/e2434a4e1917e5464eef280392a38e16da6ea870.png)\nI don\u2019t know what this means.\nThanks in advance for your help!"
    },
    {
      "role": "system",
      "text": "Agree, the full bug message will help. Also, check under the error dataset\u2019s \u201cView Details\u201d page (job-details-icon) and click on the stdout and stderr links. For many tools, there will be more information about how the job failed and clues to fix the problem. Lower down on this same page is a summary of the inputs/parameters for that job run \u2013 and this condensed view can also reveal usage issues that may have been missed when reviewing the tool form itself (duplicated input datasets; nested advanced parameters).\nIf the bug/stdin/stdout info not enough to solve the problem or it is later determined to be a server-side issue and not usage, contact information for the public Galaxy server @link http://huttenhower.sph.harvard.edu/galaxy/ is on that server\u2019s home page and also here @link https://galaxyproject.org/use/huttenhower-lab/."
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried to use the Extract MAF blocks tool via @link https://usegalaxy.org/ website.\nAs the interval, I chose a BED file of hg19 assembly that I have uploaded.\nAs the MAF source, I chose the \u201c100-ways multiZ new (hg19)\u201d out of the option of the \u201clocally fetched alignments\u201d.\nI executed the tool, and immediately received the following red error message:\n\u201cAn error occurred with this dataset:\nFatal Error: The MAF source specified (100_WAY_MULTIZ_v2_hg19) appears to be invalid.\u201d\nIs there anything I can do in order to solve it? Or is it a bug?\nThank you,\nSapirl"
    },
    {
      "role": "system",
      "text": "See updated post here: Attempting to use Stitch MAF blocks tool, but lack of documentation (@link https://help.galaxyproject.org/t/attempting-to-use-stitch-maf-blocks-tool-but-lack-of-documentation/10091)"
    }
  ],
  [
    {
      "role": "user",
      "text": "When using the FASTQ info validates single or paired fastq files  it asks if my data is single or paired in the drop down menu.\nhow do i know if my data is single or paired? im using SRR5395998"
    },
    {
      "role": "system",
      "text": "Hi @user\nplease check description of the data. For this accession it has the following:\nAll libraries were pooled and sequenced 50bp SE in two lanes\nYou are after SE here. Paired end is PE.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I did a few DESeq2 analyses and have tried to download the results file on multiple computers (Mac and Windows) and it\u2019s showing as \u201cFailed - File incomplete\u201d within a second or 2 of attempted download. Is anyone else having issues with downloading DESeq2 files? I also noticed it is now showing up as \u201cA list with 1 tabular dataset\u201d that I have to click twice to download, which it didn\u2019t before. Other generated files (DESeq2 plots and Normalized Count files) are working and downloading fine, it\u2019s only the DESeq2 results files that are immediately failing. Thanks so much in advance for any advice anyone has!"
    },
    {
      "role": "system",
      "text": "Hi @user\nhow do you download the collection? Do you click at Download icon (an arrow pointing down) near the collection name? If yes, try download of individual datasets: click on collection name > click on dataset name > click on Download (floppy disk) icon.\nI am having problems with download of a collection, but no issue with download of individual datasets from the same collection/list. It might be permission settings, but I could not figure it out for now. The settings I have on the main server, look identical to settings on another Galaxy server, where I don\u2019t have issues with collection download.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI wanted to perform differential expression analysis between 7 different cell lines, however, I just noticed that just the results are based on differential expression between just the first two cell lines which were input as factors. Is there a way we can perform pairwise differential expression between all cell lines and then isolate the genes which are differentially expressed between all cell lines as we can do in limma?\nI look forward to your suggestions and some help on this topic,\nBest,\nS"
    },
    {
      "role": "system",
      "text": "Reply posted at Gitter: galaxyproject/Lobby - Gitter (@link https://gitter.im/galaxyproject/Lobby?at=628bb7a2ef00bd1dc6f60fc2)\nTutorials: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have searched through all the threads on this topic and I have yet to find a reasonable explanation for why I continue to encounter this error. I have already run Cufflinks on my samples, but when I attempt to run Cuffmerge (cuffdiff -o /scratch/sbag/Tikshana/BANANA/cuff/Macbal_diff -b /scratch/sbag/Tikshana/BANANA/RAW/Mac.fa -p 8 -u /scratch/sbag/Tikshana/BANANA/cuff/merged_asm/merged.gtf -L leaf,root /scratch/sbag/Tikshana/BANANA/map/Top_Macbal_6127/accept\ned_hits.bam, /scratch/sbag/Tikshana/BANANA/map/Top_Macbal_6141/accepted_hits.bam), I get the following error:\nError (GFaSeqGet): end coordinate (46954453) cannot be larger than sequence length 46622217\nGffObj::getSpliced() error: improper genomic coordinate 46954453 on chrUn_random for TCONS_00084296\nPLEASE HELP ME TO CLEAR THIS\u2026"
    },
    {
      "role": "user",
      "text": "Solved by checking all paths and file name, one negligible mistake might abandon the whole experiment.\nMake it simple by checking all file name and location is deeply suggested"
    }
  ],
  [
    {
      "role": "user",
      "text": "The tool is still running, and the files I passed are quite large, but is 5 days normal?"
    },
    {
      "role": "system",
      "text": "Hi @user\nSome jobs and tools will queue then execute longer than others. The server will process the work when shared resources are available.\nThe @link http://UseGalaxy.eu server was likely just busy. @link https://status.galaxyproject.org/\nFAQs:\n\nMy jobs aren\u2019t running! (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running)\nUnderstanding job statuses (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI have the raw illumina paired-end .fastq reads from a whole genome sequencing of my own genome done by a sequencing company. All the raw reads are about 50Gb in size. Would I be able to use the @link https://usegalaxy.org/ free site to do the FastQC assessment, adapter/read trimming, alignment to the reference genome and some basic form of variant calling using just the free site alone? I have a biology background and have some experience using computational biology tools, but I have never tried to assemble my own genome and I hope this could be a good learning experience for me. Does Galaxy have a tutorial for doing an assembly to hg19 reference genome using my own whole genome sequencing data?\nThank you so much for the help."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe GTN tutorials would be a good place to start. Search with keywords like tool names and datatypes or search by domain. If you are brand new to Galaxy, running through a few introduction tutorials will be worth it. The top of each tutorial has prerequisites and \u201cwhich Galaxy\u201d types of recommendations. Galaxy Training! (@link https://training.galaxyproject.org/training-material/)\nThe GTN will be hosting a training event using those tutorials in May that you may also be interested in. Most of the materials are already available if you want to start early following those learning paths: Sm\u00f6rg\u00e5sbord 2023 (@link https://gallantries.github.io/video-library/events/smorgasbord3/)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I was running a collection of FASTA files on ALphafold2. Each FASTA file contains a single line of FASTA header and a single line of amino acid sequence. The run failed and the error message generated was \u201cCould not find HHsearch database /data/pdb70/pdb70\u201d.\n\nScreenshot from 2022-06-07 17:36:431920\u00d71080 427 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b37dc127b45548d4529ba800651fed1b759de277.png)\n\nScreenshot from 2022-06-07 17:35:281920\u00d71080 415 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e8f359788b73c28ee98111e4781d83116102d752.png)\nWhat should I do now so that I can run the structure prediction?"
    },
    {
      "role": "system",
      "text": "Known tool issue. See AlphaFold2 Error - Could not find HHsearch database /data/pdb70/pdb70 - #2 by gallardoalba (@link https://help.galaxyproject.org/t/alphafold2-error-could-not-find-hhsearch-database-data-pdb70-pdb70/8593/2)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to upload the genome of camellia sinensis in fasta file into galaxy from NCBI datasets, please help me as I am a beginner. Ref-seq id is- GCF_004153795.1 Here is the NCBI link- @link https://www.ncbi.nlm.nih.gov/datasets/genomes/?acc=GCA_013676235.1&term=&utm_source=assembly&utm_medium=referral&utm_campaign=KnownItemSensor:org I also unable to upload the genome of it using weblink into ftp fies in galaxy, I have pasted the following link there but it\u2019s cannot fetch it. @link https://www.ncbi.nlm.nih.gov/datasets/genomes/?acc=GCA_013676235.1&term=&utm_source=assembly&utm_medium=referral&utm_campaign=KnownItemSensor:org"
    },
    {
      "role": "system",
      "text": "Hi @user, I think for now you need to get the genome directly from the refseq ftp site.\nIt appears the resources for GCF_004153795.1 are at Index of /genomes/refseq/plant/Camellia_sinensis/latest_assembly_versions/GCF_004153795.1_AHAU_CSS_1 (@link https://ftp.ncbi.nlm.nih.gov/genomes/refseq/plant/Camellia_sinensis/latest_assembly_versions/GCF_004153795.1_AHAU_CSS_1/), and I believe the genomic fasta file is @link https://ftp.ncbi.nlm.nih.gov/genomes/refseq/plant/Camellia_sinensis/latest_assembly_versions/GCF_004153795.1_AHAU_CSS_1/GCF_004153795.1_AHAU_CSS_1_genomic.fna.gz"
    }
  ],
  [
    {
      "role": "user",
      "text": "I was using the \u201cAnnotated my IDs - FeatureCounts\u201d tool when i realize that some of my genes had a NA tag, so i went to the NCBI page to check and it said \u201cThis record was replaced with Gene ID: XXXX\u201d. Can i just replace them? or should i check somehow if this new gene correspond to my previous annotated ones?\nThxs"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe annotation was based on the original record. Checking to see what changed would be a very good idea. It would be your decision about whether or not any specific changes could impact the larger results in a way significant for your analysis purposes (if any, besides the name label).\nNew or different transcripts associated with that record would probably be considered a significant change since that could make the result non-reproducible, even if just in a small way.\nHope that helps"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nCan anyone advise how to create a new tabular file in galaxy? which tool should be used?\nThanks\nAlex"
    },
    {
      "role": "system",
      "text": "Hi Alex\nYou can use the \u201cUpload File - from your computer\u201d tool (see also: @link https://galaxyproject.github.io/training-material/topics/galaxy-data-manipulation/tutorials/get-data/slides.html ) and select \u2018Paste/Fetch data\u2019. This will allow you to directly enter text, e.g.:\n1 2 3\na b c\nMake sure you tick the box for 'Convert spaces to tabs which you will see, when you click on the \u2018Settings\u2019 wheel. To be on the safe side, select \u2018tabular\u2019 in the \u2018type\u2019 drop-down menu.\nNow, click on Start, and you will have you small table with 3 columns and 2 rows in your galaxy history\nI hope this helps\nHans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am planning to analyze my 16S RNA sequences using Galaxy mothur Toolset.\nI watched your youtube Tutorial GTN Training - Microbial Analysis - 16S Metagenomics and analyzed some test samples following the\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html#DeSantis2006\n\n\nGalaxy Training: 16S Microbial Analysis with mothur (extended) (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html#DeSantis2006)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\nBut the problem is that my PCR amplification targeted the V3V4 region of the 16s rRNA, using the primers set 341F and 806R.\nCan I still use the silvav4fasta database for alignment? Does it cover this region?\nI tried downloading the latest version of silva 138.1 but there is an error and I cannot perform the align.seqs command.\nI tried changing in silvav4 the start and end in the screen.seqs command but it didn\u2019t work as well.\nCould anyone please advise regarding this matter?\nThank you!!"
    },
    {
      "role": "system",
      "text": "Hi @user,\n@quote\nCan I still use the silvav4fasta database for alignment? Does it cover this region?\nyou should generate a customized datase to our region of interest using the  pcr.seqs (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/mothur_pcr_seqs/mothur_pcr_seqs/1.39.5.0) command on the  mothur-compatible dataset: Silva reference files (@link https://mothur.org/wiki/silva_reference_files/).\n@quote\nI tried downloading the latest version of silva 138.1 but there is an error and I cannot perform the align.seqs command.\nWhere did you get this dataset from?\nI recommend you to have a look at the following resources, which could help you to solve some your your doubts:\n\nMothur MiSeq SOP tutorial (@link https://mothur.org/wiki/miseq_sop/)\nGalaxy training 16S Microbial analysis tutorial (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html)\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI\u2019d trying to configure PAM auth in galaxy 20.05.\nI\u2019d like users to enter their linux credentials and make Galaxy register them automatically.\nI\u2019ve created a auth_conf.xml file like:\n\n< ?xml version=\u201c1.0\u201d?>\n< auth>\n< authenticator>\n< type>PAM< /type>\n< options>\n< auto-register>True< /auto-register>\n< pam-service>system-auth-ac< /pam-service>\n< login-use-username>True< /login-use-username>\n< /options>\n< /authenticator>\n< /auth>\n\nI then go  to the login page and use my credentials and I see the error:\n\n\u201cUncaught exception in exposed API method:\u201d\n\nLog say:\n\nrun.sh: galaxy.auth.providers.pam_auth DEBUG 2020-07-30 12:51:03,291 [p:7289,w:2,m:0] [uWSGIWorker2Core2] PAM authentication successful for diazalbe\nrun.sh: galaxy.auth.util DEBUG 2020-07-30 12:51:03,300 [p:7289,w:2,m:0] [uWSGIWorker2Core2] Email: None, auto-register with username: bob\nrun.sh: galaxy.web.framework.decorators ERROR 2020-07-30 12:51:03,301 [p:7289,w:2,m:0] [uWSGIWorker2Core2] Uncaught exception in exposed API method:\nJul 30 12:51:03 inhas06030 run.sh: Traceback (most recent call last):\n\nand the exception:\n\nrun.sh: if not(VALID_EMAIL_RE.match(email)):\nrun.sh: TypeError: expected string or bytes-like object\n\nthis is a new user but Galaxy is already expecting an email for it. That means that I need to enable user registration, register my linux account and associate an email to it (I use the very same pasword) and then login works fine.\nMy question is: is there a way to make Galaxy register any new user with valid linux credentials?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi Pelacables,\nI think you need to set login-use-email to False and also specify a\nvalue for maildomain in your auth_conf.xml file. Basically, the add the following 2 lines:\n<maildomain>example.com</maildomain>\n<login-use-email>False</login-use-email>\n\nLet us know if this resolves your issue.\nThanks,\n-Kaivan"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone!\nI followed the \u201cCalling variants in diploid systems\u201d tutorial step by step. I try to annotate my variants, but I can\u2019t find the SnpEff tool. I checked every Snp related tool but none of them have the parameters as same as in the turorial. Can anyone help me with that?\nThanks for your support in advance!"
    },
    {
      "role": "system",
      "text": "We discussed about that few days ago.\nIt isn\u2019t found when you search for it. look at the section \u201cVariant Calling\u201d without any search term entered in the search box, and you will find it ."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Community,\nI was wondering if someone could help me out. I am trying to use RStudio instance via @link http://Galaxy.eu; however, I can never install Seurat (some other packages are fine). Seurat has a dependency package png which I believe causes this problem as it also cannot be installed. Error message: /bin/bash: libpng-config: command not found\nMaybe someone from the Galaxy Team can address it?\nThank you!\nAya"
    },
    {
      "role": "system",
      "text": "From Gitter:\n\nadeellqp\n@adeellqp\nDec 24 08:37\nHello\nI want to do WGCNA in Rstudio in @link http://usegalaxy.eu but could not install the dependency \u2018png\u2019. Can anyone help me?\nBj\u00f6rn Gr\u00fcning\n@bgruening\nDec 24 09:50\n@adeellqp could you try again?\nBut I doubt this will work, we would need to add libpng to the image\nCould you use Jupyter Notebook for the time being? This also has an R kernel.\nadeellqp\n@adeellqp\nDec 24 20:21\n@adeellqp could you try again?\n\nThanks\nI have tried many times it says that libpng is not available.\nI will try to use Jupyter Notebook.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am facing the same issue now\u2026I have been using galaxy for post 6 yrs w/o any issues however, recently it keeps giving me issues\u2026and mostly around weekends. I don\u2019t understand why none of the functions work\u2026all my tasks stay in grey for hours\u2026it was never like this before"
    },
    {
      "role": "system",
      "text": "Hello @system @user\nThere was a cluster error over the weekend that has been corrected.\nWhat to do (for everyone who had problems over the weekend):\n\nIf your job failed (red dataset) \u2013 try rerunning now.\n\n\n \u2192 All jobs that failed Upload or other tools that transfer external data into Galaxy will need a rerun.\n\n\nIf your job is queued (gray dataset) \u2013 please leave your job queued for fastest processing. You can decide to delete and rerun, but know that all new jobs start at the back of the queue.\nThere is a small backlog of jobs, and those should process sometime today.\nPlease try at least one rerun for any failures.\nIf reruns started after now also fail, you can submit bug reports for confusing errors.\n\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nAfter I executed Trinity, I obtained 2 files as assembly transcripts.fasta and log.txt files.\nI would like to know that is it possible to download \u2018Trinity.fasta.gene_trans_map\u2019 from Galaxy Trinity. It is important for my downstream analysis.\nThank you so much,\nKamoltip"
    },
    {
      "role": "system",
      "text": "Update:\nThe latest version of the Trinity tool, by default, now includes an additional output dataset named \u201cGene to transcripts map\u201d. Any version of Trinity at this revision or higher will produce the same additional output:\n\n\nTrinity  de novo assembly of RNA-Seq data (Galaxy Version 2.9.1)\n\nThe overall update of the Trinity tool suite also includes several other wrapped components. For the full list, please see the Main ToolShed repository: @link https://toolshed.g2.bx.psu.edu/view/iuc/suite_trinity/4a453a8da3b5.\nPublic Galaxy servers that have chosen to host the tool suite will generally place these tools under the RNA-seq tool panel group. Or, you can use the tool search to locate tools quickly.\nThe prior version of Trinity is considered deprecated now. At Galaxy Main @link https://usegalaxy.org, the final stages for that are still in progress. Once done, this ticket will close out: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/143\nIn short, all end-users should 1) use the updated tool versions if at all possible and 2) be careful not to use the output from the older tool as an input to the updated tools.\nIf the older output is really needed for some reason, be sure to carefully review downstream tool forms and make adjustments to your data as needed. Many accept a variety of custom assembly outputs, and for some tools, the prior Trinity results should be considered to be a \u201ccustom format\u201d due to changes in key data attributes (such as fasta title lines) between the two tool versions.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was wondering if it is possible to safely use sensitive data on a local server (IP address: 192.168.xxx.xxx) without SSL (installed NGINX to listen to port 80 instead of 443 and left out the nginx_ssl variables) or if there are other options to issue certificates or secure the server more without needing a DNS name, since Let\u2019s Encrypt (which is used by nginx to issue certificates) can\u2019t issue certificates for bare IP addresses.\nAny input is appreciated. Thanks in advance!"
    },
    {
      "role": "system",
      "text": "Heya @user. Yeah you have some options\n\nno ssl, the easiest, will generate warnings when people enter passwords because it\u2019s insecure\nself-signed ssl certificate, will generate warnings in the browser because it\u2019s self signed\n\nIt\u2019s really up to you what\u2019s appropriate. If your local network is controlled, no one can access the network without e.g. a wifi password, then you might be ok to skip SSL as you can trust the devices on your network. It\u2019s not generally a good idea, but knowing about your specific case it might be fine and easier than dealing with everyone\u2019s browsers not trusting a certificate."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nthe output data of featureCounts have a header containing the data title and the same appears in line 1 again, too. Therefore DESeq2 doesn\u2019t run. Has anyone an idea to solve that?\nThanks in advance,\nBest Bernhard"
    },
    {
      "role": "system",
      "text": "Dear @user,\nyou can use this tool Remove beginning of a file.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I was wondering if the percentage in the overrepresented sequences section of the FastQC output is really a percentage, as in 5% would show as a 5? Or would 5% show as 0.05?\nThanks you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nThese are true percentages with as many significant digits as there is space to write them out.\nYour example\n\nFive percent == 5.0\nPoint zero five percent == 0.05\n\nReal data, graph view\n\nOne point five five percent == \u201c1.5504756818485261\u201d\n\nSame data, raw view\n>>Overrepresented sequences\tfail\n#Sequence\tCount\tPercentage\tPossible Source\nCGGTGCTCGACCCCTCCGACCCCCGCCGGCCGCTTCGAGCCTGAGCCCTT\t76412\t1.5504756818485261\tNo Hit\n\n\nThe usage/display in Galaxy is the same as the original tool, so the documentation linked from the bottom of the tool form can be a really useful reference wherever you are running it. There is per-module help plus example reports of \u201cgood\u201d and \u201cbad\u201d data and other common use cases.\nBabraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence Data (@link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) \u2192 Index of /projects/fastqc/Help (@link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/) \u2192 Overrepresented Sequences (@link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/9%20Overrepresented%20Sequences.html)\nThis module lists all of the sequence which make up more than 0.1% of the total. To conserve memory only sequences which appear in the first 100,000 sequences are tracked to the end of the file. It is therefore possible that a sequence which is overrepresented but doesn't appear at the start of the file for some reason could be missed by this module."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there! I have been running differential expression analysis on a lot of datasets and I have been computing z scores to identify significant genes for a few weeks now and everything was working fine when suddenly, no matter what tool I use (deseq, hisat2, table compute, filter etc), it gives me the same error: condor_submit failed. I have been using the same inputs for weeks now with no issue. The error occurred when I was calculating z scores from a normalised counts file (which I had done literally 10 seconds earlier with the same inputs, working just fine). Any idea what is happening?"
    },
    {
      "role": "system",
      "text": "We had a temporary issue on @link http://usegalaxy.eu yesterday, which resulted in this error for all new jobs over maybe half an hour, or so.\nIf you haven\u2019t done so yet, you can now rerun each of your failed jobs and they should complete as expected again."
    }
  ],
  [
    {
      "role": "user",
      "text": "<can\u2019t use my 250GB>\nHi Guys\nI am using galaxy through my university cluster @link http://maxgalaxy.abdn.ac.uk. As a registered user (as far as I know) that I have only 250GB to use. The issue is that even after deleting all histories/data still I am using 94% of the 250GB. I permanetely deleted everything but still the issue. Any way to free up my (250GB)? or whom to contact to solve this problem?\nThanks\nMarwa"
    },
    {
      "role": "system",
      "text": "This @link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/download-delete-data/tutorial.html describes how to delete old data to make more room in your account."
    }
  ],
  [
    {
      "role": "user",
      "text": "On this page @link https://galaxyproject.org/develop/resources-tools/\nThis link is broken:\nA tool interface generator is maintained at @link http://cli-mate.lumc.nl.\nIt throws this error:\n\nInternal Server Error\nThe server encountered an internal error or misconfiguration and was unable to complete your request.\nPlease contact the server administrator at I.F.A.C.Fokkema@LUMC.nl to inform them of the time this error occurred, and the actions you performed just before this error.\nMore information about this error may be available in the server error log.\nIt would be great to have a quick contact link on each page to notfy the site admin of broken links."
    },
    {
      "role": "system",
      "text": "Hi @user\nThank you for bringing this to our attention. The service appears to unsupported at this time.\nI\u2019ve updated the Galaxy Development resource reference to designate the external service as currently unavailable. We may remove it entirely. If you want to contact them directly for status, try the email address or post an issue at their GitHub repository.\n\nUnavailable June 2020: A tool interface generator at @link http://cli-mate.lumc.nl/. Github last update Aug 15, 2014 @link https://git.lumc.nl/humgen/cli-mate.\n\nRegarding:\n@quote\nIt would be great to have a quick contact link on each page to notfy the site admin of broken links.\nThe @link http://galaxyproject.org website offers two direct options for this: 1) propose a change/fix/update PR or open an issue on Github or 2) contact us via the Gitter chat to alert us about problems.\n\ngalaxyproject-org-chat-github1914\u00d7668 125 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/97a9ac09a761fbe978e5c09a14b322afe88f0ea8.png)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am a @link http://plants.usegalaxy.eu/ user and my id is @link mailto:jvd.choudhary@gmail.com. I am working on miRNA differential expression, for that, I use the miRdeep2 mapper, miRdeep2 quantifier workflow and generated the quantifier tabular output, further I need to use the tool for differential expression like DeSeq2 or edgeR.\nTo calculate the differential expression DeSeq2 or edgeR need the replicate data. Although, I have the single set of miRNA data \u201ccontrol\u201d and \u201ctreatment\u201d without replicate. I know, replicate dataset must require for genuine calculation, but I don\u2019t have the choice.\nIn plant.galaxy, I tried to use some alternative tools also, but I couldn\u2019t get the output. From the literature support I got to know edgeR can calculate the DE from the single set of data, but plant.galaxy it couldn\u2019t support the single set of data. So could you please enable the options of edgeR to calculate the DE or suggest some workflow to calculate the differential expression without replicate data in galaxy hub?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nsorry, but Galaxy restricts edgeR to at least two replicates. You can find more information about this topic in the following post: DESeq2 or EdgeR or Limma without replication - #7 by jennaj (@link https://help.galaxyproject.org/t/deseq2-or-edger-or-limma-without-replication/2802/7)\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI try to run samples based on the Reference-based RNA-Seq data analysis tutorial and meet a problem in Deseq2 part. it\u2019s three dataset collection and i check the tabular of two sample featurecounts. Don\u2019t know how to solve it.\n\nScreenshot 2023-04-28 171409616\u00d7693 43.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d9f98b567731a6a1c0416469157fa098effcfa28.png)\nHere is the error log:\nError in data.frame(sample = basename(filenames_in), filename = filenames_in,  :\nduplicate row.names: /data/dnb08/galaxy_db/files/8/b/0/dataset_8b056f6c-8022-44aa-a290-fface0163b12.dat, /data/dnb08/galaxy_db/files/f/8/d/dataset_f8daec59-4cf1-456c-be52-31850ccf1cd1.dat\nThanks\nHarvey"
    },
    {
      "role": "user",
      "text": "@system thanks for offering help. The reason was I chose two tags to determine the dataset I wanted in every factor level. I switched to one tag and the problem was solved."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am currently performing DE analysis starting from raw counts from an RNA-seq experiment using the Limma Voom script included in UseGalaxy and I would like to introduce age and sex as co-variates in the differential expression analysis.\nIn the menu \u201cinput information from factor file?\u201d, I have clicked yes and attached a txt file in which I have introduced the groups that I am interested in comparing in the second column, sex (coded as a discrete variable using 0 and 1) in the third column and age in the fourth column.\nThe script runs perfectly but I am not sure if I can introduce age like this, as age is a  continuous and not a discrete variable.\nCould someone confirm me if I am proceeding the right way?\nIf not, could anyone tell me the best way to proceed?\nMany thanks in advance.\nbest,"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI asked about it, and this is the response:\n\nUnfortunately, as far as I can see, it currently only handles discrete factors, not continuous variables. The factor information input gets converted to factor data type.\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to create a discrimination model to classify honey and fortified honey, and use it in a set of new spectral data. In the stage of creating the model, of discrimination that I am doing by FDA (Factorial Discrimant Analysis), an error appears and the model is not created. Would you know how to tell me the reason for the error?\nThe parameters I use to create the model are:\n\u201cDiscrimination\u201d\n\u201cFDA\u201d Factorial Discrimant Analysis\nSelect X data: xCAL(CovSel on (PCA on SNV(xCAL.tab):scoresXyCAL.tab(c2)):Xdata(30varsel))\nSelect Y data: yCAL(yCAL.tab)\nColumn of y class chosen for the calculation: c2: V1\nNumber of blocs for cross-validation: 10\nNumber of discriminant variables(DV): 10\nDistance choice: Mahalanobis\nExport confusion tables (CAL) for each discriminant variables (DV) models: no\nStoud: Free Access Chemometric Toolbox (FACT) version 0.10\ngetting started at: @link http://atoms.scilab.org/toolboxes/FACT\nLoad macros\nLoad gateways\n!\u2013error 4\nUndefined variable: test\nat line      46 of function conj2dis called by :\nat line      31 of function cvclass called by :\nat line       9 of function fda called by :\nres_da=fda(x,y.d(:,ycol),10,10,0);\n                        !--error 4 \n\nUndefined variable: res_da\n                        !--error 4 \n\nUndefined variable: res_da\n                                                             !--error 248 \n\nWrong value for argument #2: Valid variable name expected.\nStderr : Wrong value for argument #2: Valid variable name expected.\nThe URL is @link https://vm-chemflow-francegrille.eu/"
    },
    {
      "role": "system",
      "text": "@quote\nSelect X data: xCAL(CovSel on (PCA on SNV(xCAL.tab):scoresXyCAL.tab(c2)):Xdata(30varsel))Select Y data: yCAL(yCAL.tab)\nI am not familiar with this tool but if the above are inputs then you might want to first double check the syntax. The error describes  Wrong value for argument #2: Valid variable name expected. which tells me that you might have provided a bad variable name.\nYou should also look into the tools arguments and figure out which argument is #2. The administrator of the galaxy instance you are using should be able to help you with that."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi to all the comunity!\nI was wondering if there is the possibility to add redundans to Galaxy Europe.\nThis pipeline has an unofficial conda package (conda install -c genomedk redundans), I test it briefly and it works properly.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe recommended procedure for requesting a new tool is to open an issue in the IUC repository; this is an example (@link https://github.com/galaxyproject/tools-iuc/issues/4708). Would you mind opening it?  Also, in case you have some time, I encourage you to try to create the wrapper by yourself (it would be extremely useful for the Galaxy community :smile: ). There are some supporting materials that would help you with that, such as this short tutorial (@link https://youtu.be/33L4B9ir0aQ).\nRegards!"
    }
  ],
  [
    {
      "role": "user",
      "text": "hello,\nI wonder why my jobs is still running for some topics up to a long time (5 days). I know that trimmomatic doesn\u2019t need much CPU and memory, so do you know why this is the case?\nthanks for your help !"
    },
    {
      "role": "system",
      "text": "Hi @user\nAre these jobs queued (gray) or executing (yellow)? @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\n\nA single trimmomatic job that has been queued for five days is probably Ok.\nA single trimmomatic job that has been executing for five days would be unusual.\n\nJobs run after any upstream jobs complete (inputs for the tool are ready to use) and as public shared resources appropriate for the tool become available. Some of your jobs run, then some of other people\u2019s jobs run, then more of yours, repeat. Using workflows (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/) can significantly speed things up.\nIf that doesn\u2019t help, would you please confirm the public server URL? The @link http://UseGalaxy.eu server is up: @link https://status.galaxyproject.org/. You could also share some screenshots. The top of the Dataset Information page ( :information_source: icon within any dataset) has some timestamps. The history with expanded datasets (inputs + outputs) can also be informative."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, when I run an imported workflow, it gives me this output: Following tools missing: toolshed.g2.bx.psu.edu/repos/iuc/phyml/phyml/3.3.20190909. A user in Europe created the workflow and I am using @link http://usegalaxy.org."
    },
    {
      "role": "system",
      "text": "Hi @user\nIf possible, consider running the workflow at the same server it was developed. This might be where the workflow link was shared from (?)\nOr, you can use the tool at either @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au.\n@link http://UseGalaxy.org does not have this tool installed (yet). I\u2019ve asked our tool development team to review. Please follow the issue ticket for updates Install phyml 3.3.20190909 \u00b7 Issue #570 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-tools/issues/570)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "As the topic name states, I am wondering if it\u2019s possible to somehow pass files during the Galaxy application startup instead of uploading them? Files that I want to upload are on remote serve and it\u2019s not possible to upload them with browser upload feature (on the same host I will run Galaxy application)\u2026 Also, I would like to make a docker image from your latest stable version and it would be best if I somehow can mount all those files into the galaxy docker container."
    },
    {
      "role": "system",
      "text": "Hi @user\nLocal file browsing is only one way to get data into Galaxy.\n\nUpload with public remote file URL\u2019s. This works at any Galaxy server.\nAdd files to Data Libraries. This works at Galaxy servers where you are an admin. Scripts & Tricks \u2014 Galaxy Project 22.05.1 documentation (@link https://docs.galaxyproject.org/en/master/admin/useful_scripts.html?highlight=data+library)\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody,\nI am new un the field of bioinformatics. I am trying Variant calling on RNA-seq Data, I will appreciate some good ideas, and optional workflow, if available.\nMeny"
    },
    {
      "role": "system",
      "text": "Hi @user\nthis topic was discussed recently. Please search the forum.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI want to get goseq done for my data but i saw this error, what this about ?\nWarning message:\nIn pcls(G) : initial point very close to some inequality constraints\nLoading required package: AnnotationDbi\nLoading required package: stats4\nLoading required package: BiocGenerics\nLoading required package: parallel\nAttaching package: \u2018BiocGenerics\u2019\nThe following objects are masked from \u2018package:parallel\u2019:\nclusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\nclusterExport, clusterMap, parApply, parCapply, parLapply,\nparLapplyLB, parRapply, parSapply, parSapplyLB\n\nThe following objects are masked from \u2018package:dplyr\u2019:\ncombine, intersect, setdiff, union\n\nThe following objects are masked from \u2018package:stats\u2019:\nIQR, mad, sd, var, xtabs\n\nThe following objects are masked from \u2018package:base\u2019:\nanyDuplicated, append, as.data.frame, basename, cbind, colnames,\ndirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\ngrepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\norder, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\nrbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\nunion, unique, unsplit, which, which.max, which.min\n\nLoading required package: Biobase\nWelcome to Bioconductor\nVignettes contain introductory material; view with\n'browseVignettes()'. To cite Bioconductor, see\n'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\nLoading required package: IRanges\nLoading required package: S4Vectors\nAttaching package: \u2018S4Vectors\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\nfirst, rename\n\nThe following object is masked from \u2018package:base\u2019:\nexpand.grid\n\nAttaching package: \u2018IRanges\u2019\nThe following objects are masked from \u2018package:dplyr\u2019:\ncollapse, desc, slice\n\nAttaching package: \u2018AnnotationDbi\u2019\nThe following object is masked from \u2018package:dplyr\u2019:\nselect\n\nUsing manually entered categories.\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: runGoseq \u2026 goseq -> reversemapping -> [ -> [.table -> NextMethod"
    },
    {
      "role": "system",
      "text": "@quote\nError in[.default(summary(map), , 1) : incorrect number of dimensions\nThis looks like an input format problem or disconnected input content. Try double-checking your inputs. Extra \u201cwhitespace\u201d, empty lines, are the first item to check, and I\u2019ve shared how to find those before with you directly (here & at the galaxy-bugs mailing list, with you and others).\nA search by the tag \u201cinput\u201d at Galaxy help will list out a lot of common format/content problems in posts where format as an issue, across tools, for others reading.\nFor you @user, I would guess that \u201cdisconnected input content\u201d is the root of the problem given the error message and since you already know how to confirm basic format. But if format checks not done yet, do that fist. After, much help is on the tool form regarding input dataset format/content, so review/compare your content versus the examples. There are columns of data between inputs and tool from entry data that ALL need to match up.\nIf that doesn\u2019t work, preserve your testing of format/content in some history, that also contains the failed run, and send me a history share link with \u201cobjects shared\u201d (direct message). I\u2019ll have somewhat limited access for admin-only details at EU since I am not an admin at the EU server, but still may be able to review what may be going wrong. If not, can ping the EU server admins, or might suggest sending in a bug report via the UI (if you do that, be sure to leave all inputs undeteled and include a link to this post for context). You can also reply to this thread that you sent in the bug report, with the Public galaxy server URL included (to confirm where this is occurring).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI (as an admin) have to backup/download multiple histories from multipe users.\nIs there a way to do this more efficient than downloading every history manually?\nI think about the BioBlend API or something similar, is this the right way or am I completely wrong?\nBest Regards\nFabian"
    },
    {
      "role": "system",
      "text": "Would something like this work? @link https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#bioblend.galaxy.histories.HistoryClient.export_history"
    }
  ],
  [
    {
      "role": "user",
      "text": " How to delete rows with the same number in the bed file in usegalaxy\n (If the number of the second column is equal to the number of the third column, delete that row)\n which tool could be useful?\ninput:\nchr1 10 20 \nchr1 10 10 \nchr1 15 20 \nchr1 15 30 \nchr1 30 30\n\noutput:\nchr1 10 20 \nchr1 15 20 \nchr1 15 30\n"
    },
    {
      "role": "user",
      "text": "Thank you, very Good :+1:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy team,\nI use Aspegillus niger sequence to predict its genes using Augustus.Here is the screenshot.But I get failed always.For reference I have copied the screenshot of my workflow.\nimage1920\u00d71080 314 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/e9a5512a0762638d2e86ece9889853595753e18e.png)\nCan somebody help me to get this issue  resolved please.\nBest regards,\nJagath Ranasinghe"
    },
    {
      "role": "user",
      "text": "Dear Dr.Cristobal,\nThank you very much for your help .I shared the history in previous mail.\nBest regards,\nJagath Ranasinghe."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I like to use the ChIPseeker to annotate my ChIP-seq result.\nMy reference genome is hg19.\nAlthough I\u2019ve tried to download it from the UCSC website, none of the downloaded files can get the analysis work.\nI don\u2019t know if my GFT file is not right for the tool.\nDoes anyone know where I can download the GTF file for my analysis?"
    },
    {
      "role": "system",
      "text": "@system and @user\nThere is much Q&A at this site about how to obtain and format a GTF properly for use with tools. I added some tags to this post to help find it.\nThis FAQ may also help. It has a focus on DE analysis, but the reference annotation advice applies to any tool that accepts a GTF input.\n\n\n@link https://galaxyproject.org/support/#getting-inputs-right\n\n\n\nGalaxy Support (@link https://galaxyproject.org/support/#getting-inputs-right)\n\n\n\n\n\n\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI was following the tutorial to perform the MAGeCK count, but I cannot find this tool in the list.\nDo anybody knows if this tool is still available?\nThank you,\nFabio"
    },
    {
      "role": "system",
      "text": "Which tutorial? Can you share a link/url?\nI do find the tool on @link https://usegalaxy.eu/. Not every training is possible on every galaxy server. To check this click the \u201cworld\u201d icon on the training page.\n\nimage1128\u00d7128 5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/86398be3705f70242c6dcbbe5154be59173dc938.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have used 89% of my 250GB data on my account. I deleted 94GB, but the space still shows 89% full. Can you please help me with resolving my disk space?"
    },
    {
      "role": "system",
      "text": "@quote\nHi@systemLook under User \u2192 Histories. Open the advanced search and set \u201cstatus = all\u201d. The size of each history is the amount each is using. \nIf any of the histories are only deleted, they need to be permanently deleted (purged) to free up quota space. To find all stray deleted datasets across histories, or entire deleted histories, go to User \u2192 Preferences \u2192 Storage Dashboard and use the functions. \nAfter purging, go back to the top level page for the Storage Dashboard and click on the \u201cref\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello:)\nI am new to Galaxy :slight_smile:\nI want to run the Golm Metabolome Database search on my processed GC-MS data but i dont have RI\u2026 I have to choose \u201cnone\u201d GC column, but there is no \u201cnone\u201d-option\u2026\nIf anyone knows the trick, I would be super thankful!\ncheers, Philipp\n\nimage.png1299\u00d7456 17.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/ffd008342598c508935fdcb479cedd589813f1ca.png)"
    },
    {
      "role": "system",
      "text": "The algorithm behind Golm db search includes alkane RI as input data. So it not possible to unset this parameter or to set to none. There is no logic todo this using Golm Db. The Golm webservice proposes this option but it is a trap :wink:\nIf you have any question on metabolomics workflows/tools, please contact the workflow4metabolomics support at @link mailto:support@workflow4metabolomics.org"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nNew to Galaxy and Cloud AWS. Followed the instructions here (@link https://galaxyproject.org/cloudman/getting-started/). Got to step 1-3, where I entered the credentials of my newly setup AWS User and I get the error \u201cThe credentials you have entered are invalid.\u201d\nAny help on this would be greatly appreciated."
    },
    {
      "role": "system",
      "text": "AWS can take up to a day to process a new account so if your account is brand new, it may take a bit of time before it is usable. Beyond that, can you please double check that the IAM user has the necessary permissions, namely EC2FullAccess and S3FullAccess."
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to use the Galaxy/hutlab format to write a paper about gut microbiota.\nSo, I would like to ask four questions in order to apply to the IRB.\n\u2460If an unregistered user uploads gut microbiota data, how long does the data take to be deleted? I want to know the exact period.\n\u2461 After the data is completely deleted, will it never be restored?\n\u2462Will the data stored on the site, Galaxy/hutlab be used without the consent of the uploader?\n\u2463Will my uploaded data and analysis results are seen by anyone other than me?\nI have already browsed the following sites\uff1a\n\u30fbManaging Datasets in Galaxy - Galaxy Community Hub (@link https://galaxyproject.org/learn/managing-datasets/#delete-vs-delete-permanently)\n\u30fbDownloading and Deleting Data in Galaxy (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/download-delete-data/tutorial.html#fn:1)\nI look forward to hearing from you. Thank you."
    },
    {
      "role": "system",
      "text": "Cross-posted here Deletion of data / Secondary use of data (@link https://help.galaxyproject.org/t/deletion-of-data-secondary-use-of-data/9338)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nIs there an overview of the average or expected time it takes for every tool/algorithm from the tool panel to finish a job?\nKind regards,\nLou"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI don\u2019t think this is possible because there are many factors involved in the running time, but you can always ask us about specific jobs. If you are using the @link http://usegalaxy.eu instance, I suggest to you post your questions about waiting times in the Gitter channel (@link https://matrix.to/#/#usegalaxy-eu_Lobby:gitter.im).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am a beginner into RNA-Seq data analysis and was wondering if someone could explain what exactly the \u2018Specify strand information\u2019 setting from the featureCounts tool is doing. For my pair-end RNA-Seq data analysis I am following the workflow as per this tutorial (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html). BTW, I am performing this analysis for data (biological triplicates) generated under 3 experimental conditions A, B and C. So total 9 files.\nAfter estimation of the strandedness (using the infer experiment tool) of my pair-end RNA-Seq data alignments I find that my libraries are reverse stranded (i.e. on average for all files \u201c1\u00b1,1-+,2++,2\u2013\u201d: 0.9670). Does it mean that for featureCounts I need to \u2018Specify strand information\u2019 as stranded (Reverse)?\nI have tried both combinations of Stranded (Forward) and Stranded (Reverse) with the idea that I do not want to miss out on read counts that originated from overlapping but opposite strand genes or the antisense strand of non-overlapping genes i.e. antisense strand where there are no annotated features yet on the target genome thereby \u2018discovering\u2019 potential new transcribed regions. I then created 3 dataset collections as:\n\n\u2018all counts\u2019 - containing forward and reverse count files (total 18 tabular datasets) for each sample (A, B and C).\n\u2018all forward counts\u2019 - containing only forward count files (total 9 tabular datasets) for each sample  (A, B and C).\n\u2018all reverse counts\u2019 - containing only reverse count files (total 9 tabular datasets) for each sample  (A, B and C).\n\nSubsequently, I extracted identifiers and tagged all files from all 3 dataset collections accordingly. I then performed DESeq2, filtered out genes (2-fold change and adjusted p-value<0.05). I now have 3 dataset collections each containing filtered lists (A vs B, A vs C and B vs C) i.e. DEGs from considering \u2018all counts\u2019, \u2018all forward counts\u2019 and \u2018all reverse counts\u2019. My main question is, which of these would be the correct collection to refer to? Would it be only the collection of lists obtained from considering the \u2018all reverse counts\u2019 count files?\nI would like to thank you very much in advance for your time, efforts and patience in answering my questions.\nBest regards."
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI am a beginner into RNA-Seq data analysis and was wondering if someone could explain what exactly the \u2018Specify strand information\u2019 setting from the featureCounts tool is doing.\nSetting the stand filters which mapped read will be counted up per annotation feature.\n\nReads map to a genomic regions with strand assigned during mapping (BAM).\nFeatures are placed on genomic regions with strand assigned in reference annotation (GTF, GFF3, or the built in indexes).\nFeatureCounts compares the two, and counts up reads-per-feature at the gene summary level.\n\n@quote\nAfter estimation of the strandedness (using the infer experiment tool) of my pair-end RNA-Seq data alignments I find that my libraries are reverse stranded (i.e. on average for all files \u201c1\u00b1,1-+,2++,2\u2013\u201d: 0.9670). Does it mean that for featureCounts I need to \u2018Specify strand information\u2019 as stranded (Reverse)?\nIf your data is stranded, then you should use stranded protocols for both mapping and counting.\nWhy? The sequencing chemistry protocol was designed to produce stranded reads. Any reads mapped or counted on the opposite strand are non-specific. That can create a lot of noise in the results, and valid results could be omitted due to the over-counting.\n@quote\nMy main question is, which of these would be the correct collection to refer to? Would it be only the collection of lists obtained from considering the \u2018all reverse counts\u2019 count files?\nYes, those would be the results to consider.\nFeatureCounts produces graphs describing which reads could be assigned to a feature with specificity, and when not, the reason why. Maybe examine those reports to see if you can confirm they produce the \u201cbest\u201d results?\n@quote\nI have tried both combinations of Stranded (Forward) and Stranded (Reverse) with the idea that I do not want to miss out on read counts that originated from overlapping but opposite strand genes or the antisense strand of non-overlapping genes i.e. antisense strand where there are no annotated features yet on the target genome thereby \u2018discovering\u2019 potential new transcribed regions.\nFor this purpose, Salmon, Sailfish and Kallisto are some choices for transcriptome-based discovery analysis."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI\u2019m trying to get Galaxy to work with PBS. I\u2019ve installed openpbs wit pbs/torque drmaa, configured the job_conf.xml like this:\n\n<job_conf>\n\t<plugins workers=\"4\">\n                <plugin id=\"drmaa_default\" type=\"runner\" load=\"galaxy.jobs.runners.drmaa:DRMAAJobRunner\"/>\n\t\t#<plugin id=\"pbs\" type=\"runner\" load=\"galaxy.jobs.runners.pbs:PBSJobRunner\"/>\n                <param id=\"drmaa_library_path\">/usr/local/lib/libdrmaa.so.1</param>                 \t\n\t</plugins>\n        <destinations>\n                <destination id=\"pbs_default\" runner=\"drmaa_default\"/>\n        </destinations>\n</job_conf>\n\n\nand in systemctl I get this:\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.tours._impl INFO 2023-01-18 09:21:08,518 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded tour 'core.history'\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.tours._impl INFO 2023-01-18 09:21:08,528 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded tour 'core.scratchbook'\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs.manager DEBUG 2023-01-18 09:21:08,532 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Initializing job handler\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs INFO 2023-01-18 09:21:08,532 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Handler 'main.job-handlers.1' will load all configured r>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: galaxy.jobs.runners.state_handler_factory DEBUG 2023-01-18 09:21:08,534 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Loaded 'failure' state ha>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: pulsar.managers.util.drmaa DEBUG 2023-01-18 09:21:08,540 [pN:main.job-handlers.1,p:199275,w:0,m:1,tN:MainThread] Initializing DRMAA session from thread M>\nJan 18 09:21:08 front-d012e754-8b70-11ed-8061-0242ac110002.novalocal uwsgi[199275]: Unable to communicate with localhost(127.0.0.1)\n\nWhat seems to be the problem? UI is working and running, but the jobs dont even start they just wait.\nThank you"
    },
    {
      "role": "user",
      "text": "Sorry it was a typo yes. Problem was resolved. We\u2019ve had wrong libs installed(from Torque).\nSolution was to reinstall pbs-drmaa from sourceforge after installing openpbs-devel.\nNow everything works and jobs run on the node."
    }
  ],
  [
    {
      "role": "user",
      "text": "My Trinity assembly has been waiting to run for more than 24 hours - is this normal? Should I cancel it and run it again? Please let me know if there is something going on with the server or if this is the normal time frame for running this tool.\nThanks!"
    },
    {
      "role": "system",
      "text": "Please see this related post for details. In short, all public Galaxy servers are very busy and jobs are queuing a bit longer than usual. Avoid deleting/restarting for the fastest processing.\n@quote\nGood evening, I had started a Trimmomatic job on a paired end dataset at about 1:10 UTC. It\u2019s been nearly 3 hours now with the jobs stuck in queue. Are you currently experiencing a lot of traffic? \nAnother question I had was how many jobs I could run concurrently? I was going to queue FastQC as well, but I\u2019m unsure if queuing too many jobs might lead to an error. I apologize if the questions were a little basic, I\u2019ve just started using Galaxy. Thanks for your time!\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI am using cowpie in galaxy tool.\nSomeday, it is not working well, can not upload data file due to 502 bad gateway issue.\nShould i do something to solve this problem?\nThankyou.\n\n123872\u00d7533 38.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a72e172b736f49870546f31e25455533f5d371c5.jpeg)"
    },
    {
      "role": "system",
      "text": "This was a server side issue with the Upload tool at @link https://usegalaxy.be/ that is likely resolved.\nIf anyone runs into this same issue again, please open a new issue ticket and note the server URL."
    }
  ],
  [
    {
      "role": "user",
      "text": "I create .vcf file and use reference by Galaxy hg38 (GRCh38.p2). I downloaded my file after SnpSift Annotate and trying use it for SnpEff annotation in cygwin64. I have error \u201cGRCh38.p2 genom not found\u201d because it\u2019s not in .vcf but hg38 is there. When I write it in command line nothing happened\u2026 \u201cdatabase not installed\u201d\nHelp please\u2026\n\nimage826\u00d7419 146 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/235de0e7ee8dc0065f0d10b58e20102d6ac4e2e3.jpeg)"
    },
    {
      "role": "system",
      "text": "@quote\nSnpSift\nHi @user\nThis forum is focused on using tools within the Galaxy Platform. Hundreds of tools, including this one, are wrapped for Galaxy and available at many of the public services.\n\n\n\n@link https://galaxyproject.org/\n\n\nHome - Galaxy Community Hub (@link https://galaxyproject.org/)\nAll about Galaxy and its community.\n\n\n\n\n\nIf you are new to Galaxy, start here.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/)\n\n\nGalaxy Training: Introduction to Galaxy Analyses (@link https://training.galaxyproject.org/training-material/topics/introduction/)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\nWhen using the tool directly, or for general usage instructions, the author\u2019s website is the go-to resource. Several of the general bioinformatics forums also have discussion \u2013 a web-search would find these.\n@link https://pcingola.github.io/SnpEff/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey, I encountered an issue: I was working on Galaxy (@link http://huttenhower.sph.harvard.edu/galaxy/) and trying to run lefse but the output files were different when I changed the way microorganisms are named.\nHere is everything that I know:\nThe dataset is green, the job did not fail.\nThe lef_usual.txt is a input file whose first column contained many names of microorganisms, and these names had some special characters like \u201c()\u201d, \u201c-\u201d and \u201c'\u201d, like  \u201cClostridium_sp._DL-VIII\u201d.\nThe lef_usual_result.svg is the output file. There are 5 biomarkers for \u201cmeNorChild\u201d.\nThen, I replaced all the names of microorganisms in the first column in lef_usual.txt with another way of naming, such as s1, s2 and s3. The new input file was named lef_number.txt.\nThe  lef_number_result.svg is the new output file. There are 11 biomarkers for \u201cmeNorChild\u201d.\nThe relevant parameters of the two tasks are the same, only the naming method of the first column of the input file has changed.\nSo, why does just changing the way microorganisms are named affect the final results? What format of naming will give the most correct results?\nThe process screenshots, input files and output files have been uploaded.\n\nlef_number1906\u00d7744 77.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/91fa9fda29893aa7cab89bad0edb31f24e1f4597.png)\n\nlef_number_result1050\u00d72497 143 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/eb92bc975a6dd198107d654f68b3d1df45cc3b32.png)\n\nlef_usual1888\u00d7729 76.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cdca97ad08002bf2849140e109dbc7455976a939.png)\n\nlef_usual_result1050\u00d72287 281 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7724835daa1efcf7ceba50a66452195b6225b88b.png)\n\nprocess1080\u00d71964 181 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f0353216d18ea73995870df00b06d3e226f65b5a.jpeg)"
    },
    {
      "role": "system",
      "text": "Cross-posted here: the way microorganisms are named affect the final results of lefse (@link https://help.galaxyproject.org/t/the-way-microorganisms-are-named-affect-the-final-results-of-lefse/8337)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Minimap2 should be able to map from fastq or fasta files.  Galaxy Minimap2 cannot see fastq files.  When galaxy minimap2 is run on fasta files it fails where it previously ran effectively.  This same data was processed through galaxy a few weeks ago, something is different."
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nGalaxy Minimap2 cannot see fastq files.\nYou might just need to change the datatype Changing the datatype (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_change_datatype.html). Next time you Upload data, allow Galaxy to \u201cauto-detect\u201d the datatype.\nIf the \u201cauto-detect\u201d guess is wrong and you are working with reads, there is very likely some problem with that data (format, content, compression that Galaxy cannot uncompress,\u2026). For the compression issues, try uncompressing locally first. If that works, upload the uncompressed data to Galaxy. If that fails, some problem was introduced upstream and the data is corrupted and not usable anywhere, not just Galaxy. Maybe an upstream data transfer failed?\nOnce you have Uploaded intact data, run some QA before attempting to map.\nMore about how to learn what datatypes a tool is looking for, plus other general fastq help is in the topic below. The datatype restrictions are a very basic quality/format sanity check \u2013 not to be confused full QA. A tool can fail for odd reasons when given data in a format that it wasn\u2019t designed to work with, or produce putatively successful results that have scientific problems. The latter isn\u2019t always easy to detect.@quote\nHi@systemMost tools in Galaxy expect reads in fastqsanger or fastqsanger.gz format and with one of those datatypes assigned. Those datatypes represent a specific type of quality score scaling: Sanger Phred +33. \nThis is produced by Illumina 1.8+ pipelines. PacBio used Phred+33 scaling to start with and now the scaling is neutral. You can double check with a tool like FastQC or FASTQ info to see what those report. \nThere are some tools that expect just fastq or fastq.gz (scaling unspecified). But\u2026\nTutorials\n\n@link https://training.galaxyproject.org/training-material/search?query=minimap\n@link https://training.galaxyproject.org/training-material/search?query=quality\n\nIf you need more help, please share more details. This is the information people at this forum will usually need. Be sure to capture the server URL. Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hoping for some help.\nI have a new Galaxy 21.09 install that was done using Ansible roles following the tutorial guidance here:\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html\n\n\nGalaxy Training: Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\nbut using the updated Ansible files (since some of the older ones mentioned in the tutorial produced installation errors).\nIt installed fine and the tools seem to work fine except that I find that there are some files that I am unable to view using the \u201ceye\u201d button - although I get the correct preview in the box.  The error I get is:\n\n404 Not Found\nThe resource could not be found.\nNo route for /_x_accel_redirect/galaxyzpool/data/7/d/e/dataset_7de6b170-fac8-4d5e-88ce-9bdbe5dd97fa.dat\nThis is when trying to see the output of the Samtools Flagstat tool.\nI also find that I cannot download some (not all) files.  And I get the error that the file is not at the location.\nThis seems to have something to do with the proxying function in NGINX (I think) but I don\u2019t have enough knowledge of how this works to fix it.\nAny help would be appreciated!"
    },
    {
      "role": "user",
      "text": "OK.  Found another syntax error in the referenced tutorial:\nIn the galaxy.j2 file, \u201clocation /_x_accel_redirect\u201d should be \u201clocation /_x_accel_redirect/\u201d\n-Michael"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good day!\nI would like to inquire about how and where to acquire a .fasta file for another hypervariable region. In the tutorial, the V4 fasta file was given. I was wondering where to acquire a fasta file for other hypervariable regions such as V2 for analysis. Thank you very much."
    },
    {
      "role": "system",
      "text": "Hi @user\nMothur reference help README for the SILVA v138 reference files (@link https://mothur.org/blog/2020/SILVA-v138-reference-files/)\nMothur Q&A Creating a customized reference alignment for V1-V2 - Commands in mothur - mothur (@link https://forum.mothur.org/t/creating-a-customized-reference-alignment-for-v1-v2/20268)\nGalaxy Tutorials\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\n\n\nGalaxy Training: Metagenomics (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to do a differential gene expression analysis of the bovine oviduct. When using the featureCounts tool, I need to upload a GFF/GTF file in the \u201cGene annotation file\u201d. The reference genome I selected in the Hisat2 tool was bosTau9, so in the \u201cfeatureCounts\u201d tool I choose the option \u201cGFF/GTF file in your history\u201d and upload a GFF file from the @link http://Ensembl.org web page (@link https://www.ensembl.org/index.html), but it seems that the gene annotation file I retrieve (Index of /pub/release-109/gff3/bos_taurus (@link https://ftp.ensembl.org/pub/release-109/gff3/bos_taurus/)) do not corresponds to the same reference genome (bosTau9) I used for the alignment with the Hisat2 tool because a received an error message: Fatal error: Exit code 255 ().\nI hope someone can help me.\nMany thanks."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe bosTau9 genome available and indexed in Galaxy is based on the UCSC version of the assembly. It uses slightly different chromosome names than the Ensembl version. Tools expect exact matches, and can fail or produce odd results when there is some mismatch.\nTry using the GTF annotation from UCSC. There are a few choices, and each corresponds with a Gene and Gene Prediction track (see their browser for a description). The file you want is probably this one. Copy/paste the link into the Upload tool using all defaults and it will be ready to use.\n@link https://hgdownload.soe.ucsc.edu/goldenPath/bosTau9/bigZips/genes/bosTau9.ensGene.gtf.gz\nFAQ Working with GFF GFT GTF2 GFF3 reference annotation (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_working_with_reference_annotation.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Has this entry been addressed?:\n@link https://biostar.usegalaxy.org/p/13232/index.html\nI would like to merge three (replicate) bam files in a collection/list into a single bam file.\nCapture\nWhen I try to do this with Samtools merge, I can only select the collection but not individual bam files. And if I select the collection in which there are the bam files I see the following message.\n\nCapture11288\u00d7213 8.42 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/079f8b6e614cd94a446b290feb1b2bbc586c81f6.png)\nIs there a way to see what these data 189, data 187, and data 188 are, to know what I\u2019m merging?\nThank you!"
    },
    {
      "role": "system",
      "text": "Dear @user,\nThe tool merges the data in your collection. So your file Samtools merge on data 189, 187, and 188 contains the data from your three input files SRR103*. You could not select the individual data sets, because your data is in a data set collection. In order to select your individual files (e.g., to merge just two bam files of your input data) you would need to extract the data sets from the collection with the tool Extract element identifiers of a list collection.\nHave a good day and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have RNAseq data (Illumina Total RNA) from whole blood RNA of COVID19 patients.  Based on ddPCR, I think some have detectable virus in blood. The alignment to HG38 using HISAT2 is great, but when I substitute the SARS-CoV2 genome, I get very few alignments and it does not change between controls and COVIDs, giving maybe 50 alignments out of 17M paired end reads. Any ideas on what could be wrong, or a source for a workflow?  Many thanks, TIm"
    },
    {
      "role": "system",
      "text": "If you\u2019re just interested in potential SARS-CoV-2 sequences, I wouldn\u2019t recommend HISAT2 necessarily, at least not as the only tool.\nOptions I\u2019d explore instead:\n\n\nDo one pass of HISAT2 alignment, then filter for unmapped reads and align those using bwa-mem (or bowtie2)\n\n\nGenerate a combined human/SARS-CoV-2 FASTA, then use bwa-mem (or bowtie2) for a single round of alignment, after which you filter for alignments to the viral genome\nGalaxy | Europe | Published History | SARS-CoV-2/human combined ONT reference (@link https://usegalaxy.eu/u/wolfgang-maier/h/sars-cov-2human-combined-ont-reference) is an example history for generating the combined genome. You can import it and use it as your starting point, but since your data is Illumina data (not ONT), you would want to concatenate only the hg38 and the NC_045512.2 fasta datasets.\nHere\u2019s a workflow that illustrates the approach for ONT data and uses minimap2 as the mapper:\nGalaxy | Europe | Published Workflow | SARS-CoV-2: map ONT reads to transcripts (@link https://usegalaxy.eu/u/wolfgang-maier/w/sars-cov-2-assign-ont-reads-to-transcripts-mapping)\nIt shouldn\u2019t be hard to modify it for your use case.\n\n\nThe rational behind these recommendations: HISAT2 is a great tool for aligning eukaryotic spliced RNA reads, but it isn\u2019t optimized for viral RNA. A general mapper (like bwa-mem or bowtie2) is sufficient for the viral data and may do a better job in fact.\nMy first suggestion might be the best way to get optimal sensitivity for human and viral reads, but the second option should be very comparable (or even more sensitive) if you care about the viral reads only (it will do a good enough job of getting the vast majority of human reads out of the way and will discover SARS-CoV-2 alignments reliably)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I am trying to use this tool to compare differences in two conditions across three samples. I get the following error message when trying to import my data:\n\u201creading in files with read_tsv\n1 2 Error in tximport::tximport(files = localFiles, type = tolower(dataAnalyed$orign),  :\nall(txId == raw[[txIdCol]]) is not TRUE\nCalls: importIsoformExpression \u2192  \u2192 stopifnot\nWarning message:\nIn txId == raw[[txIdCol]] :\nlonger object length is not a multiple of shorter object length\u201d\nThe error message suggests that I have duplicate input datasets, but I do not - they\u2019re three different inputs per condition.\nI performed transcriptome assembly using the following tutorial and I don\u2019t know if this is part of the issue?\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/topics/transcriptomics/tutorials/differential-isoform-expression/tutorial.html#isoform-switching-analysis-with-isoformswitchanalyzer)\n\n\nGalaxy Training: Genome-wide alternative splicing analysis (@link https://training.galaxyproject.org/topics/transcriptomics/tutorials/differential-isoform-expression/tutorial.html#isoform-switching-analysis-with-isoformswitchanalyzer)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nMy history:\n\n@link https://usegalaxy.eu/published/history?id=b7b3102ddb80500c\n\n\nGalaxy\n            | Europe (@link https://usegalaxy.eu/published/history?id=b7b3102ddb80500c)\nGalaxy is a community-driven web-based analysis platform for life science research.\n\n\n\n\n\nAny help is greatly appreciated!"
    },
    {
      "role": "system",
      "text": "Hi @user\nNotice how the transcripts are named in the Stringtie files \u2013 they are using the default transcript names generated for novel transcripts by that tool. These instead need to match up with the reference annotation\u2019s transcript_id attributes and the >lines in the transcript fasta.\nIt looks like you missed a step that consolidates that data: Stringtie merge. Scroll up in the tutorial to find it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I would like to participate virtually in the conference to be held in Brisbane, Australia. Is poster presentation possible for virtual participation? I want to present a poster."
    },
    {
      "role": "system",
      "text": "@user Yes! Virtual presentation for posters is allowed under the assumption that you are available on Zoom/Slack during your scheduled poster session. One thing to keep in mind that, depending on your timezone, that session may fall in the middle of the night."
    }
  ],
  [
    {
      "role": "user",
      "text": "First of all, thank you for the training in the galaxy. it made me learn a lot about bioinformatics and its processing. I am a new user, I have learned from this site for a week. but there are some questions related to this tutorial.\n\nIn some tutorials shown the processing of metagenomic data from bacteria, and it uses a taxonomy in the form of \u201csilva\u201d data (silva.v4.fasta). incidentally my research is abaout diversity of \u201cdiatom\u201d and primers that I use is rbcl, the question is, what taxonomy do I use in processing data in the galaxy later? rbcl? or other?\nif my taxonomy is rbcl, where do I get rbcl.fasta data?\n\nFor information, I am waiting for the results of my sample sent by the NGS method, and I can\u2019t wait to process it.\nI appreciate your response and looking forward to it :smiley:"
    },
    {
      "role": "system",
      "text": "Hello @user\nThis particular analysis domain space (diatom metagenomics) is a bit out of my lane, but this publication seems interesting. The authors host rbcL taxonomic reference databases for both Mothur and DADA-2.\n\n\n@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6805954/pdf/41598_2019_Article_51500.pdf\n\n\n@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6805954/pdf/41598_2019_Article_51500.pdf\n@link https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6805954/pdf/41598_2019_Article_51500.pdf\n1385.45 KB\n\n\n\n\n\nMaybe have a look? And others reading are certainly welcome to comment more!"
    }
  ],
  [
    {
      "role": "user",
      "text": "*I try to install galaxy local instance on Virtual box machine via ansible follow by galaxy project tutorial (LINK: Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html#galaxy)) And I got this problems, Did anyone meet this before? Could you please help me with this error?\nThanks in andvanced\n[ERROR]\nTASK [galaxyproject.galaxy : Get current Galaxy DB version] ************************************************\nfatal: [head-node]: FAILED! => {\u201cchanged\u201d: false, \u201ccmd\u201d: [\u201c/srv/galaxy/venv/bin/python\u201d, \u201c/srv/galaxy/server/scripts/manage_db.py\u201d, \u201c-c\u201d, \u201c/srv/galaxy/config/galaxy.yml\u201d, \u201cdb_version\u201d], \u201cdelta\u201d: \u201c0:00:01.893473\u201d, \u201cend\u201d: \u201c2022-06-27 04:28:27.278930\u201d, \u201cfailed_when_result\u201d: true, \u201cmsg\u201d: \u201cnon-zero return code\u201d, \u201crc\u201d: 1, \u201cstart\u201d: \u201c2022-06-27 04:28:25.385457\u201d, \u201cstderr\u201d: \u201cTraceback (most recent call last):\\n  File \"/srv/galaxy/server/scripts/manage_db.py\", line 28, in \\n    invoke_migrate_main()\\n  File \"/srv/galaxy/server/scripts/manage_db.py\", line 20, in invoke_migrate_main\\n    config = get_config(sys.argv, use_argparse=False, cwd=os.getcwd())\\n  File \"/srv/galaxy/server/lib/galaxy/model/orm/scripts.py\", line 133, in get_config\\n    properties = load_app_properties(config_file=config_file, config_prefix=config_override, config_section=config_section)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 79, in load_app_properties\\n    properties = read_properties_from_file(config_file, config_section)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 107, in read_properties_from_file\\n    raw_properties = _read_from_yaml_file(config_file)\\n  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 124, in _read_from_yaml_file\\n    with open(path) as f:\\nPermissionError: [Errno 13] Permission denied: \u2018/srv/galaxy/config/galaxy.yml\u2019\u201d, \u201cstderr_lines\u201d: [\u201cTraceback (most recent call last):\u201d, \"  File \"/srv/galaxy/server/scripts/manage_db.py\", line 28, in \u201c, \"    invoke_migrate_main()\u201d, \"  File \"/srv/galaxy/server/scripts/manage_db.py\", line 20, in invoke_migrate_main\", \"    config = get_config(sys.argv, use_argparse=False, cwd=os.getcwd())\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/model/orm/scripts.py\", line 133, in get_config\u201d, \"    properties = load_app_properties(config_file=config_file, config_prefix=config_override, config_section=config_section)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 79, in load_app_properties\u201d, \"    properties = read_properties_from_file(config_file, config_section)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 107, in read_properties_from_file\u201d, \"    raw_properties = _read_from_yaml_file(config_file)\u201c, \"  File \"/srv/galaxy/server/lib/galaxy/util/properties.py\", line 124, in _read_from_yaml_file\u201d, \"    with open(path) as f:\", \u201cPermissionError: [Errno 13] Permission denied: \u2018/srv/galaxy/config/galaxy.yml\u2019\u201d], \u201cstdout\u201d: \u201c\u201d, \u201cstdout_lines\u201d: }\n\nimage1305\u00d7679 150 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5934b6c1a2519606ba57da6589ea800454bc6697.png)\nt"
    },
    {
      "role": "system",
      "text": "Maybe you can find some info here: galaxyproject/admins - Gitter (@link https://gitter.im/galaxyproject/admins?at=5e677032e203784a559ad71c)\nAre you executing the installation as the user \u201cgalaxy\u201d?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Apologies for asking an obvious question.\nPlease how do I correctly download the FASTA copy of the baboon reference genome on NCBI link below;\n@link https://www.ncbi.nlm.nih.gov/genome/assembly/1082401\nI used the download assembly button on the web browser which gave the option of RefSeq (which is about 512kb) and Genbank (about 912mb)\nI also downloaded the DNA sequence which has numerous files (about 4GB) but could not get in on to Galaxy.\nPlease help.\nThank you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can find the ftp URL for the baboon reference genome in the FTP directory for RefSeq assembly section (NCBI web page, right column):\n\n\n@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0/\n\n\n\nIndex of /genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0 (@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0/)\n\n\n\n\n\nnext, copy the URL which correspond to the genomic sequence and paste it directly in the Galaxy uploader:\n@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/728/515/GCF_008728515.1_Panubis1.0/GCF_008728515.1_Panubis1.0_genomic.fna.gz\nFinally, after uploading the file, select edit attributes, convert, and select convert compressed file to uncompressed."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have my galaxy instance on Cloud via Cloudman. However the site connection is insecure. Any files uploaded via FTP will be clearly visible. How do i make the Connection Secure?"
    },
    {
      "role": "system",
      "text": "Unfortunately, secure FTP is not currently supported on CloudMan out of the box.\nIt should be possible to set it up manually by obtaining an SSL certificate and installing an SFTP server, but that\u2019s likely to involve quite a bit of system administration.\nThe new GVL 5 offers SSL out of the box, although no FTP support at all. However, the latest Galaxy that comes with GVL 5 supports large file uploads so, if that\u2019s a reason for using FTP, it should be addressable.\nHope this helps."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I am trying to use the \u201cARTIC minion\u201d tool on galaxy and even though I select different version of the ARTIC primers (from the drop down menu or bed files I upload) it still uses the V1 (when I open the log file is says that it uses V1 scheme). Can anyone tell me what can be the problem? Because I get a consensus that has a lot of Ns.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "@user that log is a bit unfortunate and misleading, but what the tool actually does seems to be the right thing.\nI just tested with a primer scheme from one of my histories and the generated command line has this part:\nmkdir -p 'scheme/name/V1' && ln -s '/data/dnb-ds02/galaxy_db/files/020/532/dataset_20532843.dat' 'scheme/name/V1/name.scheme.bed'\nwhere '/data/dnb-ds02/galaxy_db/files/020/532/dataset_20532843.dat' is really where my primer scheme file lives on the server.\nIn other words, the tool merely always links your custom primer scheme into a \u201cscheme/name/V1\u201d folder in the job working directory, from where it gets picked up by the ARTIC minion pipeline, but unfortunately that leads to that confusing log."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a VCF file (format = vcf_bgzip) and I would like to obtain multiple separate VCF files by chromosome. I would appreciate any help into how to achieve this using Galaxy. Here is a sample file:\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=1,length=249250621>\n##contig=<ID=10,length=135534747>\n## several other hashes and text\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tSampleName\n1\t69869\trs548049170\tA\t.\t.\t.\t.\tGT\t0/0\n1\t565508\trs9283150\tT\t.\t.\t.\t.\tGT\t./.\n1\t727841\trs116587930\tC\t.\t.\t.\t.\tGT\t0/0\n1\t752721\trs3131972\tT\tG\t.\t.\t.\tGT\t1/1\n"
    },
    {
      "role": "system",
      "text": "Dear @user,\nYou can do this with Text reformatting with awk\nusing the following AWK Program: $0 ~ /^[1#]/ {print $0}\nYou can run this multiple times and just change the [1#] for chromosome 1 to [2#] for chromosome 2 and so on.\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I hope you can help me.  I am attempting a\n\ndownload720\u00d71280 104 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/62e9788ca34c624f7ed39c4996a2498a965ad0ea.jpeg)\nrna-seq analysis. This is a BAM file (the first ~100 lines are like above and the remaining lines are in this context with QNAME FLAG RNAME POS MAPQ CIGAR MRNM MPOS SEQ QUAL OPT).  I have the entire workflow done except for this part. I want to go from the raw data I was given to  fastqsanger.gz so I can use trimmomatic on it, etc.\nHow do I go about that? Or another question, what am I doing wrong?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThese tutorials cover many common analysis paths when working with RNA-seq data. Start with a few introduction tutorials if you are not familiar with Galaxy, then review tutorials that match the type of analysis you want to do. Both will help you to understand what to do when using your own data.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/)\n\n\nGalaxy Training: Introduction to Galaxy Analyses (@link https://training.galaxyproject.org/training-material/topics/introduction/)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\n@quote\nI am attempting a rna-seq analysis. This is a BAM file. I have the entire workflow done except for this part.\nBAM datasets are intermediate output. These represent alignments of read sequences against a reference sequence. Mapping tools generate this kind of data.\n@quote\nI want to go from the raw data I was given to fastqsanger.gz so I can use trimmomatic on it, etc.\nThe raw RNA-seq read sequencing data should already be in \u201cfastqsanger\u201d format, or \u201cfastqsanger.gz\u201d if compressed. When uploading this kind of data (fastq) to Galaxy, leave the datatype detection as \u201cauto-detect\u201d. If the data really is in this format, Galaxy will guess it correctly and QA tools like Trimmomatic will accept it as valid input.\n\nIf you need more help, please be specific when you ask a new question so we can help you. What are the analysis goals? What steps have you done so far? If you ran into job errors: what did you check already, what did the error logs report, and can you share a link to your work for context?\nFAQs:\n\nhow to ask a question others can help with (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#what-information-should-i-include-when-reporting-a-problem)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "\n\n\n\nOption 1 - Dark\nOption 2 - Pastels\nOption 3 - Original\n\n\n\n\n0\nvoters\n\n\n\n\nOption 1 - Dark\n\n49466266-84b16080-f7f7-11e8-80b9-72cb8f539ef1.png1295\u00d7935 103 KB\n (@link /uploads/galaxy/original/1X/5ff3ba8255b1bce09fcfe6006bd9f2cc386b665e.png)\nOption 2 - Pastels\n\n49466211-63507480-f7f7-11e8-8e9f-b42eea995789.png1296\u00d7928 102 KB\n (@link /uploads/galaxy/original/1X/b07f85797c31524578394149150b8975c4b2edbf.png)\nOption 3 - Original\n\ngrafik.png1295\u00d7931 81.2 KB\n (@link /uploads/galaxy/original/1X/dba99b1d8c0376adb2f36ec76e5f4b702cda4dd3.png)\nRelated to a github pull request (@link https://github.com/galaxyproject/galaxy/pull/7072)"
    },
    {
      "role": "system",
      "text": "Thanks for the feedback everyone! :slight_smile: We\u2019ll take this feedback into consideration when finalising the feature implementation."
    }
  ],
  [
    {
      "role": "user",
      "text": "I ran DESeq2 on @link http://usegalaxy.org and generated the figure on the left. I then took the VST file exported on Galaxy and ran the following code on R to generate a similar heatmap:\nsampleDists <- dist(t(assay(vsd)))\nlibrary(\"RColorBrewer\")\nsampleDistMatrix <- as.matrix(sampleDists)\nrownames(sampleDistMatrix) <- paste(vsd$condition, vsd$type, sep=\"-\")\ncolnames(sampleDistMatrix) <- NULL\ncolors <- colorRampPalette( rev(brewer.pal(9, \"Blues\")) )(255)\npheatmap(sampleDistMatrix,\n         clustering_distance_rows=sampleDists,\n         clustering_distance_cols=sampleDists,\n         col=colors)\n\nAs you can see from the figures, the distance on the clustering looks the same for both, but the groupings are drawn differently. Which is correct? Any idea what could have happened?\n\nheatmaps1920\u00d71080 109 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6096c04e9457ae39656dd75521426ffefbf985de.jpeg)"
    },
    {
      "role": "system",
      "text": "Deseq 2 nomalization method is based on deseq2 median ratio. You are using different normalization method in R"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m running Galaxy on a local on-premiss server and developing using Rscript.My tool works fine Up to the point of Rscript execution, but finally returns \u201cUnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode byte 0x8b in position 1: invalid start byte\u201d. What are the possible causes?\n\nGalaxy ver.: v20.01\nPython ver.: 3.6.8\nOS: CentOS 8\noutput file format: txt\n\ngalaxy.model.metadata DEBUG 2023-05-25 15:47:41,938 [p:1938833,w:0,m:1] [DRMAARunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 502\ngalaxy.jobs.runners ERROR 2023-05-25 15:47:41,944 [p:1938833,w:0,m:1] [DRMAARunner.work_thread-3] (303/546) Job wrapper finish method failed\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/__init__.py\", line 1522, in _finish_dataset\n    dataset.set_peek(line_count=line_count)\nTypeError: set_peek() got an unexpected keyword argument 'line_count'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"lib/galaxy/jobs/runners/__init__.py\", line 540, in _finish_or_resubmit_job\n    job_wrapper.finish(tool_stdout, tool_stderr, exit_code, check_output_detected_state=check_output_detected_state, job_stdout=job_stdout, job_stderr=job_stderr)\n  File \"lib/galaxy/jobs/__init__.py\", line 1657, in finish\n    output_name, dataset, job, context, final_job_state, remote_metadata_directory\n  File \"lib/galaxy/jobs/__init__.py\", line 1525, in _finish_dataset\n    dataset.set_peek()\n  File \"lib/galaxy/model/__init__.py\", line 2621, in set_peek\n    return self.datatype.set_peek(self)\n  File \"lib/galaxy/datatypes/data.py\", line 903, in set_peek\n    est_lines = self.estimate_file_lines(dataset)\n  File \"lib/galaxy/datatypes/data.py\", line 854, in estimate_file_lines\n    dataset_read = dataset_fh.read(sample_size)\n  File \"/home/galaxy/galaxy/.venv/lib64/python3.6/codecs.py\", line 321, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n\nThanks,\nYukie"
    },
    {
      "role": "user",
      "text": "Hi @system,\nThank you very much for providing various helpful information. Regarding the datatype defined as \u201ctxt\u201d in the output section of the tool XML file, to be honest, I\u2019m not entirely sure about it since I\u2019m not the author of the Rscript. I think it would be a good idea to reach out to the author of the Rscript and inquire about it.\nBest,\nYukie"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to follow a procedure in the following link (@link https://www.frontiersin.org/articles/10.3389/fmicb.2020.00208/full) - detailed instructions are in @link https://www.frontiersin.org/articles/10.3389/fmicb.2020.00208/full#supplementary-material. I believe I have everything working until the step where I have to run BLAST + makeblastdb on a very large file (I believe ~ 100GB, although I haven\u2019t been able to successfully download what the procedure uses - it\u2019s supposed to be all bacterial nucleotide sequences). Instead, I have downloaded the BLAST pre-formatted nucleotide database (@link https://www.ncbi.nlm.nih.gov/public/?blast/db/FASTA/nt.gz) using FTP to download it. I have a local galaxy set up to handle the large file sizes and am running it from an external hard drive. I thought these databases were supposed to be pre-formatted, but:\n\n\nWhen I try to run NCBI BLAST+ blastn on my assembled sequence against the database (after I uncompressed it via Galaxy) I get an error:\nFatal error: Exit code 137 ()\n/Volumes/MBLab/galaxy/database/jobs_directory/000/26/tool_script.sh: line 25: 87409 Killed: 9               blastn -query \u2018/Volumes/MBLab/galaxy/database/objects/7/6/c/dataset_76cf539a-0e17-416d-b488-604ddd55b8ea.dat\u2019 -subject \u2018/Volumes/MBLab/galaxy/database/objects/1/8/9/dataset_189ca2b3-5d9d-4629-bb24-2811979970ef.dat\u2019 -task \u2018megablast\u2019 -evalue \u20180.001\u2019 -out \u2018/Volumes/MBLab/galaxy/database/objects/b/5/a/dataset_b5a49b6e-b95a-40b7-9f72-cf421e820fe5.dat\u2019 -outfmt \u20186 std sallseqid score nident positive gaps ppos qframe sframe qseq sseq qlen slen salltitles\u2019 -num_threads \u201c${GALAXY_SLOTS:-8}\u201d\n\n\nWhen I try to run NCBI BLAST+ makeblastdb on the uncompressed nt file (uncompressed via Galaxy -> Edit dataset attributes -> Convert -> Convert compressed file to uncompressed) I get an error:\nFatal error: Exit code 1 ()\nBLAST Database creation error: Error: Duplicate seq_ids are found:\nGNL|BL_ORD_ID:553373\n\n\nI am currently downloading a Refseq bacterial database (fromhere (@link https://ftp.ncbi.nlm.nih.gov/refseq/release/bacteria/)) because Refseq is nonredundant. I am going to try NCBI BLAST+ makeblastdb on those once I have them up to see if that helps, although the file is massive (300+gb). Any advice is very welcome - I struggle with this and have no experience here. I will update how the RefSeq approach goes."
    },
    {
      "role": "user",
      "text": "Thank you,\nI ended up re-trying after reading something elsewhere. I tried with the parsing parameter set to \u201cYes\u201d (the instructions say to leave it set to \u201cNo\u201d . That seemed to do the trick and the database was generated! Hopefully this was not a problematic thing to do."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019ve been facing a certain problem for some time now regarding SPADES and metaSPADES. There are always error messages, of the most varied possible, at the time of requesting the work or minutes after the execution of the same. Most of these are memory errors or incorrect data entry (it sometimes mentions duplicate input files) - I always follow the manual and so far I\u2019ve only managed to run my data twice. These are paired end data (individual datasets). My data is metagenomic libraries; each file is between 400 and 500GB in size. Is this some SPADE/metaSPADE instability?\nThanks in advance for your help!"
    },
    {
      "role": "user",
      "text": "Thank you very much, solved! I don\u2019t know what the problem with @link http://galaxy.org and galaxy.au, but with @link http://galaxy.eu everything went well, everything is working fine. Thanks for the tip!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am using GALAXY USA from December 2021 using 2 email IDs. But now I am trying to log in to my account using both of my email IDs (admin-redacted-for-privacy). It says it is an invalid ID. It would be great if someone could help me with my login.\nThank you\nJawahar"
    },
    {
      "role": "system",
      "text": "@link https://usegalaxy.org/static/terms.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nCan I share data between two Galaxy servers? Like I am using Galaxy Australia and I want to share my data with Galaxy Europe.  Is this possible?"
    },
    {
      "role": "system",
      "text": "It is possible but you have to \u2018fetch\u2019 the data to the Galaxy you want to work with. One way is to right click on the \u2018download\u2019 icon on the dataset and copy the link. Then paste the link in the upload modal of the other Galaxy and let it fetch the dataset for you (from the original Galaxy)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI downloaded GTF file from NCBI and uploaded it to my Galaxy. I ran HISAT2 along with my sample but I incurred an error while using it with StringTie.\nError : no valid ID found for GFF record\nI even removed the header comments and ran Stringtie against my HISAT result but still got an error.\nDoes anyone seem to know what the problem is?\nGTF File after removing the header lines\n\n\n\n\nCP083173.1\nGenbank\ngene\n1\n1155\n.\n+\n.\ngene_id K9E40_00005; transcript_id ; gbkey Gene; gene_biotype protein_coding; locus_tag K9E40_00005;\n\n\n\n\nCP083173.1\nProtein Homology\nCDS\n1\n1152\n.\n+\n0\ngene_id K9E40_00005; transcript_id unassigned_transcript_1; gbkey CDS; inference COORDINATES: similar to AA sequence:RefSeq:WP_004115883.1; locus_tag K9E40_00005; product sugar-binding protein; protein_id UQA84770.1; transl_table 11; exon_number 1;\n\n\n\nCorresponding FNA file\n\nCP083173.1 Gardnerella vaginalis strain JNFY11 chromosome, complete genome\nATGAACATAGGTAAAAAAGCAATCGCTTTATTTGTAGGTATTGCTGTAGTTGCTGGTTTATCAGCCTGTTCAGGTTCAAG\nAGGTGGTGCATCCAAAAACGTAAGTCAAGGAATTGAAAAAGGTGCCACCATCGGTGTCTCTATGCCAACAAAGTCAGAGG\u2026\n"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe consider providing a brief description of your goal(s). Are you after gene annotation or read counting/gene expression?\nYou use HiSAT2 and StringTie on bacterial data. HiSAT2 is a gapped aligner that can align reads across introns. StringTie is often used for prediction of genes in eukaryotic genomes. On other hand, many bacterial genes present in operons. Read splitting can be disabled in HiSAT2.  Not sure if SringTie is the best option for annotation of bacterial genes, but I am not familiar with the topic. For read counting maybe consider alternative tools such as featureCounts or htseq-count. You may need to tweak the setting, depending on the annotation file and your goals. Read counting tools count reads on annotation present in 3d column (type) and aggregate results using one of attributes from the last column. By default many read counting tools count reads against exons, but exon annotations may not present in some bacterial datasets (no \u2018exon\u2019 in 3d column). In this case you need to choose something else, for example, CDS or gene. You may get different results with different settings. The same for the attributes.\nAs I don\u2019t know what you are trying to archive and cannot check the data, it is hard to answer your question.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was wondering if there is an option to display name tags in the workflow editor. This would make editing the workflow easier. As they are displayed in this tutorial Name tags for following complex histories (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/name-tags/tutorial.html)\nImage from the tutorial below -\n\nnametag-wf837\u00d7735 84.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/71ff266bd4e8ee0fa5867c44765e3ca3527ff067.png)\nI could not find any option to enable this.\nThanks and Regards"
    },
    {
      "role": "user",
      "text": "Hi Jennifer (@system ),\nThanks so much for the detailed explanation and the Matrix post. I will create a github issue depending on responses to the matrix post.\nThe workflow editor gives options to add and remove tags (on the left where one sets the parameters for the tool and the output names). These tags if available in the workflow diagram would be great. I agree that the tags set for input datasets would be available only at the runtime.\nTags are quite a nice feature of Galaxy and our local instance users find them very convenient.\nThanks Again and Regards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI would like to get DEG for my RNAseq. I have completed these steps: cufflinks, and cuffdiff. But when I am trying to run CummeRbund in the galaxy by using SQLite files output of cuffdiff as the input for CummeRbund, I get this error every time: The tool was executed with one or more empty input datasets. This frequently results in tool errors due to problematic input choices. I would appreciate it if you could guide me on how to fix it. How should I use CummeRbund in the galaxy?\nAnd are there other alternative tools to get DEG from FPKM data in the galaxy?\nThank you very much\nM"
    },
    {
      "role": "system",
      "text": "Hello @user\nThe error means that the database wasn\u2019t created correctly. The tool suite you are using is known to have issues. Using updated protocols is recommended.\nPlease see the Galaxy tutorials here. If you run into problems, this forum has much Q&A around troubleshooting: search by tool name or datatype.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have paired-end stranded libraries.Can someone please let me know what should I choose for these 3 options to run HISAT2 and htseq thereafter, given that my inferexperiment result is1\u00b1,1-+,2++,2\u2013\nQ1. For HISAT2, it aks to specify strand information (Unstranded/FR/RF) and then\nQ2. also asks to choose*\u2018Select the upstream/downstream mate orientations for a valid paired-end alignment against the forward reference strand*\u2019 with options (\u2013fr/\u2013rf/\u2013ff).\nQ3. for htseq it asks to choose setting for \u2018stranded\u2019 with options (yes/no/reverse).\nI used FR and --fr for HISAT2 and obtained 43% overall mapping. Then I used those BAM files to perfomr htseq and selected YES for the \u2018stranded\u2019 option. After doing MultiQC, I see 0% reads were assigned.\nMany thanks,G"
    },
    {
      "role": "system",
      "text": "Hi @user\nread strandness is determined by protocol used for preparation of library. Ideally, your sequencing service provider should give it to you as part of description for sequencing data. HiSAT2 and htseq-count settings are determined by the data. If you don\u2019t have information on strandness of your reads, either ask your sequencing provider or determine it as described in Strandness section of this tutorial: 1: RNA-Seq reads to counts (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html#strandness)\nGood description of strandness is available in another tutorial: De novo transcriptome reconstruction with RNA-Seq (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#mapping)\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good day,\nI\u2019ve searched whole google and only thing i\u2019ve saw is BamTools that can operate under windows, but it requires some serious setups that i\u2019m totally confused how to make it work.\nAny other programs are *nix/macos, and i dont see alternatives.\nDoes anything else exist ?\n(To be more specific, i\u2019m talking about windows 7 x86 (because that is what installed on our lab computers))"
    },
    {
      "role": "system",
      "text": "HI @user,\nwhy are you concerned about Windows compatibility? Most of the bioinformatics software is not able to run natively under Windows. If you want to learn commandline bioinformatics tools, I recommend to use Linux and Conda as a scientific package manager.\nMoreover, I think you are in the wrong forum. Galaxy does not require any installation for users. For example you could use BAM to FASTQ here @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtofastq/2.27.1. Simply with your webbrowser and no installation.\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m having some problems with my Pulsar installation and was wondering if anyone can give me some advice on what I might be doing wrong.\nIn my current setup I was able to use some tools from the tool shed using Pulsar, however internal Galaxy tools do not work. For example the internal upload tool does not work on my Pulsar server using the web variant. The job returns the following error:\n\nTraceback (most recent call last): File \u201c/data/galaxy/server/files/staging/36/tool_files/data_fetch.py\u201d, line 13, in  from galaxy.datatypes import sniff ModuleNotFoundError: No module named \u2018galaxy.datatypes\u2019\nAn error occurred with this dataset\nTraceback (most recent call last):\nFile \u201c/data/galaxy/server/files/staging/36/tool_files/data_fetch.py\u201d, line 13, in \nfrom galaxy.datatypes import sniff\nModuleNotFoundError: No module named \u2018galaxy.datatypes\u2019\n\nWhen I configure my local_env.sh in my Pulsar configuration to contain export TEST_GALAXY_LIBS= 1 I also get the below error, which make me suspect that this problem is because somehow Pulsar cannot use its local Galaxy installation correctly because I\u2019ve misconfigured something.\n\nBlockquote\nSourcing file ./local_env.sh\nTraceback (most recent call last):\nFile \u201c\u201d, line 1, in \nImportError: cannot import name \u2018eggs\u2019 from \u2018galaxy\u2019 (/data/galaxy/venv/lib/python3.8/site-packages/galaxy/init.py)\nFailed to setup Galaxy environment properly, is GALAXY_HOME (/data/galaxy) a valid Galaxy instance.\n\nMy Galaxy installation is in /data/galaxy. I\u2019ve tried changing the python path in the local_env.sh to several things like:\n\nexport PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib/galaxy/jobs/rules\n\nor\n\nexport PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib/\n\nBut neither seems to be working.\nThis is my local_env.sh configuration:\n \n ## Place local configuration variables used by Pulsar and run.sh in here. For example\n \n ## If using the drmaa queue manager, you will need to set the DRMAA_LIBRARY_PATH variable,\n ## you may also need to update LD_LIBRARY_PATH for underlying library as well.\n #export DRMAA_LIBRARY_PATH=/path/to/libdrmaa.so\n \n \n ## If you wish to use a variety of Galaxy tools that depend on galaxy.eggs being defined,\n export GALAXY_HOME=/data/galaxy\n export PYTHONPATH=${PYTHONPATH}:/data/galaxy/server/lib\n export TEST_GALAXY_LIBS=1\n\nAnd this is my job_conf.xml config on the side of the Galaxy server:\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"local_plugin\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n        <plugin id=\"pulsar\" type=\"runner\" load=\"galaxy.jobs.runners.pulsar:PulsarLegacyJobRunner\"/>\n    </plugins>\n    <destinations default=\"remote_cluster\">\n        <destination id=\"local_destination\" runner=\"local_plugin\"/>\n        <destination id=\"remote_cluster\" runner=\"pulsar\">\n            <param id=\"url\">http://pulsar-server-02:8913/</param>\n            <param id=\"dependency_resolution\">remote</param>\n        </destination>\n    </destinations>\n    <tools>\n    </tools>\n</job_conf>\n\nA workound to this problem I found is to configure the DATA_FETCH tool to run using the local_destination destination. However, I\u2019d like to know if there is a way to have all tools including internal ones run on the Pulsar side or whether there is a better way entirely to solve this problem.\nI also found this github issue that seems to be related to my question, but the fix from this issue did not work for me unfortunately: Galaxy eggs? \u00b7 Issue #134 \u00b7 galaxyproject/pulsar \u00b7 GitHub (@link https://github.com/galaxyproject/pulsar/issues/134)\nAny advice or pointers would be greatly appreciated!"
    },
    {
      "role": "user",
      "text": "This issue can be marked as solved because I found out that I shouldn\u2019t be using Pulsar in the way I described above and found a better way to run a Galaxy cluster in the meantime."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI annotated my sequencing data with SnpEff ( SnpEff eff: annotate variants (Galaxy Version 4.3+T.galaxy1)) using the database SnpEff download: SnpEff4.3 GRCh37.75 ( SnpEff download:  download a pre-built database (Galaxy Version 4.3+T.galaxy2)).\nWhen checking some variants, I found that information regarding regulatory region variants  are not supplied (as they are using the VEP for Ensemble version 75). Does the SnpEff database generally not supply this information?\nThanks!\nAll the best,\nRose\nExample: 9_139400219_G/A\nSnpEff annotation\nA|missense_variant|MODERATE|NOTCH1|ENSG00000148400|transcript|ENST00000277541|protein_coding|25/34|c.4129C>T|p.Pro1377Ser|4205/9371|4129/7668|1377/2555||,A|upstream_gene_variant|MODIFIER|NOTCH1|ENSG00000148400|transcript|ENST00000494783|processed_transcript||n.-922C>T|||||922|\noutput VEP see picture attached\n\nBildschirmfoto 2020-05-04 um 22.40.341047\u00d7273 45 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/beeb38ff4d145e7b35bfab3ca8ae3021c1573e73.png)\n\n\n\n\nUploaded Variation\nLocation\nAllele\nGene\nFeature\nFeature type\nConsequence\nPosition in cDNA\nPosition in CDS\nPosition in protein\nAmino acid change\nCodon change\nCo-located Variation\nExtra\n\n\n\n\n9_139400219_G/A\n9:139400219\nA\nENSG00000148400\nENST00000494783\nTranscript\nupstream_gene_variant\n-\n-\n-\n-\n-\nrs61751542\nSTRAND=-1;SYMBOL=NOTCH1;DISTANCE=922;SYMBOL_SOURCE=HGNC;GMAF=A:0.0087236\n\n\n9_139400219_G/A\n9:139400219\nA\n-\nENSR00001317600\nRegulatoryFeature\nregulatory_region_variant\n-\n-\n-\n-\n-\nrs61751542\nGMAF=A:0.0087236\n\n\n9_139400219_G/A\n9:139400219\nA\nENSG00000148400\nENST00000277541\nTranscript\nmissense_variant\n4205\n4129\n1377\nP/S\nCcc/Tcc\nrs61751542\nENSP=ENSP00000277541;STRAND=-1;SYMBOL=NOTCH1;HGVSc=ENST00000277541.6:c.4129C>T;HGVSp=ENSP00000277541.6:p.Pro1377Ser;SYMBOL_SOURCE=HGNC;GMAF=A:0.0087236\n\n\n\n"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis is a good question.\nSnpEff has a regulatory_region_variant effect, but regulatory effects belong to the additional annotations described in chapter 6 of the SnpEff manual (@link http://snpeff.sourceforge.net/SnpEff_manual.html), which you have to enable explicitly.\nIn the Galaxy wrapper, this is hidden in the section Regulation options just below the SnpEff4.3 Genome Data select box if you want to use it.\nBest,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Lati98,\nHow did you resolve this issue? Would you please share your solution?\nI have the same problem and my input data format (thermo.raw) and integrity looks good, as it should be, no input file or file name duplicates etc.\nThank you in advance for your help.\nBest regrds.\n(Qc)"
    },
    {
      "role": "user",
      "text": "Hi JennaJ,\nThank you for your kind suggestions.\nWe are talking about the GTN training material:  \u201cMaxQuant and MSstats for the analysis of TMT data\u201d;\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/maxquant-msstats-tmt/tutorial.html)\n\n\nGalaxy Training: MaxQuant and MSstats for the analysis of TMT data (@link https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/maxquant-msstats-tmt/tutorial.html)\nTraining material for proteomics workflows in Galaxy\n\n\n\n\n\nWhen you click on the \u201cgraduation hat\u201d icon on @link http://Galaxy.eu and search for \u201cMaxQuant and MSstats for the analysis of TMT data\u201d training material now there\u2019s a message appearing, which says;\n\" Under Development!\nThis tutorial is not in its final state. The content may change a lot in the next months. Because of this status, it is also not listed in the topic pages.\"\nProbably the specialists are checking the steps that you\u2019re suggesting.\nThank you once again for your kind assistance.\nRegards,\nQc"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Everyone,\nI am very new to Galaxy. I managed to create a countdata ,factordata, annotate form the in house bulk-RNA dataset. now I am trying to do Specify Contrast(s) of interest (based on the 2: RNA-seq counts to genes (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html)) but I keep getting errors. is anyone know how to fix this issue?\n\nDataset Error Report\nAn error occurred while running the tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.50.0+galaxy0 .\n\nDetails\nExecution resulted in the following messages:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nError in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, : line 7944 did not have 5 elements Calls: read.table \u2192 scan Execution halted\n\nTroubleshooting\nThere are a number of helpful resources to self diagnose and correct problems.\nStart here: My job ended with an error. What can I do? (@link https://galaxyproject.org/support/tool-error/)"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you share your history with me?\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m trying to carry out a DiffBind operation with peak files (in bed format) generated elsewhere. When selecting my input files and parameters, the default setting for Score Column for a narrowPeak file is column number 8. In my narrowPeak files, column 8 corresponds to the -log10(pvalue). Is this what the score refers to, or should I be changing the column number to correspond to another column, like abs_summit or fold_enrichment? I should note that none of the columns in the narrowPeak file are actually called \u2018score\u2019. Thanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nIn my narrowPeak files, column 8 corresponds to the -log10(pvalue). Is this what the score refers to\nUsually, yes, the p-value is what is used for \u201cscore\u201d when calculating the FDR.\n@quote\nI should note that none of the columns in the narrowPeak file are actually called \u2018score\u2019.\nThe file format for narrowPeak datasets is here: Genome Browser FAQ (@link https://genome.ucsc.edu/FAQ/FAQformat.html)\nYou\u2019ll notice the first few columns are similar to BED format, which has a score metric in the 5th column, but what that score represents differs between the two plus narrowPeak has two more summary attributes.\nResources\n\n\nDiffbind\n\nUsage from the tool form\u2019s help section in Galaxy:\n@link https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf\n\nBioconductor author Q&A forum: @link https://support.bioconductor.org/post/search/?query=diffbind\n\n\n\n\nMACS2\n\nSee the README and ISSUES to start with @link https://github.com/macs3-project/MACS\n\nGoogle Group author Q&A: @link https://groups.google.com/g/macs-announcement/\n\n\n\n\nTutorials\n\n@link https://training.galaxyproject.org/training-material/search?query=ChIP-seq\n@link https://training.galaxyproject.org/training-material/search?query=macs2\nAnd, the author resources have more examples\n\n\n\n\n\nThe GTN and Bioconductor groups will be hosting a training later this month. Please consider joining for direct expertise help from the original tool authors and scientists from both communities :slight_smile: Sm\u00f6rg\u00e5sbord 3: A week of Free, Online, Galaxy Training supported by the Global Galaxy Training Community (@link https://help.galaxyproject.org/t/smorgasbord-3-a-week-of-free-online-galaxy-training-supported-by-the-global-galaxy-training-community/9786)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019d like to compare the result of aligning a small RNA-seq dataset using HISAT2 vs Bowtie. (I realize Bowtie is rather old, but I\u2019d like to compare to old  data analyzed with Bowtie.) I aligned the same FASTQ file using the 2 programs, and then ran samtools stats. The \u201craw total sequences\u201d line in the stats output for the HISAT2 bam file matches up with the number of sequencing reads in the original FASTQ file (based on FASTQC), but the total sequences line from the Bowtie bam file is inflated by more than a million sequences. Can anybody see what I might be missing here? Am I misinterpreting the meaning of \u201craw total sequences?\u201d I expected to see different numbers of aligned reads, but not total input reads."
    },
    {
      "role": "system",
      "text": "Welcome, @user\nI took a look at one of your datasets mapped with both tools versus different tools that count up \u201creads\u201d (and sometimes \u201calignment lines\u201d instead!).\nFastq Groomer\n\nTotal seqs in the input fastq dataset reported: 2182610\n\nFastQC\n\nTotal sequences in the input fastq dataset reported: 2182610\nTotal sequences in HISAT2 result BAM (sorted): 2509206\nTotal sequences in Bowtie result SAM (unsorted): 4345310\nTotal sequences in Bowtie result SAM (unsorted) converted with Sam-to-BAM to a BAM (sorted): 4345310\n\nSamtools stats\n\nTotal \u201craw total sequences\u201d in HISAT2 result BAM (sorted): 2182610\nTotal \u201craw total sequences\u201d in Bowtie result SAM (unsorted): 4345310\nTotal \u201craw total sequences\u201d in Bowtie result SAM (unsorted) converted with Sam-to-BAM to a BAM (sorted): 4345310\n\nHISAT2\n\nTotal input reads reported the built-in mapping stats: 2182610\n\nBowtie\n\nNo built-in stats for this older tool\n\nLine/Word/Character count\nSome plain data manipulation to prepare first: Convert BAM-to-SAM or Sam-to-Fastq or Fastq-to-tabular, remove any headers with Select, and count the alignment lines. Then Cut out the reads names (first column) and finally get the Unique occurrences of each record. This is certainly a more complicated way to do the counts but is the \u201ctruth\u201d of the content and can work on any data, with the right prep manipulations, if you are ever unsure about what is going on with a particular tool. Manipulating data this way doesn\u2019t rely on special tools to interpret metadata/flags in non-obvious ways \u2013 plus you can compare raw text data counts to those from other tools to get a better idea about what those may be actually counting up.\n\nTotal \u201cunique reads\u201d in original Fastq (dataset 322 hidden, contained in collection dataset 323 as first dataset): 2182610\nTotal \u201calignment lines\u201d in HISAT2 result BAM: 2509206\nTotal \u201cunique reads\u201d in HISAT2 result BAM: 2182610\nTotal \u201calignment lines\u201d in Bowtie result SAM: 4345310\nTotal \u201cunique reads\u201d in Bowtie result SAM: 2182610\n\nIn summary, each reported alignment line or fastq entry (Fastq, BAM, SAM) is being counted up by some tools (including, in some cases, duplicated reported reads) and others are counting up true unique reads/sequences.\nThis is unfortunate but expected, and part of the reason the updated mapping tools (including Bowtie2) produce native tool mapping stats plus output sorted/better annotated BAMs by default.\nHope that helps, both to help make sense your data and to definitively confirm that all original reads were mapped by both tools. There are probably a hundred different ways to do this with Text/data manipulation tools (and line-command, outside of Galaxy with Unix utilities/scripts), but the tools I used should be a useful start, do not involve complicated regular expressions/programming, and are all included in Galaxy with simple usage.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi @system, today I was trying to use Methyldackel but I couldn\u2019t find it in the search tools, even shows \u2018results no found\u2019. Could that be because you are updating the tool? I tried to search for it last week but the same results. Why it does not show up in my search tool? Thank you!"
    },
    {
      "role": "system",
      "text": "That is wired. Are you on @link http://usegalaxy.eu?\nimage\nYou can try this link for the lastest version Galaxy | Europe (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/pileometh/pileometh/0.5.2+galaxy0)"
    }
  ],
  [
    {
      "role": "user",
      "text": "how to use my own google cloud"
    },
    {
      "role": "system",
      "text": "Hi @user\nWould you please explain more about your question?\nIf it about running your own Galaxy server, choices about platforms is available here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am working on Exome sequencing data.\nNeed to create PED file not sure how?\nSimilar question has been asked before but the link in the answer is not active.\nCan someone please explain how to create PED file?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis file needs to be created manually as follow:\n\nCopy the content by clicking on Copy:\n\n\nimage845\u00d7135 11.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f2b8e2796785480ca41cd74dc16e035ef229b897.png)\n\nClick on Upload data \u2192 Paste/Fetch data, and paste the content:\n\nimage932\u00d7564 54 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a62ed50f8fbc39e9b2935d2de592f8acaf711add.png)\n\nClick in Start\n\n\nIn that way, you can create the PED file.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I go to \u201cget data\u201d and then \u201c Download and Extract Reads in FASTA/Q** format from NCBI SRA\u201d. I then select input type SRA accession, and use accession number SRR643156 and then execute. I get the error \u201cThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below. {}\u201d. Can someone help me import my data? It\u2019s for an online course through coursera, this is the data they say I have to work with, and I can\u2019t contact anyone through the course for help. Thanks."
    },
    {
      "role": "system",
      "text": "@quote\nSRR643156\nHi @user\nThanks for reporting the problem. NCBI can get busy, with this type of error resulting. I\u2019m running a test with that exact accession right now at @link http://UseGalaxy.org across the different NCBI fastq get data tools, and the queries are still running and likely would have failed by now if were going to.\nPlease try again if you have not already. I\u2019ll write back with an update.\n\nUpdate:\nThe test downloads of that same accession worked fine. Please try again if you haven\u2019t already."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello , im doing internship in bioinformatic and i need more space and data storage . I\u2019m using interproscan and i have a set of 20 species/genomes at the moment . How can i get it  ?"
    },
    {
      "role": "system",
      "text": "Hey @user,\nyou can apply for an increased storage quota here: @link http://usegalaxy.eu/quota-increase\nCheers,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI am comparing microbial community in response to a treatment using lefse cladogram.\nI have 4 different experimental set up and I am planning to display the 4 lefse analysis together in order to compare result between experiment (see figure attached, \"CURRENT GRAPH). So far I have run 4 lefse analyses separately but I am realising that it makes the reading of the graph difficult, cause I have 4 different legend/letters  (see figure attached, \"CURRENT GRAPH). Is it possible to run 4 lefse analysis separately (from 4 distincts experiments) from the same table input, and therefore get one unique taxa legend for all 4 cladograms. I would like to get the result bellow (see figure attached, \u201cGRAPH EXPECTED\u201d).\nThanks very much for your help.\nScreenshot 2020-11-13 at 10.14.41440\u00d7553 58.3 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8dde5062591369ec3ff4b2302cef3d32a8c247dc.png)"
    },
    {
      "role": "system",
      "text": "In this case you are assuming that a given taxa color (up/down difference abundance) is the same in all the 4 cladograms. I wonder if that would be rare to happen.\nBut\nYou could simply \u201cmerge\u201d all the legend text (and the colored rectangles) in a graphics editor like inkscape, gimp, photoshop, etc. This is what I used to do. But you need to confirm what I said above.\nGood luck"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am trying to download RNA-seq data from the \u201cEBI\u201d website using the \u201cUpload Data\u201d. Unfortunately, only a small amount of data is loaded, for instance, 12 Mb from a 2 GB file. I have repeated this job for many several times, but, each time I encountered this problem."
    },
    {
      "role": "system",
      "text": "Hi @user\nWe\u2019ve had many reports of data download failures from EBI SRA over the prior week or so (and before that, during various time windows when they were reorganizing data). We aren\u2019t clear on what issues ENA may be having right now \u2013 perhaps just high volume \u2013 although we\u2019ve also seen some technical problems with the links one can capture from their website.\n:arrow_right: Try getting the data from the NCBI SRA instead. Entering URLs in the Upload tool is one method plus there are several dedicated tools. Search the tool panel with the keyword \u201cncbi\u201d to review the choices.\nFor fastq data, both of these tools are great choices. The first downloads individual dataset(s); the second sorts the dataset(s) into collections. Full usage options are on each tool\u2019s form. Both are equally \u201cfast\u201d \u2013 the Galaxy tool name is simply reflective of the underlying name NCBI gave these functions.\n\nDownload and Extract Reads in FASTA/Q format from NCBI SRA\nFaster Download and Extract Reads in FASTQ format from NCBI SRA\n\nHope that works out!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, we had a user email us who has a link to a file in Galaxy which the UCSC Genome Browser cannot open because the @link http://usegalaxy.org web browser doesn\u2019t support byte range requests:\ntrack name=\u201c6hr Veh 1\u201d type=bigWig bigDataUrl=@link https://usegalaxy.org/api/datasets/f9cad7b01a47213550329ab30779cd1d/display?to_ext=bigwig\nleads to:\nCouldn\u2019t open @link https://usegalaxy.org/api/datasets/f9cad7b01a47213550329ab30779cd1d/display?to_ext=bigwig\nI\u2019m pretty sure that it used to be possible to load Galaxy-hosted files into the UCSC Genome Browser. Is there something that has changed on your end?\nbest\nMax"
    },
    {
      "role": "system",
      "text": "Resolved 11/14/2022\nHi @user \u2013 we have added in a change to restore the prior functionality. This is available at @link http://UseGalaxy.org so far, and the other usegalaxy.* servers will be updated soon.\nDetails HEAD and byte-range support \u00b7 Issue #14982 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/14982)\n\n\nUpdate 11/11/2022\nWe identified a change that appears to be on our side triggering the import issue. It was likely introduced in the last release. Details are in the public chat here You're invited to talk on Matrix (@link https://matrix.to/#/!NhNWKNcgINoFZdbUbY:gitter.im/%24NKCRGmOj0OSiqnvBrsOCtsu6fyrgx7Vy-ODmjvTQWRg?via=gitter.im&via=matrix.org)\nHappy to be wrong about this method \u201cnot ever\u201d working :slight_smile: We\u2019ll post updates back here, and feel free to follow or join the chat. You can also message me directly to coordinate end-user communications around the anticipated fix when the time comes.\nThanks Max!\n\nHi @user\nI saw that thread at the UCSC forum, too. Unless something changed very recently, hosting files from Galaxy to the UCSC browser should definitely still be possible.\nAs a guess, I think the problem with the particular file the user was asking about is that it didn\u2019t have a \u201cdatabase\u201d key assigned. It didn\u2019t have one when I upload it here yesterday (seems Ok to post since is already public data, and I\u2019ll unshare/purge in a day or two): Galaxy | Accessible History | test bigwig (@link https://usegalaxy.org/u/jen/h/test-bigwig). A matching database -> dbkey is required for the UCSC link to even show up in Galaxy, so how that error was produced wasn\u2019t clear.\nOh! I reread the post at UCSC. This part has clues:\n\nThe Url links to my workspace in Galaxy, where my bigWig files are saved. When I click Submit, the genome browser gives me the following warning:\n\nMaybe the user isn\u2019t linking directly from inside Galaxy, and instead trying to upload the file to the UCSC browser by URL? I forgot how that works on your end \u2013 do users target a specific dbkey/assembly for upload? That never worked from what I remember. People had to start in Galaxy and use the link inside a dataset that points to UCSC.\nI will still take another look. More feedback, probably tomorrow."
    }
  ],
  [
    {
      "role": "user",
      "text": "I want use ce10ToCe11.over.chain file with the MiModD rebase tool (Galaxy name: Rebase Sites ) to convert my VCF variant coordinates to the SnpEff genome file coordinates just before i do the annotation.But i find Can\u2019t import the chain files to Galaxy rebase site tool"
    },
    {
      "role": "system",
      "text": "Hi,\nyou need to declare the format of the chain file as tabular.\nIf you haven\u2019t done so during upload, you can do it by clicking the pencil icon on the dataset in your history now, then select the datatype tab."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! Would it be better to use FeatureCounts or StingTie for generating count tables in a RNA-Seq analysis?\nGalaxy tutorials recommend FeatureCounts, but I\u2019ve also read that StringTie is very efficient and widely used.\nI\u2019m working with Arabidopsis thaliana so I\u2019m not really interested in novel transcripts. I\u2019d just like to know what is the difference between these tools and which one should I use, since I get better variances in the PCA plot using tables from FeatureCounts than using tables from StringTie from the exact same data."
    },
    {
      "role": "system",
      "text": "Hi @user\nIf you don\u2019t care about novel transcripts and have a reference genome, go straight to Featurecounts and include a known annotation GTF.\nStringtie would include novel transcripts/genes plus known annotation \u2013 but only if the known annotation was included. You can also adjust the settings in Stringtie to only output results based known annotation. If you don\u2019t include known annotation, only counts based on the regions your read data represent (transcripts/genes) will generated \u2013 and that may be where the biggest differences are coming from.\nSo, there are a few different ways to do this and the results would not be expected to be exact between any two DE methods/pipelines based on distinct algorithms. Using differently created transcript/gene boundaries for the \u201cannotation\u201d to determine expression levels is also a factor.\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to install a custom blast db into a locally hosted instance of Galaxy.  I have edited the blastdb.loc file and it is properly detected by Galaxy, as seen in the admin datatable:\nimage1080\u00d7107 6.71 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/efc19aa5be21fad54596d3d01ab80c7fe7433ab7.png)\nThe file path points to a folder with the following files:\nmfas.nhr  mfas.nin  mfas.nog  mfas.nsd  mfas.nsi  mfas.nsq.\nIf I go to that folder and run blast at the command line using the \u201cmfas\u201d keyword, the database is fine.\nHowever, when I run the NCBI BLAST + blastn Galaxy tool, I get this error:\n\nBLAST Database error: No alias or index file found for nucleotide database [/apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/] in search path [/apps/galaxy/galaxy_staging1/database/jobs_directory/000/9/working::]\n\nWhat am I doing wrong?  I am currently using the full filepath but I don\u2019t think it\u2019s pointing correctly.  I\u2019ve read the tutorials but it\u2019s unclear if I\u2019m supposed to provide the full filepath or a relative filepath.\nAny help would be greatly appreciated!  I\u2019m new to Galaxy admin but will need to install quite a few custom DBs for my group so I really need this working."
    },
    {
      "role": "system",
      "text": "I am not sure but should you not add the actual database to your path? Now it is pointing to a folder and not an alias or index.\nSo instead of /apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/ I think it should be /apps/galaxy/galaxy_staging1/tool-data/app_db/mfas/mfas\nor\n/apps/galaxy/galaxy_staging1/tool-data/app_db/mfas"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there!\nI am using galaxy on my classes, and would like to delete all users at the course end, but, there is no \u201cdelete user\u201d option on admin area.\nI use Galaxy as a Docker instance.\nThanks for any ideia on how to do it :wink:\nCheers"
    },
    {
      "role": "system",
      "text": "Hi,\nwhen you are on this page: https:///admin/users then click on the arrow next to the user and inside the drop down you will find the option \u201cDelete\u201d.\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have paired-end reads data of NGS in 2 fastq files that are: father_R1.fq.gz and father_R2.fq.gz\nHow can I find how many reads are included in these two fastq files and their read length respectively through Galaxy?\nIs there any specific tool for it?"
    },
    {
      "role": "system",
      "text": "Did you tried FastQC already? This would be a good start."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\uff0cI installed the workflow but encoutered the errors as follows:The server could not complete the request. Please contact the Galaxy Team if this error persists.\n{\n\u201cnew_history_name\u201d: null,\n\u201chistory_id\u201d: \u201c1cd8e2f6b131e891\u201d,\n\u201cresource_params\u201d: {},\n\u201creplacement_params\u201d: {\n\u201cplot_title\u201d: \"APEC \"\n},\n\u201cparameters\u201d: {\n\u201c0\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201chid\u201d: 108,\n\u201cid\u201d: \u201cf2db41e1fa331b3e\u201d,\n\u201cname\u201d: \u201cAPECpairedfastqs\u201d,\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c1\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c7ca8f1b7f24e5a2d\u201d,\n\u201csrc\u201d: \u201chda\u201d,\n\u201chid\u201d: 109,\n\u201cname\u201d: \u201cpp_plasmid_database.fasta\u201d,\n\u201ckeep\u201d: true,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c2\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201chid\u201d: 110,\n\u201cid\u201d: \u201cf30a35c999095ed7\u201d,\n\u201ckeep\u201d: false,\n\u201cname\u201d: \u201cplasmidfinder_plusAMR.fasta\u201d,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ctags\u201d: \n}\n],\n\u201cbatch\u201d: false\n}\n},\n\u201c3\u201d: {\n\u201csingle_or_paired|type\u201d: \u201ccollection\u201d\n},\n\u201c4\u201d: {\n\u201cnum_lines\u201d: \u201c1\u201d\n},\n\u201c5\u201d: {\n\u201ccolumn\u201d: \u201c9\u201d,\n\u201cstyle\u201d: \u201cnum\u201d,\n\u201corder\u201d: \u201cDESC\u201d,\n\u201cheader_lines\u201d: \u201c0\u201d\n},\n\u201c6\u201d: {\n\u201ccond\u201d: \u201cc9>80\u201d,\n\u201cheader_lines\u201d: \u201c0\u201d\n},\n\u201c7\u201d: {\n\u201ccolumnList\u201d: \u201cc1\u201d,\n\u201cdelimiter\u201d: \u201cSp\u201d\n},\n\u201c8\u201d: {\n\u201cexact\u201d: \u201cfalse\u201d,\n\u201cinverse\u201d: \u201cfalse\u201d,\n\u201cfile_or_type|select\u201d: \u201clist\u201d\n},\n\u201c9\u201d: {\n\u201csingle_or_paired|type\u201d: \u201ccollection\u201d,\n\u201cmlst_or_genedb|job_type\u201d: \u201ccustom_only\u201d,\n\u201cmlst_or_genedb|gene_max_mismatch\u201d: \u201c250\u201d,\n\u201coptions|select\u201d: \u201cadvanced\u201d,\n\u201coptions|min_coverage\u201d: \u201c0\u201d,\n\u201coptions|max_divergence\u201d: \u201c0\u201d,\n\u201coptions|min_depth\u201d: \u201c5\u201d,\n\u201coptions|min_edge_depth\u201d: \u201c2\u201d,\n\u201coptions|prob_err\u201d: \u201c0.01\u201d,\n\u201coptions|stop_after\u201d: \u201c1000000\u201d,\n\u201coptions|mapq\u201d: \u201c1\u201d,\n\u201coptions|baseq\u201d: \u201c20\u201d,\n\u201coptions|minins\u201d: \u201c0\u201d,\n\u201coptions|maxins\u201d: \u201c1000\u201d\n},\n\u201c12\u201d: {\n\u201cgroupcol\u201d: \u201c3\u201d,\n\u201cignorecase\u201d: \u201cfalse\u201d,\n\u201cignorelines\u201d: null,\n\u201coperations_0|optype\u201d: \u201cunique\u201d,\n\u201coperations_0|opcol\u201d: \u201c3\u201d,\n\u201coperations_0|opround\u201d: \u201cno\u201d,\n\u201coperations_0|opdefault\u201d: \u201c\u201d\n},\n\u201c13\u201d: {\n\u201ccolumnList\u201d: \u201cc1\u201d,\n\u201cdelimiter\u201d: \u201cT\u201d\n},\n\u201c14\u201d: {\n\u201cexact\u201d: \u201cfalse\u201d,\n\u201cinverse\u201d: \u201cfalse\u201d,\n\u201cfile_or_type|select\u201d: \u201clist\u201d\n},\n\u201c15\u201d: {\n\u201cchecks_0|pattern\u201d: \u201c^>\\d+(.*?).*\u201d,\n\u201cchecks_0|replacement\u201d: \u201c>\\1\u201d\n},\n\u201c16\u201d: {\n\u201cdbtype\u201d: \u201cnucl\u201d,\n\u201ctitle\u201d: \u201cMatched Plasmids\u201d,\n\u201cparse_seqids\u201d: \u201cfalse\u201d,\n\u201chash_index\u201d: \u201ctrue\u201d,\n\u201cmask_data_file\u201d: null,\n\u201ctax|taxselect\u201d: \u201c\u201d\n},\n\u201c17\u201d: {\n\u201cdb_opts|db_opts_selector\u201d: \u201chistdb\u201d,\n\u201cdb_opts|database\u201d: \u201c\u201d,\n\u201cdb_opts|subject\u201d: \u201c\u201d,\n\u201cblast_type\u201d: \u201cmegablast\u201d,\n\u201cevalue_cutoff\u201d: \u201c0.001\u201d,\n\u201coutput|out_format\u201d: \u201cext\u201d,\n\u201cadv_opts|adv_opts_selector\u201d: \u201cbasic\u201d\n},\n\u201c18\u201d: {\n\u201csureness\u201d: \u201c0.75\u201d,\n\u201clength\u201d: 0,\n\u201ccoverage\u201d: 0,\n\u201ctitle\u201d: \"APEC \",\n\u201canonymize\u201d: \u201cfalse\u201d,\n\u201ccombineincs\u201d: \u201cfalse\u201d\n}\n},\n\u201cparameters_normalized\u201d: true,\n\u201cbatch\u201d: true\n}\nanybody to fix this?"
    },
    {
      "role": "system",
      "text": "Hi @user\nFirst, try a rerun of what you were doing. Several of the public servers are undergoing updates. If you haven\u2019t tried what you were doing again, please try that now.\nIf the problem continues, please provide more details:\n\nWhere are you working? URL if a public server or describe if other.\nDid you get any error datasets? Do those errors describe what is going wrong (how to check error logs (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors))?\nAre the inputs a match to what the workflow is expecting (how to check inputs (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages))?\nWhere did you source the workflow? If it was imported (installed?), did you open the workflow after importing and check for warnings? Try this if you haven\u2019t already.\nIf you can, please post share links: to the history with the inputs, to the imported workflow, and to the output history (if not the same as the input). That will provide context the community can help with.\n\nLet\u2019s start there :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to perfrom a de novo assembly from nanopore reads. All the reads are stored in a fasta file. I tried to run the flye assembler on the dataset, but 3 hours later, It returns with an error.\ntoolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/2.9+galaxy0 .\nDoes anyone know what this error means, and point me in the direction to fix it.\nI can provide more information if needed.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you share your Galaxy history (@link https://youtu.be/_uQrN45zkmE) with me? I\u2019ll have a look. You can share it with me by using my email address Screenshot from 2022-04-03 16-52-09.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy Community,\nI am struggling to set up the GalaxyCloudMan through an AWS to my Galaxy through @link http://usegalaxy.org.au.\nFirst according to the instructions I have made AWS account > create group (selecting ec2 and s3 full access) > users > add users to group > and download the credentials access key and secrete key. These keys I have used to launch GalaxyCloudMan through CloudLaunch.\nIn CloudMan Console I access the Galaxy and I registered using my mail ID and password that I made on CloudLaunch. Galaxy on the cloud interface appear but not fully active.\nActually, I am Coursera student where according to the instructions the Galaxy CloudMan is launched from CloudMan console step to the cloud selecting new cloud cluster in the account available on @link http://usegalaxy.org.\nHere is my QUESTION as well as PROBLEM that I do not have Cloud tab or option in my galaxy account available on @link https://usegalaxy.org.au/. Therefore, I could not launch or access the Galaxy CloudMan to my Galaxy account (@link https://usegalaxy.org.au/).\nWhat step am I missing to follow? What is required for cloud option to appear in Galaxy.\nKindly resolve this problem\nNeeti Nirwal"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe way cloud Galaxy is started up has changed a bit since that tutorial was published. Launch one from here instead: @link https://launch.usegalaxy.org/catalog\nUse the GVL version for the best performance and reliability (it is based on the Galaxy release 20.01)\nThe other Cloudman option is considered deprecated. So shut down and remove the old one, and start over with the GVL version. Your account at the new cloud server will be distinct from your accounts on any public Galaxy server.\nMeaning, you need to add yourself as an administrator and create an account through the Galaxy website created inside the GVL appliance. Once both are done, you\u2019ll be the administrator of the server with full control over configuration/administration (adding cluster nodes, installing new tools, and the like).\nRelated Q&A: Galaxy Cloudman or GVL? (@link https://help.galaxyproject.org/t/galaxy-cloudman-or-gvl/3651)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\none of my friends has a workflow which is in another galaxy server.\nShe send me a work flow in pdf file and some file with .ga.\nI want to know is there anyway she can share that workflow with me in galaxy ? because i cant move it in my workflow section.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi,\nnot sure what\u2019s inside the pdf, but you should be able to import the .ga workflow definition file into any Galaxy server.\nWhile logged in, select the Workflow tab from the top menu, then select Upload or import workflow (i.e., the little icon in the top right corner of your workflow list view). On the next page, you will want to provide your .ga file as an Archived workflow file.\nNote that this imports the workflow definition, but in order to run the workflow, you also have to have all tools used in it installed on the  Galaxy instance you are working on."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI am trying to run Galaxy on the Genetics and Genomics Analysis Platform (GenAP) hosted by Compute Canada and am running into an error with various tools.\nI have fastq files from Illumina RNA sequencing that I am trying to assemble into a transcriptome using Trinity or to map it to a reference genome of a closely related species using HISAT2.\nWhen I try to run either of these tools I get the following error:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: \u2018time\u2019\nI get this error if I try to run Trinity, HISAT2, or Bowtie2. This is the full error message I get when I try to run Trinity:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: \u2018time\u2019\n{\n\u201chistory_id\u201d: \u201cf597429621d6eb2b\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1\u201d,\n\u201ctool_version\u201d: \u201c2.9.1\u201d,\n\u201cinputs\u201d: {\n\u201cpool|pool_mode\u201d: \u201cYes\u201d,\n\u201cpool|inputs|paired_or_single\u201d: \u201cpaired\u201d,\n\u201cpool|inputs|left_input\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cNS.1543.001.NEBNext_dual_i7_176\u2014NEBNext_dual_i5_176.Fish1-unfed_R1.fastq.gz (as fastqsanger)\u201d,\n\u201ctags\u201d: ,\n\u201ckeep\u201d: false,\n\u201chid\u201d: 1,\n\u201cid\u201d: \u201c0a248a1f62a0cc04\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201cpool|inputs|right_input\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cNS.1543.001.NEBNext_dual_i7_176\u2014NEBNext_dual_i5_176.Fish1-unfed_R2.fastq.gz (as fastqsanger)\u201d,\n\u201ctags\u201d: ,\n\u201ckeep\u201d: false,\n\u201chid\u201d: 2,\n\u201cid\u201d: \u201c03501d7626bd192f\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201cpool|inputs|strand|is_strand_specific\u201d: \u201cfalse\u201d,\n\u201cpool|inputs|jaccard_clip\u201d: \u201cfalse\u201d,\n\u201cnorm\u201d: \u201ctrue\u201d,\n\u201cadditional_params|min_contig_length\u201d: 200,\n\u201cadditional_params|guided|is_guided\u201d: \u201cno\u201d,\n\u201cadditional_params|long_reads\u201d: null,\n\u201cadditional_params|min_kmer_cov\u201d: 1,\n\u201c__job_resource|__job_resource__select\u201d: \u201cno\u201d\n}\n}\nI seem to be able to run other tools (FastQC, Trimmomatic) and I have also run HISAT2 on these same fastq files using the @link http://usegalaxy.org instance.\nIf anyone has any advice or can diagnose this problem, I\u2019d really appreciate it!\nThank you!"
    },
    {
      "role": "user",
      "text": "FYI, solved my own issue by playing around with some of the sample data from Galaxy tutorials.\nMy file format was \u201cfastqsanger.gz\u201d so I changed the datatype to \u201cfastqsanger\u201d and this seems to have solved the error I was getting.\nNow let\u2019s see where I\u2019ll get stuck next!\nThank you Galaxy community!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI am new to Galaxy and bioinformatics and just going through the training. I can not find the \u2018select first\u2019 or the \u2018compare two datasets\u2019 tools in the tool panel  and when I go to the Tool Shed to search for them they don\u2019t appear to be there. Has the name of these tools changed?\nThanks :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user\nSome of these basic manipulation tools are native to the Galaxy application, and won\u2019t be in the ToolShed. They should be found with a tool panel search but that has a few problems right now.\nAt UseGalaxy.* servers the two tools you asked about are under the GENERAL TEXT TOOLS group and nested in sub-groups:\n\nText Manipulation \u2192 Select first lines from a dataset (Galaxy Version 1.0.1)\nJoin, Subtract, and Group \u2192 Compare two Datasets to find common or distinct rows (Galaxy Version 1.0.2)\n\nOnce you find a tool and load the tool form, you can star it in the upper right corner to add it to your shorter list of favorites.\nWe are working to improve/repair the tool search. Let\u2019s leave this topic open in case you or others have more tools that are difficult to find. We can also share these with the developers.\nI can confirm the two tools you already posted cannot be found with a tool panel search at @link http://UseGalaxy.org or @link http://UseGalaxy.eu right now (instead, scroll through the sections above). Other general text tools are also impact. The tool search is working properly at @link http://UseGalaxy.org.au.\nYou can run through tutorials at any of the servers listed under the \u201cAvailable at these Galaxies\u201d menu at the top of each. Some tools are really only available at specific public sites or a special Docker container. The exceptions are the \u201cIntroduction to Galaxy\u201d tutorials \u2013 these should work at most public sites even if not listed.\n\nFor everyone: if a tool is difficult to find at an \u201cAvailable Galaxy\u201d, please post a reply and include the following:\n(quote and replace text)\n\n\nI am working at: public server URL\n\n\nThe tutorial is: the tutorial URL\n\n\nProblems finding the tool(s): tool names as given in the tutorial\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am currently trying to upload bed files in order to use the deepTools computeMatrix tool. The bed files appear to be loaded in correctly, but whenever I run the computeMatrix tool I get the below error.\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: objectstore, _call_method failed: get_filename on Dataset(id=29454816), kwargs: {}\nAfter the error occurs, the upload bed file job appears red and gives an error that my input file has chrX, but the genome file does not. I am using mm10 when I upload the bed files.\nAnother person in the lab has used the exact same files and it has worked for them, so any help would be appreciated."
    },
    {
      "role": "system",
      "text": "Update\nThe bug report sent in revealed that the BED dataset contained extra spaces. Problematic formatting can produce many types of tool errors, but the solution is the same \u2013 fix the formatting, then rerun tools.\nTabular datasets (including BED data) can be cleaned-up a few ways.\n1. Fix the dataset formatting locally on your computer, then  Upload  it to Galaxy after.  Do this if you plan to use the data outside of Galaxy as well. Or, fix the data in Galaxy with one of the methods below and download a copy of the format-corrected BED dataset.\n2. Remove the extra spaces during  Upload  to Galaxy.  How-to: In the  Upload  tool, click on the \u201cgear\u201d icon and check the box to \u201cConvert spaces to tabs\u201d. The tool merges all consecutive \u201cwhitespace\u201d (actual spaces or tabs) into a single \u201ctab\u201d \u2013 which is what is wanted for proper tabular dataset formatting, including BED data. The screenshot below shows where to find the option.\nNote:  If you ever decide to \u201cpaste\u201d (or type in) tabular data lines with the  Upload  tool, this is a good option to use to avoid formatting issues.\nWarning:  Use this option carefully. Example: if a dataset has header lines, converting \u201cwhitespace to tabs\u201d will be applied to those header lines, not just the data lines. That is not always desirable as it can introduce another formatting problem.\n\nupload_cleanup_whitespace.png1742\u00d7640 71.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/ec68bb47cc8be45475db6d00f8c9a7f6acb3300e.png)\n3. Run the tool  Text Manipulation > Convert delimiters to TAB  on an already loaded dataset.  Use the settings shown in the screenshot below. Once the job completes, the result will have the datatype \u201ctabular\u201d assigned. Click on the pencil icon (Edit Attributes) for the dataset to change it back to BED, and save.\n\ntext-manipulation_cleanup_whitespace2.png1410\u00d7742 50 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/123365d03a1c8f1c884a7f62fcc5c5b3c5368900.png)\nFAQs:\nGeneral error help: Troubleshooting resources for errors or unexpected results (@link https://help.galaxyproject.org/t/troubleshooting-resources-for-errors-or-unexpected-results/42/5)\nSupport links: @link https://galaxyproject.org/support/#getting-inputs-right\n\nFormat help for Tabular/BED/Interval Datasets (@link https://galaxyproject.org/support/tabular/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\nTool error? Try Sorting Your Inputs (@link https://galaxyproject.org/support/sort-your-inputs/)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone :slight_smile:\nI have a list of paired-end files from Rad-Seq sequencing, for which the barcode (which were at the start of the sequences) were removed after demultiplexing. I was wondering though, I now intend to use the stacks series for a denovo map, but I noticed that my sequences all start with a part of the restriction site used (eg in my forward sequences it starts with TGCAG, and in reverse with CGG). Should I trim (with trimmomatic by example) my data before proceeding with the Stacks series ?\nThank you in advance! :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi Joelle :wink: ,\nIf I am not saying a mistake, you don\u2019t have to trim your data between process_radtags step and others Stacks tools.\nCheers,\nYvan"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have FASTQ files and I am going to map with reference genome Aspergillus niger.But I can not find Aspergillus niger  as reference genome in Galaxy.\nCan you help me to sort out at your earliest since I have to finish this for my thesis please.\nKind regards,\nJagath Ranasinghe -Sri Lanka"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can download the A. niger genome from the NCBI website (@link https://www.ncbi.nlm.nih.gov/genome/?term=Aspergillus%20niger%5BOrganism%5D&cmd=DetailsSearch) and uploading it to Galaxy. In that way you can use it for your analysis.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to set up Galaxy to work on AWS.\nAccording to the instructions here @link https://galaxyproject.org/cloudman/getting-started/ I am supposed to go to the CloudLaunch application (@link https://beta.launch.usegalaxy.org/) and choose Galaxy CloudMan, but the link is broken and in @link https://launch.usegalaxy.org/catalog it notes that CloudMan is deprecated and GVL is preferable.\nGVL\nWhat is recommended?\nThanks"
    },
    {
      "role": "system",
      "text": "The documentation is out of date on the Getting Started page (I\u2019ll update it). The option to use is the GVL."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I am fairly new to Galaxy, and am trying to complete a differential gene expression analysis. The tools I am using are HISAT2, featureCounts, and then DESeq2. I have run all of the programs successfully until DESeq2. DESeq2 will run and complete successfully (turns green), but it gives me 0 lines for the output, so I don\u2019t actually receive any results. Why is this happening?\nAny help is appreciated! Thanks in advance!"
    },
    {
      "role": "user",
      "text": "Hi @system ,\nThanks for the reply! I did a rerun and got an error that time. My issue was that I was using different featureCount files with the same name. I have started over and DESeq2 runs successfully now and actually gives output lines, so I\u2019m all good!\nThanks for the help!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI want to upload a file, but the link \u201cUpload File\u201d is not working, no window pops up as it normally would. Has anyone else had this problem?"
    },
    {
      "role": "user",
      "text": "Sorry, it is the \u201cUpload File from your computer\u201d link under the tab \u201cGet Data\u201d. I just realized that there is a second button at the top left under the tools search bar called \u201cUpload Data\u201d which does work normally. So now that I have figured that out I can upload data."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have a question about the table compute tool. Whatever I change in the syntax the custom expression on the whole table does not work. It outputs an empty tabular file instead of a table with the new values. See the image below.\n\n2023-03-271476\u00d7759 17.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6e117d7a0c6cc814960b63e185375e6cced2d3a7.png)\nHere the steps that I followed in table compute tool:\n\n\n\u201cInput Single or Multiple Tables\u201d: Single Table\n\nparam-file \u201cTable\u201d: iris.csv with box: input data has Column names on the first row checked\n\n\u201cType of table operation\u201d: Perform a full table operation\n\n\n\u201cOperation\u201d: Custom\n\n\n\u201cCustom expression on \u2018table\u2019, along \u2018axis\u2019 (0 or 1)\u201d: table.sub(table.min(0), 0)\n\n\n\n\n\n\n\nAny help with solving this problem would be appreciated!\nDaria"
    },
    {
      "role": "system",
      "text": "Hi @user\nAre you trying to compute the minimum value per column? Please try this:\n\nScreen Shot 2023-03-27 at 12.07.29 PM1426\u00d7984 73.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/92d41ac0c09344409da9cb6ca7775383d6937a7f.png)\nIf you want to instead calculate the minimum value per column AND subtract that minimum from all the original values in that same column, change the operation to this:\n\nScreen Shot 2023-03-27 at 12.27.00 PM1060\u00d7186 9.94 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8e2d7c5a10b90a5e47dcfb932415ee50e1591693.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!!\nToday I found a complete set of Trinity tools in @link http://test.galaxyproject.org (not implemented in Main, yet) but I\u2019m concerned about the registration.\nDo I need a new account or just use the same of Main Galaxy?  I don\u2019t wanna have any problem with a duplicated account.\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Create a new account there, these are completely separate services and there is no duplication prohibition between them."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI setup my local Galaxy v20.01 and I\u2019d like to change username on User Management of Admin menu . In version v19.05, clicking on the user name button takes me to a page where I can change the email address and public name, but in v20.01, it takes me to the page of the admin user\u2019s own information. Is this a known issue?\nThanks,\nYukie"
    },
    {
      "role": "system",
      "text": "Hi @user\nChoices:\n\nHave the user update their own account preferences.\nSet up \u201cimpersonation\u201d. When you are logged in as an admin, the option will be under Admin > User Management > Users > search for the user > find it in the pull-down menu. Galaxy Configuration \u2014 Galaxy Project 21.05.1.dev0 documentation (@link https://docs.galaxyproject.org/en/master/admin/config.html?highlight=impersonation#allow-user-impersonation)\n\nThere is probably a way to do this line command against the database (or using the gxadmin function), but I don\u2019t know the exact details, and it may different between servers anyway. You could ask at either of these Gitter channels instead (public channels \u2013 don\u2019t post anything private!): galaxyproject/admins - Gitter (@link https://gitter.im/galaxyproject/admins) or maybe galaxyproject/dev - Gitter (@link https://gitter.im/galaxyproject/dev)\n\n\nThe other function was deprecated as far as I know \u2013 it presents at any server I work at as admin. Likely was a security related change. You could ask for confirmation at Gitter as well.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy team,\nI am attempting to reproduce a galaxy tutorial using command line versions of the tools(Exome sequencing data analysis for diagnosing a genetic disease (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/exome-seq/tutorial.html))\nI tried filtering the bam files using samtools (command line version) but the result I got is different if I use the galaxy platform.\nhere is the command I used on my PC samtools view -F 0x012 mapped_reads_father.bam\nPlease advise. Thanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf you are getting a different result with any tool locally versus Galaxy, try comparing the command line Galaxy used with what you are using locally.\nFind the Galaxy command string on the Dataset Information page (:information_source: icon in each dataset) in the section \u201cJob Information\u201d. This is the same page where other job logs (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors) are found.\nYou can also scroll down a bit more to the section \u201cJob Dependencies\u201d to learn the version of any dependencies that were used. For Samtools, the version probably won\u2019t make a difference, but could with other tools, including tools used upstream.\n@quote\nhere is the command I used on my PC samtools view -F 0x012 mapped_reads_father.bam\nYou are trying to reproduce the filter step here (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/exome-seq/tutorial.html#mapped-reads-postprocessing) in the tutorial, correct?\nIf yes, the command line is this, and is the same as you would find in the Galaxy command string log:\nsamtools view -o 'output.bam' -h   -b  -F 0xc input.bam\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019m running a diffbind operation between two conditions from an ATAC-seq experiment with two replicates in each. However, I get an error message resulting in my output files being buggy/red.\n\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\n6hVC1_peaks_narrowPeak 6hVC1_peaks_narrowPeak  VC  NA raw\n6hVC2_peaks_narrowPeak 6hVC2_peaks_narrowPeak  VC  NA raw\n6hdTAG2_peaks_narrowPeak 6hdTAG2_peaks_narrowPeak  dTAG  NA raw\n6hdTAG3_peaks_narrowPeak 6hdTAG3_peaks_narrowPeak  dTAG  NA raw\nconverting counts to integer mode\ngene-wise dispersion estimates\nmean-dispersion relationship\nfinal dispersion estimates\nWarning message:\nNo sites above threshold\nError in pv.getPlotData(DBA, attributes = attributes, contrast = contrast,  :\nUnable to plot \u2013 no sites within threshold\nCalls: dba.plotHeatmap \u2192 pv.getPlotData\n\nDoes \u201cNo sites above threshold\u201d in the error message just suggest that there were no differentially accessible sites (which I was expecting)? If so, why does the output seem to suggest there was an error in the process, rather than just returning an \u2018empty\u2019 interval file or some \u2018blank\u2019 result? I\u2019m quite new to using diffbind on Galaxy, so I\u2019m not too familiar with the various error messages that might occur. Thank you!"
    },
    {
      "role": "system",
      "text": "@quote\nDoes \u201cNo sites above threshold\u201d in the error message just suggest that there were no differentially accessible sites (which I was expecting)?\nYes, that is what the message means.\n@quote\nIf so, why does the output seem to suggest there was an error in the process, rather than just returning an \u2018empty\u2019 interval file or some \u2018blank\u2019 result?\nThe tool wrapper is around a Bioconductor tool written in R. Trapping errors for these cases usually relies on the output from R, and the same tool/inputs would likely fail with the same message if run directly.\n@quote\nI\u2019m quite new to using diffbind on Galaxy, so I\u2019m not too familiar with the various error messages that might occur. Thank you!\nNo problem! Some tools run in Galaxy will produce more meaningful messages than others. Much depends on how error trapping was implemented in the original 3rd party tool. :slight_smile:\nThe GTN FAQs here can help. Search this page with keywords or error messages:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/faqs/galaxy/)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\nAnd this specific FAQ explains how to find all of the failure details:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello I have been using Galaxy europe for some time without problems. Is it normal that the site does not work today?"
    },
    {
      "role": "system",
      "text": "@link http://usegalaxy.eu is unreachable today due to a network issue.  The compute center is working to fix it, but we do not have an estimate of how long it will take."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m at a standstill trying to install a local instance of the Galaxy server.  I hoping someone on this forum who has successfully installed a local instance of Galaxy might work with me to compare install notes to see where I\u2019m messing things up.\nMy issues, as already posted on this forum, are:\n\nUpload from the local computer just hangs, no error in logs.\nSort of data hangs, again no error in log files sort data seems to hang (@link https://help.galaxyproject.org/t/sort-data-seems-to-hang/9335)\n\nrpy2 module is not found whenever you try to run stats on a data file, even though the module is installed according to Galaxy web UI.   rpy2 install fails (@link https://help.galaxyproject.org/t/rpy2-install-fails/9318)\n\n\nThose are my top three issues right now.\nVersion: 22.05\nNginx: Proxy\nOS: Rocky 8\nThanks\u2026"
    },
    {
      "role": "system",
      "text": "Hi @user\nMaybe these tutorials will help?\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\n\nGalaxy Training: Galaxy Server administration (@link https://training.galaxyproject.org/training-material/topics/admin/)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to use the SnpSift dbNSFP tool to annotate some vcf files.\nThe description for the tool highlights that pre build versions of the database are available on the SnpSift website, which I did find on the page, @link https://pcingola.github.io/SnpEff/ss_dbnsfp/.\nI have attempted to import the GRCh38 database to galaxy but the download connection keeps being cut off.\nHas anyone successfully imported this database to galaxy? Is it available somewhere else?"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry getting the data from here instead: Index of /WGSAdownload/resources/dbNSFP/ (@link http://web.corral.tacc.utexas.edu/WGSAdownload/resources/dbNSFP/)\nRead this file: @link http://web.corral.tacc.utexas.edu/WGSAdownload/resources/dbNSFP/dbNSFP4.1c.readme.txt \u2013 it explains the contents of each file in the directory so you can choose the one you need.\nCapture the link for the data you want to use and paste it into the Upload tool. The tool form help explains how to label the data (\u201cdatatype\u201d) for the Galaxy wrapped version to recognize the data as an appropriate input.\nNote: This data is for hg38 only. There are other ways to get/filter the full data at the website you referenced, but all that prep would be done on the command line first, then the proper filtered result/format loaded up to Galaxy. The exact instructions are on that website - although it doesn\u2019t seem like you\u2019ll need to do that and can instead use the pre-built data linked above, which is far simpler, smaller, and is designed to work within Galaxy.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have a problem with the Cloud Authorization option on Galaxy.\nI followed these steps:\n\nI log in to my account at @link http://usegalaxy.org\n\nI click User -> Preferences -> Manage Cloud Authorization ->Create New Authorization Key\nI pick Azure and enter my details, but the Save Key button never gets activated.\n\nuntitled973\u00d7727 63.9 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/bc013900174e89c3f9287e2cc052ab24477424b9.png)\nI also tried installing Galaxy locally and followed the same steps, but the button never got activated.\nI was wondering whether anyone has had a similar problem and could give me some advice?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nThank you for testing this feature. As @system has mentioned, this feature is in beta status, and it seems it is broken through-out the development.\nWe will fix the UI, meanwhile, you can create authnz using Galaxy\u2019s API. For this, you may run the following (replacing ... with your values) in your terminal:\ncurl --header \"Content-Type: application/json\" \\\n--request POST \\\n--data '{\"provider\": \"azure\", \"config\": {\"tenant_id\": \"...\",\"client_id\": \"...\",\"client_secret\": \"...\"}}' \\\nhttps://usegalaxy.org/api/cloud/authz?key=...\n\nThis will return the configuration created in json format. For instance:\n{\n    \"model_class\": \"CloudAuthz\",\n    \"config\": {\n        \"client_id\": \"...\",\n        \"client_secret\": \"...\",\n        \"tenant_id\": \"...\"\n    },\n    \"user_id\": \"...\",\n    \"last_activity\": \"2020-12-15 15:37:56.123303\",\n    \"create_time\": \"2020-12-15 21:37:56.125248\",\n    \"description\": \"\",\n    \"last_update\": \"2020-12-15 15:37:56.123284\",\n    \"id\": \"...\",\n    \"provider\": \"azure\",\n    \"authn_id\": null\n}\n\nThen you should be able to use id to use the cloud storage api (@link ); send data to Azure or get your data from Azure to Galaxy.\nHowever, at the moment, there is a bug using this feature on @link http://usegalaxy.org as this feature requires OIDC which is not yet enabled on @link http://usegalaxy.org.\nI created an issue for the UI (@link https://github.com/galaxyproject/galaxy/issues/10926) and another one for azure on usegalaxy.org (@link https://github.com/galaxyproject/galaxy/issues/10927)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I used STAR to analyze the paired-end reads and then STAR-fusion with the chimeric junctions file from STAR to detect fusion transcripts. The K562 SRA file from NCBI is used for the test run. The reference genome, GTF file and BLAST+ result file are all obtained from CTAT-RESOURCE-LIB. The galaxy run page is set up as shown in snapshot below. Although the chimeric junctions file from STAR shows 1.4M hits, no fusion transcript was detected by the STAR + STAR fusion. The output showed zero line.\nI noticed that the version of STAR fusion 0.54 is really out of date and is probably incompatible with the STAR version 2.78a in galaxy. I actually tried different versions of STAR, but got the same results with zero lines in the outputs.\nI believe this is the compatibility issue with STAR fusion. Is it possible for galaxy developers to update the STAR fusion version in galaxy?\n\nstarfusion1691\u00d71273 176 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/10cd836361c30741b8eb952a56750f8ec7c79495.jpeg)"
    },
    {
      "role": "system",
      "text": "See star fusion validation failure (@link https://help.galaxyproject.org/t/star-fusion-validation-failure/375)"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nGalaxy.jpg1250\u00d7850 412 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/456436b7068b3f8499a79dc35aa3aeb938f0ef34.jpeg)\nHi, Greeting! I am a new user of Galaxy.\nSince last week, I am not able to format my data to select raw for class or subclass for Lefse. With the same data, it worked perfectly two weeks ago. Anyone could get some hints to this problem? Thanks!"
    },
    {
      "role": "system",
      "text": "Your data appears to be in csv format (comma separated) and the tool is expecting tabular format (tab separated) for the input.\nRead the tool form help to understand more about the expected input content/format.\nYou might need to reformat the data before uploading to Galaxy or you might be able to reformat within Galaxy (with a tool such as Convert delimiters to TAB). Note that is is just a general guideline \u2013 double check your content versus the help to confirm that any transformed data actually meets the input requirements stated to use the tool successfully.\nI\u2019m not sure why this worked two weeks ago and not now. But if you want to use the tool, at this public Galaxy server (@link http://huttenhower.sph.harvard.edu/galaxy), this is the tool version available."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am trying to analyze a ChIPseq experiment for the first time. I have a collection of ten fastq files, and I queued a series of jobs on this collection: (1) FastQC, (2) Trimmomatic, (3) FastQC on the Trimmomatic output, and (4) BWA on the Trimmomatic output. The first FastQC job stalled with only 8/10 files completed. The Trimmomatic and subsequent FastQC jobs completed. The BWA appears stalled on the first 2/10 files. These have been stalled since Friday. Wondering if this is a typical run time and I should keep waiting, or if this is a sign of a problem? Very new to Galaxy. Thank you!\n\nScreen Shot 2022-09-26 at 4.05.09 PM2012\u00d71144 328 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2bc7b3f9ea81babb3e258283557e82cccf599800.png)\n\nScreen Shot 2022-09-26 at 4.23.25 PM594\u00d71104 58.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4f4897408176481feaa905912b274b751e97349f.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe @link http://UseGalaxy.org server is very very busy. Updates are posting in this topic.\n@quote\nUpdate 09/26/2022 \nTheUseGalaxy.orgserver is still very busy. \nLeave jobs queued for the fastest processing. \n\nUpdate 09/23/2022 \nAtusegalaxy.orgwe were able to confirm the delays and fixed a small technical issue. \nPlease leave queued jobs queued. Avoid deleting and rerunning as that will lose the job\u2019s original place in the queue. \nThanks to everyone who reported the problem! \n\nHi@systemThe public servers might just be busy.https://status.galaxyproject.org/Are you working\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.\nI m running align.seqs output file from Unique.Seqs against SILVA reference file. How long this process will take. ? Its been more than 8hr. thanks."
    },
    {
      "role": "system",
      "text": "There were job delays due to server issues at Galaxy Main @link https://usegalaxy.org and Galaxy EU @link https://usegalaxy.org\nRecent Q&A with details: Job delays at Galaxy Main https://usegalaxy.org and Galaxy EU https://usegalaxy.eu (@link https://help.galaxyproject.org/t/job-delays-at-galaxy-main-https-usegalaxy-org-and-galaxy-eu-https-usegalaxy-eu/1540)\nGalaxy Main is processing jobs again now. Wait for your job(s) to finish, instead of rerunning, to preserve your placement in the current job queue."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nJust starting as a Galaxy user I got an issue conserning annotation. I have a number of featureCounts output files for DE analysis. A number of significantly differentially expressed genes with a Fc of >2 were obtained. However, when I want to annotate them using \u2018Annotate DeSeq2/DexSeq output tables\u2019 my columns with the annotation info returns only NAs. I got the impression that the cause of this is that featureCount files come with the Entrez-id while this information is not present in my .gtf file that I got from the EnSembl repository (GRCh38.102.gtf.gz; removed the header lines containing an #). Any suggestions how to change the gene Entrez-id in my featureCounts output or how to get ENS numbers in my .gtf file? Thanks for your help,\nHenk"
    },
    {
      "role": "system",
      "text": "Hi @system,\nyou can use the annotateMyIDs (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/annotatemyids/annotatemyids/3.7.0+galaxy1) tool in order to perform such format conversion by using the featureCounts file as input. Let me know if it works.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to install Galaxy on my MAC and am getting the following errors on install:\nBelow is the log.\nMy Operating system is MAC OS Catalina 10.15.6 (19G73)\nThanks !\n================================================================\nSuccessfully built attmap bagit beaker cachecontrol dictobj dogpile.cache future prettytable pyperclip pyrsistent pysam pysftp pyyaml rdflib-jsonld refgenconf sqlalchemy-utils tempita ubiquerg warlock wrapt yacman scandir\nInstalling collected packages: six, python-dateutil, chardet, certifi, idna, urllib3, requests, pyjwt, pycparser, cffi, cryptography, adal, vine, amqp, appdirs, ubiquerg, attmap, attrs, azure-nspkg, azure-common, azure-cosmosdb-nspkg, futures, azure-storage-common, azure-cosmosdb-table, oauthlib, requests-oauthlib, isodate, msrest, msrestazure, azure-mgmt-nspkg, azure-mgmt-compute, azure-mgmt-devtestlabs, azure-mgmt-network, azure-mgmt-resource, azure-mgmt-storage, azure-storage-nspkg, azure-storage-blob, pytz, babel, bagit, bcrypt, tzlocal, setuptools-scm, bdbag, beaker, pyyaml, requests-toolbelt, boto, bioblend, webencodings, pyparsing, packaging, bleach, boltons, docutils, jmespath, botocore, s3transfer, boto3, numpy, bx-python, cachecontrol, cachetools, cheetah3, pbr, stevedore, pyperclip, wcwidth, cmd2, prettytable, cliff, cloudauthz, netaddr, rfc3986, oslo.i18n, wrapt, debtcollector, oslo.config, msgpack, iso8601, netifaces, oslo.utils, oslo.serialization, os-service-types, keystoneauth1, python-keystoneclient, pyasn1, rsa, pyasn1-modules, google-auth, uritemplate, httplib2, google-auth-httplib2, google-api-python-client, simplejson, deprecation, munch, decorator, requestsexceptions, dogpile.cache, jsonpointer, jsonpatch, openstacksdk, os-client-config, osc-lib, oslo.context, oslo.log, python-neutronclient, oauth2client, python-swiftclient, deprecated, pyeventsystem, tenacity, python-novaclient, pynacl, paramiko, pysftp, zipp, importlib-metadata, pyrsistent, jsonschema, warlock, pyopenssl, python-glanceclient, python-cinderclient, cloudbridge, humanfriendly, coloredlogs, psutil, shellescape, future, typing-extensions, lockfile, mistune, rdflib, ruamel.yaml.clib, ruamel.yaml, rdflib-jsonld, schema-salad, lxml, networkx, prov, pathlib2, mypy-extensions, scandir, cwltool, dictobj, docopt, ecdsa, fabric3, galaxy-sequence-utils, gxformat2, h5py, isa-rwval, kombu, markupsafe, mako, markdown, mercurial, monotonic, nodeenv, nose, oyaml, parsley, paste, pastedeploy, pastescript, webob, pulsar-galaxy-lib, pycryptodome, pykwalify, pysam, python-jose, pyuwsgi, tqdm, yacman, refgenconf, repoze.lru, routes, defusedxml, python3-openid, unidecode, social-auth-core, sortedcontainers, sqlalchemy, tempita, sqlparse, sqlalchemy-migrate, sqlalchemy-utils, svgwrite, whoosh\nSuccessfully installed adal-1.2.3 amqp-2.5.2 appdirs-1.4.3 attmap-0.12.11 attrs-19.3.0 azure-common-1.1.14 azure-cosmosdb-nspkg-2.0.2 azure-cosmosdb-table-1.0.4 azure-mgmt-compute-4.0.1 azure-mgmt-devtestlabs-2.2.0 azure-mgmt-network-2.1.0 azure-mgmt-nspkg-3.0.2 azure-mgmt-resource-2.0.0 azure-mgmt-storage-2.0.0 azure-nspkg-3.0.2 azure-storage-blob-1.3.1 azure-storage-common-1.4.2 azure-storage-nspkg-3.1.0 babel-2.8.0 bagit-1.7.0 bcrypt-3.1.7 bdbag-1.5.6 beaker-1.11.0 bioblend-0.13.0 bleach-3.1.5 boltons-20.1.0 boto-2.49.0 boto3-1.9.114 botocore-1.12.253 bx-python-0.8.8 cachecontrol-0.11.7 cachetools-3.1.1 certifi-2020.4.5.1 cffi-1.14.0 chardet-3.0.4 cheetah3-3.2.4 cliff-2.18.0 cloudauthz-0.6.0 cloudbridge-2.0.0 cmd2-0.8.9 coloredlogs-14.0 cryptography-2.9.2 cwltool-1.0.20191225192155 debtcollector-1.22.0 decorator-4.4.2 defusedxml-0.7.0rc1 deprecated-1.2.9 deprecation-2.1.0 dictobj-0.4 docopt-0.6.2 docutils-0.15.2 dogpile.cache-0.9.1 ecdsa-0.15 fabric3-1.14.post1 future-0.18.2 futures-3.1.1 galaxy-sequence-utils-1.1.5 google-api-python-client-1.7.8 google-auth-1.14.1 google-auth-httplib2-0.0.3 gxformat2-0.11.3 h5py-2.10.0 httplib2-0.17.3 humanfriendly-8.2 idna-2.9 importlib-metadata-1.6.0 isa-rwval-0.10.7 iso8601-0.1.12 isodate-0.6.0 jmespath-0.9.5 jsonpatch-1.25 jsonpointer-2.0 jsonschema-3.2.0 keystoneauth1-4.0.0 kombu-4.6.8 lockfile-0.12.2 lxml-4.5.0 mako-1.1.2 markdown-3.1.1 markupsafe-1.1.1 mercurial-5.4 mistune-0.8.4 monotonic-1.5 msgpack-1.0.0 msrest-0.5.5 msrestazure-0.5.0 munch-2.5.0 mypy-extensions-0.4.3 netaddr-0.7.19 netifaces-0.10.9 networkx-1.11 nodeenv-1.3.5 nose-1.3.7 numpy-1.16.6 oauth2client-4.1.3 oauthlib-3.1.0 openstacksdk-0.17.0 os-client-config-2.1.0 os-service-types-1.7.0 osc-lib-2.0.0 oslo.config-7.0.0 oslo.context-2.23.0 oslo.i18n-3.25.1 oslo.log-3.45.2 oslo.serialization-2.29.2 oslo.utils-3.42.1 oyaml-0.9 packaging-20.3 paramiko-2.7.1 parsley-1.3 paste-3.4.0 pastedeploy-2.1.0 pastescript-3.2.0 pathlib2-2.3.5 pbr-5.4.5 prettytable-0.7.2 prov-1.5.1 psutil-5.7.0 pulsar-galaxy-lib-0.14.0.dev3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pycryptodome-3.9.7 pyeventsystem-0.1.0 pyjwt-1.7.1 pykwalify-1.7.0 pynacl-1.3.0 pyopenssl-19.1.0 pyparsing-2.4.7 pyperclip-1.8.0 pyrsistent-0.16.0 pysam-0.16.0.1 pysftp-0.2.9 python-cinderclient-4.0.0 python-dateutil-2.8.1 python-glanceclient-2.12.0 python-jose-3.1.0 python-keystoneclient-3.17.0 python-neutronclient-6.9.0 python-novaclient-11.0.0 python-swiftclient-3.6.0 python3-openid-3.2.0 pytz-2020.1 pyuwsgi-2.0.18.post0 pyyaml-5.3.1 rdflib-4.2.2 rdflib-jsonld-0.4.0 refgenconf-0.7.0 repoze.lru-0.7 requests-2.23.0 requests-oauthlib-1.3.0 requests-toolbelt-0.9.1 requestsexceptions-1.4.0 rfc3986-1.4.0 routes-2.4.1 rsa-4.0 ruamel.yaml-0.16.0 ruamel.yaml.clib-0.2.0 s3transfer-0.2.1 scandir-1.10.0 schema-salad-4.5.20191229160203 setuptools-scm-3.5.0 shellescape-3.4.1 simplejson-3.17.0 six-1.11.0 social-auth-core-3.3.0 sortedcontainers-2.1.0 sqlalchemy-1.3.16 sqlalchemy-migrate-0.13.0 sqlalchemy-utils-0.36.5 sqlparse-0.3.1 stevedore-1.32.0 svgwrite-1.3.1 tempita-0.5.2 tenacity-4.12.0 tqdm-4.46.0 typing-extensions-3.7.4.2 tzlocal-2.0.0 ubiquerg-0.5.1 unidecode-1.1.1 uritemplate-3.0.1 urllib3-1.25.9 vine-1.3.0 warlock-1.3.3 wcwidth-0.1.9 webencodings-0.5.1 webob-1.8.6 whoosh-2.7.4 wrapt-1.12.1 yacman-0.6.7 zipp-1.2.0\nThe Galaxy client has not yet been built and will be built now.\nInstalling node into /Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv with nodeenv.\n\nInstall prebuilt node (10.15.3) .\nTraceback (most recent call last):\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1317, in do_open\nencode_chunked=req.has_header(\u2018Transfer-encoding\u2019))\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1244, in request\nself._send_request(method, url, body, headers, encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1290, in _send_request\nself.endheaders(body, encode_chunked=encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1239, in endheaders\nself._send_output(message_body, encode_chunked=encode_chunked)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1026, in _send_output\nself.send(msg)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 966, in send\nself.connect()\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\u201d, line 1414, in connect\nserver_hostname=server_hostname)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 423, in wrap_socket\nsession=session\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 870, in _create\nself.do_handshake()\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\u201d, line 1139, in do_handshake\nself._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)\n\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/bin/nodeenv\u201d, line 8, in \nsys.exit(main())\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 1113, in main\ncreate_environment(env_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 936, in create_environment\ninstall_node(env_dir, src_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 700, in install_node\ninstall_node_wrapped(env_dir, src_dir, opt)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 723, in install_node_wrapped\ndownload_node_src(node_url, src_dir, opt, prefix)\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 552, in download_node_src\ndl_contents = io.BytesIO(urlopen(node_url).read())\nFile \u201c/Users/georgeweingart/Documents/GW_Galaxy/galaxy/.venv/lib/python3.7/site-packages/nodeenv.py\u201d, line 580, in urlopen\nreturn urllib2.urlopen(req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 222, in urlopen\nreturn opener.open(url, data, timeout)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 525, in open\nresponse = self._open(req, data)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 543, in _open\n\u2018_open\u2019, req)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 503, in _call_chain\nresult = func(*args)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1360, in https_open\ncontext=self._context, check_hostname=self._check_hostname)\nFile \u201c/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\u201d, line 1319, in do_open\nraise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)>"
    },
    {
      "role": "system",
      "text": "@quote\ncertificate verify failed: unable to get local issuer certificate\n\n\ndev2qa.com \u2013 24 Jun 19 (@link https://www.dev2qa.com/how-to-fix-python-error-certificate-verify-failed-unable-to-get-local-issuer-certificate-in-mac-os/)\n\n\n\nHow To Fix Python Error Certificate Verify Failed: Unable To Get Local Issuer... (@link https://www.dev2qa.com/how-to-fix-python-error-certificate-verify-failed-unable-to-get-local-issuer-certificate-in-mac-os/)\nWhen i run python code in mac os, i meet a certificate verify failed error like this ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). [\u2026]\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI am trying to use maaslin from yestarday but i get always the following message\n\u201cTool request failes Maaslins: Uncaught exception in exposed API method\u201d\nIs there any internal error ? I get the same message when I try to use Lefse.\nThanks for the help\nAndy"
    },
    {
      "role": "system",
      "text": "This tool is only installed on the huttenhower Galaxy server that runs a very old version of Galaxy. We cannot help you there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI\u2019m trying to develop a tool but I failed to properly filter output.\nThe aim is to repeat different type of analysis depending of the parameters. So I created a repeat parameters, with a conditional and with a select parameter. The analysis is okay but I cannot filter output data and I received empty files for non analyzed part of the code\u2026\nI tried to write the filter in different manner but it don\u2019t work into a repeat parameter\u2026\nthanks !\nAn example of my code could be :\n<inputs>\n<repeat name=\"repeat\" title=\"repeat1\">\n    <conditional name=\"conditional\">\n        <param name=\"select1\" type=\"select\">\n            <option value=\"type1\" >the type 1</option>\n            <option value=\"type2\">the type 2</option>\n        </param>\n        <when value=\"type1\">\n            <param name=\"data1\" type=\"data\" label=\"data1\">\n        </when>\n        <when value=\"type2\">\n            <param name=\"data2\" type=\"data\" label=\"data2\">\n        </when>\n    </conditional>\n</repeat>.\n</inputs>\n<outputs>\n<data name=\"output1\" format=\"pdf\" label=\"output1 from data1\">\n    <filter>repeat['conditional']['select1'] == 'type1' </filter>\n<data name=\"output2\" format=\"csv\" label=\"output2 from data2\">\n    <filter>repeat['conditional']['select1'] == 'type2' </filter>\n<outputs>\n"
    },
    {
      "role": "user",
      "text": "Hi @system thanks a lot,\nYour solution with repeat[0] is great, but finally i used the collection type to extract the output.\nThank you !"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI am trying to use Ribogalaxy and running into a lot of errors. I am aware that RiboGalaxy has just been moved over from a different domain and so bugs are to be expected, but my data analysis is time sensitive and I am not well versed in bioinformatics.\nCould I get in touch with someone developing RiboGalaxy to help me with the bugs I am encountering?\nThank you!!"
    },
    {
      "role": "system",
      "text": "Hi @user\nUpdate:\nThe servers home page @link https://ribogalaxy.genomicsdatascience.ie/ has contact information (an email address). They ask that bugs be reported there for now. The admins running the server would be best people to review for actual bugs versus usage issues they recognize.\n\nFind their contact information in the public Galaxy directory here: RiboGalaxy - Galaxy Community Hub (@link https://galaxyproject.org/use/ribogalaxy/)\nThey have a dedicated help and forum site. The news section has information about the updates @link https://gwips.ucc.ie/ but the forum appears to be still under maintenance. You\u2019ll need to either wait for them to get back up (along with the custom tools/protocols), or explore other public servers."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have been using HISAT2 for RNAseq reads alignment for quite a while.\nLast few days, the tool keeps failing with different error messages, even on inputs that I have used successfully before.\nHere a couple of examples of the error messages that I get:\n\nThis job failed for reasons that could not be determined\n\n\n(ERR): hisat2-align died with signal 9 (KILL)\n[W::sam_read1_sam] Parse error at line 34126880\nsamtools sort: truncated file. Aborting\n[main_samview] fail to read the header from \u201c-\u201d\n\n\nThis job was terminated because it used more memory than it was allocated\n\nCan anyone help?\nThanks\nLuca"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe clusters that run this tool have been very busy but these errors look like they are resulting from a job that is too large to execute. I understand that these reads mapped before, but if any of these other items changed, it could impact how \u201clarge\u201d a job is:\n\nparameters\nreference genome\nreference annotation\ntool version (always use the most current)\n\nThings to troubleshoot:\n\nTry another rerun to eliminate server side transient issues\nInspect the reads \u2013 do these still pass QA tools?\nInspect the target genome if using a custom genome input \u2013 is the fasta format correct?\nThe format/content of any included reference annotation should also be checked, but usually doesn\u2019t lead to an error: the reference annotation would in practical use be ignored instead. So, scientific not technical problems.\n\nI added some tags that link to prior Q&A about those items. In short, this root problem could be input issues OR the job is actually too large to process. The middle message suggests that the target reference genome (fasta) contains a lot of sequences \u2013 remember that this tool is expecting a somewhat intact assembly: up to ~ 1000 target sequences but no more.\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-exceeds-memory-allocation-error-messages\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-walltime-error-messages\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there a way to calculate the time consumed/needed when running a tool?"
    },
    {
      "role": "system",
      "text": "Hi @user, for completed jobs users can check information in info panel (click on dataset name, click on  info icon). Some tools put run time into log files. I am not aware about any option that allow estimation of run time for an active job through Galaxy interface, at least not for a user, assuming standard Galaxy setup.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "What Galaxy instance has a robust reliable assembly gap filling tools, please?"
    },
    {
      "role": "system",
      "text": "Have you checked SSPACE?. Also, you can search any software in the Galaxy Tool Shed (@link https://toolshed.g2.bx.psu.edu)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,  There is a repository named qiime2_wrappers (URL below).  I was unable to find this repository anywhere in the public galaxy version.  Is this only available in the locally installed galaxy version?  If not, how do I \u201clink\u201d to this repository.\n@link https://toolshed.g2.bx.psu.edu/repository?repository_id=a43ea4908816827c&changeset_revision=51b9b6b57732"
    },
    {
      "role": "system",
      "text": "Hello,\nIs this only available in the locally installed galaxy version?\n\nEach instance\u2019s tool suite is installed by administrators, so not all tools in the shed will be present in all Galaxy instances \u2013 it\u2019s at the discretion of the admin. This allows the tool shed to be permissive in accepting tools, so you can run anything you\u2019d like to download/develop on your own instance, without potentially compromising the security of every other instance (should someone upload a malicious tool).\n@link http://usegalaxy.org does not have the tool collection you mentioned, though @link http://usegalaxy.eu does appear to have some similar tools that draw from this collection:\n@link https://github.com/galaxyproject/tools-iuc/tree/master/tools/qiime/\nif those tools are insufficient, your best bet is to run that particular step in your process on your own Galaxy if possible \u2013 or befriend a Galaxy admin :wink:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI am using a LEfSe analysis for my sequencing data. I use this format pasted to .tsv file in the notepad:\n\nimage1752\u00d7263 10.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1c2a6b320f0f47c20ed0661fe9662c31f94e834e.png)\nNevertheless, I do not receive any results. I got that there does not have to be any significant difference among taxons, but I have tried a lot of variable values to obtain any result and to confirm that I work correctly and the input file is ok. So I have tried every combination of \u201cSet the strategy for multi-class analysis\u201d and \u201cDo you want the pairwise comparisons among subclasses to be\u2026\u201d at the step B (LDA Effect Size) and after that didn\u2019t do anything, I setup both Alpha values to 1. In my opinion, that means I should see at least some differences. But I do not.\nSo I would like to ask you if I am performing this analysis correctly and if so, what can be the troubleshoot?\nThank you a lot!\nMartin"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf you are working with this tool at a public Galaxy site, is it this one? If so, you\u2019ll need to contact them directly for usage issues. Click into this post \u2013 it explains why and how to reach them.\n@quote\nHi@systemThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server. \nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers). \nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (belo\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have assembled contigs (more than 64000 contigs) with a long header. How can I make the header shorter using one of Galaxy\u2019s tools? please\nRegards\nAdnan"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for posting publicly.\nFAQ Fasta format (@link https://galaxyproject.org/learn/datatypes/#fasta)\nThere are many ways to shorten or modify fasta title lines (what is on the > lines) \u2026 maybe try a few different ways to get exactly what you want.\nTips: Be sure that what remains for the >identifier content (not including anything after the first whitespace) will be unique within the file, or expect problems. Be aware that some tools will fail when any description content (whatever is after the first whitespace, if anything) is present, so that is enough for some people.\nNormalizeFasta\nwith the option to trim the header at the first whitespace\n\nGood for preserving original >identifier content and removing excess trailing description content.\n\nText transformation with sed\n\nThe tool form has examples, or see the link outs for more.\nAnchoring to title lines with ^> would probably be involved (line starts with a > character)\n\nMore manipulations, including several Replace function variations are here. Some may require tabular format, so: fasta > tabular > do stuff > fasta. Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Admins,\nI have been running a certain workflow with my galaxy eu account for the last 3 days without problems. Today I tried to run it again but I get the following message: \u201cInvocation scheduling failed - Galaxy administrator may have additional details in logs.\u201d\nI have no more additional information, sorry. The only thing I can share is that the workflow is not automatically recognising my input (paired collection) as it was doing the previous days.\nAny help or explanation would be much appreciated."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe best advice would be to try again now, especially since this data and workflow was working previously.\nAnd \u2026 if it seems that the data is still not being recognized, maybe create or copy that starting data into a new history, and try from there? That resets all of the history + dataset metadata."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi I am new to galaxy, so if someone has asked this question in the past I apologize. I downloaded 42 files to galaxy via the SRA run selector as a collection. I now have my FASTQ files and would like to use Trimmomatic; however, I am above the alloted space quota. Trimmomatic was run on the first 15 files already and the rest are waiting to be processed.\nMy question is if I delete/purge  the first 15 files in the collection ( FASTQ) will that allow Trimmoatic to resume or will it mess things up because the data is grouped together as a collection. I just not sure and it took forever to download so any advice would be much appreciated."
    },
    {
      "role": "system",
      "text": "Hi @user\nOne solution is to break out the read data into distinct collections during download and processing in batches.\n\n\nTrimmomatic creates near duplicate very large fastq outputs.\nThe idea would be to get through that step with a portion of the data, purge the original files, repeat with another portion, then combine the results into one collection for downstream processing.\nFrom your description it seems like 2-4 batches would be enough.\n\nAnother is to make a small short-term extra quota request (extra 100-250 GB for a few days to a week).\n\nThis is only available for academic researchers at the public sites. How-to for accounts at @link http://UseGalaxy.org. Account quotas - Galaxy Community Hub (@link https://galaxyproject.org/support/account-quotas/). Make the request via email as described in that FAQ \u2013 don\u2019t post your information here publicly.\n\nFor non academics, additional resources require the use of a private Galaxy server with resources scaled to fit the work. Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\n\n\nIf that is not enough help, would you please post back a share link to this history? That context will help with review and specific solutions. :slight_smile:\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am new to Galaxy.\nAre the BAM files generated after mapping using RNA STAR (version 2.7.8a+galaxy1 or newer) sorted (either by coordinate or name or tag)? After clicking on the \u2018i\u2019 logo for the \u2018*.mapped.bam\u2019 dataset details, the dataset information section says the format is BAM. The \u2018Tool Standard Output\u2019 is as follows:\nApr 28 20:52:35 \u2026 started STAR run\nApr 28 20:52:35 \u2026 starting to generate Genome files\nApr 28 20:53:23 \u2026 processing annotations GTF\nApr 28 20:53:54 \u2026 starting to sort Suffix Array. This may take a long time\u2026\nApr 28 20:54:14 \u2026 sorting Suffix Array chunks and saving them to disk\u2026\nApr 28 21:13:08 \u2026 loading chunks from disk, packing SA\u2026\nApr 28 21:14:38 \u2026 finished generating suffix array\nApr 28 21:14:38 \u2026 generating Suffix Array index\nApr 28 21:19:00 \u2026 completed Suffix Array index\nApr 28 21:19:01 \u2026 inserting junctions into the genome indices\nApr 28 21:21:04 \u2026 writing Genome to disk \u2026\nApr 28 21:21:07 \u2026 writing Suffix Array to disk \u2026\nApr 28 21:21:30 \u2026 writing SAindex to disk\nApr 28 21:21:33 \u2026 finished successfully\nApr 28 21:21:33 \u2026 started STAR run\nApr 28 21:21:33 \u2026 loading genome\nApr 28 21:21:55 \u2026 started mapping\nApr 28 21:30:30 \u2026 finished mapping\nApr 28 21:30:54 \u2026 started sorting BAM\nApr 28 21:31:45 \u2026 finished successfully\nHowever, though this output is essentially the same for my \u2018*.transcriptome-mapped.bam\u2019 file, its dataset information says the format is unsorted.bam. See attached screenshots.\n\nmappedbam1378\u00d7470 50.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c77f533871688690619a24067dab41fb2b855481.png)\n\ntransmappedbam1356\u00d7475 50.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a10c2fc012342d3c65d6e747f738adbd9204180e.png)\nThe 1st line of \u2018.mapped.bam\u2019 file is \u2018@HD VN:1.4 SO:coordinate\u2019. So am I correct in assuming that perhaps it might be sorted by coordinate? The 1st line of '.transcriptome-mapped.bam\u2019 is not the same as the \u2018*.mapped.bam\u2019 but rather is \u2018@SQ SN:NR_046018.2 LN:1652\u2019.\nDo I still need to perform the Samtools sort operation for either one or both these files? I intend to use them as inputs for featureCounts.\nThanks a lot for the help in advance. :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nDo I still need to perform the Samtools sort operation for either one or both these files? I intend to use them as inputs for featureCounts.\nA datatype metadata describes the sort formats of bam data.\n\nThe first dataset with the bam datatype assigned is coordinate sorted.\nThe second dataset with the unsorted.bam datatype assigned is not.\n\nThe first is what most people would be using with Featurecounts. Instead, for transcript level expression analysis, Salmon or Kallisto are probably \u201cbetter\u201d choices scientifically. But, you can of course try out different data/parameters with any and see what happens :slight_smile: to explore. Maybe you can figure out how to get the data you need in novel ways.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI wanted to ask if there is any way to access academic or commercial servers that allow for using more space on Galaxy.  I am planning to analyze bigger data sets that will need far more than the 250GB permitted on Galaxy. I did see some suggestions on Galaxy but could not connect to AWS or Anvil. Does anyone have experience using these servers or any suggestions for other servers that allow for additional space that can be booked?\nThanks,\nSwati"
    },
    {
      "role": "system",
      "text": "Hi @user\nSeveral choices are in the matrix here\n\n\n\n@link https://galaxyproject.org/use/#which-platform--platform-type-to-choose\n\n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy... (@link https://galaxyproject.org/use/#which-platform--platform-type-to-choose)\nAll about Galaxy and its community.\n\n\n\n\n\n@quote\nI did see some suggestions on Galaxy but could not connect to Anvil\nThe contact is here AnVIL - Galaxy Community Hub (@link https://galaxyproject.org/use/anvil/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nMy VelvetOptimiser is offering different results on galaxy vs. my command line.\nHere are the inputs i used on galaxy:\n\n11668\u00d7634 42.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/878dd4c360ca599592926c555c5b0132fbe3bd1d.png)\n\n21652\u00d71144 78.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0cdbe97424bd53c286911a785380593ae507c6bd.png)\n\n31686\u00d71380 183 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/391776dbbabf7941e3abc61bfad2142237538cba.png)\nand this is from my command line\nVelvetOptimiser.pl -s 55 -e 69 -x 2 -f '-shortPaired -fastq -separate trial2.trimmed.paired.R1.fastq trial2.trimmed.paired.R2.fastq' -f '-short -fastq trial2.trimmed.unpaired.R1.fastq' -f '-short -fastq trial2.trimmed.unpaired.R2.fastq'  \n\n(trial2.trimmed.paired.r1\u2026unpaired.R2.fastq are the same post-trimming files as the ones on galaxy)\nFor velvet, I cloned the github repo using\ngit clone https://github.com/dzerbino/velvet.git\n\nand compiled it to make the MAXKMERLENGTH = 121 with the command\nmake 'MAXMAXKMERLENGTH = 121'\n\nI\u2019ve troubleshooted quite a bit, beginning with the swapping of the format and type as per the github.\nVelvetOptimiser.pl -s 55 -e 69 -x 2 -f '-fastq -shortPaired -separate trial2.trimmed.paired.R1.fastq trial2.trimmed.paired.R2.fastq' -f '-fastq -short trial2.trimmed.unpaired.R1.fastq' -f ' -fastq -short trial2.trimmed.unpaired.R2.fastq'  \n\nI also made sure to add the velvet folder and folder containing velvet optimiser to my path.\nAny ideas what I\u2019m doing wrong? I attached pictures of my error file if that helps; any feedback would be greatly appreciated.\n\n1960\u00d71796 261 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7867216b677ba59a39328a683fb4f9c2eb3e2747.jpeg)\n\n21288\u00d71270 137 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2b2ffd5e27b404ebf54f90c62f29b494b0b39331.jpeg)"
    },
    {
      "role": "system",
      "text": "@quote\nVelvetOptimiser\nMy guess is that the underlying tool versions differ. Or, maybe there is some bug.\nThe version of this tool and the resource allocated are intended for very small tutorial-sized datasets when working at public Galaxy servers. Some servers even have the tool marked as deprecated. The last wrapper update was in 2021, details: Galaxy | Tool Shed (@link https://toolshed.g2.bx.psu.edu/repository?repository_id=ad5ca8db1b4051ed). If you want to help us to fix the wrapper, the development repository is linked from there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Greetings.\nOur Environment:\nGalaxy:  19_09 ;\nOS:  Ubuntu 18.04.2 LTS\nDatabase:  Postgres    <<<\u2014 one of the few deviations from default install\nGalaxy is nearly a default install and not configured to run behind nginx.\nnginx does not appear to be running nor can I find an nginx.conf file anywhere on the machine.\nThe PROBLEM\nI am a newbie admin and my users are running into an issue when downloading larger files.  They can download index files which are about 96bytes.  However, the bam files (~15MB) fail.  This is Galaxy 19_09 and it is not configured to run behind nginx;  a nearly default installation.  I\u2019ve read several posts about \u201cproxy_max_temp_file_size\u201d setting in nginx but nginx is not setup or running on the server.\nI found these posts but they are all related to nginx\n\nLocal galaxy nginx max file download size 1GB (@link https://help.galaxyproject.org/t/local-galaxy-nginx-max-file-download-size-1gb/1281)\n@link http://galacticengineer.blogspot.com/2019/05/fixing-dataset-download-problems-for.html\n@link http://lists.unbit.it/pipermail/uwsgi/2018-December/008955.html\n\nI\u2019m not sure where to go from here.  Hoping someone has experience with this issue.\nOne of the errors from Galaxy.log:\n[pid: 80680|app: 0|req: 2126/2264] 10.10.255.36 () {40 vars in 832 bytes} [Wed Apr 15 12:01:28 2020] GET /api/histories/2fdbd5c5858e78fb/contents/datasets/2ea0c1fdff88d4e3 => generated 7749 bytes in 31 msecs (HTTP/1.1 200) 3 headers in 139 bytes (1 switches on core 0)\n10.10.255.36 - - [15/Apr/2020:12:01:32 -0400] \"GET /datasets/2ea0c1fdff88d4e3/display?to_ext=bam HTTP/1.1\" 200 148073827 \"http://10.10.200.140:8080/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0\"\nuwsgi_response_write_body_do() TIMEOUT !!!\nIOError: write error\n[pid: 80680|app: 0|req: 2127/2265] 10.10.255.36 () {40 vars in 813 bytes} [Wed Apr 15 12:01:32 2020] GET /datasets/2ea0c1fdff88d4e3/display?to_ext=bam => generated 6160384 bytes in 6199 msecs (HTTP/1.1 200) 5 headers in 263 bytes (97 switches on core 1)"
    },
    {
      "role": "user",
      "text": "Hello,  Wanted to post an update.  I just heard from the users that after I bounced that Galaxy server the issue has magically gone away.\nFor now assume this issue is resolved.  Thank you."
    }
  ],
  [
    {
      "role": "user",
      "text": "I executed Spades genome assembly but its not running. When I click on the job history I get this message \"This is a new dataset and not all of its data are available yet \u201cspades\u201d. Help please."
    },
    {
      "role": "system",
      "text": "Welcome, @system and @user\nJobs will queue first, then execute.\nPlease see @link https://training.galaxyproject.org/training-material/faqs/galaxy/#analysis"
    }
  ],
  [
    {
      "role": "user",
      "text": "My original files for a RNA-Seq-analysis are in fastq.gz format and to map (with BWA-MEM) them I tried converting them to fastqsanger format using the FASTQ Groomer. When doing so I received the error \u201cThere was an error reading your input file. Your input file is likely malformed.\u201d regarding some of the files.\nHowever, I don\u2019t think there is any problem with my data files. How can I check them?\nI have already tied to redo the jobs (grooming, mapping) after also re-importing the files."
    },
    {
      "role": "system",
      "text": "Hi @user\nIf the reads were generated from a recent Illumina protocol, they are already in fastqsanger (uncompressed) or fastqsanger.gz (compressed) format.\n@quote\n\u201cThere was an error reading your input file. Your input file is likely malformed.\u201d\nTry running FastQC on the reads. If that tool also fails, there is either formatting or datatype assignment problem:\n\nThe uploaded read could be truncated (before upload, or during). Check the size in Galaxy versus the size from the source. Uploading again resolves this.\nCheck to make sure that the file is actually compressed or not, and adjust the datatype to match.\nIf you can\u2019t find the problem, please share your history (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history) back for more help. You can post the link back in a public reply or ask for a moderator to start up a private direct message here instead.\n\nTutorial\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nAnalyses of sequences\n\n\n\n\n\nFAQs \u2013 these cover most fastq datatypes with extra help\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#datatypes"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nThis is Roger. I logged into my galaxy\u2019s account and all my data was removed and purged. Is there any reason for that? Can anyone help me here?\nRoger"
    },
    {
      "role": "system",
      "text": "Please see this notice: Accounts deleted from Galaxy Main to enforce posted Terms and Conditions (@link https://help.galaxyproject.org/t/accounts-deleted-from-galaxy-main-to-enforce-posted-terms-and-conditions/1429/3)\nTo be very clear:\n\nPick ONE account to restore. Be very clear.\nList out your duplicated accounts to prevent this from happening again. Not all duplicated account were removed at once. Expect this to continue. We have limited resources and need to ensure fair usage.\nIf your email was from public service, you\u2019ll need to confirm your unique identity. Primary work/academic email is usually enough. Variations on the email address (common) are not accepted.\n\nAll will be private to the administrators of our server when the email is sent in and is used only to confirm your unique identity going forward, prevent accidental duplicate account detection with other unique people, prevent account loss due to new duplicates being identified (existing and/or new accounts) \u2013 IF you follow our terms of usage going forward.\nBe honest, clear, and do this immediately. We want to help you. The GCC annual conference starts on July 1st, 2019 (@link https://galaxyproject.org/events/gcc2019/) and many will begin to travel soon. Restoration request emails that arrive after early tomorrow morning (June 27th), or that are incomplete, will result in account restoration delays on our side for up to 2-3 weeks.\nIf this happened at a different public Galaxy server, you\u2019ll need to contact them. All public Galaxy servers, and many private Galaxy servers, have an account quota of one account per distinct user per server. Each handle duplicated accounts differently. Contact information is usually on the home page of the server and sometimes (or also) here: @link https://galaxyproject.org/use/\nThanks for understanding!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Two fastq files \u2014> FASTQ Groomer \u2014> Trimmomatic \u2014> 2Pared files to BWE-MEN \u2014>RmDup \u2014> GATK happlotypeCaller\nNo error found (all green boxes). However, the last command returns 0 lines and there is a message in the output saying:\njava: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory galaxy\nThe input file used to run HaplotypeCaller looks ok\nUsing database hg38\nEDIT:\nI gave ran again the same command and now I have got another error\n\nERROR MESSAGE: SAM/BAM/CRAM file input.bam is malformed. Please see gatk-docs/Collected_FAQs_about_input_files_for_sequence_read_data_(BAM_CRAM).md at master \u00b7 broadinstitute/gatk-docs \u00b7 GitHub (@link https://software.broadinstitute.org/gatk/documentation/article?id=1317for) more information. Error details: SAM file doesn\u2019t have any read groups defined in the header.  The GATK no longer supports SAM files without read groups\n\nERROR ------------------------------------------------------------------------------------------"
    },
    {
      "role": "system",
      "text": "On which server does that happen? It seems we need to fix the conda package or the conda installation."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nWhen I perform Block Cluster Test with my small RNA-seq data, I only see miRNA, tRNA and some sno RNA but no piRNA or other smallRNA. Does anybody know why?\n"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nIs this the tutorial you are following? Small Non-coding RNA Clustering using BlockClust (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/small_ncrna_clustering/tutorial.html). Tutorial data is a sub-sample of full data and while attempts are made to provide representative sub-samples, that isn\u2019t always perfect.\nMaybe the publication linked at the bottom of the tool form will help more? It is available open source. BlockClust: efficient clustering and classification of non-coding RNAs from short read RNA-seq profiles | Bioinformatics | Oxford Academic (@link https://academic.oup.com/bioinformatics/article/30/12/i274/385482)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have trouble working with roary tool.\nI use prokka to annotate my genomes in .gff format (version 3) and then I use Roary with the generated .gff files, to analyse the pangenome.\nHowever it does not work. You will find some screenshoot of the error details.\nThank you for your help,\n\nCapture_Prokka_11010\u00d7755 83.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/24910140247f55c81f979b0cffe4f2c0d4c2127a.jpeg) \nCapture_Prokka_21008\u00d7716 81.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dcfaa774c3c8c509eedabdb9750001328016b0be.jpeg) \nCapture_Roary_3970\u00d7664 73.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8b2d79f24040dac316f3045054e79c464341380a.jpeg) \nCapture_Roary_4975\u00d7495 80.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/acd22aac5414fc739025249306c64a355cc251bf.jpeg) \nCapture_Roary_51009\u00d7709 72.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a7f3d5f69cedf867c1cf6853e7634ad6a2d20953.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis is the same problem again. And it is not a problem with the tool installation, rather it is an actual tool bug. Thank you again for reporting it!\nEvidently there is a bug with the tool when the input datasets contain spaces in the names. The correction should be simple but is still in progress. And it can be avoided without too much trouble at @link http://usegalaxy.org or @link http://usegalaxy.eu (or anywhere else) until the tool itself is fixed.\nThis is the original ticket but with more updates, including the workarounds: @link https://github.com/galaxyproject/usegalaxy-playbook/issues/293#issuecomment-605089574\nIn short, remove spaces from the input file names. How-to is in the ticket above.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am just trying to find why the VCF file (via Freebayes) does not include GQ?\nOnly the following is output:\nGT:DP:AD:RO:QR:AO:QA:GL\nIs there a way to process the VCF to add GQ? Or have I made an error during the initial variant calling? Is there a way to amend this please so that I can access GQ scores?\nThank you for your help"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe try the following during FreeBayes job setup:\nChoose parameter selection level > Full list of options\nAlgorithmic features > Set Algorithmic features\nCalculate the marginal probability of genotypes and report as GQ in each sample field in the VCF output > change to Yes.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear GalaxyHelp,\nI have a problem with my local instance of galaxy (20.01). I suddenly noticed it was not starting any new jobs or uploading files (staying grey forever). When I looked at the galaxy log, it was constantly starting new jobs, but the database/job_directory folders galaxy created seem \u2018empty\u2019 (empty working and output folder). As this is a development instance, I tried to completely remove the tmp and job_directory. Still, if I restart galaxy, it will try to start new jobs. This instance doesn\u2019t consist of users that started any jobs, so it is very confusing. I must say that I am working on an automated updating of libraries via the bioblend api. I\u2019m curious if it is possible that this api invokes certain jobs that are stuck in a loop?\nAfter some dependency checks which it usually does, my logfile looks like this:\nStarting server in PID 11717.\nserving on http://xxx:8080\ngalaxy.model.database_heartbeat DEBUG 2020-06-05 13:52:26,856 [p:11717,w:1,m:0] [database_heartbeart_main.web.1.thread] main.web.1 is config watcher\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,216 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,232 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Dispatching to local runner                                            galaxy.jobs DEBUG 2020-06-05 13:52:27,253 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Persisting job destination (destination id: local)                             galaxy.jobs DEBUG 2020-06-05 13:52:27,253 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Working directory for job is: /home/galaxy/galaxy/database/jobs_directory/080/80530\ngalaxy.jobs.runners DEBUG 2020-06-05 13:52:27,278 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] Job [80530] queued (45.707 ms)\ngalaxy.jobs.handler INFO 2020-06-05 13:52:27,288 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80530) Job dispatched\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,297 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,316 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Dispatching to local runner\ngalaxy.jobs DEBUG 2020-06-05 13:52:27,337 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Persisting job destination (destination id: local)\ngalaxy.jobs DEBUG 2020-06-05 13:52:27,339 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Working directory for job is: /home/galaxy/galaxy/database/jobs_directory/080/8\n0531\ngalaxy.jobs.runners DEBUG 2020-06-05 13:52:27,389 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] Job [80531] queued (72.643 ms)\ngalaxy.jobs.handler INFO 2020-06-05 13:52:27,408 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80531) Job dispatched\ngalaxy.jobs.mapper DEBUG 2020-06-05 13:52:27,436 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80532) Mapped job to destination id: local\ngalaxy.jobs.handler DEBUG 2020-06-05 13:52:27,469 [p:11717,w:1,m:0] [JobHandlerQueue.monitor_thread] (80532) Dispatching to local runner\netc. creating more and more job directories continuously.\nDid any of you every experience something similar? I am on the edge of reinstalling the development instance, but I want to avoid this happening again of course. I was wondering if someone has an idea of forcing to stop this process for now.\nThank you so much in advance!\nAnnabel Dekker"
    },
    {
      "role": "user",
      "text": "I used gxadmin (GitHub - galaxyproject/gxadmin: Handy command line utility for Galaxy administrators (@link https://github.com/galaxyproject/gxadmin)) to purge the database queue of my galaxy instance!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve inherited a legacy galaxy (version 16.07) installed on local Ubuntu (18.04) server, with a lot of installed tools. However, when I try to do data analysis I get the error \u2018Fatal error: Exit code 1 ()\u2019. Here is the bug report for simple fastqc (Note: I got the same/similar error message for other tools I\u2019ve tried too).\nFatal error: Exit code 1 ()\n/bin/sh: 1: fastqc: not found\nTraceback (most recent call last):\nFile \u201c/home/posavi/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/fastqc/3a458e268066/fastqc/rgFastQC.py\u201d, line 166, in \nfastqc_runner.run_fastqc()\nFile \u201c/home/posavi/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/fastqc/3a458e268066/fastqc/rgFastQC.py\u201d, line 139, in run_fastqc\nsubprocess.check_call(self.command_line, shell=True)\nFile \u201c/usr/lib/python2.7/subprocess.py\u201d, line 190, in check_call\nraise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command \u2018fastqc --outdir /home/posavi/galaxy/database/jobs_directory/017/17999/dataset_35883_files --quiet --extract -f fastq tenM.fastq\u2019 returned non-zero exit status 127\nAlso, most of the installed tools have either red arrow or red triangle flag (see the image)\n\nimage2640\u00d71530 451 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/4f05b8b0c7f4c401980c3ef940f6a0f8dbfe06fc.png)\nI am also not able to either update/reinstall  any of installed tools neither to install any new tools.\nPlease let me know if there is the way to fix this problem, except to upgrade ubuntu and galaxy.\nThank you.\nMarijan"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nFatal error: Exit code 1 ()/bin/sh: 1: fastqc: not foundTraceback (most recent call last):\nThis is a configuration problem. Was the Galaxy installation moved to a new location?\n@quote\nPlease let me know if there is the way to fix this problem, except to upgrade ubuntu and galaxy.Thank you.\nYou\u2019ll need to be running a current version of Galaxy for the newer tool versions to be functional.\nUpgrading works best when done sequentially through releases. For your case, that would be a lot of work. Is there a reason why you need the older installation instead of starting over?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there,\nMy account was suddenly marked for deletion for the first time on Thursday 23 october 2020. I have requested galaxy by replying to their designated email to restore my single non-public account (admin redacted for privacy). also I had declared my public server duplicated accounts for closure. Still after one day, my account has not been restored, every email comes up with same machine generated reply.\nI don\u2019t know whom to contact for restoration of my account as the data is very important to me.\nKindly let me know is there any other way to ask them for restoration.\nThanks\nFurqan"
    },
    {
      "role": "system",
      "text": "Resolved.\n@quote\nUpdate Nov 1, 2019 \nDuplicated accounts are detected as an ongoing process. If you found your account recently deleted, or have not but do have duplicates and want to avoid problems, write to us atgalaxy-bugs@lists.galaxyproject.organd list the following: \n\nThe registered email for the account you wish to keep\nIf that email is from a public domain service, include your non-public primary email address. See \u201cReminders\u201d below if you do not understand the difference.\nList out your duplicated acco\u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy community,\nFor both eggNOG 1.0.2 or 2.0.1 implemented in @link http://usegalaxy.eu, the \u2018Version of eggNOG Database\u2019 cannot be changed (\u2018No options available\u2019 is displayed as default), but upon \u2018Execute\u2019, this drop-down menu becomes highlighted in red with a \u2018Please verify this parameter.\u2019 being displayed, and eggNOG would not start.\nWould be great if there was a way to get eggNOG started, and any help would be appreciated.\nThanks,\nMichael\neggNOG 1.0.2\n@link https://usegalaxy.eu/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Feggnog_mapper%2Feggnog_mapper%2F1.0.2\neggNOG 2.0.1\n@link https://usegalaxy.eu/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Feggnog_mapper%2Feggnog_mapper%2F2.0.1%2Bgalaxy1"
    },
    {
      "role": "system",
      "text": "Hi,\ncan you please try this again. I installed database 2.0.\nCiao,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am having trouble running the COVID-19: variation analysis on ARTIC PE. I tried different versions as well but get the following error \"Workflow submission failed: Uncaught exception in exposed API method\u201d. I thought it was my fastq files but then I also tried running the data from previous history that was successful but the same error came up. The same error occurs on both .eu and .org platforms."
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease make sure to source the most current versions of these workflows from here: GalaxyProject SARS-CoV-2 analysis effort - Galaxy Community Hub (@link https://galaxyproject.org/projects/covid19/)\nIf you need more help, the community is having trouble helping. Please provide a bit more context so others can try to reproduce and offer advice.\n\nWhere the workflow was sourced\nThe exact URL of that public workflow\nShare link to that workflow after imported at either @link http://usegalaxy.org or @link http://usegalaxy.eu\n\nShare link to a history where that workflow was executed and failed\nShare link to a history where that workflow was executed and was successful\nScreenshot of the expanded User -> Workflow Invocations report for each of those runs\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello to the Galaxy community. Very helpful indeed. Many thanks for your effort.\nI am new to sequencing and I am following the same Tutorial as above, but I am applying the protocol to mouse. Unfortunately, I ended up with the same problem, i.e., the final DEseq2 file did not contain the gene names, but instead the term N/A. I am using the file from UCSC web table, as follows:\n\nDataset Peek:\n\n\n\n\n1.Seqname\n2.Source\n3.Feature\n4.Start\n5.End\n6.Score\n7.Strand\n8.Frame\n9.Attributes\n\n\n\n\nchr1\nmm10_ncbiRefSeq\nstop_codon\n134202951\n134202953\n0.000000\n-\n.\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nCDS\n134202954\n134203590\n0.000000\n-\n1\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nexon\n134199215\n134203590\n0.000000\n-\n.\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\nchr1\nmm10_ncbiRefSeq\nCDS\n134234663\n134234733\n0.000000\n-\n0\ngene_id \u201cNM_001291928.1\u201d; transcript_id \u201cNM_001291928.1\u201d;\n\n\n\nIf I understood correctly I have not used the correct mouse file. Could you please help me, and point me to the correct file, and what I have done wrong, if not the wrong file?\nMany thanks."
    },
    {
      "role": "user",
      "text": "Hi gallardoalba,\nThanks for your email, appreciated. Following the instructions on the help site I realised the problem was the genome reference file from UCSC table platform. I downloaded the latest mouse genome from gencode and my problems about naming of genes and unassigned ambiguity were solved. However, there are other things that I would like to ask. Therefore, I open a new topic.\nMany thanks again for your effort on the help tutorials.\nBest regards,"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have been trying to follow the galaxyproject tutorial about sambam-datasets, however, when I try to map bwa-mem my fastq file, it does never run. It return with the error message that it could not align. What step shall I take? What would I be doing wrong?\nHere the exactly message: file does not contain alignment data\nBy the way, here it is the tutorial that I have been following exactly. Manupulating NGS data with Galaxy (@link https://galaxyproject.org/tutorials/ngs/#sambam-datasets)."
    },
    {
      "role": "system",
      "text": "Dear pauloalabarse,\nPlease make sure you selected the correct reference (either from Galaxy or from your local history) and also check if you fastq files are correctly formatted.\nHere is another tutorial that might help you: Mapping (@link https://galaxyproject.github.io/training-material/topics/sequence-analysis/tutorials/mapping/tutorial.html)\nCheers,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy support team,\nI have developed 3 bioinformatics workflows on the Galaxy Australia server for processing sRNAseq data from host plants for detection of viruses and viroids.\nI would like to publish these workflows so that Galaxy-main users can also access these.\nI tried to test my workflows on the main server and received the error that one tool is not available by default.\nTherefore, I am writing to request to add the following tool to the main server.\ntoolshed.g2.bx.psu.edu/repos/artbio/cap3/cap3/2.0.0\nThank you"
    },
    {
      "role": "system",
      "text": "@quote\ntoolshed.g2.bx.psu.edu/repos/artbio/cap3/cap3/2.0.0\nHi @user\nI opened the install request here: Add cap3 2.0.0 to support a new published workflow \u00b7 Issue #524 \u00b7 galaxyproject/usegalaxy-tools \u00b7 GitHub (@link https://github.com/galaxyproject/usegalaxy-tools/issues/524)\nIt looks like both @link http://usegalaxy.org and @link http://usegalaxy.eu need the tool. Please clarify in the issue ticket as needed.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve mapped RNA-seq data with Bowtie2 to the GRCm38 genome and I am trying to count using htseq.\nI have imported GTF files from bot UCSC and Ensembl but I cannot get the right annotation. It works for Ensembl transcripts IDs but when I select gene_id all counts are zero."
    },
    {
      "role": "system",
      "text": "There was probably a mismatch for the \u201cFeature type = gene\u201d when using the UCSC GTF (not a good choice anyway, as both gene_id and transcript_id are the same value from this data source - effectively meaning all counts will be \u201cby transcript\u201d even if reportedly \u201cby gene\u201d).\nAnd there was probably a chromosome mismatch problem when using the Ensembl GTF. Ensembl chromosome names will have a format like \u201c1\u201d when most of the genome pre-loaded indexes for mapping tool use the UCSC chromosome names with a format like \u201cchr1\u201d. See Mismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\nGencode (@link https://www.gencodegenes.org/) andiGenomes (@link https://support.illumina.com/sequencing/sequencing_software/igenome.html) are the best sources for reference annotation when your genome/build version is supported by either. Gencode GTFs can be loaded directly by URL. iGenomes archives need to be downloaded and unpacked locally first, then just the genes.gtf file uploaded to Galaxy."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I\u2019m following the tutorial for DNA Methylation Data Analysis (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/methylation-seq/tutorial.html#feedback)  and cannot find the Metilene tool to detect differentially methylated regions (DMRs).\nIs there a way to upload this tool from the @link https://toolshed.g2.bx.psu.edu/repository?repository_id=8bc5954a5c4e0382&changeset_revision=37abd09a0ae6?\nAlternatively, is there another tool to use for detection of DMRs or differentially methylated loci?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit will be installed in the next days in @link http://usegalaxy.org; meanwhile you can run the tutorial in the Galaxy instance @link http://usegalaxy.eu, where you can find Metilene.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI downloaded my 16S files in fastq format and I\u2019m interested in convert this files to fastq.gz files. How do I do that?\nI\u2019m leaning how to analyze 16S data.\nThanks  lot"
    },
    {
      "role": "system",
      "text": "Hi @user\ntry Edit Attributes (pencil icon) > in the middle window switch to Convert tab > in pull down menu select an appropriate option (fastqsanger.gz).\nThis is assuming you use data with Phred 33 encoding and Galaxy assigned fastasanger datatype to FASTQ files after upload.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a question about limma or edgeR or any other DEG tool. how the tool calculate the treat and control fold changes? for example in the first section we put control and second treat. So is it Control-Treat(in fold changes) or Treat-Control\nit is just about how the DEG tool calculate fold change result"
    },
    {
      "role": "system",
      "text": "Actually, for the limma and edgeR tools at least, I think you want Treat-Control. Control-Treat would be if you want to know what genes are up/down in the control relative to the treatment. Usually people want to know what\u2019s up/down in the treatment relative to the control. Even if you input the Control sample count files first and the Treat counts files second, it\u2019s the contrast order that specifies how the groups are compared. See the help section in the limma tool form (e.g. for Mut-WT the fold changes in the results will be up/down in Mut relative to WT)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI failed to connect to the galaxy ftp server. Below is the error message I got:\n\nWeChat Screenshot_20210303193605844\u00d7137 6.46 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/96714496ea234fde0180fca5bd1e5e8a6272b920.png)\nI tried to turn off the firewall but it didn\u2019t help.\nI also tried to edit the network configuration wizard and I got this error message:\nWeChat Screenshot_20210303193936\nWhat should I do? FYI, I\u2019m physically in China so I have to use vpn first to get access to galaxy, google etc\u2026 Is this the problem?\nThanks,\nYunyang"
    },
    {
      "role": "system",
      "text": "Hi @user,\nCould you try connecting again? It seems to be due to a system scheduled maintenance.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have a series of non-tool-shed tools I would like to install on my Galaxy instance.\n\n\n\n@link https://github.com/TAMU-CPT/galaxy-tools\n\n\n\n@link https://github.com/TAMU-CPT/galaxy-tools\nCPT's Galaxy Tools. Contribute to TAMU-CPT/galaxy-tools development by creating an account on GitHub.\n\n\n\n\n\nmacros.xml includes following requirements for all of the tools\nrequirement type=\u201cpackage\u201d version=\u201c2.7\u201d - python - requirement\nrequirement type=\u201cpackage\u201d version=\u201c1.65\u201d - biopython - requirement\nrequirement type=\u201cpackage\u201d version=\u201c2.12.1\u201d - requests - requirement\nAll of these are resolved by conda in my instance, but when I run a tool, it uses default galaxy virtualenv python executable instead of mulled conda environment causing errors. How should I let the job run inside conda environment?"
    },
    {
      "role": "user",
      "text": "Thanks for your response. This issue seems to be resolved, and it seems like I caused the problem. I was running run.sh inside Ubuntu Screen instance so Galaxy can keep running when I close ssh. And Screen apparently initiated another bash shell which seems to have caused this issue. I now run inside tmux and it seems to have fixed the issue."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Community,\nCould someone guide me to someone who can help me. I am getting this error when I try to login to my galaxy account. \"Uncaught exception in exposed API method: \" Does someone know why or how can I fix this?\nThank You in Advance!"
    },
    {
      "role": "system",
      "text": "Hello @user and @system\nThat message can result if a prior account was attempted to be \u201cre-created\u201d.\nTry creating an account with an email address not previously used for an account at the server where you are working. Even when some old account doesn\u2019t exist anymore, the email address still cannot be reused at most of the public servers. Meaning: please start completely over to get a new working account.\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#account"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello siam here. I am a newbie in terms of MD simulation. For my research purpose, I need to simulate 100ns. how do I incorporate that into the galaxy server? how many steps do I need to include for a 100ns simulation? Do I need to step of ps?\nthank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease see the tutorials here\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/)\n\n\nGalaxy Training: Computational chemistry (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/)\nModelling, simulation and analysis of biomolecular systems\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nPlease I would like to ask if it is okay and allowed to create another account on Galaxy because I used almost all the available space I have in my current account and I am planning to upload more raw data and there is no enough space for that!\nIf not then what do you suggest?\nThank you in advance!"
    },
    {
      "role": "system",
      "text": "Hello @user\nAt most public Galaxy servers, there is a \u201cone account per user\u201d quota. In practical terms, this means you can have many Galaxy accounts but just one at any particular public Galaxy server.\nFor Galaxy Main @link https://usegalaxy.org:\n\nAccount terms are explained in this FAQ: @link https://galaxyproject.org/support/account/\n\nHow to manage data is explained in this FAQ, including how to download data no longer in active use and how to apply for more quota space short-term for larger projects: @link https://galaxyproject.org/support/account-quotas/. From your comments, it sounds like you need to download then purge your old work to make room for new work, more help here: @link https://galaxyproject.org/support/account-quotas/#reduce-quota-usage-while-still-retaining-prior-work-data-tools-methods\n\nCreating or using a duplicated account, or someone else\u2019s account, to try to get around the account terms will cause problems like this.  Accounts deleted from Galaxy Main to enforce posted Terms and Conditions (@link https://help.galaxyproject.org/t/accounts-deleted-from-galaxy-main-to-enforce-posted-terms-and-conditions/1429) Stick to using just one account and you won\u2019t have to worry about losing your account or data unexpectedly.\n\nThe FAQs above also cover how to find out the usage terms (and if quota extensions are possible) when working at other public Galaxy servers. In short, check the server\u2019s home page or directory listing (@link https://galaxyproject.org/use) for that server\u2019s contact and quota information, and ask them you have questions.\nYou also have the option of setting up your own Galaxy server. A local server can be used to store data (in context). Cloud servers are popular choices for large computational work. If you use one at AWS, they offer grants for storage/computational resources (Galaxy itself is always free, and the Cloudman version is designed to be simple to administrate). These details are also linked from FAQs above and at the Galaxy resources directory (@link https://galaxyproject.org/use). Also, search prior Q&A with the term \u201ccloud\u201d here if interested in exploring that option.\nAll public Galaxy servers are independently administered and not all follow this forum. If you can\u2019t locate the server contact or quota info for where you are working, share back the URL and we can try to help you find it.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI am a very new galaxy user. I have 4 fastq files from the same organism, 2 forward reads and 2 reverse reads. I want to merge forward by forward and reverse by reverse before mapping the genome.\nI\u2019ve found several tutorials on mapping, which are great, but I don\u2019t know how to merge my sequences.\nCould you please kindly help me with this?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can use the Concatenate datasets tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to identify and study the differentially expressed non-coding RNA, I used the FeatureCount Built-in genome for the counting, but I am not getting non-coding RNA. Any help is welcome. Thanks!"
    },
    {
      "role": "system",
      "text": "Hello @user\nThe built-in annotation sources are for coding regions of the genome.\nIf you want to look for ncRNA, Gencode is one annotation source. Use the GTF version of the annotation, format it correctly, and adjust the tool form setting to match the annotation features/attributes.\nAnnotation:\n@link https://www.gencodegenes.org/human/\nFAQs:\n@link https://galaxyproject.org/learn/datatypes/#gtf\n@link https://galaxyproject.org/support/diff-expression/\nTutorials:\n\n\n@link https://training.galaxyproject.org/training-material/search?query=ncrna\n\n\nGalaxy Training: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=ncrna)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\nI also added some tags to your post that point to related prior Q&A.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nScreenshot 2022-05-02 at 00.44.351920\u00d71200 141 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/569ade2f9e27338d5c5f53530fda6d634951b61a.jpeg)\nMy FASTQC raw data (collection of paired-end reads) does not appear in the dropdown menu of MultiQC when I select multiple datasets or dataset collection, I have tried it with version but no avail\nPlease help"
    },
    {
      "role": "system",
      "text": "Try flatten collection as explained here MultiQC not working correctly (@link https://help.galaxyproject.org/t/multiqc-not-working-correctly/7386)\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nPlease I am getting error using SnpEff since yesterday, I had results a day before using the same settings and data type. Please I want to know if there is something wrong somewhere thank you.\nGavers"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe @link http://UseGalaxy.eu server was addressing maintenance the last few days. @link https://status.galaxyproject.org/\nReruns have worked to resolve odd behavior for other people. That might involve backing up and examining/rerunning tools run upstream from this tool (and those outputs). Please also give that a try.\nIf you are still having problems after that, send in a bug report and consider asking if the problem might be still related to any lingering maintenance issue at the EU chat @link https://matrix.to/#/#usegalaxy.eu:matrix.org. You can post a link to this topic for reference in a chat or in a bug report.\nThe EU moderators will be back next Monday their time (and maybe sooner!) but let\u2019s start there. If you want to post a shared history link with your final rerun errors, that will speed things up if there is maybe a new data problem. Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there any way where I can upload my VCF files (myvcf.vcf) and annotate the rsid of dbSNPs?\nI understand I should use SnpSift Annotate and I need the VCF File with ID field annotated (such as dbSNP.vcf). Is it possible that I don\u2019t have to upload this file and can access remotely via FTP? I am not sure how to do this. I am looking for dbSNPs for hg19 genome.\nI am trying to do this in the online version of Galaxy.\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user,\nno I don\u2019t think there is currently a way around uploading the file to a history of yours at least once.\nIf you need to do this for more than once it may be convenient to maintain a dedicated \u201cresource\u201d files history, from which you just drag the dataset over to each analysis history. This way, as explained in Galaxy Community Hub - Galaxy Community Hub (@link https://galaxyproject.org/admin/disk-quotas/), the file will be counted in your storage quota only once.\nIn the long run, this tool should be extended to work also with preinstalled annotation data on the server. I\u2019ll give this some thought."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi , I am learning about Mapping ,and when using the RNA STAR,I cannot find a reference genome called Fly (Drosophila melanogaster): dm6 Full. What should I do? How to find it or how to insert it? Thank you verymuch"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can find the indexed genomes in @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.8a+galaxy0. Otherwise, you can download the reference genome from NCBI (@link https://www.ncbi.nlm.nih.gov/genome?term=vih&cmd=DetailsSearch); you just need to copy the following link:\n\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/215/GCF_000001215.4_Release_6_plus_ISO1_MT/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.fna.gz\n\nThen you need to click in Upload data in the Galaxy interface, then click in Paste/Fetch data, paste the link there and finally Start.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi the jobs are waiting to run in galaxy for long time. I have started before 8 hours and the job is not running at all."
    },
    {
      "role": "system",
      "text": "Hello @system @user\nPlease see Cluster issue at UseGalaxy.org -- Resolved 06/05/2023. Rerun failed (red) or leave queued (gray) - #3 by jennaj (@link https://help.galaxyproject.org/t/cluster-issue-at-usegalaxy-org-resolved-06-05-2023-rerun-failed-red-or-leave-queued-gray/10185/3)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nPlease I have a question about create a single Pie chart for each sample.\nAs attached , the photo represents the Krona pie chart for two samples, now I need to make a single pie chart for each sample for comparative. Any one can help me to do that?\nI will appreciate\nbest regards\n\nScreenshot 2021-07-07 at 2.48.30 pm1440\u00d7900 198 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/19c1d95dcbd48c0829fc88ce52eeaac4c03e15bf.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI recommend you to have a look at this tutorial 16S Microbial analysis with Nanopore data (@link https://training.galaxyproject.org/training-material//topics/metagenomics/tutorials/nanopore-16S-metagenomics/tutorial.html#analyze-taxonomic-assigment). It generates different pie charts for different samples.\nRegards."
    }
  ],
  [
    {
      "role": "user",
      "text": "HI,\nWe just recently moved all of our Galaxy data to a new server with python 2.7 so that we could upgrade to the latest version of galaxy. Everything seems to be working pretty well except that when I try to re-run old GATK or deeptools tasks, the job fails.\nI have checked/compared \u201cmoved data\u201d versus \u201cre-uploaded data\u201d using these two methods:\n\n\nmoved bam data: job errors: checked file locations to make sure they match, checked permissions, run jobs with new data (error)\n\nre-uploaded\" bam data: job success: copied data (from the new server to my local machine) and re-uploaded them. Jobs run successfully with this data creating the same output as the old jobs\n\nI am wondering if anyone might know why this is happening with method 1. There does not seem to be anything wrong with the bam files or the tools.\nAny ideas would be great, thanks!\nBelow are the errors output by some of the tools after BAM were only moved (not re-uploaded) using method 1\n\nDeeptools bam compare output\nFatal error: Exit code 1 ()\nThe file one.bam does not exist\n\nGATK haplotype caller\n##### ERROR MESSAGE: File /data/galaxy/tmp/tmp-gatk-PBNEqR/gatk_input_0.bam.bai \nis malformed: Premature end-of-file while reading BAM index file /data/galaxy/tmp/tmp-gatk-\nPBNEqR/gatk_input_0.bam.bai. \nIt's likely that this file is truncated or corrupt -- \nPlease try re-indexing the corresponding BAM file.\n\nIndel realigner\n##### ERROR MESSAGE: File /data/galaxy/tmp/tmp-gatk-tXbYgP/gatk_input.bam.bai is malformed: \nPremature end-of-file while reading BAM index file /data/galaxy/tmp/tmp-gatk-tXbYgP/gatk_input.bam.bai. \nIt's likely that this file is truncated or corrupt -- \nPlease try re-indexing the corresponding BAM file.\n\n\n\nNote: Moderator reformatted post for clarity\n"
    },
    {
      "role": "user",
      "text": "Update* I have located the bam indexes on our original server and discovered that the _metadata files (containing the indexes) in the files dir were not copied over with the rest of the files. Copying these files over has fixed this issue."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone.\nI am trying to operate htseq-count in galaxy. I have used HISAT2 BAM files (I am using Arabidopsis WT and Double Knock Out) and TAIR10.GFF3 annotation file.\nBut every time it shows fatal error after a while.\nRequesting you all to provide your invaluable suggestions.\nThanking you,\nMahmudul"
    },
    {
      "role": "system",
      "text": "Hi,\nTry using the gtf version of the annotation instead. This tool does not accept gff3 formatted annotation.\nThis can get a bit confusing since the tool form states gff as an input, and will accept files with that datatype assigned. This is because many data sources provide gff+gft hybrid data \u2013 and the only difference between those two is the last column (attributes). Some also release gff3 data that is given a gff datatype (including this source: @link https://www.arabidopsis.org). The gff3 format is structured and labeled much differently than gff or gtf.\nThese data are incredibly difficult to automatically assign a datatype to, because there can be so much variation from sources. Custom header lines can interfere, blank lines, etc. It is usually a good idea to check the format yourself once in Galaxy to confirm the actual type. Sometimes headers need to be removed in gff or gtf datasets before certain tools will consume them properly.\nThis FAQ explains each, how they are similar/different, and reformatting/transformation tips.\n\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have a differential expressed genes files with Gene ID and Log fold change values.\nI want to find the top 20 expressed genes using limma but as I do not have the feature counts with me it is giving me error.\nCan you please suggest which tool within galaxy I can use"
    },
    {
      "role": "system",
      "text": "Based on information you provide it is very difficult to discern what you are trying to do. Please consult the following resources first:\n\nRNAseq in galaxy Tutorial (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html)\nCornell RNAseq course (@link http://chagall.med.cornell.edu/RNASEQcourse/Intro2RNAseq.pdf)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear support team of the GALAXY,\nI would like to use Bowtie2 for mapping NGS (ChIP-seq) data for two different microorganism, Aspergillus fumigatus and Verticillium dahliae.\nI have already checked for the corresponding reference genomes of these two microbes and they are not available as options in the GALAXY platform.\nWould it be possible please to include them in the reference genome inventory?\nPlease find details of these genomes below:\n\nAspergillus fumigatus:\nLink:\n@link https://fungidb.org/common/downloads/Current_Release/AfumigatusAf293/fasta/data/\nFile:\n[FungiDB-48_AfumigatusAf293_Genome.fasta]\n\n(@link https://fungidb.org/common/downloads/Current_Release/AfumigatusAf293/fasta/data/FungiDB-48_AfumigatusAf293_Genome.fasta)\n2)Verticillium dahliae:\nLink:\n@link ftp://ftp.ensemblgenomes.org/pub/fungi/release-48/fasta/verticillium_dahliaejr2/dna/\nFile:\n[Verticillium_dahliaejr2.VDAG_JR2v.4.0.dna.toplevel.fa.gz]\n(@link ftp://ftp.ensemblgenomes.org/pub/fungi/release-48/fasta/verticillium_dahliaejr2/dna/Verticillium_dahliaejr2.VDAG_JR2v.4.0.dna.toplevel.fa.gz)\nMany thanks in advance,\nBest regards,\nManolis"
    },
    {
      "role": "system",
      "text": "Hi @user\nUpload the fasta version of each and use them as Custom Genomes \u201cfrom the history\u201d. You can also promote any fasta to a Custom Build to generate a \u201cdatabase\u201d that can be assigned to datasets (some tools may require that metadata assignment).\nThere is much prior Q&A about using Galaxy this way that covers the how-to FAQs plus details and other troubleshooting. I added a few tags to your post to help in finding those.\nThe @link http://usegalaxy.eu administrators may write back, if they decide to add the genomes directly. There is a hackathon going on today, so replies may be delayed. But you don\u2019t need to wait \u2013 using a custom genome/build is accessible to everyone at all usegalaxy.* servers and most other public Galaxy servers.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI\u2019ve recently encountered some issues with Unicycler assembly. I\u2019ve tried to perform hybrid assembly with use of\n\n\ntrimmed Illumina reads R1 (0.17 Gb) + R2 (0.16 Gb); format: fastqsanger.gz\n\n\nnanopore reads (2.3 Gb); format: fasqsanger\n\n\nUnicycler readily deals with individual assembly of either Illumina or Nanopore reads. However, it fails to generate hybrid assembly. Any suggestions?\nmaybe is it about available working memory @ Galaxy server?\nthanks in advance,\nPiotr\nPS here is the error report\n\ntput: No value for $TERM and no -T specified \n\ntput: No value for $TERM and no -T specified \n\ntput: No value for $TERM and no -T specified\n\n/pylon5/mc48nsp/xcgalaxy/main/staging/23588931/command.sh: line 95:\n38467 Segmentation fault      \n\n(core dumped) unicycler -t\n\"${GALAXY_SLOTS:-4}\" -o ./ --verbosity 3 --pilon_path $pilon -1'fq1.fastq.gz' -2 'fq2.fastq.gz' -l lr.fastq --mode 'conservative' --min_fasta_length '100' --linear_seqs '0' --min_kmer_frac '0.2' --max_kmer_frac '0.95' --kmer_count '10' --depth_filter '0.25' --start_gene_id '90.0' --start_gene_cov '95.0' --min_polish_size '1000' --min_component_size '1000' --min_dead_end_size '1000' --scores '3,-6,-5,-2'\n\n"
    },
    {
      "role": "system",
      "text": "Hi @user!!\nPlease try running the job again now. I would suggest trying at least two, to see if you can hit a non-problematic cluster node.\nThere were some cluster issues yesterday and a few earlier today (different reason). A very small number of jobs failed, but most didn\u2019t. There is likely some small bit of cluster tuning that still needs to be done on our side, but that is unlikely to happen until Monday.\nThese are the important parts of the error message. The \u201ctput\u201d lines are a technical side-effect error output that can be ignored.\n@quote\n/pylon5/mc48nsp/xcgalaxy/main/staging/23588931/command.sh: line 95: 38467 Segmentation fault(core dumped) unicycler -t \u201c${GALAXY_SLOTS:-4}\u201d -o ./ --verbosity 3 --pilon_path $pilon -1\u2019fq1.fastq.gz\u2019 -2 \u2018fq2.fastq.gz\u2019 -l lr.fastq --mode \u2018conservative\u2019 --min_fasta_length \u2018100\u2019 --linear_seqs \u20180\u2019 --min_kmer_frac \u20180.2\u2019 --max_kmer_frac \u20180.95\u2019 --kmer_count \u201810\u2019 --depth_filter \u20180.25\u2019 --start_gene_id \u201890.0\u2019 --start_gene_cov \u201895.0\u2019 --min_polish_size \u20181000\u2019 --min_component_size \u20181000\u2019 --min_dead_end_size \u20181000\u2019 --scores \u20183,-6,-5,-2\u2019\nWhat to do if a few unmodified reruns fail:\n\nMake sure that you have done some QA/QC on your data. FastQC is a great tool for diagnosing what could be done to improve the data quality (example: removing adaptor and quality trimming with Trimmomatic; the presence of other sequencing artifacts or contamination \u2013 both of which can lead to assembly issues). Help for interpreting FastQC output: Babraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence Data (@link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) >> Documentation > Analysis modules Index of /projects/fastqc/Help (@link https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/).\nTry tuning the alignment parameters to better fit the profile of your inputs. Unicycler usage documentation is here: GitHub - rrwick/Unicycler: hybrid assembly pipeline for bacterial genomes (@link https://github.com/rrwick/Unicycler)\n\nReview a Galaxy GTN tutorial that covers the tool (bacterial assembly, but could give usage cues, even if assembling some other type of organism): Unicycler Assembly (@link https://galaxyproject.github.io/training-material/topics/assembly/tutorials/unicycler-assembly/tutorial.html)\n\n\n@quote\nSegmentation fault\n^^ can be due to many reasons, including a job that is too big to run with the given inputs/parameters. The dataset sizes seem Ok, so if this turns out to be the reason, there is more going on content-wise. QA/QC might address that so be sure to not skip doing it. I know these worked independently, but combining the two types of inputs will impact how the overall assembly is processed.\n\nAfter trying the above, and the jobs still fail after a few reruns/parameter tunings, please send in a bug report from one of the failed assemblies and we can review the full error message, input data, settings, et cetera and offer more advice. Please include a link to this post in the comments and be sure to leave the error output and all input datasets undeleted. If the history contains multiple failed jobs (it should IF you have tested different parameters/reruns), leave all of those undeleted as well). Also, please leave your FastQC results undeleted so we don\u2019t have to run the tool (that is usually where I start when troubleshooting analysis problems involving fastq data, unless something more obvious is clearly going wrong, meaning: tool usage problems or server issues).\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI am trying to connect to the Galaxy ftp server via command line. I followed the tutorial (@link https://galaxyproject.org/ftp-upload/); however, it is not working.\nWhen I try:\n$ lftp -u user@email.edu ftps://usegalaxy.org\nI get the error, \u201clftp: @link http://usegalaxy.org: Name or service not known\u201d\nI can successfully run:\n$ lftp -u user@email.edu htpps://usegalaxy.org\nbut when I try to transfer files I get the following error, \"\nput: Fatal error: gnutls_record_send: Error in the push function.\"\nNot sure what is going on, so I am reaching out to determine how best to connect to the Galaxy ftp so i can upload my files to my Galaxy account.\nThanks!!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nPlease try again. I just tested both methods and they work correctly with a few simple commands:\n$ put my_file\n< chat from the server about the transfer >\n$ ls\n< my_file is listed >\n\nUsage from FAQ @link https://galaxyproject.org/ftp-upload/\nFor explicit FTPS:\n$ lftp -u user@email.edu usegalaxy.org\n\nFor implicit FTPS:\n$ lftp -u user@email.edu ftps://usegalaxy.org\n\nIf you cannot get line-command ftp to work, using a client like Filezilla is another choice. How-to is on the same FAQ.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, i am trying to learn galaxy as i think it can be very useful. I am currently trying to use it for QIIME2.\nI have my paired end sequences which are joined ( archaea practice nano) and my metadata but I have no idea how to import them in Galaxy as a QIIME2 artefact using the qiime2 tools import . There is a tutorial but that only shows you how to copy and paste the tutorial data.\n\nimage1635\u00d7897 62.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c629961121f03987709b4d60e09ae6de67c8b9e4.png)\nCan anyone offer any advice?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tool author is working to improve the way the import tool functions for the QIIME2 suite when in Galaxy. Details: Lint failure for deprecated feature, need advice on the best way to transition QIIME 2 Tools (@link https://help.galaxyproject.org/t/10206)\nFor now, you\u2019ll need to try to match up your data with the descriptions available, or try some guesses for the others. The tool form will change when different options are selected, which can offer more clues.\nIt looks like you have some type of paired-end read data, in a collection (aka \u201cdirectory\u201d), along with some metadata files, so that is where I would suggest starting. It may take a few tries to see which fits best. The author\u2019s help site may help more. Find it linked all the way down in that help section.\nMaybe @system has more specific help?\nXref: TSV upload problems - #4 by jsaintvanne (@link https://help.galaxyproject.org/t/tsv-upload-problems/10277/4)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI used SnpEff download (Galaxy Version 4.3+T.galaxy2) to download the database SnpEff4.3 GRCh37.75  for vcf annotation. However, I have difficulties to find the information of the ensemble build used? I would be happy for help!\nThanks!\nAll the best,\nRose"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis is the ENSEMBL release used to build the SnpEff database: release 75 (@link http://ftp.ensembl.org/pub/release-75/). However, I recommend you to use the GRCh38.86 database, which is the most recent one.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "dear when i do this operation the galaxy produce this error \u201cFATAL: container creation failed: mount\u201d what mean?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis error is caused by a temporal problem in the server. Sorry for the inconvenience.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to filter reads with skipped regions/spliced alignments which have N bases in the cigar string. I\u2019m using the following tool: Filter BAM datasets on a variety of attributes (Galaxy Version 2.5.1+galaxy0). I selected the option to filter on cigar string but do not know how to give a command to filter only reads with the \u2018N\u2019 base. Does anyone know how to do this?"
    },
    {
      "role": "system",
      "text": "Answer: use the term *N*\nScreenshot:\n\nfilter-bam-cigar-example1690\u00d71440 192 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8274184310033c7db877f42e9aebad66e78e91a1.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "please what the differences between the using of cutadapt option and trimmomatic when we want to do trimming after checking  the quality score?\nI will appreciate your help"
    },
    {
      "role": "system",
      "text": "Hi @user,\naccording to this paper (@link https://www.frontiersin.org/articles/10.3389/fbioe.2020.00817/full), both tools perform similarly regarding data quality after processing. However, you should take into account that Trimmomatic is specifically designed for processing Illumina paired-end and single-ended data (@link https://github.com/usadellab/Trimmomatic#running-trimmomatic). If your datasets have been generated by using a different technology, you should use Cutadapt.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve tried to use HISAT2 on some of my Cutadapt output files, however the summary files from HISAT2 are always empty for my 48 files (i.e. 0 lines).  I tried re-uploading my data and starting the analysis again, re-running HISAT2 multiple times, and even trying the files on the main galaxy site (which didn\u2019t work either).  Strangely, I got a \u2018normal\u2019 summary file just once after re-running HISAT2 on one file, but I deleted it thinking the problem had been solved to re-run the tool on all the samples again.\nThe genome file I\u2019m aligning to has been successfully used in previous analyses, and the bam file from HISAT2 is fine for downstream analyses.  Can anyone give me some insight on what I can do to fix this?\nThe stderr output is:\nSettings:\nOutput files: \"genome..ht2\"\nLine rate: 6 (line is 64 bytes)\nLines per side: 1 (side is 64 bytes)\nOffset rate: 4 (one in 16)\nFTable chars: 10\nStrings: unpacked\nLocal offset rate: 3 (one in 8)\nLocal fTable chars: 6\nLocal sequence length: 57344\nLocal sequence overlap between two consecutive indexes: 1024\nEndianness: little\nActual local endianness: little\nSanity checking: disabled\nAssertions: disabled\nRandom seed: 0\nSizeofs: void:8, int:4, long:8, size_t:8\nInput files DNA, FASTA:\ngenome.fa\nReading reference sizes\nTime reading reference sizes: 00:00:03\nCalculating joined length\nWriting header\nReserving space for joined string\nJoining reference sequences\nTime to join reference sequences: 00:00:01\nTime to read SNPs and splice sites: 00:00:00\nUsing parameters --bmax 48513222 --dcv 1024\nDoing ahead-of-time memory usage test\nPassed!  Constructing with these parameters: --bmax 48513222 --dcv 1024\nConstructing suffix-array element generator\nConverting suffix-array elements to index image\nAllocating ftab, absorbFtab\nEntering GFM loop\nExited GFM loop\nfchr[A]: 0\nfchr[C]: 72099572\nfchr[G]: 129346882\nfchr[T]: 186624101\nfchr[$]: 258737183\nExiting GFM::buildToDisk()\nReturning from initFromVector\nWrote 90670663 bytes to primary GFM file: genome.1.ht2\nWrote 64684300 bytes to secondary GFM file: genome.2.ht2\nRe-opening _in1 and _in2 as input streams\nReturning from GFM constructor\nReturning from initFromVector\nWrote 117491317 bytes to primary GFM file: genome.5.ht2\nWrote 65856150 bytes to secondary GFM file: genome.6.ht2\nRe-opening _in5 and _in5 as input streams\nReturning from HierEbwt constructor\nHeaders:\nlen: 258737183\ngbwtLen: 258737184\nnodes: 258737184\nsz: 64684296\ngbwtSz: 64684297\nlineRate: 6\noffRate: 4\noffMask: 0xfffffff0\nftabChars: 10\neftabLen: 0\neftabSz: 0\nftabLen: 1048577\nftabSz: 4194308\noffsLen: 16171074\noffsSz: 64684296\nlineSz: 64\nsideSz: 64\nsideGbwtSz: 48\nsideGbwtLen: 192\nnumSides: 1347590\nnumLines: 1347590\ngbwtTotLen: 86245760\ngbwtTotSz: 86245760\nreverse: 0\nlinearFM: Yes\nTotal time for call to driver() for forward index: 00:01:50\nError, fewer reads in file specified with -1 than in file specified with -2\nterminate called after throwing an instance of \u2018int\u2019\n(ERR): hisat2-align died with signal 6 (ABRT)\n[bam_sort_core] merging from 40 files and 1 in-memory blocks\u2026"
    },
    {
      "role": "system",
      "text": "Merging with Bowtie2 error-bowtie2-align died with signal 6 (ABRT) (@link https://help.galaxyproject.org/t/bowtie2-error-bowtie2-align-died-with-signal-6-abrt/8795)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello community!\nI have tried to use EasyFig on Windows and on MacOS but neither of them works. It seems that is a blast issue, but in several forums people are facing the same problem. I used to run EasyFig on my MacOS but after the Monterey 12.5 a lot of things stopped working.\nThus I was wondering whether is possible to include EasyFig on Galaxy?"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tool is wrapped for Galaxy and available at this public server.\n\n\n\n@link https://galaxyproject.org/use/center-for-phage-technology-cpt/\n\n\nCenter for Phage Technology (CPT) - Galaxy Community Hub (@link https://galaxyproject.org/use/center-for-phage-technology-cpt/)\nAll about Galaxy and its community.\n\n\n\n\n\nIt is a new wrapper (@link https://toolshed.g2.bx.psu.edu/view/cpt/cpt_easyfig/2139c39b727e), so should work at that site. If you have problems with it, contact that domain-specific server admin for the quickest feedback. Or, if you want the feedback public, you could post here (new topic!) then share the link with the admins and request they answer here to help build up some knowledge about the server and hosted tools. You could also post back the answer they provide if you think it would help others.\nHow to move data between Galaxy servers (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#transfer-entire-histories-from-one-galaxy-server-to-another). Moving individual datasets, not entire histories, is probably best for this case. Since you are currently working at @link http://UseGalaxy.eu, remember to set the history to a \u201cshared by URL\u201d state before capturing the dataset link. Or upload the data directly from your computer if it is there."
    }
  ],
  [
    {
      "role": "user",
      "text": "In stringtie merge, do we merge the replicates of the treatment samples"
    },
    {
      "role": "system",
      "text": "In short, the answer is usually \u201cyes\u201d. The tool can do this always but whether you want to or not depends on your analysis details.\nUsage and which inputs to include (sample GTFs/known GTF) depend on whether you are going to perform the DE analysis on only known transcripts/genes, or on only discovered (sample) transcripts/genes, or on both.\nStingtie Merge can be used to do the following:\n\n\nPre-process an existing reference annotation GTF dataset, by itself, so it can be used with Stringtie to guide assembly, and with other downstream tools. Not all public GTFs are formatted in a way other tools can interpret and this tool can be used to \u201cgroom\u201d a GTF.\n\n\nCombine the per-sample GTF results of Stingtie, and optionally the reference GTF (knowns), into a unified GTF assembly that can be used with downstream differential expression tools.\n\n\nMore details that summarize the different use-cases are in this prior Galaxy Biostars Q&A post: Question: StringTie and StringTie merge - when to apply the Guide gff (reference annotation file)? (@link https://biostar.usegalaxy.org/p/29938/).\nPlease also see the Stringtie tool manual (@link http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual#de).\nIt would probably also help to review the Galaxy Tutorials for DE analysis. Several of the RNA-seq protocols include it \u2013 and importantly, HISAT2 (settings to create Stringtie-readable BAMs).\n\n\nGalaxy Tutorials for Galaxy Main (@link https://galaxyproject.org/learn/): Start here: RNA-seq: Discovering and quantifying new transcripts (@link https://galaxyproject.org/tutorials/nt_rnaseq) - an in-depth transcriptome analysis example\n\n\nGalaxy Training Network Tutorials  (@link https://galaxyproject.github.io/training-material/): Some GTN  (@link https://galaxyproject.org/teach/gtn/) tutorials are appropriate for Galaxy Main (@link https://usegalaxy.org) and some are not. Where you can run each is noted per tutorial \u2013 click on the Galaxy instances gear icon galaxyserversGTN to review the Public Galaxy (@link https://galaxyproject.org/use/) server choices. If a tutorial is supported by a pre-configured Galaxy Docker (@link https://hub.docker.com/r/bgruening/galaxy-stable/) training image, instructions for how to get it will be listed below the tutorial listings, per category.\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hellow everyone\nI work with my colleagues on RNA-seq human data where we are trying to analyze these data on @link http://usegalaxy.eu. we have +50 raw single-end sequencing data files. We have started our analysis by running FastQC on all data files with no problem. Now we are trying to run \u201cTrimmomatic flexible read trimming tool for Illumina NGS data\u201d on the data files to cut the first 5 bases but we are getting the error in attached image for 8 of these data files. We have tried to run these 8 data files on @link http://usegalaxy.org to see if the files will run correctly but we got the same error. Hope you can help us to troubleshoot this problem.\n\nIMG_20220728_0936321920\u00d71440 267 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/ac124b0e32b62da31596cba7601c2e29ee7df4c2.jpeg)"
    },
    {
      "role": "user",
      "text": "Problem solved"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am experiencing very slow speed of @link http://galaxy.org, I am not able to do a basic mapping using BWA-MEM application, has anyone experienced similar problem?"
    },
    {
      "role": "system",
      "text": "Hi @user\nYes, the clusters are very busy at @link http://UseGalaxy.org after a small server side issue detected/fixed today.\nJobs will still run in the order submitted. The best strategy is to leave your jobs queued to preserve the original queue placement.\nI\u2019m going to close this topic out to avoid duplicated posts.\nPlease follow this ticket instead for updates: usegalaxy.org is super slow: 11/8/2022 Confirmed job delays, please leave queued jobs queued - #3 by jennaj (@link https://help.galaxyproject.org/t/usegalaxy-org-is-super-slow-11-8-2022-confirmed-job-delays-please-leave-queued-jobs-queued/8986/3)\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nwould it be possible to have the Arabidopsis thaliana TAIR10 genome installed for HISAT2?\nthanks a lot\nbest"
    },
    {
      "role": "system",
      "text": "Hello @user\nThat genome has not been indexed for HISAT2 at @link https://usegalaxy.org. We do plan to do that as part of a larger project but it won\u2019t be completed for a bit more time.\nYou don\u2019t need to wait. There are two choices:\n\nUpload the genome in fasta format and use it as a Custom Reference genome with HISAT2 (and any other tool it is not indexed for) at Galaxy Main @link https://usegalaxy.org. How-to: Preparing and using a Custom Reference Genome or Build (@link https://galaxyproject.org/learn/custom-genomes/). The exact genome used in Galaxy is here (a good place to source from, since the genome is already indexed for some other tools, and using the same exact genome for all steps in the analysis is very important): @link http://datacache.galaxyproject.org/\n\nUse the Galaxy EU @link https://usegalaxy.eu/ server instead, they do have the genome indexed for HISAT2 already.\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am working on galaxy 101 tutorial. I have shared my history below.\n@link https://usegalaxy.org/u/ashgourd/h/galaxy101-29mar22\nMy workflow is different from what I am seeing in the tutorial especially in the first step\u2026 Appreciate your help. Thanks"
    },
    {
      "role": "user",
      "text": "Figured out. Thanks"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI ran Kallisto and Dsse2 to get my list of DE genes. In the results of Dseq2, I can see there are about 20000 lines - I am assuming 1 line for each gene. However, when I download the file, I only get 9000 lines. Can you please let me know why this is happening? And how can I download the entire dataset of 20000 lines?"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry the create/download \u201cHistory Archive\u201d method.\nMore details are in this prior Q&A:\n@quote\n@systemYes, the data is large. The connection dropped for me too. \nI remembered that this has come up before. See this prior Q&A on how to \u201cresume\u201d downloads with curl and wget and see if one of those works for you. \n\n\nYour other option is to create a History archive and download that. Once uncompressed, you\u2019ll find your data inside of it. Tip: Copy just the data you need into a new history to make it smaller/faster to process this way. Copies of dataset you already have in your account do not c\u2026\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Can anyone tell me how to find \u201cMap with Bowtie for Illumina\u201d from the tools? I searched \u201cBowtie\u201d in the tool search, I only got \u201cBowtie2\u201d and no \u201cMap with Bowtie for Illumina\u201d.  However according to what I learned, I need use Bowtie version1 since my chip-seq sequence are short than 50bp and single ended."
    },
    {
      "role": "system",
      "text": "Hi @user\nBowtie version 1 has been deprecated by the original tool authors.\nAlternatives include: Bowtie2, HISAT2, BWA and minimap2\nMaybe test out mapping with several tools and compare what works best for your specific data?\nTutorials\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/epigenetics/)\n\n\nGalaxy Training: Epigenetics (@link https://training.galaxyproject.org/training-material/topics/epigenetics/)\nDNA methylation is an epigenetic mechanism used by higher...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am new to exome sequencing analysis. I have been looking into the theory and tried using galaxy for the same. I got 2 mutations following a published pipeline from the shared data (which uses freebayes) but only one of them was confirmed by Sanger Sequencing.\nLooking back at my SNP-Sift output, I see that although the unconfirmed variant is of good quality [\u201cQUAL\u201d is good], the CIGAR string for that variant is just represented as \u201cCIGAR=1X\u201d.\nFrom a theoretical point of view, what I could understand is that , CIGAR strings is generally represented as some matches, then some mismatches/insertion/deletion and then some matches. So, I am unable to understand how it can just be \u201cCIGAR=1X\u201d. If it is possible, please tell what does it say about the quality of that variant.\nAny help would be appreciated.\n-Debanjan Roy"
    },
    {
      "role": "system",
      "text": "Hi,\nif the variant in question is a single nucleotide change, then CIGAR=1X is just fine though also totally meaningless. CIGAR strings in the VCF INFO field (because that\u2019s what I think we are talking about here) describe how to align an alternate allele to the reference allele. If your REF and your ALT allele both have a length of 1 nt, then the only possibility to align the two is with one mismatch, which is exactly what 1X means.\nWhat you might confuse this with is the CIGAR string associated with aligned reads in a BAM file. This version describes how to align the read to the reference sequence and that\u2019s what would probably be more relevant for assessing the quality of the variant call. However, it\u2019s typically easier to look at reads aligned to the reference in a genome browser (like IGV) than to do it manually by inspecting CIGAR strings one by one."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am having issues proxying my production server of Galaxy. I have set up NGINX to proxy as described in the admin documentation and can access galaxy via the proxy however, it shows up as plain text in the web browser without any graphics(see attached image). I can click on links and navigate through Galaxy, but it\u2019s difficult to do anything. Any help would be greatly appreciated. Screen Shot 2020-12-29 at 11.00.23 PM1570\u00d71692 275 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0b98e23506129caaadc9ff85983d3d1a5bd8913d.png)"
    },
    {
      "role": "system",
      "text": "Do you have this part in your config?\n        # serve framework static content\n    location blue {\n        alias $galaxy_root/static/style/;\n        expires 24h;\n    }\n    location static {\n        alias $galaxy_root/static;\n        expires 24h;\n    }\n    location robots.txt {\n        alias $galaxy_root/static/;\n        expires 24h;\n    }\n    location favicon.ico {\n        alias $galaxy_root/static/;\n        expires 24h;\n    }\n\nAnd do you have set the right location for $galaxy_root"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to analyze my Infinium Human Methylation BeadChip data, and I have seen this is a possibility using the \u201cInfinium Human Methylation BeadChip\u201d tool. This is the tutorial I found: Infinium Human Methylation BeadChip (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/ewas-suite/tutorial.html)\nI am confused about which Galaxies this tool is available on. I can not see the tool on my current instance using @link http://usegalacy.org.\nPlease let me know if it is possible to use the \u201cInfinium Human Methylation BeadChip\u201d tool, and which Galaxy I should be using.\nThank you very much for your help!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can find that tool in the @link http://usegalaxy.eu instance: Infinium tool (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/kpbioteam/ewastools/minfi_analysis/2.1.0).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "First I need to select variants that are exonic and not in the 1000 genomes project. The 100G is coded in columns 11 to 16 as a \u201c.\u201d or \u201c\u201d used when its missing and whether its exonic or not in column 6. I uploaded a wANNOVAR csv and converted it to tsv. Im the usibf the regular filter option and no matter how many variations of the below I use it wont work.\n6==\u2018exonic\u2019 and c11==\u2018.\u2019 and c12==\u2018.\u2019 and c13==\u2018.\u2019 and c14==\u2018.\u2019 and c15==\u2018.\u2019 and c16==\u2018.\u2019\nIm also supposed to do a simple non coding variant filter on a vcf file from snpeff eff but I not see a column or any data specifically indicating exons or 1000G. How can I do ths?\nNext i tried collectinsertsizemetrics and Im getting this error\n/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/048/054/48054991/_job_tmp -Xmx7g -Xms256m\n00:13:00.688 INFO NativeLibra\nNEED help URGENTLY"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nMaybe you found these already?\n\nSince the data is in tabular format, you can use general text manipulation tools to perform filters.\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n\n\nGalaxy Training: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n\nAnd, variant analysis tutorials are here.\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\n\n\nGalaxy Training: Variant Analysis (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\nIf you are having problems with a specific tool, please ask a new questions and include more details so the community here can help. Items like the server where you are working, the exact tool name/version, parameters, and example data lines provide context. All would be included in a shared history link or you can post that information directly.\nGuidance is here:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)\n\n\nGalaxy Training: Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Everyone,\nI want to run Racon on Canu output files. It is my first time running this pipeline, hence I have a few basic questions:\n\nFor Racon\u2019s \u201cOverlap\u201d field, which file of Canu should be used?\nI see that all outputs of Canu are in fasta format. But Racon needs the input format in MHAP/PAF/SAM for \u201cOverlap\u201d field. Do I need to change the file format? If yes, which format should I use?\n\nThanks :smiley:"
    },
    {
      "role": "system",
      "text": "Hi @user,\nRacon requires to be provided with the mapping of the reads to the assembly generated by Canu. You can generate it by using minimap2.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI followed the tutorial \u2018Visualisation with Circos\u2019 and tried both hg18_karyotype_withbands.txt (custom) and Human hg18 (preset). After I ran the tool I obtained the following error in both cases:\nCan\u2019t call method \u201ccolorExact\u201d on an undefined value at /usr/local/tools/_conda/envs/mulled-v1-3cf5c7d734898be0f8d76d0fbf3face5f99ca4b3a9b294235f9271dd27115368/bin/\u2026/lib/Circos/Colors.pm line 348,  line 886.\nHere\u2019s my history: Galaxy | Europe (@link https://usegalaxy.eu/u/03f5b39cf2494d8481d02cede3edbd0b/h/circos)\nPlease, can you help me with that?\nAdam\nEdit: tried 0.69.8+galaxy9, 0.69.8+galaxy10, 0.69.8+galaxy12"
    },
    {
      "role": "user",
      "text": "Hi,\nthanks @system\nThe version on @link http://usegalaxy.org seems to be working fine.\n@quote\nPlease submit a bug from the red error dataset to let those administrators know about the problem.\nI have submitted it.\nAdam"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI tried to analyse my recently performed a MD simulation using GROMACS following a tutorial (Running molecular dynamics simulations using GROMACS (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/md-simulation-gromacs/tutorial.html) ).\nParameters of the MD simulation were slightly modified to have shorter simulation (because my protein is around 350 residues).\nI then tried to analyse the results following Bio3D tutorial (Analysis of molecular dynamics simulations (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/analysis-md-simulations/tutorial.html)).\nEverything seemed to work fine exept when I tried to analyse the results. In fact lauchning RMSD Analysis using Bio3D resulted in \" /corral4/main/jobs/046/826/46826281/tool_script.sh: line 9: Rscript: not found \" alias \" Fatal error: Exit code 127 ()\"\nDo you have an idea why?\nThank you so much,\nMartin\nWorkflow : Galaxy (@link https://usegalaxy.org/u/martinveinsteinorg/w/221114-md-test-fgfr-rsk-gromacs-v3)\nDataset :87414315 (bbd44e69cb8906b5a3ee8b8c60394a49)\nHistory : 6677271 (ba081d172fac65b7)"
    },
    {
      "role": "system",
      "text": "Resolved, the tool is available at all of the UseGalaxy.* servers."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody,\nI have paired end fastq files FO_1 and FO_2 (forward and reverse, respectively). I want to use them with qiime2. I uploaded them successfully to galaxy, however when I choose qiime2 tools import I receive error messages. The options that I am using are: SampleData[PairedEndSequencesWithQuality], Paired End fastq Manifest Phed33V2. My colleague was able to import the same two files to qiime2 on his machine using command-line codes (native qiime2), but I am unable to import them to qiime2 on galaxy. Please help. Many thanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nWe are troubleshooting this tool in another thread. Please read through that about why this particular tool has been troublesome, and current advice. Creating QIIME2 artefacts - #13 by Martyn (@link https://help.galaxyproject.org/t/creating-qiime2-artefacts/10388/13)\nLet\u2019s combine the two."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to login and get data using \u2018@link http://usegalaxy.org\u2019.\nHowever, I was not able to login and get message as \u2018Uncaught exception in exposed API method:\u2019"
    },
    {
      "role": "system",
      "text": "Hi @user\nYou created at least four accounts yesterday at @link http://UseGalaxy.org. Those email addresses cannot be used for a new account at @link http://UseGalaxy.org because you already created accounts using those in the past at @link http://UseGalaxy.org (plus many others).\nThere is a quota of one account per person per public Galaxy server. Everyone can have one account at each server \u2013 but no more than one account at any. These are free public shared services. When someone uses multiple accounts they are competing for resources not only against themselves, but creating an unfair resource competition environment for everyone else working at the same public server.\nAnyone using multiple accounts to subvert the public quotas has their accounts administratively removed. There is not a special notification when administrative action is applied. Instead, the reminders about terms of service are directly on the account registration form and included directly in account activation emails exactly where the link to confirm the account is located. We want everyone to understand and not have problems.\nOptions\n\nIf you have one single active account at @link http://UseGalaxy.org, keep using that one.\nIf you have two or more active accounts at @link http://UseGalaxy.org, please manage your data into one account and delete the others under User > Preferences.\nCreating a single new account with a different email address is OK if you do not have any active account(s) at @link http://UseGalaxy.org.\n\nFAQs\nOur goals are to help you successfully use public Galaxy servers and avoid future multiple account problems.\n\nRegistering Accounts - Galaxy Community Hub (@link https://galaxyproject.org/support/account/)\nAccount quotas - Galaxy Community Hub (@link https://galaxyproject.org/support/account-quotas/)\n\nIf you need to use Galaxy differently, please review\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\n\nThe GVL Cloudman version is a single or multi-user choice and AWS offers grants. Amazon Web Services (AWS) - Galaxy Community Hub (@link https://galaxyproject.org/use/aws/) && AWS Programs for Research and Education (@link https://aws.amazon.com/grants/)\n\nThe AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. AnVIL - Galaxy Community Hub (@link https://galaxyproject.org/use/anvil/)\n\n\nThank you for understanding, and for helping us to keep @link http://UseGalaxy.org a free and fair public shared resource for researchers worldwide.\nGalaxy Team"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am attempting to complete the trimmomatic step, I keep receiving the error below:\nPicked up _JAVA_OPTIONS: -XX:MaxPermSize=2G -Xmx6G -Xms1G -Djava.io.tmpdir=/data/2/galaxy_db/tmp -Duser.home=/data/2/galaxy_db/tmp\nOpenJDK 64-Bit Server VM warning: Ignoring option MaxPermSize; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning:"
    },
    {
      "role": "system",
      "text": "Hi @user\nI checked Trimmomatic in Europe with SE and PE data: no issues detected.\nMaybe consider doing a relevant tutorial, eg Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI have two tools \u201cRStudio\u201d and \u201cInteractive JupyTool and notebook\u201d in my history, but when I want to call them from the Active Interactive Tools section,I face with the message you do not have active interactive tools yet\nHow can I use these two tools?"
    },
    {
      "role": "system",
      "text": "How long ago did you start the interactive tools? If it\u2019s been a long time you may have gotten shut down (for example for server maintenance) and restarting the tools should help. I just started Rstudio and it all seemed to work ok for me at least"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi - looking for any advice or solution. I am running some workflows and an unable to complete them because a step crashes giving the \u201ccannot allocate memory\u201d error. However, it randomly occurs during different steps in the workflow. None of my steps are very demanding, they are simple things like Sort or Join Two Datasets, although I am doing a blast search on a transcriptome that I uploaded. Is there a memory cache I need to purge somewhere? I have purged deleted files and am nowhere near by disk quota.\nThanks for any help, I am stuck."
    },
    {
      "role": "system",
      "text": "Welcome, @user!\nThe memory used to execute jobs is different from the memory used to store account data. How you describe the problem has in the past indicated that the jobs are approaching the memory limit of what the public server can process. In short, sometimes the jobs hit a cluster node that can execute successfully and sometimes not.\nSorting data can require quite a bit of memory, depending on the datatype and size of the dataset. There are a few different \u201csort\u201d tools, so I can\u2019t offer many more details without knowing which you are using.\nThat said, a highly fragmented transcriptome can easily exceed resources (BAM or tabular results). Filtering out smaller target sequences can often help, especially if they are too small to capture meaningful hits anyway. Modifying the hit parameters can also help (more stringent criteria). In a BAM result, the header can get very large (includes each target), causing a Sort BAM type of job to fail at the indexing or sorting steps. Often tabular output is a better choice. Only \u201chits\u201d are retained, that can be later filtered to reduce redundancy, before doing further manipulations that compare the data to other data (join functions). Tools to filter might include: Select, Filter, Sed, Awk, etc. Blast can create a great deal of \u201ccommon key\u201d, \u201ccoordinate overlapping\u201d, and \u201clow specificity\u201d hits. Sometimes running with strict parameters first, then remapping whatever didn\u2019t map originally, at a lower stringency threshold, is a good strategy to avoid generating too much output (often these are uninformative sub-hits).\nJoin Two Datasets can also be memory intensive, both with processing and with handling the output. It is possible to create pathologically large outputs where each row of the first dataset matches with each row of the second. We do recommend inputting the larger of the two datasets second on the tool form to maximize how the tool uses resources. You might need to modify your query to restrict the output/be more specific or split up the larger dataset into chunks first (tools might include: Line/Word/Character count, Select first/last lines, Remove beginning of a file), then run the join in distinct jobs, and at the end combine the results (tool: Concatenate).\nIf it turns out that you need your jobs processed as-is, and they are exceeding resources on the public Galaxy Main server at @link https://usegalaxy.org (or any usegalaxy.* server), the option to move to your own cloud, docker or local Galaxy is available. Enough job processing memory resources would need to be allocated, and that is usually easier with a cloud solution for a few reasons (less administrative setup, custom high-memory cluster choices). Keep in mind that Galaxy itself is always free, but commercial cloud services are not (although AWS does offer grants for education/research).  You could also check to see if your affiliations include access to academic cloud resources. If interested, please see:\n\n@link https://galaxyproject.org/choices/\n@link https://galaxyproject.org/use/\n\nFAQs: @link https://galaxyproject.org/support/#troubleshooting\n\nMy job ended with an error. What can I do? (@link https://galaxyproject.org/support/tool-error/)\nStart here since it doesn\u2019t sound like data format problems are a factor: @link https://galaxyproject.org/support/tool-error/#type-exceeds-memory-allocation\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, my Unicycler jobs at @link http://usegalaxy.org are stalled at \u201cThis job is waiting to run\u201d for 12 hours. Looking through past issues, this may be a PCS Bridges issue. Any advice on getting these to run?\nadmin-edit: Remove end user\u2019s email address for privacy"
    },
    {
      "role": "system",
      "text": "Welcome @user\nIt is true that certain tools only run at PCS Bridges and that cluster\u2019s queue can get quite long, requiring a longer-than-usual wait (as compared to other tools/clusters).\nPlus, right now that cluster is down for scheduled maintenance. Details: @link https://portal.xsede.org/user-news/-/news/item/11554\nTools that run at PCS Bridges:\n\nRNA-STAR\nSTAR-Fusion\nTrinity\nUnicycler\n\nThe best strategy is to:\n\nQueue up jobs that run at this cluster as soon as you can. Batch processing features such as Workflows (and Dataset Collections) can aid with this.\nPlan for the tools above taking a bit longer to complete than most other tools.\nAvoid deleting/rerunning \u2013 that will only put your new job back at the end of the queue, extending the wait time, each time it is done.\n\nIn short, queue up jobs and allow them to stay queued until execution is completed. These tools are computationally expensive and do not run well on other clusters. Practically, this means the turnover for cluster availability is a bit slower, but the resources (memory + runtime allocations) are higher. A good thing, in the big picture, but requires some patience.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019ve been having a similar issue for a few days now. I\u2019ve been trying to upload a file but it keeps stopping and showing the red error message \u201cWarning: Internal Server Error (500)\u201d. I\u2019ve tried rerunning the upload and have made sure there are no connection issues but the upload is still failing.\nPlease advise, and thank you in advance.\nKimia"
    },
    {
      "role": "system",
      "text": "Welcome @user !\nThanks for asking about this problem. It is a temporary server-side issue impacting jobs, not just Upload, and the message is not always reported within the failed dataset itself in the History view. But it is on the Job Details page under stdout or stderr reports (\u201ci\u201d icon).\nWhat to do: Try a rerun. For this and any other jobs that fail in a similar way.\nHow to know if this is your issue: Check the stderr report.\nFor errors like this one per dataset \u2013 reported in the stderr at the very end:\n\nOSError: [Errno 122] Disk quota exceeded\n\nLikely related: Uploads fail directly within that tool, before the data is loaded into the history.\nThat said, it seems you were not getting that far into the processing, and the Upload tool is reporting a problem instead.\n\nWarning: Internal Server Error (500)\n\nBoth types of errors are most likely related to the same root issue right now.\nIf there is an actual problem with the file itself (contents or transfer), that same error in the Upload tool can result.\nSo, double-check your local data first for any obvious problems (truncated file, compression problems). If Ok, then try a rerun letting Galaxy set the datatype (auto-detect, the default), and consider using FTP to load data if your connection is slower and/or the file is very large.\nDetails:\nThe errors are related to administrative tasks running in the background at @link http://UseGalaxy.org. Specifically, we have been migrating data to a larger storage system. The prior is full \u2013 resulting in odd transient errors like those above for a fraction of jobs. Rerun those tools.\nApologies for the confusion, and thanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nHas anything changed in uploading files via the browser? or is there a temporary outage?\nAfter dragging the files (small bam files from my local PC), there is a rolling circle but upload does not begin.\nThanks for your help"
    },
    {
      "role": "system",
      "text": "There were some server issues around the time this question was asked. All should be resolved by now.\nHistorical and ongoing usegalaxy.* server status: @link https://status.galaxyproject.org/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been working on galaxy for quite some time. my account is deleted suddenly.\nI have all my work there. could you please help me restore my account.\nemail: (admin redacted for privacy)\nusername : supriyan\nThank you"
    },
    {
      "role": "system",
      "text": "Hello @user\nCreating multiple accounts in order to circumvent our data and job quotas degrades the service for all other users. Excess accounts are administratively removed (purged) at this server to ensure fair access to the free public resource.\nIf the terms of service seem unclear, go back and review the account activation emails you received for each (the email with the link). You can also log out and look at the account registration form again, or review the terms under the Help menu. @link https://usegalaxy.org/static/terms.html\n\nregistation1658\u00d7930 170 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d9abe11d57a5fb1e4296eeb594c5da42b2341ca2.png)\n\n\nCurrently, you still have at least one active account.\nIf you want to switch which account is active, and can agree to use the public resource fairly going forward, reply-all back to our existing email conversation (not publicly here) and include your answers for the four questions in this topic Accounts deleted from Galaxy Main to enforce posted Terms and Conditions - #18 by jennaj (@link https://help.galaxyproject.org/t/accounts-deleted-from-galaxy-main-to-enforce-posted-terms-and-conditions/1429/18). At this point, that even being possible is extremely time sensitive but we can try. Please follow up by email."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI tried to install some tools to my Galaxy instance from the \u201cadmin\u201d page, but found all of them stuck on \u201ccloning\u201d. And there is no error in the \u201cerror log\u201d page. Does anyone have a clue on it? I am using Galaxy 20.09 on GVL 5.0 beta 5.\nThank you!"
    },
    {
      "role": "system",
      "text": "When installing tools on the GVL, you must use the following values under the advanced settings during tool installation. Otherwise, the installation will not work correctly as you\u2019ve seen.\n\nimage900\u00d7566 45.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/cce811a7174214e094ca537d619cef8e7e534771.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nmy username does not exist anymore. apparently.\nWhat to do?\nBest regards,\nAntonello"
    },
    {
      "role": "system",
      "text": "Hi @user\nFor account help, please write in to the administrators of the Galaxy server where you were working from the email address used for account registration.\n\n\nIf you are working at @link http://UseGalaxy.org, the contact email is: @link mailto:galaxy-bugs@lists.galaxyproject.org\n\n\nIf working somewhere else, the contact information for most public Galaxy servers is usually on the homepage of the server or included in their directory listing here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use). The information is also (probably) in the original account activation email \u2013 for both public or private servers.\n\n\nFAQs @link https://training.galaxyproject.org/training-material/faqs/galaxy/#account\n\n\nIf you cannot find the contact, please post back the URL of the server and we can try to help more."
    }
  ],
  [
    {
      "role": "user",
      "text": "I got my samples sequenced from a commercial facility. They used the following adapter sequences for sequencing\nimage\n|Sample ID|Index i7|Index i5|\n|S0A|GTTCTTCTGT|CCAAGGTATT|\n|S0B|CCAAGGTATT|GTTCTTCTGT|\n|S0C|TGTGGAGAGG|AACGAACCGG|\n|S5A|TTCCTTGAGG|TGCTGTCTTA|\n|S5B|GTTAATTGCG|GAGGAGATAA|\n|S5C|GAGGAGATAA|GTTAATTGCG|\n|S15A|GTTCCTCATT|AATGTCGGTT|\n|S15B|AATGTCGGTT|GTTCCTCATT|\n|S15C|TTGACAGGTA|TGGTGGTTGT|\nI am confused with the nomenclature of adapters. I am trying to use the Cutadapt tool in galaxy, and I am not sure out of the given adapter sequences which are 3\u2019(End) adapters, 5\u2019(Front) adapters, or 5\u2019 or 3\u2019 (Anywhere) Adapters. I also tried using the trimmomatic tool but I am not sure if I should use the TruSeq2 (pair ended) or TruSeq3 (pair ended). Could anyone please guide me through this so that I can start my data analysis? Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user,\ni7 index adapter corresponds to the 3\u2019 adapter and i5 index adapter to 5\u2019 adapter.  Regarding the ILLUMINACLIP options,  it depends on the technology used; according to the Trimmomatic manual (@link http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf) TrueSeq2 is used in samples generated by GAII systems, while TrueSeq3 should be used when the samples have been generated by HiSeq and MiSeq machines."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone, I already used LEfSe in 2018 and have now a new dataset to analyse. I noticed when I upload my .tsv file that it looks different when I view the data with the \u201cview data\u201d function (see pictures attached). In 2018 the table had a grey bar with numbers at the top and it looked like a proper table. Now my data looks like a text file. I was wondering if something is wrong with my file, although LEfSe doesn\u2019t complain. My further analysis says there are no significant discriminative features in my 133 samples. I am not sure if I can trust the results now. Any help and/or advice is appreciated! Lefse_input_20181666\u00d7959 267 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6834b840cdbce1351843b6e97897b50ccacc9480.jpeg) Lefse_input_20201667\u00d7945 397 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d31327085d8bd9ea8df994000540220c8b3c4d99.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI already used LEfSe in 2018 and have now a new dataset to analyse. I noticed when I upload my .tsv file that it looks different when I view the data with the \u201cview data\u201d function (see pictures attached). In 2018 the table had a grey bar with numbers at the top and it looked like a proper table. Now my data looks like a text file. I was wondering if something is wrong with my file, although LEfSe doesn\u2019t complain.\nThe first screenshot is what would be expected for a dataset with the datatype \u201ctabular\u201d or \u201ctsv\u201d assigned.\nThe second screenshot is what would be expected for a dataset with the datatype \u201ctxt\u201d assigned. It is the default view for any plain text file (\u201ctabular\u201d, \u201ctxt\u201d, plus others).\nA very large dataset of many datatypes will revert back to the view shown in the second screenshot (\u201clarge\u201d can differ by Galaxy server). That seems to be what is happening \u2013 the files are different sizes but both have the datatype \u201ctabular\u201d assigned? Meaning, dataset 47 has fewer lines than dataset 56 (?). That would explain the difference in the \u201cView data\u201d display. This really is \u201cdisplay\u201d only and doesn\u2019t change the actual contents of the file/dataset. If you look at the expanded dataset 56 \u201cpeek\u201d view\u2019 in the history panel, you can see that it was small enough to display as \u201ctabular with the column numbers\u201d since only the first 5 lines are used for that function (for practical/design reasons).\nNote: A compressed dataset (of various types) might display as plain text (second screenshot) as well. This is not your situation but seemed worth clarifying for you (other display use-cases) or for anyone else reading this Q&A :slight_smile:\n@quote\nMy further analysis says there are no significant discriminative features in my 133 samples. I am not sure if I can trust the results now. Any help and/or advice is appreciated!\nHow data displays differently (by datatype or size) is part of the larger application settings. That is unrelated to how tools function/execute or wouldn\u2019t lead to any downstream differences analysis results \u2013 the dataset\u2019s content/data itself isn\u2019t modified, and that is what tools use as inputs.\nYou are working at this public server, correct?  Galaxy (@link https://huttenhower.sph.harvard.edu/galaxy/)\nThe server is set up a bit differently than other public servers and hosts custom versions of tools, in particular the Lefse tool suite. Their support/admin team does not follow this forum, but there are other ways to contact them about odd or non-reproducible results. I don\u2019t think that is what your issue is (unless those results are not reproducible, maybe run the analysis again and compare?). But if you have tool concerns or odd results with a rerun comparison, you could contact the Huttenhower support/admin team for more help, especially if you think there is a technical problem unrelated to the display differences. Please see this prior Q&A for how:\n@quote\nWelcome@systemYour header line does not match those in the examples. Scroll down on the form to the help section for the details, including linked example inputs. \nThehttps://huttenhower.sph.harvard.edu/galaxy/public Galaxy server hosts customized versions of the LefSe tools. If you need more help than a tool form provides, contact the server support team directly. This particular server\u2019s support/admin team do not track this forum as far as I know. \nContact information for public Galaxy \u2026\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I wish to upload a file (112 Mb) to @link http://usegalaxy.eu but it failed. After I clicked the \u201cUpload Data\u201d button and uploaded the file, it reached 100% quickly. However, after hours, the file is still loading to be available in the history (job still running and appeared in orange color). May I have your help or advice on this issue? Thanks."
    },
    {
      "role": "system",
      "text": "Hello @user\nThe @link http://UseGalaxy.eu server was undergoing server changes over the last few days. From my experiences during that time frame, the service was either completely inaccessible (with a fall-over landing page stating the service is not available), or I could reach the server and some functions were Ok and others were slower or stalled (Upload \u2013 both by local file and by URL, loading up tool forms \u2013 but other functions could also present as odd) .\n@link http://UseGalaxy.eu now has a banner up on the server with an explanation.\n\n17.05.2022. Since this morning, we are experiencing issues on our main storage. The service is, currently, not reliable. We are working on a solution\n\nIf you notice odd behavior again sometime in the future:\n\nA server restart might cause odd behavior for a minute or two. Refreshing your browser window and starting the problematic action over again is usually enough.\nCheck the overall status for any of the UseGalaxy.* servers and related primary resources, at the publicly posted status page here: @link https://status.galaxyproject.org/\n\nAsk questions at this forum if both of those don\u2019t help, or apply, or if you need more details.\nWhile waiting for a reply, you can work at any of the other UseGalaxy.* servers. One account at each has benefits: different storage/computational server resources plus your account at each has distinct resources (data storage quota).\n\nNote: I\u2019m going to modify the topic title and put this as the solution to make it easier for other people to find your question and the advice in the reply, should they run into a problem at the EU server and are not sure what is going on.\nThanks for posting!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nMy issue is quite strange. I have in total 16 RNA seq data. After trimming, I move on to HISAT2 for alignment.\n6 out of 16 sample give me error while other turned green.\n\"An error occurred with this dataset:\nformat bam database CHM13_T2T_v2.0 (@link https://usegalaxy.eu/datasets/4838ba20a6d867655ec30d60e75bf7bf/details#)\"\n\" Error, fewer reads in file specified with -1 than in file specified with -2 terminate called after throwing an instance of \u2018int\u2019 (ERR): hisat2-align died with signal 6 (ABRT) (core dumped) gzip: stdout: Broken pipe [bam_sort_core] merging from 32 files\ni dont know why it should merging 32 file, I did these pair individullly.\nThank you so much"
    },
    {
      "role": "system",
      "text": "Hi @user\nthis is the key message: Error, fewer reads in file specified with -1 than in file specified with -2.\nPlease check the input files used for the failed jobs. Properly paired reads should have identical number of F and R reads present in the same order. Two common causes: F and R reads are from different samples, or reads were trimmed as single end data resulting in different number of F and R reads.\nThe \u2018merging\u2019 part of the message probably refers to processing temporary files. Many aligners produce small files during mapping, and these files were merged into a single alignment.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI am running a local instance of Galaxy (build 22.05). I installed the latest version of Deepvariant (1.4.0+galaxy0) which installed without any errors. However, when I try to run Deepvariant on BAM files output from HISAT2, the error \u201cFatal error: Exit code 127 ()\u201d comes up. Further, it says that the tool generated the following error: \u201cline 9: run_deepvariant: command not found\u201d.\nWhen I look at the backend to see what process Galaxy is going through, even after installation of the tool, the following line keeps repeating on the command line interface:\nuvicorn.access INFO 2022-12-22 13:43:26,629 [pN:main.1,p:100965,tN:MainThread] 127.0.0.1:58158 - \u201cGET /api/tool_shed_repositories?name=deepvariant&owner=iuc HTTP/1.1\u201d 200\nI don\u2019t understand this error. Could someone please help me out?\nI am running the same job on @link http://Galaxy.eu server and it is running (for a few hours now) but in the local instance in errors out pretty much instantly.\nThanks!"
    },
    {
      "role": "system",
      "text": "Hello, I can\u2019t give a full answer but I can maybe guide you in the right direction and maybe someone that can give a better answer will reply.\nGiven the error it looks like deepvariant is not installed (not found). The tool is using a \u201cdocker tool dependency\u201d, in other words it needs a container where deepvariant is installed. If you have not checked this yet then I think this is the place to start. Below two links where you may find some more information.\n@link https://docs.galaxyproject.org/en/master/admin/special_topics/mulled_containers.html\n\n\n@link https://training.galaxyproject.org/training-material/topics/dev/tutorials/containers/slides.html#1\n\n\nGalaxy Training: Tool Dependencies and Containers (@link https://training.galaxyproject.org/training-material/topics/dev/tutorials/containers/slides.html#1)\nGalaxy is an open-source project. Everyone can contribute...\n\n\n\n\n\nThe requirement can be seen here:\n\n@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\n\n\n@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml\n<macros>\n    <token name=\"@TOOL_VERSION@\">1.4.0</token>\n    <token name=\"@SUFFIX_VERSION@\">0</token>\n    <xml name=\"edam_ontology\">\n        <edam_topics>                                                                                  \n            <edam_topic>topic_0199</edam_topic>\n        </edam_topics>\n        <edam_operations>\n            <edam_operation>operation_3227</edam_operation>\n        </edam_operations>\n    </xml>\n    <xml name=\"requirements\">\n        <requirements>\n            <container type=\"docker\">google/deepvariant:@TOOL_VERSION@</container>\n        </requirements>\n    </xml>\n    <xml name=\"citations\">\n        <citations>\n            <citation type=\"doi\">10.1038/nbt.4235</citation>\n        </citations>\n\n\n\n\n  This file has been truncated. show original (@link https://github.com/galaxyproject/tools-iuc/blob/master/tools/deepvariant/macros.xml)\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Sir,\nKindly help in this regards I was trying to make a de novo contig using trinity and it is running since from one week.\nIs it ok??? or did I out something wrong\nKindly help"
    },
    {
      "role": "system",
      "text": "Hello @user\nIf the job is running (yellow/peach dataset), it is usually best to allow it to run. The same is true for queued jobs (grey dataset). This applies to jobs (any tool) executed at a public Galaxy server.\n20 GB of fastq data \u2013 uncompressed \u2013 creates a very large assembly job. If it fails later on for exceeding resources (red dataset), you\u2019ll need to do one or more of these:\n\nTry a rerun to eliminate cluster issues\nMore QA/QC on the input reads (always recommended)\nConsider downsampling the reads (tool: Seqtk)\nPossibly need to move to your own Galaxy server where more resources can be allocated. The GVL version of Cloudman is one option: @link https://launch.usegalaxy.org/catalog\n\n\nI added some tags to your post that will find prior Q&A about the above actions. Or, you can search the forum with those keywords (not all posts get tagged).\nYou didn\u2019t state where you are working. But, if by chance at Galaxy Main @link https://usegalaxy.org, I can let you know that the cluster that runs Trinity (and Unicycler + RNA-Star) is very busy. Longer queue times are expected. If you delete the current job and rerun, that will only place your job back at the end of the queue again, extending wait time.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!\nI\u2019m new around here, learning how to use Galaxy. I\u2019m running the tutorial 16S Microbial Analysis with mothur (extended) (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html) and I keep getting an error when trying to \u201cgenerate count table\u201d using Count.seqs\nFatal error: Exit code 1 ()\nFatal error: Matched on [ERROR]\nCan anyone help me with that, please?\nHere\u2019s a link to my history:\n@link https://usegalaxy.org/u/isissilveira/h/tutorial\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you try again? I tried to reproduce the error, but it finished correctly:\n@link https://usegalaxy.org/u/gallardoalba/h/imported-tutorial\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there, I performed a BUSCO with the hope of documenting its results for a thesis. At the end of the analysis, The analysis generated only the BUSCO full table.\nHow do I get the \"summary file activated again? Or, what other alternatives can I use to make a summary table myself?\nThank you.\n\nimage994\u00d7274 9.16 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/33041cf55df5c3d26d68e69554796c3ff72e7e9e.png)"
    },
    {
      "role": "system",
      "text": "Dear @user,\nUnder Advanced Options \u2192 Which outputs should be generated you can select a summary image about the characteristic values of BUSCO.\nIs that what you meant?\nHave a good weekend and good success with your thesis!\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI got a weird graphic, and the parameter settings are all default, as follows, what\u2019s the problem,?thank you\uff01\nGalaxy34-D)_Plot_Cladogram_on_data_324800\u00d73600 4 MB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dc38812fa3857afe3d4dcc28a944ed9156fd6724.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server.\nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers).\nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (below).\n\nContact: Huttenhower Lab (@link https://galaxyproject.org/use/huttenhower-lab/)\n\n\nHelp/tutorials: (also linked from the server\u2019s home page, with example data available):\n\nlefse \u2013 The Huttenhower Lab (@link https://huttenhower.sph.harvard.edu/lefse/)\nlefse \u00b7 biobakery/biobakery Wiki \u00b7 GitHub (@link https://github.com/biobakery/biobakery/wiki/lefse)\n\nPrior Q&A and other related resources including other versions of this tool and different tools that may be helpful when working with this type of data:\n\nLefse Cladogram prior Q&A at this forum: Search results for 'Cladogram' - Galaxy Community Help (@link https://help.galaxyproject.org/search?q=Cladogram)\n\nAll Galaxy resources: Searching the Galaxy (@link https://galaxyproject.org/search/?q=cladogram#gsc.tab=0&gsc.q=cladogram&gsc.page=1)\n\n\nI also added a tag for that public server to your post. Click on the tag to review all Q&A at this forum related to troubleshooting and help/content for that server."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I\u2019m new to Galaxy and to bioinformatics too, and actually trying to install it on my CentOS 7 server. I ran the installation as suggested on the website, but unfortunately it seems to get stuck at some point (while trying to communicate with a server?)\nHere is a screenshot\u2026\n\nErrore Galaxy.png917\u00d7696 37.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/d8e15e34b36c6445ce387e6616a75affbe738784.png)\nWhat could be the problem?\nThanks in advance for your time!"
    },
    {
      "role": "system",
      "text": "There is no problem :smile:\nThis is Galaxy\u2019s way of telling you that it\u2019s ready and waiting for requests. Open your web browser on the same machine and point it to 127.0.0.1:8080. You should see Galaxy\u2019s web interface."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am trying to use EdgeR for differential gene expression analysis. When I try to run EdgeR using htseq-count files for each group in the same factor as the input (two count files per group), I end up with the following error:\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: row.names<- \u2192 row.names<-.data.frame \u2192 .rowNamesDF<-\nWarning message:\nnon-unique value when setting \u2018row.names\u2019: \u2018X0\u2019\nI\u2019m unsure how to troubleshoot this and have searched through here and was unable to find answers.\nI\u2019m sorry if this has been asked and I accidentally overlooked it!\nThanks for any/ all help!\nDan"
    },
    {
      "role": "user",
      "text": "Hi jennaj,\nThanks for your advice! I was using individual HTseq-count files produced by the HTseq tool without modifying them in any way, so I\u2019m unsure why it was giving me trouble. I ended up combining the files into a single count matrix and that fixed the problem.\nThanks again!\nDan"
    }
  ],
  [
    {
      "role": "user",
      "text": "I was notice that collection files that were recognized by FastQC (for example) are not recognized by Unicycler.\nThanks!\nIsrael"
    },
    {
      "role": "system",
      "text": "You\u2019ll need to set the datatype to fastqsanger. This is usually auto-detected but fails for nanopore files which have unusual quality scores. You can set the datatype at upload or change it for existing datasets by clicking the pen icon on the dataset\nScreenshot 2019-12-19 at 18.35.46\nthen click on the datatype tab and select fastqsanger or fastqsanger.gz depending on whether your data is compressed or not:\n\nScreenshot 2019-12-19 at 18.35.53740\u00d7434 26.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/54869126e5416628a4669aaae38dae38609ef482.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI trying to do a QC for my files. And I keep encountering this error. This data set had previously pass the FastQC without any error before. However, when re-upload the file to do some re-analysis, now it keeps failing to pass QC. Anyone encounters the same problem and know how to solve it?\n\u201cPicked up _JAVA_OPTIONS: -Djava.io.tmpdir=/corral4/main/jobs/040/717/40717303/_job_tmp -Xmx7g -Xms256m\nFailed to process file AG10_Phi_DeltaLL_S8_L001_R1_001_fastq_gz.gz\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry.  Your file is probably truncated\nat uk.ac.babraham.FastQC.Sequence.FastQFile.readNext(FastQFile.java:179)\nat uk.ac.babraham.FastQC.Sequence.FastQFile.next(FastQFile.java:125)\nat uk.ac.babraham.FastQC.Analysis.AnalysisRunner.run(AnalysisRunner.java:76)\nat java.base/java.lang.Thread.run(Thread.java:834)\ncp: can\u2019t stat \u2018/corral4/main/jobs/040/717/40717303/working/dataset_53bb4f0c-c6c8-45b3-a706-13605487098b_files/*/fastqc_data.txt\u2019: No such file or directory\u201d"
    },
    {
      "role": "system",
      "text": "@quote\nuk.ac.babraham.FastQC.Sequence.SequenceFormatException:Ran out of data in the middle of a fastq entry. Your file is probably truncated\nHi @user \u2013 Check the size of your dataset as the error message indicates the data is truncated. That could have occurred during the re-upload, or maybe it wasn\u2019t fully downloaded originally.\nIf the data is intact wherever you downloaded it, you probably just need to load it again. And if it is large or you are working on a slower connection, try using FTP upload as that tool will report a partial upload (plus you can resume the transfer).\nI added some tags to your post that point to prior Q&A about how to do this."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nPlease can I just ask how to filter SnpEff output for exonic variants only with SnpSift?\nI have tried by calling the initial SnpEff job with sequence ontology annotations.\nWhen I then try to filter as per the sequence ontology annotations it fails. When I\u2019ve viewed the SnpEff VCF output, the sequence ontology annotations don\u2019t seem to be present despite selecting this option when running the initial annotation.\nApologies if this is a basic q, very new to bioinformatics\nThank you for your help"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe brackets have wildcard characters, but these were interpreted as format symbols. If you decide to go with SnpSift Filter, just follow example at the bottom of job setup page.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to run through @link https://mcmicro.org/instructions/galaxy/galaxy-running.html using the TMA workflow, which is asking for a cell type map, but that\u2019s not included in the zip file for the exemplar data. Is this something that anyone has a solution for?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nplease, report the problem in Topics tagged mcmicro (@link https://forum.image.sc/tag/mcmicro) under the mcmicro tag.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy community,\nthank you for the great work. Is it possible to add the --cut_right option to the GUI for fastp?\nIt belongs in the \u201cPer read cutting by quality\u201d options. According to GitHub - OpenGene/fastp: An ultra-fast all-in-one FASTQ preprocessor (QC/adapters/trimming/filtering/splitting/merging...) (@link http://github.com/OpenGene/fastp#per-read-cutting-by-quality-score) the version in Galaxy supports this. It would be great to have this!\nThank you very much,\nMichel"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nI added your request to a new enhancement issue for the tool developers to consider. Please follow the ticket for updates, and feel free to join the comments/discussion: fastp: add options for \"-r, --cut_right\" to the tool form \u00b7 Issue #5183 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/5183)\nShould you be interested in helping to make the changes, please see the contributing guidelines for how to get started :hammer_and_wrench: tools-iuc/CONTRIBUTING.md at main \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/blob/main/CONTRIBUTING.md)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Input is a NCBI Fasta file. Executed  successfully about a week ago on same input but getting this error yesterday and today. Any suggestions.\nFATAL:   could not open image /cvmfs/singularity.galaxyproject.org/all/prokka:1.14.6\u2013pl5262hdfd78af_1: failed to retrieve path for /cvmfs/singularity.galaxyproject.org/all/prokka:1.14.6\u2013pl5262hdfd78af_1: lstat /cvmfs/singularity.galaxyproject.org: no s"
    },
    {
      "role": "user",
      "text": "Thanks. Rerunning today work. Guest it was just a transient error."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI was trying de novo assembly for a fish species, but I keep getting the same error message no matter what programs I used, ABySS, SPAdes, VelvetOptimiser. Here is what the error message looks like:\n2020/11/03 15:03:16.35 parrot_run[70105] child:70213 (@link ) notice: warning: system call 237 (mbind) not supported for program /cvmfs/main.galaxyproject.org/deps/_conda/envs/galaxy/bin/python\nThe input data set should be fine since I can do trimming, mapping stuff. So I\u2019m completely confused. What should I do?\nThanks,\nYunyang Wang"
    },
    {
      "role": "system",
      "text": "GTN Tutorials for assembly (including Unicycler):\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/assembly/\n\n\nGalaxy Training: Assembly (@link https://training.galaxyproject.org/training-material/topics/assembly/)\nDNA sequence data has become an indispensable tool for Mo...\n\n\n\n\n\n@quote\nUpdate \nThe assembler ABySS has been technically problematic atUseGalaxy.orgfor some time now. \nThe tool has been removed fromUseGalaxy.org. \nAn alternative usegalaxy.* server for this tool isUseGalaxy.eu\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Greetings,\nI am trying to combine paired-end data into a collection for use in the tool: Stacks: de novo map  the Stacks pipeline without a reference genome (denovo_map.pl) (Galaxy Version 1.46.0). I uploaded the files individually and tried creating a collection a few ways.\nFirst, I selected all files (24) and created a \u201clist of pairs.\u201d This seemed to work well (it created a list of 12 pairs), but I cannot get the Stacks: de novo map tool to recognize this list as a valid type of input.\nNext, I tried to build a collection using rules, but could not figure out how to pair the files, so it ended up creating a list of 24 files. The tool recognized this collection, but I didn\u2019t want to proceed without pairing the data.\nFinally, I tried selecting each pair of files, building a dataset pair for each, and then combining these into a collection\u2026but apparently you cannot build a collection from lists of data?\nSo, my questions are:\n\nIs there a Collection operation I need to use in order to proceed using the \u201clist of pairs?\u201d One that will make the tool recognize the list as a valid collection to input?\n\nOR\n\nIs there a way to pair files when building a collection from rules?\n\nMany thanks!"
    },
    {
      "role": "system",
      "text": "Hello,\nThe currently wrapped version of Stacks (version implied is 1.0) does not accept pair-end reads, or rather, not directly as a paired dataset (individual dataset or collection datasets). Inputs are expected to represent R1 reads, either as individual datasets or in a list collection. This means that a paired-end collection won\u2019t be recognized by the tool as a proper input. Please see the manual link below for details \u2013 including how to perform the analysis on certain types of paired-end data.\nThere is an updated version of the underlying tool: Stacks 2.0. It does accept pair-end data directly. However, Stacks 2.0 has not been wrapped for Galaxy (yet) but there is development in progress for this update. I would expect that pair-end collections would be accepted as an input in this version once completed. Please see this IUC issue ticket and related linked discussions/code changes (PRs) for status: @link https://github.com/galaxyproject/tools-iuc/issues/2073\nThe accepted inputs for the two two tool versions are explained in the manuals here under the section \u201cWhat types of data does Stacks (or Stacks 2.0) support?\u201d:\n\nStacks @link http://catchenlab.life.illinois.edu/stacks/manual-v1/#data\n\nStacks 2.0 @link http://catchenlab.life.illinois.edu/stacks/manual/#data\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI have trouble using the GATK DepthOfCoverage tool.\nThe following error appears:\nERROR ------------------------------------------------------------------------------------------\nERROR stack trace\njava.lang.ExceptionInInitializerError\nat org.broadinstitute.sting.gatk.GenomeAnalysisEngine.(GenomeAnalysisEngine.java:\nAccording to a Google search that could be because of a too old version of java or of GATK.\nIs it possible that java or GATK are not up to date? Or is it because I\u2019m doing something wrong? Does GATK works for someone?\nThank you!\nMathias"
    },
    {
      "role": "system",
      "text": "Hello,\nMany GATK tools currently wrapped for Galaxy are several releases outdated and are considered deprecated. They should not be used.\nThere is a newer version: @link https://toolshed.g2.bx.psu.edu/view/lz_hust/gatktools/01ff8dd37d4d\nIf that new version (released in June 2019) is presenting with problems, please create an issue at the development repository: @link https://github.com/huster16/gatk-tools\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone!\nI am working on developing some Galaxy tools.\nA problem I have come across a couple of times is programs requiring input files with specific file extensions e.g. .JPG.\nThis is challenging for me as it is my understanding that the Galaxy representation of uploaded files is arbitrarily named with the .dat extension.\nIs there a way I can control the extension of uploaded files? Can I rewrite my tool XML to work around this?\nI would really appreciate any thoughts or suggestions :slight_smile:\nAll the best and many thanks,\nOliver"
    },
    {
      "role": "system",
      "text": "I think you can do something with a symlink. See here for an example: Wrapping a script which implicitly expects a local file - #6 by astrov (@link https://help.galaxyproject.org/t/wrapping-a-script-which-implicitly-expects-a-local-file/6164/6)\nIf it is a python script you could do something like (no working code just an idea):\n    if os.path.splitext(galaxyinputfile)[1] == \".dat\":\n        cwd = os.getcwd()\n        jpginputFile=cwd+os.path.splitext(os.path.basename(galaxyinputfile))[0]+\".jpg\"\n        shutil.copy(galaxyinputfile, jpginputFile)\n\nThere may be better ways but this could give some inspiration already"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear everyone,\nI tried to analyze RNA-seq data successfully. Unfortunately, after running the DeSEQ2 tool I get the following error:\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nestimating size factors\nestimating dispersions\ngene-wise dispersion estimates: 6 workers\nmean-dispersion relationship\nfinal dispersion estimates, MLE betas: 6 workers\nError in (function (cond)  :\nerror in evaluating the argument \u2018args\u2019 in selecting a method for function \u2018do.call\u2019: BiocParallel errors\n3 remote errors, element index: 1, 2, 3\n0 unevaluated and other errors\nfirst remote error: error in evaluating the argument \u2018x\u2019 in selecting a method for function \u2018t\u2019: no right-hand side in \u2018b\u2019\nCalls: DESeq \u2192 DESeqParallel \u2192 do.call \u2192 bplapply \u2192 bplapply\nAt the same I can use edgeR and Lima without any problem.\nI just wanted to compare the three different tool\u2019s results!!\nDoes anybody know about it?\nIni"
    },
    {
      "role": "system",
      "text": "Hi @user , hi @system\nI realized it is way too late, but came across a similar error recently. In that case the issue was caused by genes with zero counts in every sample. Filtering out zero count genes helps. Alternatively, try other tools, such as edgeR.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Exception in thread \u201cmain\u201d htsjdk.samtools.util.RuntimeIOException: java.io.IOException: No space left on device\nat htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:245)\nat htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:165)\nat htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:182)\nat pt2.se2pe.Pop2FileWriter.write(Pop2FileWriter.java:74)\nat pt2.se2pe.Se2PeFramework.run(Se2PeFramework.java:95)\nat pt2.se2pe.Se2PeParser.parseCommandLine(Se2PeParser.java:76)\nat pt2.Main.main(Main.java:62)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis happened at the public Galaxy server @link http://UseGalaxy.org?\nIf so, this was probably a transient cluster problem (the disc was full when this job ran). Please try a rerun.\nShould you get that error again, please send in a bug report (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors) from the red error datasets (all). It would be helpful to include the URL of this Q&A topic in the comments."
    }
  ],
  [
    {
      "role": "user",
      "text": "How to use featureCounts as gene_name model"
    },
    {
      "role": "system",
      "text": "Hi @user\nyou can change gene identifier value in Advanced options during featureCounts job setup.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m analysing the small RNA-seq data. I\u2019m trying to split my collection into two, corresponding to wild type and mutant. I used to use \u2018Filter list from contents of a file\u2019 option but it is missing now. What option I can use instead?"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe try Filter collection.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I\u2019m trying to assemble a genome with Spades. As output files, I need to get only contigs, which I indicated when I uploaded read 1 and read 2. Spades took already more than 24 hours to load. Can you please tell me how long it can take to load Spades? Does it make sense to wait or is it better to try to reset everything and load it again?"
    },
    {
      "role": "system",
      "text": "Spades can take a lot of time and simply deleting the job and starting it again won\u2019t do you any good.\nWhat can help though is if you first downsample your input reads so spades simply has less data to process."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, there all\nWhile I am new to galaxy, I have a basic query\nAfter uploading my datasets, I joined the paired end data by using fastq-join, with each read about 4.8 gb in size\nIt produced three different files as\n1 fastq-join on data 27( unmatched 2) ( size 4.6 gb)\n2. Fastq-join on data 27 ( unmatched 1 ) size 4.6gb\n3. Fastq-join on data 27 ( joined) size 210.4 mb\nNow my question is which file I have to choose for further downstream analysis.\nThanks in advances\nAny possible help will.be highly appriciated"
    },
    {
      "role": "system",
      "text": "Why in particular are you using fastq-join? This joins reads only when they are overlapping, and most fastq reads will not overlap. From the size of your outputs it certainly looks like you have non-overlapping reads. Depending on what you are trying to achieve, you will either use the two read datasets (which here are largely preserved in the \u201cunmatched 1\u201d and \u201cunmatched 2\u201d datasets or you will use the joined one (but this seems very small in your example).\nIf you explain a bit more about the analysis you are trying to do it will help people advise better."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am new to galaxy and have little knowledge on coding. I was running differentially expressed genes using three replicates of RNAseq data from controls and samples. My workflow was from FASTQC >> Trimmomatic >> HISAT2 >> Stringtie >> Stringtie merge >> Stringtie (using Stringtie merge as a reference genome) >> DESeq2. However, the results from Stringtie giving out gene count file which had no information in it. The assembled transcript file showed mapped transcripts. So when I performed DESeq2, the error showed up. Can anyone help me out with this issue? Thanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you share your Galaxy history with me? I\u2019ll have a look at the results. My email is\nScreenshot from 2022-04-03 16-52-09.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone, I\u2019m a new member :slight_smile: I\u2019m wrinting you because I have some issue in Galaxy when I use the tool \u201cUnique.seqs\u201d. The following error message appairs : \"Fatal error 1 () with the message \u201cTERM environment variable not set\u201d. When I look to the previous step (Screen.seqs), the contigs.report and the align.report contain 0 line. Does someone already had this issue and know how to fix it ? Thank you very much :slight_smile:"
    },
    {
      "role": "system",
      "text": "@quote\n\u201cUnique.seqs\u201d. The following error message appairs : \"Fatal error 1 () with the message \u201cTERM environment variable not set\u201d.\nThis is a technical artifact and can be ignored as a non-specific warning.\n@quote\nWhen I look to the previous step (Screen.seqs), the contigs.report and the align.report contain 0 line.\nTo clarify: the input to Unique.seqs does not contain any data? This means that some upstream step was problematic (usage), or none of your data actually passed the filtering criteria applied by Screen.seqs (or possibly another upstream step/tool). Not all tools will \u201cfail\u201d (turn red) when given empty inputs \u2013 some instead just produce more empty inputs.\nTraceback through your analysis to find out where the problem was introduced \u2013 and it wasn\u2019t necessarily the most recent upstream steps. The metagenomics GTN tutorials that include Mothur tools can help with usage and example data/workflows that are known to work.\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/metagenomics/\n\n\nGalaxy Training: Metagenomics (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\nMore help here, but to be clear, the issue you explain is a tool usage or data content issue, not a tool/server issue (if I am understanding correctly!). Why there are empty results from the upstream tool should be addressed first.\n@quote\nCheck to see if there is a known usegalaxy.* server issue. \n\nServer Status:https://status.galaxyproject.org/If the server was down when you first ran your job, try a rerun once back up. A rerun can also eliminate transient server issues too short in duration to trigger a statuspage warning. \nNext, start by reviewing the troubleshooting FAQ. Common reasons and solutions for tool errors are explained. Most job errors can be resolved by correcting your input data\u2019s format/content. Others indica\u2026\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I got the DEG table with the limma package in R, I searched for the SNAI1 gene in the DEG table and now I want to compare my gene expression in the samples. I want to know in which samples SNAI1 is up-regulated and in which samples it is reduced. How can I do that? With what code? Does anyone know about this ? Thanks in advance\n   library(limma)\n    library(edgeR) \n    dge <- DGEList(rawdata)\n    keep <- filterByExpr(dge, design = design)\n    keep2 <- data.frame(keep)\n    filt <- dge[keep, ,keep.lib.size = F]\n    norm <- calcNormFactors(filt, method = \"TMM\")\n    v <- voom(norm, design = design, plot = T)\n    E <- v$E\n    ex <- 2^E\n    Ex <- log2(ex + 1) \n\n    fit <- lmFit(Ex, design = design)\n    contrast <- makeContrasts(cancer-normal, levels = design)\n    fit2 <- contrasts.fit(fit = fit,contrasts = contrast)\n    fit2 <- eBayes(fit = fit2)\n    diff <- topTable(fit = fit2, number = Inf, sort.by = \"P\" )"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis forum is for help using the UseGalaxy.* resources: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\nFor help with Bioconductor packages command-line, see the package vignettes here @link http://bioconductor.org/ and their support forum here Bioconductor - Help (@link http://bioconductor.org/help/).\nIf you want to try in Galaxy instead, or want to review those resources for general analysis guidelines/examples, see: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a hairpin dataset and i cant convert it with RNA to DNA convertor because this dataset has some things like Y or R in their sequence, So i cant remove them one by one, is there any tool which can do it all together.\nFor example clear all sequence except the ACTGactgNn ?"
    },
    {
      "role": "system",
      "text": "You could use the \u2018Text transformation with sed\u2019 tool with this SED Program\n/^[^>]/s/[^ACTGNactgn]+//g\nThis will remove all non-ACTGnactgn characters from all non-header lines in FASTA."
    }
  ],
  [
    {
      "role": "user",
      "text": "I keep getting this error for my fastq files when uploading to GalaxyTrakr. I\u2019ve kept trying for the past 2 weeks with no resolution. Any help would be appreciated!"
    },
    {
      "role": "system",
      "text": "Hi \u2013 this was most likely related to the other Q&A here Uploads require you to log in -- Status: Resolved bug (@link https://help.galaxyproject.org/t/uploads-require-you-to-log-in-status-resolved-bug/7244)\nMost public Galaxy servers should have updated by now to capture the fix. If not, please open a new topic, and include the server URL along with the other context question in the linked post."
    }
  ],
  [
    {
      "role": "user",
      "text": "sh run.sh\n\u2026/\u2026\nyarn install v1.15.2\n[1/4] Resolving packages\u2026\n[2/4] Fetching packages\u2026\nerror Couldn\u2019t find match for \u201c6a1028fb562f9aa68c451f0901f8cfeb43cad140\u201d in \u201crefs/heads/develop,refs/heads/gh-pages,refs/heads/master,refs/heads/namespaced-events,refs/heads/no-deps,refs/heads/refactor,refs/heads/separate-orphan-step-template,refs/tags/0.10.3,refs/tags/0.2.0,refs/tags/v0.10.0,refs/tags/v0.10.1,refs/tags/v0.10.2,refs/tags/v0.10.3,refs/tags/v0.11.0,refs/tags/v0.12.0,refs/tags/v0.2.0,refs/tags/v0.3.0,refs/tags/v0.4.0,refs/tags/v0.5.0,refs/tags/v0.5.1,refs/tags/v0.6.0,refs/tags/v0.6.1,refs/tags/v0.6.2,refs/tags/v0.7.0,refs/tags/v0.7.1,refs/tags/v0.7.2,refs/tags/v0.7.3,refs/tags/v0.8.0,refs/tags/v0.8.1,refs/tags/v0.9.0,refs/tags/v0.9.1,refs/tags/v0.9.2,refs/tags/v0.9.3\u201d for \u201c@link https://github.com/sorich87/bootstrap-tour.git\u201d.\ninfo Visit @link https://yarnpkg.com/en/docs/cli/install for documentation about this command.\nERROR: Galaxy client dependency installation failed. See ./client/README.md for more information, including how to get help."
    },
    {
      "role": "system",
      "text": "When I\u2019ve seen something similar before it was because git was unavailable, I believe.  Can you tell me more about the platform you\u2019re running Galaxy on?  And, does that error still happen when you attempt to re-run Galaxy?"
    }
  ],
  [
    {
      "role": "user",
      "text": "We recently upgraded our HPC environment and can\u2019t seem to get our Galaxy server to properly submit jobs. The issue seems to be with slurm-drmaa not working properly with Slurm 23.02:\n[harvey@bisonnet-hpc ~]$ /software/apps/slurm-drmaa/current/bin/drmaa-run testscript.sh\nSegmentation fault (core dumped)\nHas anyone gotten slurm-drmaa to work with Slurm 23.02?"
    },
    {
      "role": "user",
      "text": "Thankfully, the slurm-drmaa devs patched the code and the changes should be merged into the main branch soon."
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to use Kallisto for pseudo alignment, I need a reference transcriptome (homosapien), there is no option available in the tool.\nwhere can I find it?\n\n1753\u00d7489 16 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6c954b2410388da67dccfdcecfc0ad11bd6f4e6b.png)"
    },
    {
      "role": "system",
      "text": "Dear @user,\nIn which galaxy instance are you working? You can finde reference transcriptomes on @link http://usegalaxy.eu. It seems they are not supported on @link http://usegalaxy.org. So if you are working on .org, then you can download your reference transcriptome, e.g., from UCSC (@link https://genome.ucsc.edu/cgi-bin/hgTables), GENCODE (@link https://www.gencodegenes.org/), or Ensembl (@link http://www.ensembl.org/info/data/ftp/index.html/).\nHave a good day and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to use DESeq2 to compare between samples before treatment and after treatment  individually as paired samples.\nFor example\nPt.1 before vs Pt.1 after\nPt.2 before vs Pt.2 after\nPt.3 before vs Pt.3 after\n\u2026\nI followed this tutorial:\n\n\n@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html\n\n\nGalaxy Training: Reference-based RNA-Seq data analysis (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nBut I\u2019m not sure if the DESeq2 in the above tutorial it compares patients individually or as a group, and How to do DESeq2 for paired samples for each patients individually in usegalaxy?"
    },
    {
      "role": "system",
      "text": "Dear @user,\nDESeq2 would group them.\nYou would need to apply DeSeq2 to each pair individually. I am not sure, if there is a more handy approach to this.\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello @system,\nI am having the same problem. I am the admin for the Galaxy instance that we use at our institution. I imported the RNA-seq deg-analysis workflow shared by you earlier in an earlier post (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/workflows/) from Galaxy workflows. It ran without any errors and generated outputs for the example dataset. However, I had someone double-check the workflow, with data from NCBI. I have another workflow designed to get counts and it ran on this data without any errors. However, when they ran these counts on the above workflow, it gave an error at the goseq stage.\nUsing manually entered categories.\nError in `[.default`(summary(map), , 1) : incorrect number of dimensions\nCalls: run_goseq ... goseq -> reversemapping -> [ -> [.table -> NextMethod\n\nIn the above answer, you indicated double-checking my inputs. I have downloaded and looked at it, but is there another way to double-check in Galaxy? The tag \u201cinput\u201d didn\u2019t give me many posts.\nI compared the input files that are given to goseq with the test dataset and the input files in the case of the example dataset that worked earlier. It looks very similar. Is there anything else I could do? Is there a way I can share the history and objects within so someone can have a look?\nThank you so much!\nPriyanka"
    },
    {
      "role": "user",
      "text": "Thank you so much for the detailed answer. I figured out the problem! The issue was with selecting the gene names in the drop-down menu in the goseq tool. The gene annotation file was using gene symbols and the default for goseq in the workflow was the Ensembl gene IDs. This inconsistency caused the problem. It was really hard to pin the error down though because the R error message wasn\u2019t helpful. Thank you so much @system ! Your reply helped me think through what could be going wrong!\nThank you,\nPriyanka Bhandary"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI used Salmon for quatification and now I\u2019m trying to run DESeq2. I\u2019m using a mapping file from Transcript ID to GeneID but I get the following error:\nreading in files with read.delim (install \u2018readr\u2019 package for speed up)\n1 2 3 4 5 6\nremoving duplicated transcript rows from tx2gene\nError in $<-.data.frame(*tmp*, \u201cTXNAME\u201d, value = character(0)) :\nreplacement has 0 rows, data has 997581\nCalls: get_deseq_dataset \u2192 $<- \u2192 $<-.data.frame\nI think my mapping file does not have header bur I do not know how to set header=false and I do not know if that is the real problem.\nWould anyone help me with this issue?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\n1 2 3 4 5 6\nThis implies that your mapping file has more than two columns if I am understanding the log.\nThe mapping file should contain:\n\nNo header line\nTwo columns separated by a single tab\nTranscriptIDs in the first column\nGeneIDs in the second column\nNo extra spaces, tabs, or empty lines\nThe datatype should be assigned as \u201ctabular\u201d. If Galaxy doesn\u2019t guess this datatype with the @link https://training.galaxyproject.org/training-material/faqs/galaxy/#detecting-the-datatype-file-format function, then there is a format problem.\n\nGeneral help\n\nThis same mapping file should have been used when running Salmon (parameter = \u201cFile containing a mapping of transcripts to genes\u201d)\nIf you used a GTF file instead with Salmon, you should also use that with DESeq2.\n\nGFF3 annotation won\u2019t work with these tools \u2013 you need either GTF or a tabular mapping. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation\n\nMake sure that you are inputting the correct output from Salmon == \u201cquant.genes.sf\u201d\n\nI added more tags to your post that link to prior Q&A that helped others, and tutorials are here:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nIf you need more help \u2013 you can either create a share link to your history (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history) and post that back, or it might be enough to post back just the first few lines from each of your inputs.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I shared a history with a colleague and then purged it from my account since I had no space. Colleague gets error \u201cUser doesn\u2019t have permission to use dataset\u201d. I understand I should have given all permissions when sharing but is there anything that can be done now to allow use of the datasets?"
    },
    {
      "role": "system",
      "text": "For anyone reading this later on \u2013 once a dataset is purged from a server it cannot be recovered.\nConsider backing up important work: download locally, or to a server or cloud storage you control"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to generate MultiQC report from multiple QualiMap RNA-Seq QC files. I am using \u2018@link http://usegalaxy.org\u2019.\nThe MultiQC runs but does not give any output (0 bytes). I think the problem is because MultiQC is expecting genome_coverage to be the first file in the QualiMap RNA-Seq QC output. However, I don\u2019t see any such report. genome_coverage is present in the QualiMap report, but not in the raw data folder. I\u2019m not sure why this is happening.\nVersions:\nMultiQC v1.8\nQualiMap RNA-Seq QC v2.2.2d\nP.S. MultiQC works fine with STAR .log output (v2.7.1a) and FastQC (v0.72) raw data."
    },
    {
      "role": "system",
      "text": "Hi,\nTry using the tool QualiMap Counts QC instead to generate a summary. That means you\u2019ll want to output the optional \u201ccounts\u201d output, not just \u201cstatistics\u201d, when running QualiMap RNA-Seq QC.\nThe version of MultiQC wrapped in Galaxy doesn\u2019t interpret QualiMap RNA-Seq QC reports (raw data). It expects QualiMap BamQC (raw data) input, only, despite the top menu label stating that the input could be from either tool. The lower menu does specify BamQC but we can see how that can be confusing. The tool wrapper could probably be updated to be clearer \u2013 we\u2019ll follow up on that for later potential changes.\nAppreciate your patience for a reply!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I can\u2019t reset my password (I\u2019m using Galaxy (@link http://pedagogix-tagc.univ-mrs.fr/galaxy/)), i tried but i never received the mail to reset it (it\u2019s the righ adress). How can i reset it?"
    },
    {
      "role": "system",
      "text": "@quote\nGalaxy\nHi @system & @user\nThis looks like a private Galaxy server internal to your university.\nYou\u2019ll need to contact the administrators of the server to find out what can be done. No information is available unless one has an account, so we can\u2019t help you with who to contact here. Maybe try contacting whoever originally granted you access?\nHope that works out!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everybody,\nI\u2019m writing the Python code for a tool that taking a matrix as input, it returns an heatmap as result.\nWhen running the tool on Galaxy I get this error:\nTraceback (most recent call last):\n  File \"/Users/andrea/Desktop/galaxy/tools/personal_tools/sys_workflow/sys_heatmap.py\", line 16, in <module>\n    s.savefig(sys.argv[3])\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/seaborn/axisgrid.py\", line 65, in savefig\n    self.figure.savefig(*args, **kwargs)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/figure.py\", line 3046, in savefig\n    self.canvas.print_figure(fname, **kwargs)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2259, in print_figure\n    canvas = self._get_output_canvas(backend, format)\n  File \"/Users/andrea/Desktop/galaxy/database/dependencies/_conda/envs/mulled-v1-e8e5c838ec73328e9b82c3fbfb3ba227afa87188721e8f9187093dbc4fdd3cb3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2188, in _get_output_canvas\n    raise ValueError(\nValueError: Format 'dat' is not supported (supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff)\n\nI think that the problem is that Galaxy is passing a dat file as storage for the output.\nIn the xml file I specified that the output should be a png file but it seems to not be enough, how can I solve this?\nHere\u2019s the code:\nxml file:\n<tool id=\"sys_heatmap_tool\" name=\"Heatmap \" version=\"0.1.0\">\n  <description>of phenotype counts</description>\n  <requirements>\n    <requirement type=\"package\">seaborn</requirement>\n\t<requirement type=\"package\">pandas</requirement>\n  </requirements>\n  <command>\n\t  <![CDATA[python3 \"$__tool_directory__/sys_heatmap.py\" '$input' '$cluster' '$heatmap_out']]>\n  </command>\n  <inputs>\n          <param name=\"input\" type=\"data\" label=\"Input file:\" help=\"Enter the matrix with the counts of phenotypes\"/>\n\t\t  <param name=\"cluster\" type=\"select\" label=\"Indicate which method use for clustering:\">\n\t\t\t  <option value=\"single\">single</option>\n\t\t\t  <option value=\"complete\">complete</option>\n\t\t\t  <option value=\"average\">average</option>\n\t\t\t  <option value=\"weighted\">weighted</option>\n\t\t\t  <option value=\"centroid\">centroid</option>\n\t\t\t  <option value=\"median\">median</option>\n\t\t  </param>\n\t\t  <param name=\"dist\" type=\"text\" label=\"Distance metric to use for the data:\" help=\"Enter the desired metrics\"/>\n  </inputs>\n  <outputs>\n\t  <data format=\"png\" name=\"heatmap_out\" file=\"heatmap.png\" label=\"Pheno count heatmap\"/>\n  </outputs>\n  <help><![CDATA[\n\t  **What it does**\n\t  \n\t  Given a matrix with counts of phenotypes, the tools retrieves the corresponding heatmap with clustering.\n\t  There user can choose between different methods of clustering:\n\t  \n\t  -single\n\t  \n\t  -complete\n\t  \n\t  -average\n\t  \n\t  -weighted\n\t  \n\t  -centroid\n\t  \n\t  -median\n\t  \n\t  More info about those metods can be found at: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n\t  \n\t  The default distance used for clusters is the euclidean. The user can enter a different distance metric from here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html?highlight=pdist#scipy.spatial.distance.pdist\n  ]]></help>\n</tool>\n\npython code:\nimport sys\nimport seaborn as sns\nimport pandas as pd\n\nmat = pd.read_csv(sys.argv[1], sep=\"\\t\", index_col=0)\n\nmat.index.name = 'MGI_id'\nmat.columns.name = 'phen_sys'\n\nclust=str(sys.argv[2])\n\ns = sns.clustermap(mat,row_cluster=True,col_cluster=False, method=clust, cbar_pos=(0.01,0.01,0.01,0.2), cmap=\"mako\")\ns.savefig(sys.argv[3])\n\nThank you for the help."
    },
    {
      "role": "system",
      "text": "You can do something like:\npython:\ns.savefig(\"output.png\")\n\nxml\n<data format=\"png\" name=\"output\" from_work_dir=\"output.png\" label=\"Pheno count heatmap\" />\n\nor\npython:\ns.savefig(\"output.png\")\nshutil.copyfile(\"output.png\", sys.argv[2])\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am going through the \u201cFrom peaks to genes\u201d tutorial and keep running into the same error after re-running the job and double checking that the input files are correct.\n\n\n@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html#part-1-naive-approach\n\n\nGalaxy Training: From peaks to genes (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html#part-1-naive-approach)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\nI am trying to add the promoter region to gene records (in the Analysis step of Part 1 Naive Approach) with the Get Flanks tool, but keep getting \u201cAn error occurred setting the metadata for this dataset\u201d. The information in the attributes \u201c/corral4/main/jobs/040/971/40971917/galaxy_40971917.sh: line 123: galaxy-set-metadata: command not found\u201d\nAny help to bypass this would be much appreciated\nThanks!"
    },
    {
      "role": "system",
      "text": "Update Resolved March 8, 2022\nDetails in the linked topic below.\n\nRelated recent Q&A: Error: galaxy-set-metadata: command not found (@link https://help.galaxyproject.org/t/error-galaxy-set-metadata-command-not-found/7551)\nThere is a technical problem with this set of Galaxy tools at @link http://UseGalaxy.org. It is being looked into.\nPlease try running the tutorial at @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au for now.\nApologies for the trouble!"
    }
  ],
  [
    {
      "role": "user",
      "text": "We have WGS PacBio sequenced data of environmental samples. As the data is huge, we don\u2019t have any high spec machine to analyse it. Therefore, we thought we should try the Galaxy platform. Is there any specific tool in Galaxy that can do the assembly of PacBio WGS metagenomic reads? Or any other online tool / pipeline that may do it?\nLooking for kind suggestions and help. Any leads?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nIs there any specific tool in Galaxy that can do the assembly of PacBio WGS metagenomic reads?\nYes, there is at least Flye. Find this and tools under the \u201cAssembly\u201d tool group at any of the UseGalaxy.* servers.\nFull options\n\n\nFind publications that cover analysis methods you\u2019d like to follow, use comparable tools that Galaxy hosts, and create a workflow for batch analysis.\n\n\nReview public Galaxy servers that have an analysis domain focus on Metagenomics. Search here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use/)\n\n\nAdapt the methods in our tutorials here\n\n\nMetagenomics topic\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\n\n\nGalaxy Training: Metagenomics (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\nPacBio search \u2013 the first currently listed covers assembly\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/search?query=pacbio)\n\n\nGalaxy Training: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=pacbio)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n\nHope that helps to get your started. Please ask a new question if you need more help with any specific tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\ni would like to calculate FRiP score for my CHiPseq analysis, does everyone has an idea on how to do it ?\nthanks a lot!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit can be computed by using the following tools:\n\nbedtools SortBed: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_sortbed/2.30.0\n\nbedtools intersect intervals: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.30.0\n\nsamtools view: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.13+galaxy2\n\nawk: @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/1.1.2\n\n\nYou can find the details about the parameters in the following post (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/1.1.2).\nIt requires to include a new parameter in one of the tools; I opened a Pull Request (@link https://github.com/galaxyproject/tools-iuc/pull/4452) in order to modify it.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have data with rows that contain count data for each gene and columns that contain a specific cell line.  I wish to perform a clustering analysis to determine which cell lines are most similar.  I am unsure which tool is most useful for this as well as how to filter the data since some genes aren\u2019t expressed (or only expressed in some of the cell lines)."
    },
    {
      "role": "system",
      "text": "Hi @user\nFind the Galaxy single-cell tutorials here for example tools/workflows:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nIf you are not sure where to start, the training event from last March has a suggested program, and single-cell analysis is covered with supplementary videos. The second video in the series is seems like a good match for where you are currently at with your own analysis.\n\n\n\n@link https://gallantries.github.io/video-library/events/smorgasbord2/tapas.html\n\n\nSm\u00f6rg\u00e5sbord 2022: Tapas Edition (@link https://gallantries.github.io/video-library/events/smorgasbord2/tapas.html)\nFree, Global, Online Week of Galaxy Training\n\n\n\n\n\n\n\n\n@link https://gallantries.github.io/video-library/modules/single-cell\n\n\nSingle-Cell Analysis (@link https://gallantries.github.io/video-library/modules/single-cell)\nThis module will guide you through single-cell transcriptomics analysis in Galaxy.\n\n\n\n\n\nNote: the Slack channel mentioned in this content is not active right now \u2013 so please use this forum or the GTN gitter chat instead for Q&A. FAQs:\n\nhow to ask a question others can help with (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#what-information-should-i-include-when-reporting-a-problem)\ntroubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors)\nsharing your history (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am using ChIPseeker for yeast experiment, however I didn\u00b4t find the gtf file of yeast (sacCer3). Where can I find it?\nThank you."
    },
    {
      "role": "system",
      "text": "Welcome @user\niGenomes hosts a quality GTF file for UCSC\u2019s sacCer3 genome.\nHow to get it out of the archive and uploaded to Galaxy is covered in this prior post:\n@quote\nForiGenomes, the archive corresponding to the target genome/build needs to be locally downloaded, the tar archive unpacked, and then just thegenes.gtfdata uploaded to Galaxy (browse the local file, or use FTP). Find all available genome/builds here:https://support.illumina.com/sequencing/sequencing_software/igenome.html\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI have been following tutorials and have been trying to get admin acess to the galaxy that i installed locally in my PC.\nAs per the instructions, i renamed the  config/galaxy.yml.sample to galaxy.yml and added my email to the admin_users section. However, i have not been able to get admin access even after restarting my galaxy.\nI am very new with linux and the galaxy setup so any help would be highly appreciated.\nThank you"
    },
    {
      "role": "system",
      "text": "Did you also logged in with the same email after that? By default you don\u2019t need to log in to use galaxy itself."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am new to bioinformatic.\nI expect to get the table summarizing SNP or indel variants out of reads mapped to the reference genome together with coverage information (number of reads). I used BCFtools mpileup and continued with BCFtools call. I got .bcf file and could not figure out how to open or view it.\nAppreciate all the help,\nThank you"
    },
    {
      "role": "system",
      "text": "Dear @user,\nA bcf file is a compressed version of a vcf file (see here (@link https://samtools.github.io/bcftools/howtos/index.html)). You can look into the content of a bcf file by converting it into a vcf file with the tool bcftools view Convert, filter, subset VCF/BCF files.\nHave a good day and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nis it possible to upload files from google drive to galaxy? I tried generating direct link to the google drive file (which was public) and then using it on @link http://usegalaxy.eu, but it returned an error: \u201cThe uploaded file contains invalid HTML content\u201d. Any advice?\nThanks!"
    },
    {
      "role": "user",
      "text": "Hi @system,\nthanks for the response. Unfortunately this doesn\u2019t work for me, but I finally figured out why: the files are too large for the google drive direct download link to work. Knowing where the problem was it was easy to find out the solution: Google Drive Direct Download Link for Large Files (2022) (@link https://bytesbin.com/google-drive-direct-download-link-large-files/) - it is necessary to use google API: https://www.googleapis.com/drive/v3/files/FileID?alt=media&key=APIKey so it is even more complicated, but it works, I finally managed to upload the data to Galaxy. :sweat_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am so sorry to bother you to help me solve this problem. I wanna do the Lefse analysis, the first step of format data seems failed .  I can\u2019t get the point about the error. I would be so appreciated if this will be solved. Thx!!\nDetails:\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 429, in \nclass_sl,subclass_sl,class_hierarchy = get_class_slices(zip(*cls.values()))\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/format_input.py\u201d, line 145, in get_class_slices\nprevious_class = data[0][0]\nIndexError: list index out of range"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nYou are working at this public server, correct?GalaxyThe server is set up a bit differently than other public servers and hosts custom versions of tools, in particular theLefsetool suite. Their support/admin team does not follow this forum, but there are other ways to contact them about odd or non-reproducible results.\nThat said, this error in technical terms means that the tool is looking for specific data in part of the input that isn\u2019t being found. There is probably some problem with the column or row selected for the option \u201cSelect which [row|column] to use as subclass\u201d.\nTry double-checking your tabular input (first option on the tool\u2019s form) to make sure there is content in the row or column you selected. Tutorials and example data are hosted on that public Galaxy server for comparison. Scoll down on that tool form for an example of input data organized by rows.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nWhile running bowtie2 I am getting following error continuously\n\"An error occurred while trying to schedule this job. Please retry it and if it continues to fail, report it to an administrator using the bug icon."
    },
    {
      "role": "system",
      "text": "This error was caused by an upgrade problem that has now been resolved, please try your job again and let us know if you still encounter errors."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, can anyone tell me how to extract all the reads which have a certain sequence from a multi fasta file? \u2018Fasta extract sequence\u2019 seems to only be able to extract using the Name of the sequence, not the sequence itself. All reads have the same name/ID so that isn\u2019t usable for extracting a subset.\nMy situation is that I have amplicon-seq files each containing reads from 2 different PCR products, and just want to split them into 2 files using a sequence which is unique to either.\nHelp!"
    },
    {
      "role": "system",
      "text": "Sounds like the Filter FASTA (@link https://usegalaxy.org.au/root?tool_id=toolshed.g2.bx.psu.edu/repos/galaxyp/filter_by_fasta_ids/filter_by_fasta_ids/2.3) tool is what you are looking for?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to upload control sgRNA file (.txt) in MAGeCK count and MAGeCK test.\nAfter complete uploading in history, system showed that this file was unavalible in the selection list\nof control sgRNA file.\nI\u2019ve tried to upload different file format (.txt/.csv/.tsv), then all of them were failed.\nI do not know if anyone has experience with that problem and could help me.\nThanks a lot for the help in advance.\n\ncontrol1913\u00d7929 179 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/92850f875fd3107dc199c7582e5f3a25d35645a8.png)"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nTools screen for specific datatypes when accepting inputs. For this input, the tool is expecting a dataset with the tabular datatype assigned.\n\nScreen Shot 2023-05-25 at 6.06.47 PM1646\u00d7870 99.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/55781eac3bd3907de8e8a09d83cf00fc683f3e26.png)\nHow to know which datatype a tool is expecting is in this topic and a few others.\n@quote\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:Create a new empty historyLoad up the tool formReview the input datatypes listed in the tool form\u2019s input areaNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.Examplewhat-datatype-is-accepted-input2346\u00d7554 87.2 KB\nSearch: Search results for 'tool doesn't recognize input' - Galaxy Community Help (@link https://help.galaxyproject.org/search?q=tool%20doesn%27t%20recognize%20input)\n\n\nSituations when the (unavailable) message can be generated\n\nDataset 72 was originally accepted by a tool run.\nIt had a tabular datatype assigned, or was drag-and-dropped.\nThe latter breaks some built-in checks \u2013 you can try this way but expect problems exactly like this.\nOr, the first was done, but the datatype assigned to dataset 72 was modified to be something other than tabular.\n \u2192 When the \u201crerun\u201d function is used to load up the tool form, it preloads what was originally input. If the metadata datatype was changed to be anything other than tabular, it won\u2019t be accepted.\n\nKeep in mind that none of this is about the file content. The data still needs to be in tabular format and have data in the columns it can understand.\nExamples are down in the help section of the tool form. The GTN also has tutorials.\n@link https://training.galaxyproject.org/training-material/search?query=MAGeCK\nThere is a training event going on all week if interested. It is not too late to join. The bonus is a dedicated chat per tutorial, and the authors and other Galaxy scientists are there to help (including me). Sm\u00f6rg\u00e5sbord 3: A week of Free, Online, Galaxy Training supported by the Global Galaxy Training Community (@link https://help.galaxyproject.org/t/smorgasbord-3-a-week-of-free-online-galaxy-training-supported-by-the-global-galaxy-training-community/9786)\nHope that helps! :hammer_and_wrench: :scientist: :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am unable to view html output from MultiQC on @link http://usegalaxy.org. Perhaps the tool has been updated and needs to be whitelisted? I can successfully download and view the local html report. Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nYes, that was exactly the problem. Should be fixed now \u2013 please let us know if not.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hye, I have a account in @link http://usegalaxy.org and was working on important project on it. I took break from work for a month and today when I tried to log in, I am not able to access my account. While ai am resetting my password, it is showing \u201cFailed to produce password reset token. User not found.\u201d.My user email id is (admin-redacted). Is anyone else has faced such issue. Kindly help me to retrieve my account. It has very important data and months of work."
    },
    {
      "role": "system",
      "text": "@user\nWe are discussing this privately via the @link http://UseGalaxy.org mailing list. That is best place to address administrative account issues, although if you search this forum you will find other posts related to this topic. For privacy reasons, I have redacted your email address from the original question.\nYou still have one active account at @link http://UseGalaxy.org. This meets the terms of service: one account per person per public Galaxy server. Please have/use one account at each, and never two or more at any. This information is clearly stated in plain language on the account registration form and the account activation email exactly where the link is located. Both also include the full terms of service with an explanation about why these terms are in place. In short, the free public resources have practical limits. We must ensures fair resource access for you, and for everyone else working at the same service.\nIf you need more resources than the combined resources of all public servers can support or if you simply want more control over how resources are scaled/allocated, private Galaxy server options are available. Several options require very little to no administration. These and other deployment choices are widely utilized globally by both technical and non-technical scientists. Some for short term analysis projects, and some for longer term analysis needs. You can of course still have one account at each public server while using private options. Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use/)\nI\u2019m going to close this topic out. Please direct future administrative account concerns to the private communications channels. Such contact information is usually on the home page of the server or in the public directly listing above. If you can\u2019t find it, and there is not already a topic covering known contact methods for the server you are working at, please post a new question with the URL of the server and we can help."
    }
  ],
  [
    {
      "role": "user",
      "text": "I ran roary on galaxy europe for 37 e coli genomes but the core genome alignment file output is empty, there is no sequences in there."
    },
    {
      "role": "system",
      "text": "Duplicated post, see Roary troubleshooting (version 3.13.0+galaxy2 or later) (@link https://help.galaxyproject.org/t/roary-troubleshooting-version-3-13-0-galaxy2-or-later/9592)\nIf you would like more help, please post back a shared history link to the original topic.\nBefore you do that, please try at least one more rerun. The @link http://UseGalaxy.eu server had some technical issues earlier this week that may have contributed to failures."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear community,\nI read that control is important to get reliable peaks when using MACS2. However, I do not find any control for my treatment regarding the experiment that  I am analyzing. I am using one dataset found in Chip Atlas and they get peak calling reports for the sample. But I do not know how do they define this control. The paper mentioned they used MACS2 but no explicit input controls were given. How is usually the best way to proceed when you lack this information without biasing the results or doing too much data manipulation?  Thank you in advance for any helpful information."
    },
    {
      "role": "system",
      "text": "Hi @user\nGTN provides at least couple tutorials for MACS2\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html)\n\n\nGalaxy Training: ATAC-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html)\nDNA methylation is an epigenetic mechanism used by higher...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/tal1-binding-site-identification/tutorial.html)\n\n\nGalaxy Training: Identification of the binding sites of the T-cell acute l... (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/tal1-binding-site-identification/tutorial.html)\nDNA methylation is an epigenetic mechanism used by higher...\n\n\n\n\n\n\nHope it helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I recieved two already aligned to hg19 and filtered BAM files and wanted to proceed with Variant Calling on VarScan Somatic for comparing normal and diseased samples. I came across the following error message:\n[W::hts_idx_load2] The index file is older than the data file: normal.bam.bai\n[E::mpileup] fail to parse region \u2018chr11_gl000202_random:\u2019 with normal.bam\nIs there something I can do to solve this problem and use these files in VarScan or should I restart the quality control and mapping with the respective FastQ files? Although there is a workflow for that on Galaxy Training, I\u00b4d rather use the BAM files because I have never mapped data before, I usually work with BAM and VCF files. Thanks in advance, any help would be most welcome!"
    },
    {
      "role": "system",
      "text": "Hi Anna,\nfirst, please don\u2019t post issues in multiple places (VarScan error: index file older than data file and fail to parse certain regions \u00b7 Issue #1879 \u00b7 galaxyproject/galaxy-hub \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy-hub/issues/1879)) - it won\u2019t help anyone.\nNow, in your error message E stands for error and W for warning so what causes your tool run to fail is the \u201cchr11_\u201d\u2026 issue. Most likely, your BAM file has been generated against a version of hg19 that contained only the canonical chromosomes, chr11 for example, but not incompletely assembled scaffolds like the one mentioned. Varscan is really strict about this and insists on identity between the reference genome sequence names and the names listed in the header of your BAM file (which you can check yourself by clicking on the eye icon of that dataset and looking for header lines starting with @SQ.\nYou didn\u2019t mention where you have been running this job, but on @link http://usegalaxy.eu, there is a reference genome called \u201chg19 Canonical\u201d, which is identical to the full hg19 for all the standard chromosomes, but is lacking the additional scaffolds, and which you may try as your reference.\nThe warning about the index file is of no relevance in Galaxy and you can safely ignore it.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone, could you please help me?\nWhy is there no \u201cinteractive_tool_simtext_app\u201d tool in Galaxy? I can not do the step after \u201cPMIDs to PubTator\u201d because this tool does not exist at all and gives me this error: \u201cDownload tool interactive_tool_simtext_app Failed: Tool with \u201cinteractive_tool_simtext_app\u201d ID not found\u201d.\nThank you in advance."
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can find that tool in the @link http://usegalaxy.eu instance: Interactive tool simtext app (@link https://usegalaxy.eu/root?tool_id=interactive_tool_simtext_app).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have 4 different groups named as group_1, group_2, group_3, and group_4. 6 samples in each group. The input data was a text file obtained from picrust 2 pathway abundance feature table. I encountered the following error;\n\"/galaxy_venv/local/lib/python2.7/site-packages/rpy2/rinterface/init.py:185: RRuntimeWarning: Error in lda.default(x, grouping, \u2026) :\nvariable 36 appears to be constant within groups\nwarnings.warn(x, RRuntimeWarning)\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/run_lefse.py\u201d, line 89, in \nif params[\u2018rank_tec\u2019] == \u2018lda\u2019: lda_res,lda_res_th = test_lda_r(cls,feats,class_sl,params[\u2018n_boots\u2019],params[\u2018f_boots\u2019],params[\u2018lda_abs_th\u2019],0.0000000001,params[\u2018nlogs\u2019])\nFile \u201c/export/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/lefse.py\u201d, line 189, in test_lda_r\nz = robjects.r(\u2018z \u2190 suppressWarnings(lda(as.formula(\u2019+f+\u2018),data=sub_d,tol=\u2019+str(tol_min)+\u2018))\u2019)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/init.py\u201d, line 359, in call\nres = self.eval(p)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/functions.py\u201d, line 178, in call\nreturn super(SignatureTranslatedFunction, self).call(*args, **kwargs)\nFile \u201c/galaxy_venv/local/lib/python2.7/site-packages/rpy2/robjects/functions.py\u201d, line 106, in call\nres = super(Function, self).call(*new_args, **new_kwargs)\nrpy2.rinterface.RRuntimeError: Error in lda.default(x, grouping, \u2026) :\nvariable 36 appears to be constant within groups\"\nAny suggestions will be appreciated!\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nTools sourced from the Test Toolshed may have technical problems and produce weird errors.\nInstall and use tools from the Main Toolshed here instead: @link https://toolshed.g2.bx.psu.edu/\nIf you run into problems at your own server, try running the same data through the same tool/same tool version at a public usegalaxy.* server (@link https://galaxyproject.org/use) to compare. That will help to isolate usage problems from technical/configuration problems (bugs). You can also first try an internet search using content from an error report if it seems to be produced by the underlying tool itself (unrelated to the Galaxy wrapper). Most tools, including this one, have a lot of discussion at the different public bioinformatics forums, and that discussion is usually helpful even if it is based on running the tool line command instead of Galaxy.\nExample: the immediate problem is probably similar to the one reported in this Lefse forum Q&A: I can't see why I am getting this error in lefse in conda? - LEfSe - The bioBakery help forum (@link https://forum.biobakery.org/t/i-cant-see-why-i-am-getting-this-error-in-lefse-in-conda/3304).\nThat was found with a search using this part of the error message \u201cError in lda.default\u201d + lefse. You can also do that same search to find more Q&A, at that same tool dedicated forum hosted by the tool authors, or just in general. That finds Q&A specific to this tool, if it exists, whether run in Galaxy or not.\nWhat to try: Fix your input data to be compatible/understood by the underlying tool. If that produces an error that has not been discussed at a public forum, then you can try updating the Galaxy tool to be a stable version and proceed from there with troubleshooting via comparisons with public servers running that same tool. If the problem can be reproduced at a public site, you can use that as an example to get more troubleshooting help at this forum. Sometimes sharing the error message is enough, other times including a share a link to the history and noting the datasets involved is better.\nFAQ with more advice: Understanding-input-error-messages (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages)\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nSPades1282\u00d7865 25.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/64aabfc28541c57d3bacebf61a5253744a763e13.png)\nI am trying to run assembly of SARS-COV2 genome from data set SRR10971381. When it shows completion, I don\u2019t get the contig fand scaffold files. It displays 0 bytes. There is a bug that needs to be fixed.\nI get the log file Log File (@link https://usegalaxy.org/datasets/bbd44e69cb8906b5ac60b4288ecdd288/display?to_ext=txt) where no error spotted."
    },
    {
      "role": "system",
      "text": "Hi @user,\nI reported the problem to the Galaxy sysadmins in order to update SPAdes up to version 3.15.4; probably it will fix the problem.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am working on RNA seq data to get the differential gene expression data of sulfate-reducing bacteria. My bioinformatic pipeline is raw read \u2192 fastqc \u2192 trimmomatic \u2192 hisat2 \u2192 featurecounts \u2192 deseq2. I could get results until hisat2. But when I upload the hisat2 BAM file for featurecounts, I am getting an error \u201cAn error occurred with this dataset:\u201d I checked with three different genomes with their corresponding reference genomes and I am getting an error with the featurecounts. Could any one help me with this?\nThank you\nJawahar"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas I told you by email, you need to use paired-end lists, instead of collections of paired-end reads.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to upload a very small txt.file of several bits to galaxy eu. I tried to upload from local files and using ftp. When uploading from local file it says: the job is waiting to run and it does not run at all. When I upload from ftp it says: curl: (7) Failed to connect to @link http://ftp.usegalaxy.eu port 21 after 6085 ms: Network is unreachable.\nHow can I upload my file?\nThanks for answering!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis problem seems to be related to the maintenance tasks that are currently being carried out. Sorry for the inconvenience.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello all,\nI am currently developing synthetic biology tools for Galaxy. We try to use standards of the field including SBML and SBOL (@link https://sbolstandard.org/). The former is supported by Galaxy but I could not find the latter. Is there a roadmap to include SBOL? It simply is an XML, and we are currently defining input/output of the tools to take XML. However this is not ideal.\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nGalaxy filetypes are defined in this file:  @link https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/datatypes_conf.xml.sample#L748\nYou can create a pull request on GitHub to include the SBOL filetype. Here is a link to the documentation: @link https://docs.galaxyproject.org/en/latest/dev/data_types.html\nHope this helps, feel free to ask any further questions.\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am new to galaxy. Just installed local galaxy in my computer and changed admin_users to myself. Yes, after login, I am able to have the admin tab. However, after I clicked on the admin, I realized that the interface of my Galaxy is different from what others showed. See below for a screen shot. It doesn\u2019t show me the Tool Shed option and many other buttons.\nWhat could be wrong? Thanks for help!\nFrankie!\n17|690x375 (@link /uploads/short-url/vRSaJZdWbmyRKm8QgijiEVPsFG4.png)"
    },
    {
      "role": "system",
      "text": "Interface will generally only show you options relevant for your Galaxy. E.g. if you don\u2019t have quotas configured, it won\u2019t show you the UI for defining them. Similar when you have no toolshed tools installed, it will not show you \u2018manage installed tools\u2019 menu item.\nYou can install tools from toolshed using the \u2018install or uninstall\u2019 menu item. (However if this is the \u2018dev\u2019 branch of Galaxy, that won\u2019t work atm, please use the latest release version of Galaxy instead - @link http://getgalaxy.org/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good day. Is there a workflow that I can use to convert fastq files directly to fasta. I need to analyse the raw reads in fasta format and not the consensus sequence. I tried a workflow called fastq to fasta but it doesn\u2019t seem to be working."
    },
    {
      "role": "system",
      "text": "Have you tried FASTQ to FASTA tools (instead of a workflow)? You can search for them in the tool menu. I see two:\nFASTQ to FASTA converter from FASTX-toolkit\nFASTQ to FASTA converter"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have a install galaxy but when i run tools, i don\u2019t see it use multi- thread and more memory. How can I config to use more ram and thread ?\nI have cloned galaxy from github\nI have a computer with 64cpu and 150 gb ram."
    },
    {
      "role": "system",
      "text": "I think most tools from the toolshed use the variable ${GALAXY_SLOTS:-1}. You can change this variable in the file galaxy/config/job_conf.xml. In this file you will see a line like <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"4\"/> You could increase the workers. If you dont have the file job_conf.xml you could use job_conf.xml.sample_basic by doing cp job_conf.xml.sample_basic job_conf.xml. You need to restart galaxy after this change."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have a RNA-seq have no replicates, so I am using edgeR to do DE. and I got counts data from featurecount. I run edgeR, but keep turning red\u2026 the warning message see below.\nCould anyone help me with that?\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nWarning message:\nIn estimateDisp.default(y = y$counts, design = design, group = group,  :\nNo residual df: setting dispersion to NA\nError in plotMDS.default(y, top = top, labels = labels, pch = pch, cex = cex,  :\nOnly 2 columns of data: need at least 3\nCalls: plotMDS \u2192 plotMDS.DGEList \u2192 plotMDS \u2192 plotMDS.default\n"
    },
    {
      "role": "system",
      "text": "hi @user\nit seems you have only two samples, while the latest versions of edgeR require replicates, hence \u201cOnly 2 columns of data: need at least 3\u201d. The best option is to have replicates, at least for one condition. Alternatives: old versions might support analysis without replicates. Try an earlier versions of DESeq2 and/or edgeR. Cuffdiff supports analysis without replicates, at least old versions did.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "The display option after running a tool doesn\u2019t function anymore. I can download the output, but sometimes it\u2019s more convenient to view it real time as the titles of the download are not very intuitive when you run multiple histories in one session."
    },
    {
      "role": "system",
      "text": "Update: Displaying the content of datasets should be working again."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Admin,\nI am writing to request the addition of Plasmodium species (Plasmodium vivax, Plasmodium malaria, Plasmodium ovale and Plasmodium falciaparum) reference genome in the galaxy main webpage. Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nThese genomes can be used as custom reference genomes (fasta) files from the history. If you need a \u201cdatabase\u201d metadata key, promote each fasta to a custom build with a unique name.\nAt the very start of an analysis project, the best practise is to prepare each genome fasta in a standardized format along with the correctly paired and formatted reference annotation (if you plan to incorporate that kind of data).\nFAQs:\n\nworking-with-fasta-datasets (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fasta-datasets)\nreference-genomes (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#reference%20genomes)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#adding-a-custom-database-build-dbkey\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have my personal WGS 100x data in such a big CRAM file (183 GB) that I am not able to work on using public Galaxy servers. So I tried to launch my own GVL instance on AWS.\nUnfortunately now everyone can register and use my GVL instance and I doubt I could allow this financial wise.\nSo I my Q is: how to prevent others from using my private GVL instance deployed on AWS?"
    },
    {
      "role": "user",
      "text": "Solved\ninsert into Galaxy-app/config/galaxy.ini the following lines:\nallow_user_creation = False\nrequire_login = True"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am loading the Gemini load file into Gemini query in \u201cInsert Region filter\u201d section with chromosome number and Genomic coordinates.\nAll the queries are returned with columns name only.\n|gene|chrom|start|end|ref|alt|impact|impact_severity|max_aaf_all|rs_ids\nAny idea why this might be happening?\nAm I doing anything wrong?\nPlease advice.\nThank you.\nCheers,\nShuchi."
    },
    {
      "role": "system",
      "text": "The fixed version will be available in the next days; could you meanwhile use Advanced query construction? You need to use an expression with the following pattern:\nSELECT chrom, start, end, ref, alt, gene, impact FROM variants WHERE chrom = 'chr1' AND  start >= 0 AND end <= 13327\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, can someone help me with this? I am trying to perform a workflow to achieve motif discovery and genome ontology data about the motifs found. I have started with fasta files from a ChIP-seq experiment, then I run FastQ Groomer to my data, followed by Mapped with Bowtie for Illumina, then I used MACS - model-based analysis for ChIP-seq. Based on ABCAM chip-seq tutorial by Xi Cheng, I would be able to get the tabular data from the MACS, generate an interval for the peak summit found using this workflow, to run on MEME-ChIP (another website), and, then, also with the interval, to run on GREAT (genome ontology), however, my MACS are coming out with no results. Please, help me! I had already run 7 different workflows,  using 7 different tutorials, and I have not found how to get there! My public name is paulovga. Help is really appreciated!!!"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you share your Galaxy history with me? I\u2019ll have a look at the MACS results. My email is\nScreenshot from 2022-04-03 16-52-09.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have followed all steps on find my mobile but my galaxyS20 5G will not give location, I can ring it but not locate it"
    },
    {
      "role": "system",
      "text": "Hi, this is the help forum for the galaxy project (@link http://usegalaxy.org) not the Galaxy range from Samsung."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, how can I find out the highest and lowest expression values in a large matrix of single cell gene expression data whose number of columns and rows is greater than 1000?"
    },
    {
      "role": "system",
      "text": "Hi @user\nYou could Sort the file by a specific column. Consider removing any header lines first, as you can always add one back in (exact or just parts of it) later on and some tools might complain about it.\nThis tutorial has a similar method described.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101/tutorial.html#sort-the-exons-by-snps-count)\n\n\nGalaxy Training: Galaxy 101 (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101/tutorial.html#sort-the-exons-by-snps-count)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\nAnd this tutorial includes a bunch of data manipulation methods. Most are \u201cUNIX utility\u201d or \u201cspreadsheet functions\u201d analogues and definitely work on larger tabular text files.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet)\n\n\nGalaxy Training: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#cheatsheet)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nScreen Shot 2022-08-10 at 11.24.48 AM1239\u00d7347 43.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/ee553b9a70dab2915c1f75be85ebbf108840c021.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nHopefully you have discovered this already, but public Galaxy servers usually do not include indexed reference annotation.\nFor the reference genome, try one of these:\n\n\nUse an available build-in reference genome index without annotation.\n\n\nUse a fasta file from the history that represents the reference genome.\n\n\nFor the reference annotation, try:\n\nUse a matching reference annotation provided by you from the history. GTF will work best with this tool and many others.\n\nFAQs. Skip the first three and just review the last one if using a built-in genome index.\n@quote\nworking-with-fasta-datasetsreference-genomesadding-a-custom-database-build-dbkeyworking-with-gff-gft-gtf2-gff3-reference-annotation"
    }
  ],
  [
    {
      "role": "user",
      "text": "Can anyone advice on how to upload a .embl file on galaxy for the purpose of running Maker? When I upload it as auto detect it uploads as txt and maker does not recognize this file. maker also doesn\u2019t seem to accept fasts or gff3 or gtf formats. How have others loaded this file?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin order to use Maker, you should convert the .embl file in fasta format; it can be done with the seqret tool."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m looking for a tool/way to extract the barcode from fastq file and to split fastq file into separate fastq files based on the indices. The indices are placed in the sequence ID. Here is the example:\n@NB552014:62:HGCLWBGXJ:1:11101:8614:1055 1:N:0:ACTATAAC+TAAGATTA\nGATCGNAAGAGCACACGTCTGAACTCCAGTCACACTA\n+\nAAAAA#EEEAEEEEEEAEE/AEEEEEAE//EE</A//\nIncorrect index sequences were entered in the sample sheet and Illumina demultiplexing filed. I have the fastq files and try to demultiplex on my own\u2026any suggestions, please?"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncurrently, there\u2019s not a tool able to perform such operation; I started working on demuxbyname (more information here (@link https://www.seqanswers.com/forum/bioinformatics/bioinformatics-aa/46256-efficient-way-to-split-fastq-files-based-on-illumina-indexes-in-the-id?t=56429)), since I will provide that functionality. I will try to make it available from the next week.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "In the data libraries, afetr clicking on some datasets, it redirects to the covid 19 dataset. Please help in resolving the problem. These datasets ate unrestricted,even then this problem is arising. Please help"
    },
    {
      "role": "system",
      "text": "We believe this has been fixed in [21.05] Fix breadcrumb in dataset details in libraries by OlegZharkov \u00b7 Pull Request #11998 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/pull/11998) we will deploy an update in the coming days with this fix.\nThanks for reporting!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI have a tabular file as output of my tool and I would like to use the first row as header (displayed in blue in the view tab).\nI read an old discussion where they suggested to use the \u2018#\u2019 on the first row to let galaxy knowing that this is the header row but it seems that it is not working since the header is still considered part of the data.\nThank you for the help"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI read an old discussion where they suggested to use the \u2018#\u2019 on the first row to let galaxy knowing that this is the header row\nThat is correct. In the \u201cpeek view\u201d, it will look like this for a tabular datatype with one header row starting with a #.\n\ntabular-header-peek-view526\u00d7532 25.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a4bea8ae4ad2062745ec258e2b1c29d001ed28cc.png)\nHow to interpret:\n\nColumns are numbered with a dark blue background.\nTechnically, and the header is counted up as a header line, other lines are counted as data lines.\n\ntabular is one of the most general datatypes.\n\nThe header line and data lines are passed to downstream tools.\nSome tools will have an option on the form where you can specify the input has a header line, or how many rows to consider as a header line. Other tools won\u2019t, and for these, the user would need to strip header lines out first.\nThere are other defined datatypes that have \u201cbuilt in\u201d headers. Example: bed. What is seen in the \u201cpeek view\u201d for that datatype, under the dark blue background, is defined in the bed datatype specification (built into Galaxy), and not included in the file contents.\n\n\n\nIf you know what tools are likely to consume the output of your tool, you can test out how those interpret your header. Why? Several of the tools under \u201cData Manipulation\u201d expect no header lines in any data. If you want people to be able to use those tools without extra formatting steps, consider not including a header or making it an optional output on your tool\u2019s form. Maybe put some examples in your tool\u2019s help section to alert users about how to use the output with example tools that interpret it differently."
    }
  ],
  [
    {
      "role": "user",
      "text": "hey i deleted my account on accident please may you assist me and reactivate it i need it for school"
    },
    {
      "role": "system",
      "text": "Hello @user\nYou still have one active account at @link http://UseGalaxy.org using your Gmail email address. The other was removed. If you want to swap which single account is active, see this prior topic for how-to: Account marked deleted - #2 by jennaj (@link https://help.galaxyproject.org/t/account-marked-deleted/9015/2)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#account"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am having great difficulty reporting on alpha diversity for kraken2 results! In Galaxy there is no option for Kraken tools and there is no option to generate OTU tables from your results, which would make it possible to analyze alpha diversity using other programs. Does anybody know how to solve this? What is the best path to follow, that is, how to analyze alpha diversity from kkraken2 results?vvvv"
    },
    {
      "role": "system",
      "text": "Hi @user\nPlease consider working at Metagenomics - Galaxy Community Hub (@link https://galaxyproject.org/use/metagenomics/). You\u2019ll find the full suite of available metagenomics tools that have been wrapped for Galaxy hosted there. Those include format conversion tools.\nIf you already have an account at @link http://UseGalaxy.eu, that same account will be your account at @link http://metagenomics.UseGalaxy.eu/, too. If you need to move data between other servers, that can be copied over by dataset URLs pasted into the Upload tool and/or see @link https://training.galaxyproject.org/training-material/faqs/galaxy/#transfer-entire-histories-from-one-galaxy-server-to-another\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\n\n\nGalaxy Training: Metagenomics (@link https://training.galaxyproject.org/training-material/topics/metagenomics/)\nMetagenomics is a discipline that enables the genomic stu...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I had a @link http://usegalaxy.org account and tried to log in, but it said \u201cUser not found.\u201d Please help me. i need my data\u2019s from this account."
    },
    {
      "role": "system",
      "text": "Hi @user\nWe discussed this over email, but for any others that run into a similar problem\u2026\n\nAccounts at different public Galaxy servers are distinct.\n\n\n\nExample 1 these are all different servers with different accounts: @link http://usegalaxy.org.au, @link http://usegalaxy.eu, @link http://usegalaxy.org\n\n\nExample 2 your account at https://@link http://usegalaxy.eu and @link https://covid19.@link http://usegalaxy.eu will be the same. Why? Some servers host multiple domain-specific servers.\nFAQs @link https://training.galaxyproject.org/training-material/faqs/galaxy/#account\n\n\n\nCheck the original account activation email (the one with the link) for details.\n\n\nserver URL\nthe exact email address used for registration (case sensitive)\nadministrative contact\n\n\nAll known public servers are in our directory listing.\n\n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\n\n\nData can be transferred from one server to another without a download step: workflows, histories, and individual datasets.\n\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#workflows\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#histories\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#datasets\n\n\nEach account will have distinct a distinct data quota and jobs will use resources specific to that server. The homepage of the server will usually have more details. Please ask questions if anything is unclear :slight_smile:\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "My galaxy active 2 watch has muted colors, its not unbearable l, but it makes it realy hard to read.\n\n166265408815377412474651538983661920\u00d72560 298 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/874b10445b86eabb7afaddd4ea235e806707f3da.jpeg)\n\n166265414575691729090521960157371920\u00d72560 277 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/12cab23790685503fa4d30c28c64f6f3c6c50bfd.jpeg)\nI\u2019ve checked the settings and i couldnt find anything that could jave been causing it."
    },
    {
      "role": "system",
      "text": "Hello, this forum is made for the scientific open source galaxy project. This is a webapplication that makes bioinformatics tools easily accessible. It looks like you have question about a samsung galaxy product. This forum has nothing to do with samsung products. :slightly_smiling_face:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am analyzing sequences obtained from leukemic cells and I am comparing them with hematopoietic stem cells, but the most used reference genes show a lot of variation \u2026 there is a Galaxy tool that allows me to determine which or these genes are the most suitable \u2026"
    },
    {
      "role": "user",
      "text": "Hello @gallardoalaba, I thank you for your suggestion, it was very useful to me."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am working RNA-seq dataset in influenza virus infection. I am doing differential expression using DeSeq2 in usegalaxy. I am keep getting this error message\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in DESeqDataSet(se, design = design, ignoreRank) :\nCould anyone please give me advice on how to resolve this? This is my first time analyze RNAseq\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis is a technical error.\nFirst, try a rerun.\nNext, check your inputs/parameters. Content and format both matter. The tool form has many details, including a link out to the Bioconductor forum. Galaxy tutorials (@link https://training.galaxyproject.org/) and this forum are also good places to review. All can be searched with keywords like tool names.\nIf you cannot solve the problem and are working at a public Galaxy server, please share back the server URL and the full error report (@link https://training.galaxyproject.org/training-material/faqs/gtn/#what-information-should-i-include-when-reporting-a-problem).\nLet\u2019s start there :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI need to use obitools, and it is available in the toolshed. How do I do to access this within galaxy?\nbest,\nPeter"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe obitools package is available in @link http://usegalaxy.eu.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nSome days ago I realised that the testing toolshed was down. Yesterday it worked again, but it still fails when trying to install any tool from it into a Galaxy instance. Do anyone knows about this error?\nGreetings,\nCarlos"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe Test ToolShed (TTS) has been undergoing some changes in the last few days. It appears to be up now. Maybe try again?\n(( I\u2019m assuming that you already know to use the Main ToolShed (MTS) instead for \u201cproduction-ready\u201d tools (where most people should source most tools). ))\nThe current status for these and other core usegalaxy.* servers, plus associated key resources, can always be found here: @link https://galaxyproject.statuspage.io/\nLonger or planned events will usually have some kind of notation added. But you can always ask for more details at this forum \u2013 or for even quicker Q&A suitable for a chat, reach us at Gitter @link https://gitter.im/galaxyproject/Lobby.\nWe can try to clarify at either.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I just downloaded a 90 Gb file to use as a database and now galaxy won\u2019t open.\nError message is below:\nThis page isn\u2019t working\n@link http://usegalaxy.org  is currently unable to handle this request.\nHTTP ERROR 500"
    },
    {
      "role": "user",
      "text": "Side note - typing @link http://usegalaxy.org into the site isn\u2019t working, but I tried @link http://usegalaxy.org/login and it worked! The page opened and allowed me to log in! (In case anyone is having a similar issue)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use DESEQ2 dor DGE analysis.  I am using paired datasets uploaded from GEO, aligned with Hisat2 and counted with HTseq-counts.  I am selecting the input files, Have the source set as HTseq-counts, but I keep getting the error below.\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in .rowNamesDF<-(x, value = value) :\nduplicate \u2018row.names\u2019 are not allowed\nCalls: rownames<- \u2026 row.names<- \u2192 row.names<-.data.frame \u2192 .rowNamesDF<-\nWarning message:\nnon-unique value when setting \u2018row.names\u2019: \u2018SRR16947329\u2019\nI am not manipulating the dataframe from the counting program at all prior to input.  It has worked before.  At a loss on what to correct.\nThanks for any help!"
    },
    {
      "role": "user",
      "text": "Nevermind.  For all other newbies bumbling around out here  I will share my mistake\u2026  One of my input files was a duplicate right from the start.  I used tags etc on the downloads but must have accidently double downloaded it right in the beginning, so the underlying file names were the same.  I merged the collections of paired end runs and it dropped the duplicate.  When I ran with the new merged collections worked fine."
    }
  ],
  [
    {
      "role": "user",
      "text": "I run the bowtie2 to map reads against reference genome,It\u2019s show:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Uncaught exception in exposed API method:\n{\n\u201chistory_id\u201d: \u201c87610437856c28e6\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.3+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c2.3.4.3+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201clibrary|type\u201d: \u201csingle\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33a134933e9419880e8\u201d,\n\u201chid\u201d: 21,\n\u201cname\u201d: \u201cCaenorhabditisbriggsae_AD859-R01_good_1.fq\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201clibrary|unaligned_file\u201d: \u201cfalse\u201d,\n\u201clibrary|aligned_file\u201d: \u201cfalse\u201d,\n\u201creference_genome|source\u201d: \u201chistory\u201d,\n\u201creference_genome|own_file\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c11ac94870d0bb33ae87e4fc510d80ae4\u201d,\n\u201chid\u201d: 3,\n\u201cname\u201d: \u201cWS220.64.fa\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201crg|rg_selector\u201d: \u201cset\u201d,\n\u201crg|read_group_id_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_id_conditional|ID\u201d: \u201c001\u201d,\n\u201crg|read_group_sm_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_sm_conditional|SM\u201d: \u201cyq565\u201d,\n\u201crg|PL\u201d: \u201cILLUMINA\u201d,\n\u201crg|read_group_lb_conditional|do_auto_name\u201d: \u201cfalse\u201d,\n\u201crg|read_group_lb_conditional|LB\u201d: \u201c\u201d,\n\u201crg|CN\u201d: \u201c\u201d,\n\u201crg|DS\u201d: \u201c\u201d,\n\u201crg|DT\u201d: \u201c\u201d,\n\u201crg|FO\u201d: \u201c\u201d,\n\u201crg|KS\u201d: \u201c\u201d,\n\u201crg|PG\u201d: \u201c\u201d,\n\u201crg|PI\u201d: \u201c\u201d,\n\u201crg|PU\u201d: \u201c\u201d,\n\u201canalysis_type|analysis_type_selector\u201d: \u201csimple\u201d,\n\u201canalysis_type|presets\u201d: \u201cno_presets\u201d,\n\u201csam_options|sam_options_selector\u201d: \u201cno\u201d,\n\u201csave_mapping_stats\u201d: \u201cfalse\u201d\n}\n}"
    },
    {
      "role": "system",
      "text": "Dear @user,\nThere might be different reasons for that. Please try to run bowtie2 again on your datasets and maybe wait half an hour and tr again, to see if it is a server error.\nIf the error persists, then check if you data is correct, meaning, your input data might be corrupted or not in the correct format. Or an option in the tool is wrong.\nHave a good weekeend and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nWhen I try to browse through my files to select input files for a tool, I get the error message \u2018Internal Server Error (500)\u2019.\nKind regards,\nLou"
    },
    {
      "role": "system",
      "text": "Hello @user\nThe @link http://UseGalaxy.eu server is up and working without known issues: @link https://status.galaxyproject.org/\nPlease try again. Sometimes a server will reject connections temporarily due to being busy, or restarting.\nIf that is not enough, check the input datasets. Is there anything special about them? Can you expand the datasets and review the contents? Were the data fully uploaded with a datatype assigned? If you directly assigned that datatype, instead of allowing Galaxy to detect it, you could try re-autodetecting the datatype. You might want to do that anyway to see if it resolve the problem. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#detecting-the-datatype-file-format\nLet\u2019s start there. If you still need more help after checking on your own, please post back a share link to your history publicly as a reply or ask for a moderator to set up a direct message to share it in privately. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy,\nI noticed that HTML outputs from QUAST and antismash can\u2019t be viewed in a browser (Safari or Chrome) while it works for FastQC. It might be the case for other tools, I didn\u2019t test (e.g, MultiQC).\nCould be a whitelisting issue.\nIt would be cool if Galaxy would manage this automatically.\nThanks for you fantastic work.\nLaurent"
    },
    {
      "role": "system",
      "text": "Hi @user\nYes, those two needed to be added to the AllowList. I just did that at @link http://UseGalaxy.org. Please test it out. Should work on data that is already generated \u2013 but you might need to reload the browser page first.\n@quote\nIt would be cool if Galaxy would manage this automatically\nAgree! But we like to have a person review even if sometimes these get missed. Just let us know if you run into this again. Including/confirming the server URL is super helpful.\nThanks :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear support team at GALAXY,\nI am trying to perform a RNA-seq analysis for D. melanogaster. I did the mapping with Bowtie2, using the genome assembly: dmelo_r6.32 and all were fine, nice mapping stats etc. Then, I tried to generate the necessary matrices for the DEseq tool via using the tool HTseq:\nWhen I used the corresponding .GFF ( dmel-all-r6.32.gff.gz ) for this particular assembly, I got the error below:\nimage\nWhen I used the .GFF for the assembly 6 from the NCBI, then I got the error below:\n\nimage770\u00d7272 38.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1ffc69405f92a6435f1ec4b6452db73ab261942c.jpeg)\nAny input on how to overcome this obstacle in my analysis will be highly appreciated.\nManolis"
    },
    {
      "role": "user",
      "text": "Hi,\nThank you very much for your response and my apologies for my delayed answer.\nI have resolved the issue by using a different (an older) assembly genome file and its corresponding GFF.\nGreetings,\nManolis"
    }
  ],
  [
    {
      "role": "user",
      "text": "I keep getting errors when trimming my data with Cutadapt. Please find the error below.\nI ran exactly the same thing last week with the @link http://galaxy.org server and there it worked.\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\n{\n\u201chistory_id\u201d: \u201cff75df6a1c92239a\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.0+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c4.0+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201clibrary|type\u201d: \u201cpaired_collection\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201c72822dfb6c647067\u201d,\n\u201chid\u201d: 21,\n\u201cname\u201d: \u201cPaired_List_NAPC2_FASTA\u201d,\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: ,\n\u201cmap_over_type\u201d: \u201cpaired\u201d\n}\n],\n\u201cbatch\u201d: true\n},\n\u201clibrary|r1|cut\u201d: \u201c0\u201d,\n\u201clibrary|r2|cut2\u201d: \u201c0\u201d,\n\u201clibrary|r2|quality_cutoff2\u201d: \u201c\u201d,\n\u201cadapter_options|action\u201d: \u201ctrim\u201d,\n\u201cadapter_options|internal\u201d: \u201c\u201d,\n\u201cadapter_options|error_rate\u201d: \u201c0.1\u201d,\n\u201cadapter_options|no_indels\u201d: \u201cfalse\u201d,\n\u201cadapter_options|times\u201d: \u201c1\u201d,\n\u201cadapter_options|overlap\u201d: \u201c3\u201d,\n\u201cadapter_options|match_read_wildcards\u201d: \" \",\n\u201cadapter_options|revcomp\u201d: \u201cfalse\u201d,\n\u201cfilter_options|discard_trimmed\u201d: \u201cfalse\u201d,\n\u201cfilter_options|discard_untrimmed\u201d: \u201cfalse\u201d,\n\u201cfilter_options|minimum_length\u201d: \u201c20\u201d,\n\u201cfilter_options|maximum_length\u201d: \u201c\u201d,\n\u201cfilter_options|length_R2_options|length_R2_status\u201d: \u201cFalse\u201d,\n\u201cfilter_options|max_n\u201d: \u201c\u201d,\n\u201cfilter_options|pair_filter\u201d: \u201cany\u201d,\n\u201cfilter_options|max_expected_errors\u201d: \u201c\u201d,\n\u201cfilter_options|discard_cassava\u201d: \u201cfalse\u201d,\n\u201cread_mod_options|quality_cutoff\u201d: \u201c20\u201d,\n\u201cread_mod_options|nextseq_trim\u201d: \u201c0\u201d,\n\u201cread_mod_options|trim_n\u201d: \u201cfalse\u201d,\n\u201cread_mod_options|strip_suffix\u201d: \u201c\u201d,\n\u201cread_mod_options|shorten_options|shorten_values\u201d: \u201cFalse\u201d,\n\u201cread_mod_options|length_tag\u201d: \u201c\u201d,\n\u201cread_mod_options|rename\u201d: \u201c\u201d,\n\u201cread_mod_options|zero_cap\u201d: \u201cfalse\u201d,\n\u201coutput_selector\u201d: [\n\u201creport\u201d\n]\n}\n}"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you try again by using the tool version 3.7 (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/3.7+galaxy0)? It is the version available in @link http://usegalaxy.org. Perhaps there\u2019s a bug in the last update (version 4.0).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m doing the NGS tutorial from NGS data logistics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-ngs-data-managment/tutorial.html).\nAt one of the steps, I have to import a genome NC_045512.2 from Genbank via a link:\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.fna.gz\n\nI get a warning: \u201cUnsupported Media type (415)\u201d and the upload doesn\u2019t happen. It looks like this:\n\nimage899\u00d7549 22.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/620406c01ee6d43b5f260692208cd1522a0db0dd.png)\nI check and the link itself works if I just put it into my browser.\nMy history is here: Galaxy | Europe | Accessible History | Galaxy Introduction Part 2 (@link https://usegalaxy.eu/u/pashadag/h/galaxy-introduction-part-2)\nThank you,\nPaul"
    },
    {
      "role": "system",
      "text": "This looks like .*gz files are failing to download under Chromium \u00b7 Issue #12330 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/12330).\nThis is usually temporary, but of so far unknown cause. You could actually help us debug this if you could go to the developer console of your browser (with F12) and get us query and response details while you have this error from the \u201cNetwork\u201d tab.\nThanks in advance!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m getting an error when using the last step of the LEfSe galaxy module ( Plot LEfSe results). Error text posted below\nTraceback (most recent call last):\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 151, in\nelse: plot_histo_hor(params[\u2018output_file\u2019],params,data,len(data[\u2018cls\u2019]) == 2)\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 54, in plot_histo_hor\nif bcl: data[\u2018rows\u2019].sort(key=lambda ab: fabs(float(ab[3]))*(cls.index(ab[2]) 2-1))\nFile \u201c/shed_tools/testtoolshed.g2.bx.psu.edu/repos/george-weingart/lefse/a6284ef17bf3/lefse/./plot_res.py\u201d, line 54, in\nif bcl: data[\u2018rows\u2019].sort(key=lambda ab: fabs(float(ab[3])) (cls.index(ab[2])*2-1))\nValueError: invalid literal for float(): 3wo\nI\u2019ve encountered the same error before, but that was due to having more than 2 classes in the data. This set only has 2, and otherwise worked for a different subset that I entered just a few minutes prior.\nAny help would be great, thank you so much!"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nValueError: invalid literal for float(): 3wo\nThis part of the error message indicates that there is a problem with the input data to this step. Maybe check your data/settings again? Maybe the wrong column of data was selected for plotting. Or, there is an unexpected header?\nYou are also using a version of the tool that was sourced from the Test ToolShed, which means that it might not be \u201cproduction-ready\u201d or have unresolved development bugs. A better choice would be to install and use the versions of the tool suite from the Main ToolShed: @link https://toolshed.g2.bx.psu.edu/\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nhow can I find the read length in a fq.gz file for RNA Star?\nThanks in advance,\nBernhard"
    },
    {
      "role": "system",
      "text": "@user , are you talking about Compute sequence length (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/fasta_compute_length/fasta_compute_length/1.0.3) ?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have files in .loom format from a very large single cell RNA seq experiment, and I would like to try to analyse it in Galaxy. I was happy to see that in the upload page, it asks what kind of file you are uploading, and the dropdown menu has .Loom in the list. However, there are absolutely no built-in tools to do anything with it.\nHas anyone ever used Galaxy for loom files, and if so, can you provide any guidance?"
    },
    {
      "role": "system",
      "text": "Hi there,\nI am currently incorporating LOOM support into a single-cell RNA tool RaceID here (@link https://github.com/galaxyproject/tools-iuc/pull/2187).\nThere has been much talk of incorporating LOOM support into many of the scRNA tools that will be present in galaxy, as evidenced by this thread here (@link https://github.com/galaxyproject/tools-iuc/issues/2057).\nPlease stay tuned for updates throughout the year!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am trying to have DEGs but I put all files .sf and I executed that but these process don\u2019t started to run. I deleted all information that I don\u2019t need to free space but is the same, could you help me please?\nthank you very much"
    },
    {
      "role": "system",
      "text": "Update\nDelays are confirmed. See the post below for details.\n\nHi @user\nAt least two of the public servers have been busy for other people.\nFor @link http://usegalaxy.org, we\u2019ll post any updates here: Fastp not running: Confirmed job delays at UseGalaxy.org 09/23/2022. Solution: allow queued jobs to process (@link https://help.galaxyproject.org/t/fastp-not-running/8720)\n@quote\nI deleted all information that I don\u2019t need to free space but is the same\nHow much free storage space an account has is unrelated to how jobs queue and process.\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to configure Slurm following the tutorial from may 2022 (I am using an older version of the tutorial since I am using an older version, version 22.01, of Galaxy as well) (admin tutorial: Connecting Galaxy to a compute cluster (@link https://training.galaxyproject.org/archive/2022-05-01/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html#running-a-job)). Here I saw that the tutorial for connecting to a compute cluster contained some setting for Singularity, but Singularity wasn\u2019t part of the requirements and isn\u2019t installed on my server. I am wondering if I could just skip these lines containing the setting for singularity or if it is necessary to install singularity.\nBesides this, I am quite lost on how to exactly set the usage of CPU and RAM for each tool, since this is my first time using Slurm (or any workload manager/job scheduler for that matter). Can I just create new partitions and set these partitions for the destination id together with slurm as the runner? And what is the param id? I had the following settings in mind for Slurm, but am wondering if this would actually work or not.\nAny feedback/help is appreciated! Thanks in advance!\nParts of the files containing my settings:\nrequirements.yml\n- src: galaxyproject.repos\n  version: 0.0.2\n- src: galaxyproject.slurm\n  version: 0.1.3\n\ngalaxy.yml\n roles:\n    # SLURM\n    - galaxyproject.repos\n    - galaxyproject.slurm\n\ngalaxyservers.yml\n# Slurm\nslurm_roles: ['controller', 'exec']\nslurm_nodes:\n- name: localhost \n  CPUs: 14 # Host has 16 cores total\n  RealMemory: 110000 # 110000MB, viewed with command 'free --mega' which shows 135049MB total free\n  ThreadsPerCore: 1\n# slurm_partitions:\n#  - name: main\n#    Nodes: localhost\n#    Default: YES\nslurm_config:\n  SlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2\n  SelectType: select/cons_res\n  SelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node\n\njob_conf.xml.j1\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"local_plugin\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n        <plugin id=\"slurm\" type=\"runner\" load=\"galaxy.jobs.runners.slurm:SlurmJobRunner\"/>\n            <param id=\"drmaa_library_path\">/usr/lib/slurm-drmaa/lib/libdrmaa.so.1</param>\n    </plugins>\n    <destinations default=\"slurm\">\n        <destination id=\"local_destination\" runner=\"local_plugin\"/>\n        <destination id=\"partition_1\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=1 --mem=4000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n        <destination id=\"partition_2\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=2 --mem=6000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n        <destination id=\"partition_3\" runner=\"slurm\">\n            <param id=\"?\">--nodes=1 --ntasks=1 --cpus-per-task=4 --mem=16000</param>\n            <param id=\"tmp_dir\">True</param>\n        </destination>\n    </destinations>\n    </limits>\n     <tools>\n        <tool id=\"tool_example_1\" destination=\"partition_1\"/>\n        <tool id=\"tool_example_2\" destination=\"partition_2\"/>\n        <tool id=\"tool_example_3\" destination=\"partition_3\"/>\n     </tools>\n</job_conf>\n"
    },
    {
      "role": "system",
      "text": "@quote\nbut Singularity wasn\u2019t part of the requirements and isn\u2019t installed on my server. I am wondering if I could just skip these lines containing the setting for singularity or if it is necessary to install singularity.\nYes you can just skip that then. It\u2019s not necessary, it\u2019s an additional thing.\n@quote\nBesides this, I am quite lost on how to exactly set the usage of CPU and RAM for each tool, since this is my first time using Slurm (or any workload manager/job scheduler for that matter). Can I just create new partitions and set these partitions for the destination id together with slurm as the runner? And what is the param id? I had the following settings in mind for Slurm, but am wondering if this would actually work or not.\nyou are very much on the right path. Setting multiple destinations, each with their own submission parameters is the right answer. You\u2019re looking for the next tutorial in the series: Mapping Jobs to Destinations (@link https://training.galaxyproject.org/archive/2022-05-01/topics/admin/tutorials/job-destinations/tutorial.html#running-with-more-resources)\nyour id=\"?\" should be \u201cnativeSpecification\u201d:\n<param id=\"nativeSpecification\">--nodes=1 --ntasks=1 --cpus-per-task=2</param>\n\nAnd then it should be fine. Next you\u2019ll have to map specific tools to those specific destinations as you\u2019ve started doing. Good luck! Apologies for the delayed response."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019m beginner of this Seq analysis starting with Galaxy.\nI have studied this about 2 months.\nI learned rna-seq analysis using galaxy through tutorial.\nReference-based RNA-Seq data analysis (galaxyproject.org) (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html)\nI followed the tutorial perfectly using data set provided by tutorial, not any error.\nbut I can not execute \u201cGoseq\u201d process with my data, before this process everything is fine.\nimage\nI checked my two input data ( [geneid, logic], [gene id, length])\nBut I checked my logic data after \u201ccompute (bool<0.05)\u201d became short line (55402 ->17081).\nimage\nI found the reason is my adj-p value of lots of genes are NA.\nI am not sure It is reason of error.\nbut, I downloaded my data to my computer as a excel format.\nand I intentionally changed NA to False and upload this file to galaxy history again.\nimage\nbecause I think the reason of Goseq error is not the same number of row between two input data.\nBut It is also not excuted.\nPlease help me."
    },
    {
      "role": "system",
      "text": "Hi @system and @user\n\nError in [.default(summary(map), , 1) : incorrect number of dimensions\nCalls: run_goseq \u2026 goseq \u2192 reversemapping \u2192 [ \u2192 [.table \u2192 NextMethod\n\nThe error means that the tool couldn\u2019t construct a matrix of the two inputs correctly. Check to make sure that the identifiers in the \u201cDifferentially expressed genes file\u201d are all contained in the \u201cGene lengths file\u201d and that the identifiers in the same sort order.\nAll this should already be correct if the steps in either of the tutorials below were all done. If for some reason you need to do that again, the files could be joined and split up again using the original tutorial methods or using manipulations explained in the last tutorial.\nExamples of accepted formats and manipulations are here. The pre-filters remove any rows with lower scores (and NA values) before the compute step: 3: RNA-seq genes to pathways (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-genes-to-pathways/tutorial.html#gene-set-testing) and here Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#extraction-and-annotation-of-differentially-expressed-genes)\nThere should never be a need to download data, manipulate it, then load it back to Galaxy. Especially not using Excel as that can introduce many format issues including hidden characters. If you are not sure how to do text manipulations in Galaxy, see this tutorial  Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI would like to combine samples from different studies to do differential gene expression. For example, paper 1 has published 10 nasal samples from healthy children and 10 nasal samples from children with COVID infections; and paper 2 from a different research group has published 10 nasal samples with influenza infections. They may or may not have used the same sequencing platform/kits and seq depths.\nCan I download the samples from both papers and do the the following DEG/pathway analyses etc on the dataset:\nhealthy children from paper 1 (as Factor 1), COVID from paper 1 (as factor 2)\nhealthy children from paper 1 (as Factor 1) and influenza from paper 2 (as Factor 2)?\nCOVID from paper 1 (as Factor 1) and influenza from paper 2 (as factor 2)\nHealthy children (factor 1) vs COVID (factor 2) vs. Influenza (factor 3)\nSince I will be using the same bioinformatic tools and commands to process both sets in the same analysis and all transcripts will be aligned to the same assembly, will this normalize data correctly and allow for valid comparisons?\nThanks,\nSwati"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe ask people at a bioinformatics forum or DGE software developers. How do you distinguish between batch effect and potential biological effect in the described scenario with one condition coming from a different source?\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am unable to sign in on the GalaxyTrakr platform since it states that I have an invalid password. When I want to change it i get an error:\nFailed to submit an email. Please contact the administrator: (535, b__sq__5.7.8 Username and Password not accepted. Learn more atXn5.7.8 Check Gmail through other email platforms - Gmail Help (@link https://support.google.com/mail/?p=BadCredentials) ay34-20020a05620a17a200b006bad7a2964fsm9454685qkb.78 - gsmtp__sq__)\nIs it possible to fix the problem?\nThank you for your help.\nKatarina B"
    },
    {
      "role": "system",
      "text": "I think you need to email them directly, see:\n\n@link https://galaxy-genometrakr-docs.s3.amazonaws.com/userguides/FAQs+for+GalaxyTrakr.pdf\n\n\n@link https://galaxy-genometrakr-docs.s3.amazonaws.com/userguides/FAQs+for+GalaxyTrakr.pdf\n@link https://galaxy-genometrakr-docs.s3.amazonaws.com/userguides/FAQs+for+GalaxyTrakr.pdf\n476.13 KB\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nPlease inform,\nI have stringtie reads count table (-B from Ballgown function) generated of which I used to perform deseq2 analysis. Please inform me how to use read count as input in deseq2 in galaxy. Tximport is not working for data.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe output generated when using the -B option in Stringtie cannot be used as input for DESeq2. The help section of the Strintie tool includes the instructions for using Stringtie + DESeq2:\n\n@link https://bioconductor.org/packages/release/bioc/html/DESeq2.html, @link https://bioconductor.org/packages/release/bioc/html/edgeR.html and @link https://bioconductor.org/packages/release/bioc/html/limma.html are three popular Bioconductor (@link https://www.bioconductor.org/) packages for analyzing differential expression, which take as input a matrix of read counts mapped to particular genomic features (e.g., genes). This read count information can be extracted directly from the files generated by StringTie (run with the -e parameter) by selecting DESeq2/edgeR/limma-voom under Output files for differential expression? above. This uses the StringTie helper script prepDE.py to convert the GTF output from StringTie into two tab-delimited (TSV) files, containing the count matrices for genes and transcripts, using the coverage values found in the output of StringTie -e.\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All,\nI am attempting to analyze a genic locus starting from Whole genome sequencing data. I am a newbie in this field (to date I have always used specific software that provided directly to give a final result.\nNow I need to analyze a specific genic sequence that I am not able to obtain with previously cited software, but I should use Galaxy (easier than linux with command line scripts).\nI have fastQ files (by Illumina paired end) that I have firsly analyzed by using Trimmomatic and then align (with BWA) to a reference genome. So, I have obtained a .bam file. I am attempting to obtain the consensus fasta sequence from this file\u2026.is it possible? o I am skipped some steps???\nI have been using Galaxy tools.\nThank you in advance for you help"
    },
    {
      "role": "system",
      "text": "Hi - Is your goal to explore variation?\nIf so, please review the tools below and see these Galaxy tutorials for examples of complete analysis workflows that include intermediate tools.\n\n\nCommon tools to use from the step you are at now can be reviewed/used at Public Galaxy servers such as Galaxy Main @link https://usegalaxy.org, Galaxy EU @link https://usegalaxy.eu, and Galaxy AU @link https://usegalaxy.org.au (All Public Galaxy servers: @link https://galaxyproject.org/use/). Try searching for tools by these keywords: pileup or variant or vcf.\n\n\nThe Galaxy ToolShed @link https://usegalaxy.org/toolshed can also be searched for tools that can be used in your own Galaxy instance @link https://galaxyproject.github.io/.\n\n\nGTN Tutorials: Variant Analysis\n\n\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/variant-analysis/\n\n\n\nGalaxy Training: Variant Analysis (@link https://galaxyproject.github.io/training-material/topics/variant-analysis/)\nGenetic differences (variants) between healthy and diseas...\n\n\n\n\n\n\nMore about how/where to use Galaxy GTN tutorials and the included tools/workflows for your own work.\n\n@quote\nGalaxy Training Network Tutorials: SomeGTNtutorials are appropriate forGalaxy Mainand some are not. Where you can run each is noted per tutorial \u2013 click on theGalaxy instancesgear iconto review thePublic Galaxyserver choices. If a tutorial is supported by a pre-configuredGalaxy Dockertraining image, instructions for how to get it will be listed below the tutorial listings, per category.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI would like to do some modification to a vcf (changing one column name in the vcf header starting with #). Can I just do this modification with TextEdit and save the vcf in its .vcf format or will this be problematic for downstream analysis?\nIn order to have the most privacy for my data possible I would like to set the settings accordingly. I read the the information about data privacy in Galaxy (@link https://galaxyproject.org/learn/privacy-features/) and I am wondering if this \u201cMake all data private\u201d in user preferences is what I would select and if there are more possible safety settings to change?\nThanks a lot.\nRose"
    },
    {
      "role": "system",
      "text": "@quote\nI would like to do some modification to a vcf (changing one column name in the vcf header starting with #). Can I just do this modification with TextEdit and save the vcf in its .vcf format or will this be problematic for downstream analysis?\nIt depends on what exactly you are changing in the \u201c#\u201d header line. The first eight (or nine) columns are required metadata. If you are just updating a sample name, that should be fine. VCF format: @link https://vcftools.github.io/specs.html\n@quote\nIn order to have the most privacy for my data possible I would like to set the settings accordingly. I read the the information about data privacy in Galaxy (https://galaxyproject.org/learn/privacy-features/) and I am wondering if this \u201cMake all data private\u201d in user preferences is what I would select and if there are more possible safety settings to change?\nYes, that is how to make all data private, existing and newly uploaded."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nit seems that the testtoolshed is still since a week and I need a repository that is on it.\nDo you know if a mirror exists somewhere please ?\nThank you"
    },
    {
      "role": "system",
      "text": "Afaik there is no mirror, but we\u2019ll get it up shortly. Sorry for the inconvenience.\nedit: up now"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am creating a galaxy tool and I need to get as output a number of collections that depends on a value contained in an input file. I got the value from my file and did a for loop using this value. However, I still get only one collection as output. Is it possible to get a variable number of collections? And if it is possible, how can I do it?"
    },
    {
      "role": "user",
      "text": "Thanks @system, I managed to work around the problem by using nested collections but the solution is not perfect."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nAfter successfully installing Ansible I have tried setting up my own Galaxy server on a VM. During the tutorial: \u201cGalaxy Installation with Ansible\u201d (Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)), I came across an error.\nI have added the uWSGI to the playbook (galaxyservers.yml). However, when I run the playbook (\"Installing Galaxy; Galaxy; point 5: Run the playbook: ansible-playbook galaxy.yml), I receive the following error message:\nubuntu@packer-Ubuntu-18:~/galaxy/group_vars$ ansible-playbook galaxy.yml\nPLAY [galaxyservers] **********************************************************************************************************************************************************************************************************************************************************\nTASK [Gathering Facts] ********************************************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [Install Dependencies] ***************************************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nincluded: /home/ubuntu/galaxy/group_vars/roles/galaxyproject.postgresql/tasks/debian.yml for 145.100.59.212\nTASK [galaxyproject.postgresql : Install pgdg package signing key (Debian/pgdg)] **********************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install pgdg repository (Debian/pgdg)] *******************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install PostgreSQL (Debian)] *****************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Get installed version] ***********************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set version fact] ****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set version fact] ****************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set OS-specific variables] *******************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set pgdata fact] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set conf dir fact] ***************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Create conf.d] *******************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Check for conf.d include in postgresql.conf] *************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set conf.d include in postgresql.conf] *******************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Include 25ansible_postgresql.conf in postgresql.conf] ****************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Set config options] **************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : Install pg_hba.conf] *************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.postgresql : include_tasks] *******************************************************************************************************************************************************************************************************************************\nskipping: [145.100.59.212]\nTASK [galaxyproject.postgresql : Ensure PostgreSQL is running] ****************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [natefoo.postgresql_objects : Revoke extra privileges] *******************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Drop databases] ****************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create and drop users] *********************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create groups] *****************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Add or remove users from groups] ***********************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Drop groups] *******************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Create databases] **************************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Grant user database privileges] ************************************************************************************************************************************************************************************************************\nTASK [natefoo.postgresql_objects : Grant extra privileges] ********************************************************************************************************************************************************************************************************************\nTASK [geerlingguy.pip : Ensure Pip is installed.] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [geerlingguy.pip : Ensure pip_install_packages are installed.] ***********************************************************************************************************************************************************************************************************\nTASK [galaxyproject.galaxy : Ensure that mutually exclusive options are not set] **********************************************************************************************************************************************************************************************\nok: [145.100.59.212] => {\n\u201cchanged\u201d: false,\n\u201cmsg\u201d: \u201cAll assertions passed\u201d\n}\nTASK [galaxyproject.galaxy : Set privilege separation default variables] ******************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.galaxy : Include layout vars] *****************************************************************************************************************************************************************************************************************************\nok: [145.100.59.212]\nTASK [galaxyproject.galaxy : Set any unset variables from layout defaults] ****************************************************************************************************************************************************************************************************\nfatal: [145.100.59.212]: FAILED! => {\u201cmsg\u201d: \u201cThe task includes an option with an undefined variable. The error was: \u2018galaxy_server_dir\u2019 is undefined\\n\\nThe error appears to be in \u2018/home/ubuntu/galaxy/group_vars/roles/galaxyproject.galaxy/tasks/layout.yml\u2019: line 7, column 3, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n- name: Set any unset variables from layout defaults\\n  ^ here\\n\u201d}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\n145.100.59.212             : ok=18   changed=0    unreachable=0    failed=1    skipped=17   rescued=0    ignored=0\nindent preformatted text by 4 spaces\nI speculate that it may have something to do with either the loop (with_items)  or the set_fact line in the layout.yml file. The variable galaxy_server_dir, however, is undefined whilst galaxy_venv_dir is not (see the added contents of the .yml files). Do you maybe have an idea what causes this error or how I am able to fix it?\nFor any information about the contents of the .yml files:\n\nPath to galaxy.yml\n\n#~/galaxy/groups_vars/galaxy.yml\n\nhosts: galaxyservers\nbecome: true\npre_tasks:\n\nname: Install Dependencies\npackage:\nname: [\u2018git\u2019, \u2018make\u2019, \u2018python3-psycopg2\u2019, \u2018virtualenv\u2019]\nroles:\ngalaxyproject.postgresql\nrole: natefoo.postgresql_objects\nbecome: true\nbecome_user: postgres\ngeerlingguy.pip\ngalaxyproject.galaxy\nrole: uchida.miniconda\nbecome: true\nbecome_user: galaxy\n\n\n\n\nPath to galaxyservers.yml\n\n~/galaxy/groups_vars/galaxyservers.yml\n\npython3 support\npip_virtualenv_command: /usr/bin/python3 -m virtualenv # usegalaxy_eu.certbot, usegalaxy_eu.tiaas2, galaxyproject.galaxy\ncertbot_virtualenv_package_name: python3-virtualenv    # usegalaxy_eu.certbot\npip_package: python3-pip                               # geerlingguy.pip\n\nPostgreSQL\npostgresql_objects_users:\n\nname: galaxy\npostgresql_objects_databases:\nname: galaxy\nowner: galaxy\n\n\nGalaxy\ngalaxy_create_user: true\ngalaxy_separate_privileges: true\ngalaxy_manage_paths: true\ngalaxy_layout: root-dir\ngalaxy_root: /srv/galaxy\ngalaxy_user: {name: galaxy, shell: /bin/bash}\ngalaxy_commit_id: release_20.01\ngalaxy_config_style: yaml\ngalaxy_force_checkout: true\nminiconda_prefix: \u201c{{ galaxy_tool_dependency_dir }}/_conda\u201d\nminiconda_version: \u201c4.6.14\u201d\ngalaxy_config:\ngalaxy:\nbrand: \u201c@@@@@\u201d\nadmin_users: @@@@@@@@@@@@@@@@@@@\ndatabase_connection: \u201cpostgresql:///galaxy?host=/var/run/postgresql\u201d\nfile_path: /data\ncheck_migrate_tools: false\ntool_data_path: \u201c{{ galaxy_mutable_data_dir }}/tool-data\u201d\nuwsgi:\nhttp: 0.0.0.0:8080\nbuffer-size: 16384\nprocesses: 1\nthreads: 4\noffload-threads: 2\nstatic-map:\n- /static/style={{ galaxy_server_dir }}/static/style/blue\n- /static={{ galaxy_server_dir }}/static\n- /favicon.ico={{ galaxy_server_dir }}/static/favicon.ico\nstatic-safe: client/galaxy/images\nmaster: true\nvirtualenv: \u201c{{ galaxy_venv_dir }}\u201d\npythonpath: \u201c{{ galaxy_server_dir }}/lib\u201d\nmodule: galaxy.webapps.galaxy.buildapp:uwsgi_app()\nthunder-lock: true\ndie-on-term: true\nhook-master-start:\n- unix_signal:2 gracefully_kill_them_all\n- unix_signal:15 gracefully_kill_them_all\npy-call-osafterfork: true\nenable-threads: true\nmule:\n- lib/galaxy/main.py\n- lib/galaxy/main.py\nfarm: job-handlers:1,2\n\nPath to layout.yml\n\n~//galaxy/groups_vars/roles/galaxyproject.galaxy/tasks/layout.yml\n\n\nConfigure any unset variables using layout defaults\n\n\nname: Include layout vars\ninclude_vars: \u201clayout-{{ galaxy_layout | default(\u2018legacy\u2019) }}.yml\u201d\n\n\nname: Set any unset variables from layout defaults\nset_fact:\n\u201c{{ item }}\u201d: \u201c{{ lookup(\u2018vars\u2019, \u2018__\u2019 ~ item) }}\u201d\nwhen: item not in vars\nwith_items:\n\ngalaxy_venv_dir\ngalaxy_server_dir\ngalaxy_config_dir\ngalaxy_mutable_data_dir\ngalaxy_mutable_config_dir\ngalaxy_shed_tools_dir\ngalaxy_cache_dir\ngalaxy_local_tools_dir\n\n\n\nname: Check that any explicitly set Galaxy config options match the values of explicitly set variables\nassert:\nthat:\n- lookup(\u2018vars\u2019, \u2018galaxy_\u2019 ~ item) == galaxy_config[galaxy_app_config_section][item]\nmsg: \u201cValue of \u2018{{ \u2018galaxy_\u2019 ~ item }}\u2019 does not match value of \u2018{{ item }}\u2019 in galaxy_config: {{ lookup(\u2018vars\u2019, \u2018galaxy_\u2019 ~ item) }} != {{ galaxy_config[galaxy_app_config_section][item] }}\u201d\n\nTODO: requires Ansible 2.7\n#success_msg: \u201cValue of \u2018{{ \u2018galaxy_\u2019 ~ item }}\u2019 matches galaxy_config value of \u2018{{ item }}\u2019 in galaxy_config\u201d\nwhen: \u201c\u2018galaxy_\u2019 ~ item in vars and item in ((galaxy_config | default({}))[galaxy_app_config_section] | default({}))\u201d\nwith_items:\n\ntool_dependency_dir\nfile_path\njob_working_directory\nshed_tool_config_file\n\n\n\nname: Set any unset variables corresponding to Galaxy config options from galaxy_config or layout defaults\nset_fact:\n\u201c{{ \u2018galaxy_\u2019 ~ item }}\u201d: \u201c{{ ((galaxy_config | default({}))[galaxy_app_config_section] | default({}))[item] | default(lookup(\u2018vars\u2019, \u2018galaxy\u2019 ~ item)) }}\"\nwhen: \"'galaxy\u2019 ~ item not in vars\u201d\nwith_items:\n\ntool_dependency_dir\nfile_path\njob_working_directory\n\n\n\nThank you in advance!"
    },
    {
      "role": "system",
      "text": "Hi,\nI did the same Galaxy tutorial as you and I got the same error message.\nI simply added the galaxy_server_dir variable declaration in the galaxy.yml file :\n- hosts: galaxyservers\n\u2002vars:\n\u2002 \u2002 galaxy_server_dir: /srv/galaxy\n\u2002become: true\n\u2002pre_tasks:\n\u2002 \u2002 - name: Install Dependencies\n\u2026\nand it is working.\nHave a good day,\nL. Burlot"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am in the process of mapping my reads to the genome using Tophat in galaxy. However, I see that the Tophat tool has been deprecated in the galaxy platform. Can I still use it to perform my analysis or should I use HISAT2?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nTophat developers recommend using HISAT2 (@link https://ccb.jhu.edu/software/tophat/index.shtml) (developed by the same group), as they state that it is more accurate and much more efficient.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am using snpsift rminfo on EU and annotate the variants with snpeff eff annotate afterwards. Before I use vcftotab-delimited in order to parse my data  (the great EU Team made this tool working for my data on Galaxy EU- MANY THANKS!), I would like to get rid of some vcf headerlines with the info tags from the original annotation, where there is no data for the info tag existent anymore. Otherwise I would end up with various empty columns using vcftotab-delimited. I tried a couple thinks and it is a possibility to use use the tool Cut  columns from a table (cut) (Galaxy Version 1.1.0) AFTER using vcftotab-delimited. However, as in my workflow I annotate my variants with lots of further information I would prefer to keep the vcf clean and amount of headerlines small. So I would rather find a tool to remove specific header lines BEFORE I used vcftotab in the vcf itself.\nBelow is  a list of the headers I would like to remove!\nMany thanks!\nAll the best,\nRose\n##SnpSiftVersion=\u201cSnpSift 4.2 (build 2015-12-05), by Pablo Cingolani\u201d\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/common_all_20160601.vcf.gz 5BC_S15.temp0.vcf\u201d\n##INFO=<ID=CDA,Number=0,Type=Flag,Description=\u201cVariation is interrogated in a clinical diagnostic assay\u201d>\n##INFO=<ID=OTH,Number=0,Type=Flag,Description=\u201cHas other variant with exactly the same set of mapped positions on NCBI refernce assembly.\u201d>\n##INFO=<ID=S3D,Number=0,Type=Flag,Description=\u201cHas 3D structure - SNP3D table\u201d>\n##INFO=<ID=WTD,Number=0,Type=Flag,Description=\u201cIs Withdrawn by submitter If one member ss is withdrawn by submitter, then this bit is set.  If all member ss\u2019 are withdrawn, then the rs is deleted to SNPHistory\u201d>\n##INFO=<ID=dbSNPBuildID,Number=1,Type=Integer,Description=\u201cFirst dbSNP Build for RS\u201d>\n##INFO=<ID=SLO,Number=0,Type=Flag,Description=\u201cHas SubmitterLinkOut - From SNP->SubSNP->Batch.link_out\u201d>\n##INFO=<ID=NSF,Number=0,Type=Flag,Description=\u201cHas non-synonymous frameshift A coding region variation where one allele in the set changes all downstream amino acids. FxnClass = 44\u201d>\n##INFO=<ID=R3,Number=0,Type=Flag,Description=\u201cIn 3\u2019 gene region FxnCode = 13\u201d>\n##INFO=<ID=R5,Number=0,Type=Flag,Description=\u201cIn 5\u2019 gene region FxnCode = 15\u201d>\n##INFO=<ID=NSN,Number=0,Type=Flag,Description=\u201cHas non-synonymous nonsense A coding region variation where one allele in the set changes to STOP codon (TER). FxnClass = 41\u201d>\n##INFO=<ID=NSM,Number=0,Type=Flag,Description=\u201cHas non-synonymous missense A coding region variation where one allele in the set changes protein peptide. FxnClass = 42\u201d>\n##INFO=<ID=G5A,Number=0,Type=Flag,Description=\">5% minor allele frequency in each and all populations\">\n##INFO=<ID=COMMON,Number=1,Type=Integer,Description=\u201cRS is a common SNP.  A common SNP is one that has at least one 1000Genomes population with a minor allele of frequency >= 1% and for which 2 or more founders contribute to that minor allele frequency.\u201d>\n##INFO=<ID=RS,Number=1,Type=Integer,Description=\u201cdbSNP ID (i.e. rs number)\u201d>\n##INFO=<ID=RV,Number=0,Type=Flag,Description=\u201cRS orientation is reversed\u201d>\n##INFO=<ID=TPA,Number=0,Type=Flag,Description=\u201cProvisional Third Party Annotation(TPA) (currently rs from PHARMGKB who will give phenotype data)\u201d>\n##INFO=<ID=CFL,Number=0,Type=Flag,Description=\u201cHas Assembly conflict. This is for weight 1 and 2 variant that maps to different chromosomes on different assemblies.\u201d>\n##INFO=<ID=GNO,Number=0,Type=Flag,Description=\u201cGenotypes available. The variant has individual genotype (in SubInd table).\u201d>\n##INFO=<ID=VLD,Number=0,Type=Flag,Description=\u201cIs Validated.  This bit is set if the variant has 2+ minor allele count based on frequency or genotype data.\u201d>\n##INFO=<ID=ASP,Number=0,Type=Flag,Description=\u201cIs Assembly specific. This is set if the variant only maps to one assembly\u201d>\n##INFO=<ID=ASS,Number=0,Type=Flag,Description=\u201cIn acceptor splice site FxnCode = 73\u201d>\n##INFO=<ID=REF,Number=0,Type=Flag,Description=\u201cHas reference A coding region variation where one allele in the set is identical to the reference sequence. FxnCode = 8\u201d>\n##INFO=<ID=U3,Number=0,Type=Flag,Description=\u201cIn 3\u2019 UTR Location is in an untranslated region (UTR). FxnCode = 53\u201d>\n##INFO=<ID=U5,Number=0,Type=Flag,Description=\u201cIn 5\u2019 UTR Location is in an untranslated region (UTR). FxnCode = 55\u201d>\n##INFO=<ID=WGT,Number=1,Type=Integer,Description=\u201cWeight, 00 - unmapped, 1 - weight 1, 2 - weight 2, 3 - weight 3 or more\u201d>\n##INFO=<ID=MTP,Number=0,Type=Flag,Description=\u201cMicroattribution/third-party annotation(TPA:GWAS,PAGE)\u201d>\n##INFO=<ID=LSD,Number=0,Type=Flag,Description=\u201cSubmitted from a locus-specific database\u201d>\n##INFO=<ID=NOC,Number=0,Type=Flag,Description=\u201cContig allele not present in variant allele list. The reference sequence allele at the mapped position is not present in the variant allele list, adjusted for orientation.\u201d>\n##INFO=<ID=DSS,Number=0,Type=Flag,Description=\u201cIn donor splice-site FxnCode = 75\u201d>\n##INFO=<ID=SYN,Number=0,Type=Flag,Description=\u201cHas synonymous A coding region variation where one allele in the set does not change the encoded amino acid. FxnCode = 3\u201d>\n##INFO=<ID=KGPhase3,Number=0,Type=Flag,Description=\u201c1000 Genome phase 3\u201d>\n##INFO=<ID=CAF,Number=.,Type=String,Description=\u201cAn ordered, comma delimited list of allele frequencies based on 1000Genomes, starting with the reference allele followed by alternate alleles as ordered in the ALT column. Where a 1000Genomes alternate allele is not in the dbSNPs alternate allele set, the allele is added to the ALT column.  The minor allele is the second largest value in the list, and was previuosly reported in VCF as the GMAF.  This is the GMAF reported on the RefSNP and EntrezSNP pages and VariationReporter\u201d>\n##INFO=<ID=VC,Number=1,Type=String,Description=\u201cVariation Class\u201d>\n##INFO=<ID=MUT,Number=0,Type=Flag,Description=\u201cIs mutation (journal citation, explicit fact): a low frequency variation that is cited in journal and other reputable sources\u201d>\n##INFO=<ID=KGPhase1,Number=0,Type=Flag,Description=\u201c1000 Genome phase 1 (incl. June Interim phase 1)\u201d>\n##INFO=<ID=NOV,Number=0,Type=Flag,Description=\u201cRs cluster has non-overlapping allele sets. True when rs set has more than 2 alleles from different submissions and these sets share no alleles in common.\u201d>\n##INFO=<ID=VP,Number=1,Type=String,Description=\u201cVariation Property.  Documentation is at @link ftp://ftp.ncbi.nlm.nih.gov/snp/specs/dbSNP_BitField_latest.pdf\u201d>\n##INFO=<ID=SAO,Number=1,Type=Integer,Description=\u201cVariant Allele Origin: 0 - unspecified, 1 - Germline, 2 - Somatic, 3 - Both\u201d>\n##INFO=<ID=GENEINFO,Number=1,Type=String,Description=\u201cPairs each of gene symbol:gene id.  The gene symbol and id are delimited by a colon (:slight_smile: and each pair is delimited by a vertical bar (|)\u201d>\n##INFO=<ID=INT,Number=0,Type=Flag,Description=\u201cIn Intron FxnCode = 6\u201d>\n##INFO=<ID=G5,Number=0,Type=Flag,Description=\">5% minor allele frequency in 1+ populations\">\n##INFO=<ID=OM,Number=0,Type=Flag,Description=\u201cHas OMIM/OMIA\u201d>\n##INFO=<ID=PMC,Number=0,Type=Flag,Description=\u201cLinks exist to PubMed Central article\u201d>\n##INFO=<ID=SSR,Number=1,Type=Integer,Description=\u201cVariant Suspect Reason Codes (may be more than one value added together) 0 - unspecified, 1 - Paralog, 2 - byEST, 4 - oldAlign, 8 - Para_EST, 16 - 1kg_failed, 1024 - other\u201d>\n##INFO=<ID=RSPOS,Number=1,Type=Integer,Description=\u201cChr position reported in dbSNP\u201d>\n##INFO=<ID=HD,Number=0,Type=Flag,Description=\u201cMarker is on high density genotyping kit (50K density or greater).  The variant may have phenotype associations present in dbGaP.\u201d>\n##INFO=<ID=PM,Number=0,Type=Flag,Description=\u201cVariant is Precious(Clinical,Pubmed Cited)\u201d>\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/CosmicAllMuts_v69_20140602.vcf.gz 5BC_S15.temp1.vcf\u201d\n##INFO=<ID=CDS,Number=1,Type=String,Description=\u201cCDS annotation\u201d>\n##INFO=<ID=AA,Number=1,Type=String,Description=\u201cPeptide annotation\u201d>\n##INFO=<ID=GENE,Number=1,Type=String,Description=\u201cGene name\u201d>\n##INFO=<ID=CNT,Number=1,Type=Integer,Description=\u201cHow many samples have this mutation\u201d>\n##INFO=<ID=STRAND,Number=1,Type=String,Description=\u201cGene strand\u201d>\n##SnpSiftCmd=\u201cSnpSift annotate -id /srv/qgen/data/annotation/clinvar_20160531.vcf.gz 5BC_S15.temp0.vcf\u201d\n##INFO=<ID=CLNSIG,Number=.,Type=String,Description=\u201cVariant Clinical Significance, 0 - Uncertain significance, 1 - not provided, 2 - Benign, 3 - Likely benign, 4 - Likely pathogenic, 5 - Pathogenic, 6 - drug response, 7 - histocompatibility, 255 - other\u201d>\n##INFO=<ID=CLNALLE,Number=.,Type=Integer,Description=\u201cVariant alleles from REF or ALT columns.  0 is REF, 1 is the first ALT allele, etc.  This is used to match alleles with other corresponding clinical (CLN) INFO tags.  A value of -1 indicates that no allele was found to match a corresponding HGVS allele name.\u201d>\n##INFO=<ID=CLNORIGIN,Number=.,Type=String,Description=\u201cAllele Origin. One or more of the following values may be added: 0 - unknown; 1 - germline; 2 - somatic; 4 - inherited; 8 - paternal; 16 - maternal; 32 - de-novo; 64 - biparental; 128 - uniparental; 256 - not-tested; 512 - tested-inconclusive; 1073741824 - other\u201d>\n##INFO=<ID=CLNSRC,Number=.,Type=String,Description=\u201cVariant Clinical Chanels\u201d>\n##INFO=<ID=CLNREVSTAT,Number=.,Type=String,Description=\u201cno_assertion - No assertion provided, no_criteria - No assertion criteria provided, single - Criteria provided single submitter, mult - Criteria provided multiple submitters no conflicts, conf - Criteria provided conflicting interpretations, exp - Reviewed by expert panel, guideline - Practice guideline\u201d>\n##INFO=<ID=CLNDSDB,Number=.,Type=String,Description=\u201cVariant disease database name\u201d>\n##INFO=<ID=CLNACC,Number=.,Type=String,Description=\u201cVariant Accession and Versions\u201d>\n##INFO=<ID=CLNDBN,Number=.,Type=String,Description=\u201cVariant disease name\u201d>\n##INFO=<ID=CLNDSDBID,Number=.,Type=String,Description=\u201cVariant disease database ID\u201d>\n##INFO=<ID=CLNHGVS,Number=.,Type=String,Description=\u201cVariant names from HGVS.    The order of these variants corresponds to the order of the info in the other clinical  INFO tags.\u201d>\n##INFO=<ID=CLNSRCID,Number=.,Type=String,Description=\u201cVariant Clinical Channel IDs\u201d>"
    },
    {
      "role": "system",
      "text": "Hi Rose,\nit shouldn\u2019t surprise you at this point that the answer lies in yet another regexp and the Select lines tool @link https://usegalaxy.eu/root?tool_id=Grep1\nYou want to keep all lines NOT Matching a pattern like:\n^##INFO=<ID=((ANN)|(DP)|(WHATEVER)|(AND_SO_ON))"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI can\u2019t upload all of my .gff local data. Some dataset could not be read and it seems to occur accidentally.\nThe error info is\nDataset Error Report\nAn error occurred while running the tool DATA_FETCH.\nAmong 246 files, only half past of the data are correctly uploaded by the server, whereas the remaing failed. I don\u2019t understand the reason because the gff files are in the same format and were downloaded from the same repository. The correct and uncorrect upload seems to be random.\nHow can I do?\nThank you"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe try upload in small(er) batches or upload datasets from failed jobs again.\nHope that helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nWhen I mapped my RNAseq fastQ files using STAR in Galaxy, I got only one mapped.bam file. I was wondering if there is any way I could also get a  Aligned.toTranscriptome.out.bam, which you would get from the STAR mapping using a command line platform.\nMany thanks,\nKhanh"
    },
    {
      "role": "system",
      "text": "Hi Khanh,\nthe Galaxy tool version of STAR installed on the big public servers does not let you produce that file.\nTheir is an updated version available from the toolshed (so you could install it into your own instance of Galaxy right now), which supports this, but it will still take a bit of time before the new version makes it to public servers (the new version is based on STAR 2.7 so needs new genome indices built before people can really use it productively).\nBest,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI have a dataset of 3Rad data, which have been created by using two pairs of enzymes (pstI/mspI and NsiI/MspI). I used to use Stacks but I now realize it only allows a maximum of 2 enzymes in process radtags. Does anyone know of an other tool or a way to pass by this problem? My first thought was to do 2 consecutive runs in process radtags, one for each pairs, but I\u2019m afraid it will leave residues in my dataset.\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI checked the Stacks documentation (@link https://catchenlab.life.illinois.edu/stacks/manual/), and I think that it is not possible to provide three enzymes as input.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI have a BAM file in the following format (some of the data).\n\ucea1\ucc9811088\u00d7675 150 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6085ba88735810406d02f800032191297890f9fc.jpeg)\nI would like to subset my BAM via specific cell barcode sequences (e.g, AAAGAACCAGGACATG, AACCAACAGAGATTCA, GCCATTCTCACAGTGT) into a new BAM.\nIs there any way to do so efficiently?\nIt would be great if someone knew how to do this.\nStay healthy.\nMany thanks,\nMiae"
    },
    {
      "role": "system",
      "text": "Hi Miae\nIf you are just interested in a particular cell barcode, have you tried \u201cSelect lines that match an expression\u201d (@link https://usegalaxy.eu/root?tool_id=Grep1), assuming your file is actually in SAM format.\nRegards, Hans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am experiencing a problem with the tool Pureclip on the Galaxy / CLIP-explorer server. The problem is that the tool has been running for 4-5 days now with the notice: Job currently running. Previously I have already restarted the job after 1 week of waiting with job currently running.  I was wondering why it could take so long and what I can do to speed things up.\nI have uploaded .BAM files for the CLIP and input data and also a reference genome Hg19 in the Fasta format. Could someone please help me with this issue?\n\nimage1396\u00d7744 35.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a7c9a7c5fde1fce822e3a3092fc32fb7d9dad5f2.png)"
    },
    {
      "role": "system",
      "text": "Hey. PureCLIP sometimes takes really long to train the peak calling model. Try to reduce the training on important chromosomes. There is a field where you can do that: Genomic chromosomes to learn HMM parameters. Reduce it maybe to three chromosomes.\nSo try this first and really pick chromosomes which are important for your study."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to install some tools into my local galaxy instance (v22.01) with Ephemeris. For the installation I followed the Ephemeris tutorial from the Galaxy Training Network (Galaxy Tool Management with Ephemeris (@link https://training.galaxyproject.org/archive/2022-05-01/topics/admin/tutorials/tool-management/tutorial.html)).\nWhenever I try to install tools from a list it keeps telling me: Timeout during install of tool_name, extending wait to 1h.\nI used (a personalized version of) the following command to install the tools from a list:\n\n\nshed-tools install -g @link https://your-galaxy -a  -t workflow_tools.yml\n\n\nWhen I look at the output of journalctl -f -u galaxy it it keeps giving the same output of uwsgi generating bytes.\nI also stopped Ephemeris when it took more than an hour fornothing to happen. I then tried to manually install the tools from the admin tab in galaxy, but this also didn\u2019t work.\nAny help would be appreciated!\nThanks!"
    },
    {
      "role": "user",
      "text": "The problem was ultimately fixed by downloading another version of the tool which was first to be installed, which was NanoPlot. The latest version of this tool was however compatible with the server (tool: >=20.01, server: 22.01)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Community,\ni have problem to analyze the simply fastqsanger file made with HiSeq illumina on custom oncological panel qiagen. is if possible to have a workflow analysis for those file?.\nthanks a lot\ncarlo"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nproblem to analyze the simply fastqsanger file made with HiSeq illumina\nReads from that protocol should already be in \u201cfastqsanger\u201d format. Upload the data to Galaxy and let the process guess the datatype using the default option \u201cautodetect\u201d.\nWhat to do next depends on your analysis goals. The GTN tutorials can be navigated by topic or searched with keywords. Many tutorials include a workflow, and there are tutorials that cover how to use, modify, and create workflows.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n\nIf you need more help, please be specific when you ask a new question so we can help you. What are the analysis goals? What steps have you done so far? If you ran into job errors: what did you check already, what did the error logs report, and can you share a link to your work for context?\nFAQs:\n\nhow to ask a question others can help with (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#what-information-should-i-include-when-reporting-a-problem)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I tried several times to run jobs on Galaxy main but I can not get any job to run. I tried different tools with no success. There is an announcement that there are reduced resources to allow for the Smorgasbord training but I am unable to run job for the last 2 days. I do not know if this is related to the training or there is something wrong with my account. Thanks."
    },
    {
      "role": "system",
      "text": "Welcome, @user\n@quote\nI can not get any job to run\nDo you mean that jobs are queued (gray)? That is expected, even outside of training events. I added a tag to this topic that explains how the processing works at public servers. The warning on the server is stating that jobs may queue longer than usual.\nIf jobs are failing (red), please send in a bug report directly from the red error dataset. Please include a link to this topic in the comments so we can link the two together. The warning is also stating that very large jobs might not have the chance to execute for as long as usual. This situation should be somewhat rare.\nThe alternative is to share back a link to your history and we can take a closer look. You can start up a rerun to eliminate transient failures at the same time. Put that all in the same history, or share back links to multiple histories if needed.\nHow to do all of the above is here: Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)\nWith @link http://UseGalaxy.org specific error types listed here: Galaxy Training! (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting)\nLet\u2019s start there, thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to import a workflow created in Galaxy 18.09.  No log entries as to why the Import failed.  GA file is below:\n\n{\n\u201ca_galaxy_workflow\u201d: \u201ctrue\u201d,\n\u201cannotation\u201d: \u201c\u201d,\n\u201cformat-version\u201d: \u201c0.1\u201d,\n\u201cname\u201d: \u201cLow coverage module\u201d,\n\u201csteps\u201d: {\n\u201c0\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: null,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 0,\n\u201cinput_connections\u201d: {},\n\u201cinputs\u201d: [],\n\u201clabel\u201d: \u201cBAM file\u201d,\n\u201cname\u201d: \u201cInput dataset\u201d,\n\u201coutputs\u201d: [],\n\u201cposition\u201d: {\n\u201cleft\u201d: 145.5,\n\u201ctop\u201d: 262\n},\n\u201ctool_id\u201d: null,\n\u201ctool_state\u201d: \u201c{}\u201d,\n\u201ctool_version\u201d: null,\n\u201ctype\u201d: \u201cdata_input\u201d,\n\u201cuuid\u201d: \u201c84fd58a9-9317-4517-8afe-432a3cef699d\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: null,\n\u201coutput_name\u201d: \u201coutput\u201d,\n\u201cuuid\u201d: \u201c4bc20bcb-a432-4c77-82c2-89adcd1d8ee5\u201d\n}\n]\n},\n\u201c1\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: null,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 1,\n\u201cinput_connections\u201d: {},\n\u201cinputs\u201d: [],\n\u201clabel\u201d: \u201cCoverage reporting regions\u201d,\n\u201cname\u201d: \u201cInput dataset\u201d,\n\u201coutputs\u201d: [],\n\u201cposition\u201d: {\n\u201cleft\u201d: 147.5,\n\u201ctop\u201d: 391\n},\n\u201ctool_id\u201d: null,\n\u201ctool_state\u201d: \u201c{}\u201d,\n\u201ctool_version\u201d: null,\n\u201ctype\u201d: \u201cdata_input\u201d,\n\u201cuuid\u201d: \u201caf39455e-ebcf-4a45-a3e9-7324e71f45e9\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: null,\n\u201coutput_name\u201d: \u201coutput\u201d,\n\u201cuuid\u201d: \u201c32434cbf-71d6-4710-b1a5-5129c349036e\u201d\n}\n]\n},\n\u201c2\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.27.0.0\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 2,\n\u201cinput_connections\u201d: {\n\u201cinput_type|input\u201d: {\n\u201cid\u201d: 0,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool Genome Coverage\u201d,\n\u201cname\u201d: \u201cinput_type\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cGenome Coverage\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201coutput\u201d,\n\u201ctype\u201d: \u201cbedgraph\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 366.5,\n\u201ctop\u201d: 238\n},\n\u201cpost_job_actions\u201d: {\n\u201cHideDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {},\n\u201caction_type\u201d: \u201cHideDatasetAction\u201d,\n\u201coutput_name\u201d: \u201coutput\u201d\n},\n\u201cRenameDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {\n\u201cnewname\u201d: \u201c{SAMPLE_ID}.final.bedgraph\"\n                }, \n                \"action_type\": \"RenameDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.27.0.0\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"__page__\\\": null, \\\"d\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"input_type\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 1, \\\\\\\"input\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"input_type_select\\\\\\\": \\\\\\\"bam\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"three\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"five\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"report\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"report_select\\\\\\\": \\\\\\\"bg\\\\\\\", \\\\\\\"scale\\\\\\\": \\\\\\\"1.0\\\\\\\", \\\\\\\"zero_regions\\\\\\\": \\\\\\\"true\\\\\\\"}\\\", \\\"dz\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"2109aa97-4fb3-4a80-9c3d-748486476b6c\", \n        \"workflow_outputs\": []\n    }, \n    \"3\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 3, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 1, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 366.5, \n            \"top\": 456\n        }, \n        \"post_job_actions\": {\n            \"ChangeDatatypeActionout_file1\": {\n                \"action_arguments\": {\n                    \"newtype\": \"bed\"\n                }, \n                \"action_type\": \"ChangeDatatypeAction\", \n                \"output_name\": \"out_file1\"\n            }, \n            \"HideDatasetActionout_file1\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Cut1\", \n        \"tool_state\": \"{\\\"columnList\\\": \\\"\\\\\\\"c1,c2,c3,c4\\\\\\\"\\\", \\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"delimiter\\\": \\\"\\\\\\\"T\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"f94970a8-e554-49f1-815c-90072afe1595\", \n        \"workflow_outputs\": []\n    }, \n    \"4\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Filter1\", \n        \"errors\": null, \n        \"id\": 4, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 2, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Filter\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Filter\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 598.5, \n            \"top\": 242\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionout_file1\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Filter1\", \n        \"tool_state\": \"{\\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"header_lines\\\": \\\"\\\\\\\"0\\\\\\\"\\\", \\\"cond\\\": \\\"\\\\\\\"c4<30\\\\\\\"\\\", \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.1.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"d749fb31-e8d3-4a0d-bc74-bdb8a59aaa96\", \n        \"workflow_outputs\": []\n    }, \n    \"5\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.27.0.3\", \n        \"errors\": null, \n        \"id\": 5, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 4, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Compute both the depth and breadth of coverage\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Compute both the depth and breadth of coverage\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Compute both the depth and breadth of coverage\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"bed\"\n            }\n        ], \n        \"position\": {\n            \"left\": 684.5, \n            \"top\": 562\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.27.0.3\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"__page__\\\": null, \\\"overlap_a\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"d\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"a_or_b\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_b\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"hist\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"strandedness\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"reciprocal_overlap\\\": \\\"\\\\\\\"false\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.3\", \n        \"type\": \"tool\", \n        \"uuid\": \"b80c314f-649a-4fe8-a165-240e3e4e34f5\", \n        \"workflow_outputs\": []\n    }, \n    \"6\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"errors\": null, \n        \"id\": 6, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 4, \n                \"output_name\": \"out_file1\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Intersect intervals\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 814.5, \n            \"top\": 368\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"count\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"__page__\\\": null, \\\"reciprocal\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_mode\\\": \\\"null\\\", \\\"invert\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"header\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"fraction\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"once\\\": \\\"\\\\\\\"true\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"792f3eb0-3c6c-4fb0-8941-8ca962cc7fe1\", \n        \"workflow_outputs\": []\n    }, \n    \"7\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 7, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 5, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 964.890625, \n            \"top\": 704.84375\n        }, \n        \"post_job_actions\": {\n            \"RenameDatasetActionout_file1\": {\n                \"action_arguments\": {\n                    \"newname\": \"{SAMPLE_ID}.allregions.coverage.txt\u201d\n},\n\u201caction_type\u201d: \u201cRenameDatasetAction\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201ctool_id\u201d: \u201cCut1\u201d,\n\u201ctool_state\u201d: \u201c{\u201ccolumnList\u201d: \u201c\\\u201cc1,c2,c3,c4,c6,c8\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cdelimiter\u201d: \u201c\\\u201cT\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cpage\u201d: null}\u201d,\n\u201ctool_version\u201d: \u201c1.0.2\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201ccf8609df-6454-48b8-ab74-03991b88baa2\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: \u201cLow coverage, all regions table\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d,\n\u201cuuid\u201d: \u201cb0b8afdc-8190-4e10-a7b2-0ff72049ee67\u201d\n}\n]\n},\n\u201c8\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.27.0.0\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 8,\n\u201cinput_connections\u201d: {\n\u201cinput\u201d: {\n\u201cid\u201d: 6,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool MergeBED\u201d,\n\u201cname\u201d: \u201cinput\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cMergeBED\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201coutput\u201d,\n\u201ctype\u201d: \u201cbed\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 1051.5,\n\u201ctop\u201d: 380\n},\n\u201cpost_job_actions\u201d: {\n\u201cHideDatasetActionoutput\u201d: {\n\u201caction_arguments\u201d: {},\n\u201caction_type\u201d: \u201cHideDatasetAction\u201d,\n\u201coutput_name\u201d: \u201coutput\u201d\n}\n},\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.27.0.0\u201d,\n\u201ctool_shed_repository\u201d: {\n\u201cchangeset_revision\u201d: \u201ce19bebe96cd2\u201d,\n\u201cname\u201d: \u201cbedtools\u201d,\n\u201cowner\u201d: \u201ciuc\u201d,\n\u201ctool_shed\u201d: \u201ctoolshed.g2.bx.psu.edu\u201d\n},\n\u201ctool_state\u201d: \u201c{\u201cpage\u201d: null, \u201cdistance\u201d: \u201c\\\u201c0\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cheader\u201d: \u201c\\\u201cfalse\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cc_and_o_argument_repeat\u201d: \u201c[]\u201d, \u201cstrand\u201d: \u201c\\\u201d\\\u201d\"}\",\n\u201ctool_version\u201d: \u201c2.27.0.0\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201ca7054b80-047d-45cd-a61a-597dc6674467\u201d,\n\u201cworkflow_outputs\u201d: []\n},\n\u201c9\u201d: {\n\u201cannotation\u201d: \u201c\u201d,\n\u201ccontent_id\u201d: \u201cFilter1\u201d,\n\u201cerrors\u201d: null,\n\u201cid\u201d: 9,\n\u201cinput_connections\u201d: {\n\u201cinput\u201d: {\n\u201cid\u201d: 7,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201cinputs\u201d: [\n{\n\u201cdescription\u201d: \u201cruntime parameter for tool Filter\u201d,\n\u201cname\u201d: \u201cinput\u201d\n}\n],\n\u201clabel\u201d: null,\n\u201cname\u201d: \u201cFilter\u201d,\n\u201coutputs\u201d: [\n{\n\u201cname\u201d: \u201cout_file1\u201d,\n\u201ctype\u201d: \u201cinput\u201d\n}\n],\n\u201cposition\u201d: {\n\u201cleft\u201d: 1188.5,\n\u201ctop\u201d: 747\n},\n\u201cpost_job_actions\u201d: {\n\u201cRenameDatasetActionout_file1\u201d: {\n\u201caction_arguments\u201d: {\n\u201cnewname\u201d: \u201c{SAMPLE_ID}.lessthan30x.coverage.txt\"\n                }, \n                \"action_type\": \"RenameDatasetAction\", \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"tool_id\": \"Filter1\", \n        \"tool_state\": \"{\\\"input\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"header_lines\\\": \\\"\\\\\\\"0\\\\\\\"\\\", \\\"cond\\\": \\\"\\\\\\\"c5>0\\\\\\\"\\\", \\\"__page__\\\": null}\", \n        \"tool_version\": \"1.1.0\", \n        \"type\": \"tool\", \n        \"uuid\": \"d0ff2e13-8ae9-4612-a3ea-ecfe0288b182\", \n        \"workflow_outputs\": [\n            {\n                \"label\": \"Low coverage, regions of concern table\", \n                \"output_name\": \"out_file1\", \n                \"uuid\": \"3b2df0a9-788e-4b85-9f4d-141d992903be\"\n            }\n        ]\n    }, \n    \"10\": {\n        \"annotation\": \"\", \n        \"content_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"errors\": null, \n        \"id\": 10, \n        \"input_connections\": {\n            \"inputA\": {\n                \"id\": 8, \n                \"output_name\": \"output\"\n            }, \n            \"reduce_or_iterate|inputB\": {\n                \"id\": 3, \n                \"output_name\": \"out_file1\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"inputA\"\n            }, \n            {\n                \"description\": \"runtime parameter for tool Intersect intervals\", \n                \"name\": \"reduce_or_iterate\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Intersect intervals\", \n        \"outputs\": [\n            {\n                \"name\": \"output\", \n                \"type\": \"input\"\n            }\n        ], \n        \"position\": {\n            \"left\": 1101.5, \n            \"top\": 561\n        }, \n        \"post_job_actions\": {\n            \"HideDatasetActionoutput\": {\n                \"action_arguments\": {}, \n                \"action_type\": \"HideDatasetAction\", \n                \"output_name\": \"output\"\n            }\n        }, \n        \"tool_id\": \"toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.27.0.2\", \n        \"tool_shed_repository\": {\n            \"changeset_revision\": \"e19bebe96cd2\", \n            \"name\": \"bedtools\", \n            \"owner\": \"iuc\", \n            \"tool_shed\": \"toolshed.g2.bx.psu.edu\"\n        }, \n        \"tool_state\": \"{\\\"count\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"__page__\\\": null, \\\"reciprocal\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"overlap_mode\\\": \\\"[\\\\\\\"-wao\\\\\\\"]\\\", \\\"invert\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"header\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"inputA\\\": \\\"{\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}\\\", \\\"reduce_or_iterate\\\": \\\"{\\\\\\\"__current_case__\\\\\\\": 0, \\\\\\\"inputB\\\\\\\": {\\\\\\\"__class__\\\\\\\": \\\\\\\"RuntimeValue\\\\\\\"}, \\\\\\\"reduce_or_iterate_selector\\\\\\\": \\\\\\\"iterate\\\\\\\"}\\\", \\\"split\\\": \\\"\\\\\\\"false\\\\\\\"\\\", \\\"fraction\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"__rerun_remap_job_id__\\\": null, \\\"strand\\\": \\\"\\\\\\\"\\\\\\\"\\\", \\\"once\\\": \\\"\\\\\\\"false\\\\\\\"\\\"}\", \n        \"tool_version\": \"2.27.0.2\", \n        \"type\": \"tool\", \n        \"uuid\": \"e4c50b5a-1d04-4f79-95bc-51cc7a5fded4\", \n        \"workflow_outputs\": []\n    }, \n    \"11\": {\n        \"annotation\": \"\", \n        \"content_id\": \"Cut1\", \n        \"errors\": null, \n        \"id\": 11, \n        \"input_connections\": {\n            \"input\": {\n                \"id\": 10, \n                \"output_name\": \"output\"\n            }\n        }, \n        \"inputs\": [\n            {\n                \"description\": \"runtime parameter for tool Cut\", \n                \"name\": \"input\"\n            }\n        ], \n        \"label\": null, \n        \"name\": \"Cut\", \n        \"outputs\": [\n            {\n                \"name\": \"out_file1\", \n                \"type\": \"tabular\"\n            }\n        ], \n        \"position\": {\n            \"left\": 1328.5, \n            \"top\": 586\n        }, \n        \"post_job_actions\": {\n            \"ChangeDatatypeActionout_file1\": {\n                \"action_arguments\": {\n                    \"newtype\": \"bed\"\n                }, \n                \"action_type\": \"ChangeDatatypeAction\", \n                \"output_name\": \"out_file1\"\n            }, \n            \"RenameDatasetActionout_file1\": {\n                \"action_arguments\": {\n                    \"newname\": \"{SAMPLE_ID}.lessthan30x.bed\u201d\n},\n\u201caction_type\u201d: \u201cRenameDatasetAction\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d\n}\n},\n\u201ctool_id\u201d: \u201cCut1\u201d,\n\u201ctool_state\u201d: \u201c{\u201ccolumnList\u201d: \u201c\\\u201cc1,c2,c3,c7\\\u201d\u201d, \u201cinput\u201d: \u201c{\\\u201cclass\\\u201d: \\\u201cRuntimeValue\\\u201d}\u201d, \u201cdelimiter\u201d: \u201c\\\u201cT\\\u201d\u201d, \u201crerun_remap_job_id\u201d: null, \u201cpage\u201d: null}\u201d,\n\u201ctool_version\u201d: \u201c1.0.2\u201d,\n\u201ctype\u201d: \u201ctool\u201d,\n\u201cuuid\u201d: \u201cf3b35bac-09ee-4fe2-97be-a0e2d0c754f7\u201d,\n\u201cworkflow_outputs\u201d: [\n{\n\u201clabel\u201d: \u201cLow coverage, intervals\u201d,\n\u201coutput_name\u201d: \u201cout_file1\u201d,\n\u201cuuid\u201d: \u201cf45ee41a-bdee-495c-9adc-a498199cfabf\u201d\n}\n]\n}\n},\n\u201ctags\u201d: [],\n\u201cuuid\u201d: \u201c54492bd2-73be-4bbb-89df-dc2a4a28d1d5\u201d,\n\u201cversion\u201d: 14\n"
    },
    {
      "role": "system",
      "text": "Update\nMore help for verifying workflows Non-specific Error on Malformed gxformat2 Import - #3 by mvdbeek (@link https://help.galaxyproject.org/t/non-specific-error-on-malformed-gxformat2-import/8692/3)\n\nHi @system\nIf you can get your workflow uploaded to a server (try a usegalaxy.* server), open it in the editor to review the warnings. If it won\u2019t load at all, the web browser logs might have some clues."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am trying to extract loci sequence from the genome using bed file. I have uploaded the bed file and the fasta file of ref genome. But I am getting the error of unspecified build. Kindly help me sort this out\n@link https://usegalaxy.org/u/bhavani_puttaswamygowda/h/unnamed-history"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou need to specify the reference genome database also in the BED file; it can be done by clicking in the Edit attributes menu  (pencil icon), and then select the reference genome in the Database/build field.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nCan anybody shed some light on the concept of forms ? In the administration panel, user management section there is a menu entry called forms. What can you do with it ? How to use it ?\nThanks and regards,\nMarc"
    },
    {
      "role": "system",
      "text": "It has no real application in modern Galaxies afaik, it is rather a remnant of removed features, we should get rid of it, sorry for the confusion."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi!\nI have 12 samples in total - 2 genotypes, and 4 treatments, 3 replicates each. After running DESeq2, I see with the PCA plots that there is a strong batch effect - the samples from one genotype aggregate according to the batch of cells and not the treatments.\nI am trying to use RUVSeq in @link http://usegalaxy.org but I am not sure how. As factors, should I input genotype, treatment and the batches as well?\nThank you!!!\nBest,\nCarolina"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthese kind of known or easily inferred batch effects can be accommodated by DESeq2 without using RUVseq.\nYou can add multiple factors (not factor levels) in the DESeq2 wrapper by clicking on Insert Factor. The first factor is the primary factor, so should probably be treatment. You can then add a genotypes and batch information as secondary and tertiary factors.\nIf you want to use RUVseq in Galaxy you would only use the primary factor levels and let the algorithm find the batch factors that you can then use in DESeq2.\nHope that helps,\nMarius"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nI am using PROKKA to annotate my bacterial assembly.fasta file\nAs I read in several discussions, PROKKA enables active annotation against  HAMAP database of HMMs, but not Pfam and TIGRfams.\nIs there any method to direct transfer the 2 databases, below, into galaxy, and use them in the annotation process?\nTIGRFAMs: ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.LIB.gz\nPfam: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\n\nThank you,\nMD"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nAs I read in several discussions, PROKKA enables active annotation against HAMAP database of HMMs, but not Pfam and TIGRfams.\nCorrect.\n@quote\nIs there any method to direct transfer the 2 databases, below, into galaxy, and use them in the annotation process?\nNot those specific files directly with Prokka. The first has a permissions problem (URL is incorrect?). The second could probably be used with other tools \u2013 search the tool panel at the EU servers for available choices. One example is PfamScan. Or, search the GTN tutorials with tool names or datatypes. Each has a listing of the public Galaxy servers that host a specific tutorial\u2019s tools.  Galaxy Training! (@link https://training.galaxyproject.org/training-material/)\nNote: Galaxy doesn\u2019t modify the original underlying tool. Tools are instead \u201cwrapped\u201d in a way that makes it easier to use them all together all in one place: to build a workflow or to share exact methods/data with others. Or if you just prefer using a GUI :slight_smile:. The usage is (mostly) otherwise the same as the command-line.\nRelated: if you can find the fasta for other protein database sources, those can be included with Prokka using this option: Optional FASTA file of trusted proteins to first annotate from\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am new here and I am using @link http://usegalaxy.org, I started with interested tutorials for me in metagenomics, in this tutorial \u201cAnalyses of metagenomics data - The global picture\u201d I faced 2 problems first with visualization data by Phinch although I already changed the file name form .biom1 to .biom and tried many times,but the error message that the file is not valid. Second, the tool \u2018MetaPhlAN2\u2019 is not available in the list of tools. I will be appreciated for your help\nthank you"
    },
    {
      "role": "system",
      "text": "@quote\nAnalyses of metagenomics data - The global picture\nNot every public Galaxy is set up for this tutorial. The Galaxies that made sure everything for it is available are listed on the trainings page: Analyses_of_metagenomics_data_-_The_global_picture"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve used CloudLaunch to run GVL/Galaxy on AWS but I\u2019ve been having trouble getting files to upload. Actually, they upload fine but then when the go to History, the wheel spins forever\u2026 I\u2019ve even tried small text files. I\u2019ve even tried pasting in data. Anyone have any thoughts?"
    },
    {
      "role": "system",
      "text": "Hi @user, did you get this issue resolved? You had also mentioned a featurecounts issue (featureCount doesn't work on GVL/Galaxy (@link https://help.galaxyproject.org/t/featurecount-doesnt-work-on-gvl-galaxy/6774)) so perhaps these are both tied together. I would propose continuing the discussion there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello dear community, I have just started using galaxy on a local server, followed a few guides, and stopped at one of them where I needed to use join tool\u2026\nAll good, except 2 \u201cerrors\u201d, one a lot more important than other.\nWhen running join tool for the \u201cData Manipulation Olympics\u201d - training I have encountered this error, I have tried other variations with this tool - same error\u2026\nJoin two Datasets side by side on a specified field - error\nTraceback (most recent call last):\nFile \u201c/home/arian/galaxyproject/galaxy/tools/filters/join.py\u201d, line 19, in \nfrom galaxy.util import stringify_dictionary_keys\nModuleNotFoundError: No module named \u2018galaxy\u2019\nWith Sort data in ascending or descending order (built in tool) \u2013 is simply loading indefinitely without an error - but I have installed another type of sort tool (of the same kind), and it works flawlessly, but still I\u2019d like to understand why it\u2019s not working\u2026(unfortunately I don\u2019t have an output error)\nCould anyone help me?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthanks for the report. We have been informed previously about those bugs:\n@quote\nIn the last two weeks the tool join two datasets has been upgraded from version 2.1.2. to 2.1.3 and now I can\u2019t get the tool to run.it keeps failing with the error Traceback (most recent call last): \nFile \u201c/cvmfs/main.galaxyproject.org/galaxy/tools/filters/join.py\u201d, line 19, in  \nfrom galaxy.util import stringify_dictionary_keys \nModuleNotFoundError: No module named \u2018galaxy\u2019 \nIt also now says that This tool requires galaxy-util (Version 19.9.0). Clickherefor more information. \nCan any one plea\u2026\nSorry for the inconvenience.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, we hope that this email will find you well. We are trying to assemble the genome from Pacbio SMRT sequence data. Those sequences are present in the recent history, however, when we try to proceed the genome assembly by Unicycler, we get the message that the data is unavailable. Kindly help us in this matter."
    },
    {
      "role": "system",
      "text": "Hi @user!\nAre you seeing one of these on the tool form?\nShort read input:\n\nfastq-inputs.png890\u00d790 4.42 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/43779518abcc1c2e73ae481f5bbb1e519de8fe52.png)\nLong read input:\n\nfastq-fastq-inputs.png910\u00d790 4.76 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/3ced397fd6c5547bb81893debeb19c5bf67f30d4.png)\nThe short reads need to be in fastq format to use with most Galaxy tools, including this one. Long reads can be in fastq or fasta format. Galaxy will autodetect the correct format and assign the datatype upon upload. Or, if is guessed incorrectly, you can directly assign the datatype.\nPlease see these FAQs for more details: @link https://galaxyproject.org/support/\n\nHow to format fastq data for tools that require .fastqsanger format? (@link https://galaxyproject.org/support/fastqsanger/)\nUnderstanding compressed fastq data (fastq.gz) (@link https://galaxyproject.org/support/compressed-fastq/)\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nAlthough the installation seems to succeed and everything seems to work, we get the following error when installing v22.05 using ansible.\nTASK [geerlingguy.docker : Ensure docker users are added to the docker group.] ***\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: KeyError: \u2018getspnam(): name not found\u2019\nfailed: [galaxy.classe.cornell.edu] (item=galaxy) => {\u201cansible_loop_var\u201d: \u201citem\u201d, \u201cchanged\u201d: false, \u201citem\u201d: \u201cgalaxy\u201d, \u201cmodule_stderr\u201d: \u201cTraceback (most recent call last):\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 107, in \\n    _ansiballz_main()\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 99, in _ansiballz_main\\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\\n  File \"/root/.ansible/tmp/ansible-tmp-1657316637.2950892-6369-118566750342709/AnsiballZ_user.py\", line 47, in invoke_module\\n    runpy.run_module(mod_name=\u2018ansible.modules.user\u2019, init_globals=dict(_module_fqn=\u2018ansible.modules.user\u2019, _modlib_path=modlib_path),\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 207, in run_module\\n    return _run_module_code(code, init_globals, run_name, mod_spec)\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 97, in _run_module_code\\n    _run_code(code, mod_globals, init_globals,\\n  File \"/opt/rh/rh-python38/root/usr/lib64/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 3221, in \\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 3207, in main\\n  File \"/tmp/ansible_user_payload_6dj59spr/ansible_user_payload.zip/ansible/modules/user.py\", line 1055, in set_password_expire\\nKeyError: \u2018getspnam(): name not found\u2019\\n\u201d, \u201cmodule_stdout\u201d: \u201c\u201d, \u201cmsg\u201d: \u201cMODULE FAILURE\\nSee stdout/stderr for the exact error\u201d, \u201crc\u201d: 1}\n\nPLAY RECAP *********************************************************************\ngalaxy.classe.cornell.edu  : ok=111  changed=8    unreachable=0    failed=1    skipped=50   rescued=0    ignored=0\n\nOur python version is 3.8.11, and in group_vars/galaxyservers.yml we have:\ndocker_users:\n\n\u201c{{ galaxy_user.name }}\u201d\n\n\nWhere galaxy_user.name is galaxy.  Our galaxy user is in our central AD domain, and is in the docker group:\n(venv) [galaxy@lnx229 ansible]$ id\nuid=31292(galaxy) gid=6023(galaxy) groups=6023(galaxy),1500(docker)\nAny help or suggestions would be greatly appreciated.\nMany thanks,\nDevin"
    },
    {
      "role": "system",
      "text": "Hi @user\nHopefully you have solved the problem here '__galaxy_user_group' is undefined\" (@link https://help.galaxyproject.org/t/galaxy-user-group-is-undefined/6600).\nIf not, it might faster to reach out to the Galaxy Docker maintainers at this Gitter chat for more admin help bgruening/docker-galaxy-stable - Gitter (@link https://gitter.im/bgruening/docker-galaxy-stable)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi and congrats on the new site! It leaves a really good first impression on me.\nOne question though: with the old biostars site it was possible to subscribe to the rss feed of latest posts. Does the new site come with an RSS feed, too, or is this planned for the future?\nBest,\nWolfgang"
    },
    {
      "role": "system",
      "text": "Hi @user, add .rss to the url of any listing of posts to get the feed."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello! I am a new user on Galaxy. I wanted to use Lefse to look for differences in my 16S dataset. When I ran the job through, it went well up until  B) LDA Effect Size, where I was met with the following error.\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, \u2018lefse_run.py\u2019)())\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse_run.py\u201d, line 90, in lefse_run\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse.py\u201d, line 222, in test_lda_r\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018rownames\u2019\nI then attempted to re-run the job with no luck. I proceeded to download the aerobiosis_small example file and run it through Lefse again. I was met with the same error:\nFatal error: Exit code 1 ()\nTool generated the following standard error:\nTraceback (most recent call last):\nFile \u201c/galaxy_venv/bin/lefse_run.py\u201d, line 33, in \nsys.exit(load_entry_point(\u2018lefse==1.1.2\u2019, \u2018console_scripts\u2019, \u2018lefse_run.py\u2019)())\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse_run.py\u201d, line 90, in lefse_run\nFile \u201c/galaxy_venv/lib/python3.7/site-packages/lefse-1.1.2-py3.7.egg/lefse/lefse.py\u201d, line 222, in test_lda_r\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018rownames\u2019\nIs there anything I may be doing wrong? Thank you!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nI\u2019m going to merge this question with another topic at this forum. It links to a topic at the forum hosted by the administrators of that server. There does seem to be a technical problem, and only they can fix it.\n@quote\nHi@systemHave you tried to ask at forum the server hosts? It is linked from that server\u2019s homepage and is the only way to contact them as far as I know. \nThis threads and a few others have details. I\u2019ve also added a tag to this post that will find more (guessing that was the server where the original question asker was working). \n\n\n\n\nHope that helps, and if someone does make contact and gets feedback, posting back the solution would be helpful here, tooUpdate: I \u2026\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I modified the RNA-Seq protocol, so that 5\u2019 end adapters were ligated prior to RNA fragmentation and 3\u2019-end adapter ligation. I have mapped the reads to a reference bacterial genome. Now I would like to get a list of the 5\u2019 start positions (transcription start sites) along with the frequency of occurrence for each starting position. What is the best way to do this?\nChaitanya Jain"
    },
    {
      "role": "system",
      "text": "Dear @user,\nThere is a tools at least on the @link http://usegalaxy.eu, which is called \u201cExtract alignment ends from SAM or BAM\u201d. With that you get the 5\u2019 and 3\u2019 ends in bed format. To count your start or ends sites your can use \u201cCount occurrences of each record\u201d. If you are operating on @link http://usegalaxy.org or other instances without the first tool, then what you can do is to convert the bam into a bed file with \u201cbedtools BAM to BED converter\u201d.\nI hope I could help.\nBest wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all I m trying to running Dist. seqs.But its taking too long to complete. Can you please help me with this. Thanks"
    },
    {
      "role": "system",
      "text": "Generally, there is no reasonable way to speed up a running job. But you can deploy your own Galaxy on your resources that are faster: @link https://galaxyproject.org/choices/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear sir/ Madam,\nThe latest reference genome for rats is currently available. But it has been updated in galaxy. I downloaded and used for the mapping. But unfortunately, when I wanted to change my bedgraph to bigwig, I got errors that Build/database is unspecified. I could not continue my analysis. Could you please help me with that?\nCould you please let me know If it is possible to add an effective reference genome for rats?\nThank you\nFereshteh"
    },
    {
      "role": "system",
      "text": "Please find the genome index at the @link http://UseGalaxy.eu and @link http://UseGalaxy.org.eu Galaxy servers.\nRelated Q&A: Mapping to a uploaded (large) reference genome dataset -- Platform Choices (@link https://help.galaxyproject.org/t/mapping-to-a-uploaded-large-reference-genome-dataset-platform-choices/5937)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to evaluate values in columns 1 and 2. If value in column 2 is larger than value in column 1 then column 3 should be +. else if value is larger in column 1 it should be -.\nThen a second round where values in column 1 and 2 are swapped if column 3 is -.\nDont understand the syntax in the custom table operation. Need some help.\nAnyone have some tips on how to do this?"
    },
    {
      "role": "system",
      "text": "Hi @user\ntry Text reformatting with awk.\nAdding + and -:\n{\nif ($2 > $1) {print $0,\u201c+\u201d} else {print $0,\u201c-\u201d}\n}\nSwapping columns and adding + and -:\n{\nif ($2 > $1) {print $1,$2,\u201c+\u201d} else {print $2,$1,\u201c-\u201d}\n}\nIf the same value is present in c1 and c2, the output gets \u201c-\u201d\nIf you are after intervals in BED format, make sure the start coordinate uses zero offset, and the is one based.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI would like to know if there is any tool installed to visualize progressive mauve alignments.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI\u2019ve placed the tool on the @link https://usegalaxy.eu (maybe easier to use; much more popular/more tools/well maintained/bigger quota)\nHere is an example history (@link https://usegalaxy.eu/u/helena-rasche/h/xmfa-test) where you can see the output. This was written by Eleni Mijalis at the CPT. It worked well enough for our purposes, basic structural rearrangement visualisation, but it is not being actively developed.\nThe X-Vis tool takes the XMFA file and a GFF3 file showing annotations, and visualises it:\nimage1078\u00d7625 260 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/245ca8ffd152c51dad7cbaaec8cf9b5d7e198353.png)\nEach contig is placed vertically, the blue boxes represent an LCB region, and the grey lines connect them. All features from the gff3 are visualised as black arrow boxes.\n\nClicking on a black feature will show the feature name at the bottom\nDouble clicking a blue LCB box will re-align the visualisation to center those LCBs across all contigs\nYou can scroll vertically to zoom in on a single region\nClick and drag to pan around.\n\nMauve is more full featured, but, this was a convenient way to include it into our comparative genomics workflows"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI upgraded our Galaxy instance from 16.xy to 19.01.\nI succeeded to configure it with our deprecated galaxy.ini file.\nBut I face problems to migrate configuration to galaxy.yml file and set it up to use our SGE 8.1.8 cluster :\n\nwith galaxy.ini, galaxy jobs are correctly shifted and run by SGE\nbut with galaxy.yml, galaxy jobs keep waiting to be shifted but are not received by SGE.\n\nI believe I must have misconfigured something but can\u2019t find what.\nWould you have any hint ?\nHere are extracts from :\n\nour galaxy.yml :\n\nuwsgi:\n\n  http: :7090\n  buffer-size: 16384\n  processes: 2\n  threads: 4\n  offload-threads: 1\n  static-map: /static/style=static/style/blue\n  static-map: /static=static\n  static-map: /favicon.ico=static/favicon.ico\n  master: true\n  pythonpath: lib\n  module: galaxy.webapps.galaxy.buildapp:uwsgi_app()\n  thunder-lock: true\n  die-on-term: true\n  hook-master-start: unix_signal:2 gracefully_kill_them_all\n  hook-master-start: unix_signal:15 gracefully_kill_them_all\n  py-call-osafterfork: true\n  enable-threads: true\n\ngalaxy:\n\n  mule: lib/galaxy/main.py\n  farm: job-handlers:1\n\n\nour job_conf.xml :\n\n<job_conf>\n    <plugins workers=\"4\">\n        <plugin id=\"sge\" type=\"runner\" load=\"galaxy.jobs.runners.drmaa:DRMAAJobRunner\">\n            <param id=\"drmaa_library_path\">/SGE/8.1.8/lib/lx-amd64/libdrmaa.so</param>\n        </plugin>\n    </plugins>\n    <destinations default=\"sge_default\">\n        <destination id=\"sge_default\" runner=\"sge\">\n            <param id=\"nativeSpecification\">-V -j n -q short.q</param>\n        </destination>\n    </destinations>\n\n\nour galaxy.log :\n\ngalaxy.jobs.command_factory INFO 2019-04-03 16:42:44,377 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] Built script [/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/tool_script.sh] for tool command [python '/gs7k1/home/galaxydev/galaxy_19.01/tools/data_source/upload.py' /gs7k1/home/galaxydev/galaxy_19.01 /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/registry.xml /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/upload_params.json 46441:/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/dataset_46441_files:/gs7k1/home/galaxydev/galaxy_19.01/database/files/046/dataset_46441.dat]\ngalaxy.jobs.runners DEBUG 2019-04-03 16:42:44,449 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) command is: rm -rf working; mkdir -p working; cd working; /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/tool_script.sh; return_code=$?; cd '/gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532';\ngalaxy.jobs.runners.drmaa DEBUG 2019-04-03 16:42:44,483 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) submitting file /gs7k1/home/galaxydev/galaxy_19.01/database/jobs_directory/018/18532/galaxy_18532.sh\ngalaxy.jobs.runners.drmaa DEBUG 2019-04-03 16:42:44,483 [p:14860,w:2,m:0] [DRMAARunner.work_thread-3] (18532) native specification is: -V -j n -q short.q\n\nRegards."
    },
    {
      "role": "system",
      "text": "@quote\nmule: lib/galaxy/main.py farm: job-handlers:1\nShould be under uwsgi section in the config per Scaling and Load Balancing \u2014 Galaxy Project 21.09.1.dev0 documentation (@link https://docs.galaxyproject.org/en/master/admin/scaling.html#uwsgi-mule-job-handling)\nI am not sure that is the root cause but definitely firs thing to try."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to upload files to the server:\n\nChose local file\nI managed to upload I think, but looking at the window on the right, where my datasets are, and it\u2019s written \u201cThis job is waiting to run\u201d. What does it mean, waiting for what?\nChose FTP file\nI cannot connect to @link http://usegalaxy.eu or @link http://usegalaxy.org through Filezilla (passive mode checked). |Response:|530 Login incorrect.|\n|Error:|Critical error: Could not connect to server|\n\nCould someone please help me spotting what is wrong?\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user!\nOur FTP server is at a different address. You can find more information about it in our documentation (@link https://galaxyproject.eu/ftp/).\nThere were some temporary problems with running jobs on @link http://usegalaxy.eu today, please see the notice in the center panel. These should be resolved now. If your uploads still do not succeed, please retry at your convenience.\nThanks for using @link http://UseGalaxy.eu! Sorry for the inconvenience."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m using the 16S Microbial Analysis with Mothur tutorial but I\u2019m having troubles with the step of visualization data. I have an biom file which I can visualize in Phinch server (link in the galaxy output of make.biom) (@link http://phinch.org/), the graphs looks great, even so when I try to export the graphs the server doesn\u2019t respond and never ends up exporting anything. There\u2019s a way I could export these files??"
    },
    {
      "role": "system",
      "text": "Where to get help with the phinch service: @link https://www.phinch.org/Community"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage1920\u00d71080 459 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/de8a63fa0822407cc46bd97088662bde55219516.png)\neven if I opened hidden datasets my account is still full.\nCould you please erase everything?\nbest"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe issue seems to be that datasets have been deleted, but not permanently deleted (purged).  Data needs to be purged before it is actually removed (and no longer recoverable, even by an administrator).\nTry clicking on the \u201c186 deleted\u201d toggle at the top of the left history panel.\nIf a dataset has only been deleted but not permanently deleted (purged) you\u2019ll see a message within the dataset with note/link to purge.\nSince you have so many datasets and are removing all in the same history, individually clicking on those links, or even using the \u201cOperations on multiple datasets\u201d function, will take more time to do.\nTwo shortcuts, either should work:\n\nPermanently delete (purge) the entire History. Do this from the User > Histories view. The pull-down menu per history has an option for \u201cPurge History\u201d. Or, you can select multiple histories and use the \u201cDelete Permanently\u201d button at the bottom of your history list to purge in batch.\nPermanently delete (purge) all datasets that are deleted but not permanently deleted, in batch, using the History menu function \u201cPurge Deleted Datasets\u201d in the Analyze Data view (the view in your screenshot).\n\nNotes:\n\n\nHistories cannot be in a shared state if you want to permanently delete (purge) them. Unshare as needed first.\nPurging is a two-step process using any method. A pop-up box is the second step. Click to confirm the action.\nWhen purging a large amount of data, the server may take some time to update and accurately reflect quota usage. You can wait, or you can speed up the process by logging out then logging in again.\n\nFAQ with full details: @link https://galaxyproject.org/support/account-quotas/\nI also added some tags to your post. Click on them to review prior Q&A about the topics. You can also search all topics with the keyword \u201cpurge\u201d. Many have screenshots that may help even more.\nPlease give this a try and let us know if you need more help.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am new to Galaxy and currently doing Galaxy 101 tutorial. When trying to get the data from UCSC main table browser and after sending the output to Galaxy, it does not send to the current open session/history. It logs me out and create a new history.\nThanks\nTrupti"
    },
    {
      "role": "system",
      "text": "There was a prior issue that logged you out of Galaxy when downloading data from the UCSC Table Browser. It wasn\u2019t a consistent fail and we believe it was related to query-quotas at UCSC, or session identifiers in Galaxy (how UCSC knows where to download the data). Logging into Galaxy will add the newly downloaded data into a new history in your account, and you will need to consolidate datasets between histories. Copies of datasets in your own account do not consume extra quota space, but you can still purge the histories you don\u2019t need once you have a copy of the data in the history you want to work in.\nInconvenient, but we don\u2019t have another solution right now. This is the open ticket: Redirect from data sources sometimes loses session cookies \u00b7 Issue #11374 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/11374)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m new to Galaxy, and especially new to using cloud resources. I set up a cloud instance of Galaxy, but am 100% completely lost. There are a few thing things that don\u2019t really seem to be adding up. I\u2019m very sorry for, what are probably trivial questions, but I would appreciate some help,\nSo in the cloudLaunch \u201cMy Appliances\u201d page, it says I\u2019m running a genomics virtual lab on the AWS cloud. When I click on the access link, it takes me to the GVL dashboard where it says that Galaxy isn\u2019t running, but Cloudman is. So I click on the Cloudman access link, and it tells me that I\u2019m running out of disk space. Now, I managed to get Galaxy working a few weeks ago (not sure how), and managed to download a fastq file that was probably about 6 GB. But the Cloudman console was telling me that I was running out of space. I deleted the file, and then the history pane on Galaxy showed that my history was taking up 12 GB of storage, but hte number on the top right of the console said I was using about 6.8 GB. Then, under cluster status in the cloudman console, it says I\u2019ve used 54 GB / 54 GB, but in the admin console, under file systems, it\u2019s telling me that the different fiel systems are taking up almost 150 GB of storage. So I\u2019m having trouble making sense of all this information. All I did was download a 6 GB file, which I then deleted. So why am I getting all this mixed information?\nI\u2019m sorry for the mess of information, but I would greatly appreciate it if someone could point towards something that could help me understand what\u2019s going on. How do I make sense of the memory usage?\nAlso, I tried restarting Galaxy from the cloudman admin console, but it seems to be taking a long time. Is this normal?"
    },
    {
      "role": "system",
      "text": "Sorry to hear about the storage troubles. Hopefully together we can figure out what exactly is going on. So, by default, Galaxy does not really delete files from disk when a file is deleted from a history. It just marks a dataset as deleted in case you change your mind about wanting to un-delete later. To actually remove a dataset from disk, it is necessary to purge it, which is available under the history settings cog by clicking Purge Deleted Datasets.\nA more system-level approach would be to ssh to your instance and run the cleanup scripts, as described here: @link https://galaxyproject.org/admin/config/performance/purge-histories-and-datasets/\nOverall, the reason the CloudMan console is showing more disk usage than what you have uploaded to Galaxy is because of Galaxy and other supporting/accompanying software that is running on the same disk.\nFinally, if all you have done is uploaded a single file to your instance and realized the disk is too small, I would recommend you just delete that particular instance and launch a brand new one with a larger disk. Things will just go a lot quicker and smoother than fighting with the storage and/or waiting for a disk resize (which can take many, many hours to complete in the case of Amazon)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I finished an analysis and I would like to download a printable version of the workflow to keep for records.\nsomething similar to what is presented in \u201cview\u201d, but without printing or copy-pasting form browser that looks bad.\nI downloaded the *.ga but I dont know what to do with it.\nthanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nThere isn\u2019t a way to directly export a PDF of the Workflow \u201cview\u201d with formatting preserved, although that has been discussed and may be included in a future release. Or if you are interested in helping out, a development PR to add in this functionality would be welcomed. Many views could use this type of output.\nThe page can be saved in .htm format (\u201ccontrol+right-click+save-as\u201d on MAC OSX) which will remove a bit of the extra formatting. You\u2019ll end up with just the text, which may be enough.\nYou could also try importing into MyExperiment and see if their viewing/printing options work for you. How-to: @link http://wiki.myexperiment.org/index.php/Galaxy_Help#To_access_myExperiment_from_Galaxy_and_enable_the_Galaxy_perspective\nThe .ga file is a complete archive if that is your overall goal. It can be uploaded into any Galaxy server and viewed again, or shared, etc.\nNot a very satisfying answer, but hope that helps a bit.\nping @system \u2013 Is there a better/alternative way to do this I\u2019m missing?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am using version 17.05. I am trying to disable automatic tool reloading. I know in galaxy.ini \u201cwatch_tools = False\u201d won\u2019t disable the automatic reload for the tools that are listed in the tool_conf.xml. Please let me know if there is a way to disable it. Thanks.\nBTW, the reason I want to disable reloading is: When there is a running job, if I edit any tool xml file, somehow Galaxy deletes a tmp file when reloading the tools. When the job finishes, it throw an error complaining missing a tmp file. This is annoying because I cannot edit any tool file when there are running jobs. Here is an error message:\nTraceback (most recent call last):\nFile \u201c/scratch/galaxy/job_working_directory/011/11813/set_metadata_LwWNda.py\u201d, line 1, in \nfrom galaxy_ext.metadata.set_metadata import set_metadata; set_metadata()\nFile \u201c/opt/galaxy/lib/galaxy_ext/metadata/set_metadata.py\u201d, line 85, in set_metadata\ndatatypes_registry.load_datatypes( root_dir=galaxy_root, config=datatypes_config )\nFile \u201c/opt/galaxy/lib/galaxy/datatypes/registry.py\u201d, line 106, in load_datatypes\ntree = galaxy.util.parse_xml( config )\nFile \u201c/opt/galaxy/lib/galaxy/util/init.py\u201d, line 214, in parse_xml\nroot = tree.parse( fname, parser=ElementTree.XMLParser( target=DoctypeSafeCallbackTarget() ) )\nFile \u201c/usr/lib/python2.7/xml/etree/ElementTree.py\u201d, line 647, in parse\nsource = open(source, \u201crb\u201d)\nIOError: [Errno 2] No such file or directory: \u2018/scratch/galaxy/tmp/tmp4u4oxV\u2019"
    },
    {
      "role": "system",
      "text": "You can disable tool reloading in your (shed_)tool_conf.xml files by changing <toolbox monitor=\"true\"> to  <toolbox monitor=\"false\"> . 17.05 is extremely old though, this is a bug we fixed a very long time ago, I\u2019d strongly recommend updating Galaxy to a more recent version."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m trying to visualize genes from a gff3.gz file using JBrowse, with gff3.gz and fasta.gz files downloaded from NCBI into Galaxy. The gff3.gz file is not recognized by JBrowse, so it is displayed as a bed file, and as a result the \u201cType\u201d column (third column) is displayed instead of the gene names (which are in the Attributes column 9). If I try and convert the gff3.gz dataset to a gff3 datatype before displaying in Jbrowse, then no genes or labels appear. If I download the gff3.gz file to my laptop and decompress and reupload, then the gene names do appear. I\u2019m wondering if you can help me figure out how to display the gene names without downloading and decompressing first.\nHere are my steps:\n\n\ndownload genome and gff3 into Galaxy\n@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.fna.gz\n@link https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/858/895/GCF_009858895.2_ASM985889v3/GCF_009858895.2_ASM985889v3_genomic.gff.gz\n\n\nDisplay in Jbrowse by selecting \u201cgenome from history\u201d and adding a track \u201cGFF/GFF3/BED Features\u201d\n\n\nHere is the history showing the gff3.gz file downloaded from NCBI, which does not display gene names in JBrowas, and the one that was downloaded, decompressed and  reuploaded from my laptop, which does show gene names in JBrowse:\n@link https://usegalaxy.org/u/rbator01/h/intro-to-ngs\nThank you for any suggestions!\nRebecca"
    },
    {
      "role": "user",
      "text": "OK, I figured it out! If you run the \u201cConvert\u201d->\u201cConvert compressed file to uncompressed file\u201d and use the uncompressed file in JBrowse, it is read as the correct datatype and gene names are displayed. I\u2019m wondering why it\u2019s not decompressed automatically, as the fasta is. In any case, this solution works for me."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there, I\u2019m trying to obtain a consensus sequence from sequences that I have already aligned.\nWhat has been done so far is that I have uploaded multiple data obtained from PCR on galaxy. I then used Bowtie 2 to map reads against a reference genome (obtained from Genbank). I then ran ivar consensus on each of the bowtie outputs to get the consensus and aligned them on MEGA11. How do I get a single consensus from the aligned data? I tried to repeat the whole process by reuploading the aligned dataset onto galaxy and running Bowtie 2 and ivar consensus again but this gave me a sequence with alot of Ns. The aligned data set I had consists of about 30 sequences and they all aligned very well on MEGA11.\nDoes anyone know what I am doing wrong/what I can do to circumvent this? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nI believe MEGA can create a consensus sequence for an alignment.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having an issue getting PAM based user logins to work in v20.01. The problem has to do with Galaxy not having sufficient permissions to create a new user account folder.\nFor some unknown reason, it wants to append our org\u2019s domain to the user folder. This is unnecessary because the correctly named username folder (sans suffix) already exists in /home. I\u2019ve tested manually creating the fully qualified folder prior to login and it works.\ngalaxy.log:\ngalaxy.webapps.galaxy.controllers.user DEBUG 2020-05-29 13:15:34,638 [p:93475,w:1,m:0] [uWSGIWorker1Core2] trans.app.config.auth_config_file: /hpc/software/installed/galaxy/20.01/config/auth_conf.xml\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,639 [p:93475,w:1,m:0] [uWSGIWorker1Core2] use username: True use email False email None username test.user\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,639 [p:93475,w:1,m:0] [uWSGIWorker1Core2] PAM auth: will use external helper: False\ngalaxy.auth.providers.pam_auth DEBUG 2020-05-29 13:15:34,951 [p:93475,w:1,m:0] [uWSGIWorker1Core2] PAM authentication successful for test.user\ngalaxy.auth.util DEBUG 2020-05-29 13:15:34,955 [p:93475,w:1,m:0] [uWSGIWorker1Core2] Email: @link mailto:test.user@domain.com, auto-register with username: test.user\ngalaxy.web.framework.decorators ERROR 2020-05-29 13:15:35,102 [p:93475,w:1,m:0] [uWSGIWorker1Core2] Uncaught exception in exposed API method:\nTraceback (most recent call last):\nFile \u201clib/galaxy/web/framework/decorators.py\u201d, line 282, in decorator\nrval = func(self, trans, *args, **kwargs)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 122, in login\nreturn self.__validate_login(trans, payload, **kwd)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 147, in __validate_login\nmessage, user = self.__autoregistration(trans, login, password)\nFile \u201clib/galaxy/webapps/galaxy/controllers/user.py\u201d, line 105, in __autoregistration\ntrans.handle_user_login(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 720, in handle_user_login\nself.user_checks(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 665, in user_checks\nself.check_user_library_import_dir(user)\nFile \u201clib/galaxy/web/framework/webapp.py\u201d, line 657, in check_user_library_import_dir\nsafe_makedirs(os.path.join(self.app.config.user_library_import_dir, user.email))\nFile \u201clib/galaxy/util/path/init.py\u201d, line 114, in safe_makedirs\nmakedirs(path)\nFile \u201c/hpc/software/installed/galaxy/20.01/.venv/lib64/python3.6/os.py\u201d, line 220, in makedirs\nmkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: \u2018/home/test.user@domain.com\u2019\nauth_conf.xml:\n<?xml version=\u201c1.0\u201d?>\n<auth>\n<authenticator>\n<type>PAM</type>\n<options>\n<auto-register>True</auto-register>\n<maildomain>domain.com</maildomain>\n<login-use-username>True</login-use-username>\n<pam-service>sshd</pam-service>\n</options>\n</authenticator>\n</auth>\nsssd.conf:\n[sssd]\ndomains = @link http://domain.com\nconfig_file_version = 2\nservices = nss, pam, sudo\n[domain/domain.com]\nad_domain = @link http://domain.com\nkrb5_realm = @link http://DOMAIN.COM\nrealmd_tags = manages-system joined-with-samba\ncache_credentials = True\nid_provider = ad\nkrb5_store_password_if_offline = True\ndefault_shell = /bin/bash\nldap_sasl_authid = galaxy1$\nldap_id_mapping = False\nuse_fully_qualified_names = False\nfallback_homedir = /home/%u\naccess_provider = simple\nsimple_allow_groups = Group1, Group2\nAnyone able to point me in the right direction?\nThanks."
    },
    {
      "role": "user",
      "text": "Solution: Disable all user_library_import settings."
    }
  ],
  [
    {
      "role": "user",
      "text": "I was working on @link http://usegalaxy.org\nI started working through the Galaxy training/Sequence analysis/Quality control hands-on training on Friday, July 12. At the time I had no problems. However, today (Tue, July 16) when I returned to the history and tried to view the results for \u2018FastQC on data_1:Webpage\u2019 I didn\u2019t see any of the graphical displays.\nThe green box underneath the step name contained: \u2018Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxy-repl/main/jobdir/024/239/24239388/_job_tmp -Xmx7g -Xms256m.\u2019\nWhen I clicked on the icon to view details and then on \u2018stderr,\u2019 it displayed the same message: 'Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/galaxyrepl/main/jobdir/024/239/24239388/_job_tmp -Xmx7g  -Xms256m\nI am wondering if something needs to be reset as described in the response to a similar message 5 days ago \u2018FastQC file not visible - Add tool to HTML display \u201cwhitelist\u201d\u2019\nThank you!"
    },
    {
      "role": "system",
      "text": "@quote\nas described in the response to a similar message 5 days ago \u2018FastQC file not visible - Add tool to HTML display \u201cwhitelist\u201d\u2019\nexactly, thanks for the report, I think I fixed it now for good (until the next version of fastqc comes out :D)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Sir,\nI\u2019m sorry because, earlier I wasn\u2019t aware of the galaxy policy and hence I created multiple accounts for my work.\nI, therefore, kindly request you to delete all my previous accounts and grant me permission to use one account for carrying out my research work.\nI apologise for my previous mistakes and assure this will not happen in future.\nThanking you\nYours sincerely\nVikas Gupta"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for understanding. If you don\u2019t have any accounts that are still active, you can create a new account right now \u2013 just use an email address that you haven\u2019t used for an account at the server before (option 3 below). Should you remember any other stray accounts later on, please consolidate data and remove excess accounts yourself under User > Preferences (option 2 below).\n@quote\nOptionsIf you have one single active account atUseGalaxy.org, keep using that one.If you have two or more active accounts atUseGalaxy.org, please manage your data into one account and delete the others under User > Preferences.Creating a single new account with a different email address is OK if you do not have any active account(s) atUseGalaxy.org.\nThank you! :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have run a microRNA analysis using BWA and HISAT2 as mapping tools and with both I used HTSeq-count and then tried using DESeq2. With both different runs (for the different aligners) I have gotten the same error multiple times and I am not sure why. (I used the UCSC genome browser microRNA database for both). I have also ran other aligners and have gotten DESeq2 to work so I am just confused. The HTSeq count has 2,315 lines of data so I am no sure why DESeq2 states that there are no counts.\nThis is the error code:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nError in DESeqDataSet(se, design = design, ignoreRank) :\nall samples have 0 counts for all genes. check the counting script.\nCalls: get_deseq_dataset \u2026 DESeqDataSetFromHTSeqCount -> DESeqDataSetFromMatrix -> DESeqDataSet\nThank you!"
    },
    {
      "role": "system",
      "text": "Take a look at this post (@link https://help.galaxyproject.org/t/error-deseq-every-gene-contains-at-least-one-zero/564). It might help."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, hope everyone well .\nI have a project where I am given  57 sequences ( including control).\nI would not know what data there is there , it is related to the pituitary adenoma ( different forms ) and it is also including controls . What I need - UP & DOWN regulated genes. Can you recommend work flow or tutorial exists?\nThank you in advance ."
    },
    {
      "role": "system",
      "text": "Hi @user, there is a range of tutorials here: @link https://training.galaxyproject.org/training-material/topics/transcriptomics/. Most of them have workflows attached which you can use.\nI am not really that familiar with RNA-seq, but maybe this one is a good start? @link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "I currently have a tool that our team has developed to help with data analysis regarding genomic and proteomic datasets. I wanted to deploy this tool using Galaxy, so is there any way that I can add it to the website? By the way, our tool is currently written in Python."
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for the clarification.\nResources for tool development:\n\nTools working group Galaxy Working Groups and Projects - Galaxy Community Hub (@link https://galaxyproject.org/community/wg/)\n\nPlanemo for tool development @link https://planemo.readthedocs.io/\n\nToolShed (the Galaxy \u201capp store\u201d) @link https://toolshed.g2.bx.psu.edu/\n\nGalaxy Training Network (GTN) @link https://training.galaxyproject.org/\n\n\nThe general idea is to:\n\nDevelop a Galaxy compatible tool repository\nAdd the tool to the ToolShed.\nThe tool/tool suite is then available for any Galaxy server to install and use.\nThe tool form itself can include external links to author created web resources and publications in the help section, along with summarized usage help.\nConsider creating a GTN tutorial for how to use the tool, or create your own documentation with a published workflow.\n\nOnce the core basics are all in place (up to the Toolshed step is the minimum), then the public Galaxy sites may choose to include it. The Tools working group can help with those details.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI am struggling to get a custom interactive tool working. From gitter/admin I got suggested by Bj\u00f8rn and Helena to base my attempt on existing tools. The panoply tool seemed most suitable to me.\n\n\n@link https://github.com/usegalaxy-eu/galaxy/blob/release_21.05_europe/tools/interactive/interactivetool_panoply.xml\n\n\n@link https://github.com/usegalaxy-eu/galaxy/blob/release_21.05_europe/tools/interactive/interactivetool_panoply.xml\n<tool id=\"interactive_tool_panoply\" tool_type=\"interactive\" name=\"Panoply\" version=\"@VERSION@\">\n    <description>interative plotting tool for geo-referenced data</description>\n    <macros>\n    <token name=\"@VERSION@\">4.5.1</token>\n    </macros>\n    <requirements>\n        <container type=\"docker\">quay.io/nordicesmhub/docker-panoply:@VERSION@</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"Panoply on $infile.display_name\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n    mkdir output &&\n        mkdir /config/home  &&\n        mkdir /config/home/output &&\n        export HOME=/config/home &&\n        cp '$infile' '/config/home/$infile.display_name' &&\n\n\n\n  This file has been truncated. show original (@link https://github.com/usegalaxy-eu/galaxy/blob/release_21.05_europe/tools/interactive/interactivetool_panoply.xml)\n\n\n\n\n\nWhat I have done\n\n\nAdded the panoply tool as it is. Works as expected on our galaxy instance. I use the same job destination definition as my custom tool (interactive_slurm_general, see below).\n\n\nTesting custom tool outside galaxy:\n\n\ndocker run --rm -p 5800:5800  maikenp/comphep-ubuntu-gui\nRuns as expected. I see the comphep tool and can use it.\n\nScreenshot 2021-10-19 at 10.18.261920\u00d71343 231 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/88f5938b3f99fd021a37a3e90800eedb7ea38a7b.jpeg)\n\nImplementation in galaxy: First of all the tool exits right away. Putting a sleep 360 I can get it running. However I see: \u201cProxy target missing\u201d when I try to access the interactive tool running in galaxy. I use the same destination for my custom tool as for the panoply tool.\n\n\nScreenshot 2021-10-19 at 09.42.441305\u00d7210 20 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/81dded28cb7039b2b4da9bd82e5fd06857892dfa.png)\n\nThe docker run command that the galaxy produces for the comphep tool is the following:\n\ndocker run -e \"GALAXY_SLOTS=$GALAXY_SLOTS\" -e \"HOME=$HOME\" -e \"_GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR\" -e \"_GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR\" -e \"TMPDIR=$TMPDIR\" -e \"TMP=$TMP\" -e \"TEMP=$TEMP\" -p 5800 --name 88eba70b849646718ebe23e4cb668b88 -v /srv/galaxy/server:/srv/galaxy/server:ro -v /srv/galaxy/local_tools/interactive:/srv/galaxy/local_tools/interactive:ro -v /storage/galaxy/jobs_directory/001/1741:/storage/galaxy/jobs_directory/001/1741:ro -v /storage/galaxy/jobs_directory/001/1741/outputs:/storage/galaxy/jobs_directory/001/1741/outputs:rw -v /storage/galaxy/jobs_directory/001/1741/configs:/storage/galaxy/jobs_directory/001/1741/configs:rw -v /storage/galaxy/jobs_directory/001/1741/working:/storage/galaxy/jobs_directory/001/1741/working:rw -v /storage/galaxy/files:/storage/galaxy/files:rw -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v \"$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR:rw\" -w /storage/galaxy/jobs_directory/001/1741/working --net bridge --rm maikenp/comphep-ubuntu-gui:latest /bin/sh /storage/galaxy/jobs_directory/001/1741/tool_script.sh > ../outputs/tool_stdout 2> ../outputs/tool_stderr; return_code=$?; cd '/storage/galaxy/jobs_directory/001/1741'; \n\n\nThe docker run command for the panoply tool (which works) is the following:\n\ndocker run -e \"GALAXY_SLOTS=$GALAXY_SLOTS\" -e \"HOME=$HOME\" -e \"_GALAXY_JOB_HOME_DIR=$_GALAXY_JOB_HOME_DIR\" -e \"_GALAXY_JOB_TMP_DIR=$_GALAXY_JOB_TMP_DIR\" -e \"TMPDIR=$TMPDIR\" -e \"TMP=$TMP\" -e \"TEMP=$TEMP\" -p 5800 --name 49e8f15c4be9441593615daa32392a89 -v /srv/galaxy/server:/srv/galaxy/server:ro -v /srv/galaxy/local_tools/interactive:/srv/galaxy/local_tools/interactive:ro -v /storage/galaxy/jobs_directory/001/1742:/storage/galaxy/jobs_directory/001/1742:ro -v /storage/galaxy/jobs_directory/001/1742/outputs:/storage/galaxy/jobs_directory/001/1742/outputs:rw -v /storage/galaxy/jobs_directory/001/1742/configs:/storage/galaxy/jobs_directory/001/1742/configs:rw -v /storage/galaxy/jobs_directory/001/1742/working:/storage/galaxy/jobs_directory/001/1742/working:rw -v /storage/galaxy/files:/storage/galaxy/files:rw -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v /srv/galaxy/var/tool-data:/srv/galaxy/var/tool-data:ro -v \"$_GALAXY_JOB_TMP_DIR:$_GALAXY_JOB_TMP_DIR:rw\" -w /storage/galaxy/jobs_directory/001/1742/working --net bridge --rm quay.io/nordicesmhub/docker-panoply:4.5.1 /bin/sh /storage/galaxy/jobs_directory/001/1742/tool_script.sh > ../outputs/tool_stdout 2> ../outputs/tool_stderr; return_code=$?; cd '/storage/galaxy/jobs_directory/001/1742'; \n\nQuestions:\na) At the moment I dont need anything actually done in the tool except that it should run. Can the comand field of the tool wrapper be more or less empty as I have it now? It seems not, as the tool then exits right away? Or could the exit be due to other underlying problems?\nb) Can you spot other things that don\u2019t seem right with the Dockerfile/startapp.sh/tool-wrapper?\nDockerfile:\nFROM jlesage/baseimage-gui:ubuntu-18.04\nMAINTAINER Maiken Pedersen, maikenp@uio.no\n\nRUN apt-get update -y && \\\n     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n         build-essential \\\n\t libx11-dev \\\n\t libgfortran5 \\\n\t emacs \\\n\t wget \\ \n\t gfortran && \\\n     rm -rf /var/lib/apt/lists/*\n\nUSER root\nCOPY startapp.sh /startapp.sh\n\nRUN mkdir -p /opt/comphep && cd /opt/comphep/ && wget https://theory.sinp.msu.ru/lib/exe/fetch.php/comphep/download/comphep-4.5.2.tgz && tar -zxf  comphep-4.5.2.tgz && \\\n    cd /opt/comphep/comphep-4.5.2 && \\\n    ./configure && \\\n    make && \\\n    make setup WDIR=/opt/comphep_wd\n\nENV APP_NAME=\"COMPHEP\"\nRUN chown 1000:1000 -R /opt/comphep_wd\nWORKDIR /opt/comphep_wd\n\nstartapp.sh:\n#!/bin/sh\ncd  /opt/comphep_wd\nexec /opt/comphep_wd/comphep\n\ntool-wrapper:\n<tool id=\"interactive_tool_comphep_gui\" tool_type=\"interactive\" name=\"COMPHEP ubuntu-gui\" version=\"1.0.0\">\n    <requirements>\n      <container type=\"docker\">maikenp/comphep-ubuntu-gui:latest</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"COMPHEP\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    \n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n        mkdir output &&\n\tsleep 360\n    ]]>\n    </command>\n\n    <outputs>\n      <data name=\"outputs\" label=\"COMPHEP outputs\">\n\t<discover_datasets pattern=\"__name_and_ext__\"  directory=\"output\"/>\n      </data>\n    </outputs>\n    \n</tool>\n\njob_conf.xml relevant section for the destination:\n<destination id=\"interactive_slurm_general\" runner=\"slurm\">\n            <param id=\"nativeSpecification\">--time=72:00:00 --mem-per-cpu=8</param>\n            <param id=\"docker_enabled\">true</param>\n            <param id=\"docker_volumes\">$defaults</param>\n            <param id=\"docker_sudo\">false</param>\n            <param id=\"docker_net\">bridge</param>\n            <param id=\"docker_auto_rm\">true</param>\n            <param id=\"docker_set_user\"></param>\n            <param id=\"require_container\">true</param>\n        </destination>\n"
    },
    {
      "role": "user",
      "text": "Thanks to Anne she pointed out that I was missing /init statement which is needed by galaxy.\nThat solved it all, everything seems to be working perfectly.\nSo my tool now looks like:\n<tool id=\"interactive_tool_comphep_gui\" tool_type=\"interactive\" name=\"COMPHEP ubuntu-gui\" version=\"1.0.0\">\n    <requirements>\n      <container type=\"docker\">maikenp/comphep-ubuntu-gui:latest</container>\n    </requirements>\n    <entry_points>\n        <entry_point name=\"COMPHEP\" requires_domain=\"True\">\n            <port>5800</port>\n        </entry_point>\n    </entry_points>\n    \n    <command detect_errors=\"exit_code\">\n    <![CDATA[\n        mkdir output &&\n\tcd - &&\n        /init ;\n\ttouch output/test.txt &&\n        echo \"Hi!\" > output/test.txt &&\n\tsleep 360\n    ]]>\n    </command>\n\n    <outputs>\n      <data name=\"outputs\" label=\"COMPHEP outputs\">\n\t<discover_datasets pattern=\"__name_and_ext__\"  directory=\"output\"/>\n      </data>\n    </outputs>\n   \n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I ran Cuffdiff on some RNAseq data last weekend, and it completed in about 48 hours. However, I discovered that I didn\u2019t select the option to generate the SQLite file for use in CummeRbund. So, I re-ran the same analysis, but it\u2019s been running for four days now. I have two questions:\n\nShould it take this long to run?\nHow consistent are Cuffdiff\u2019s outputs between runs? Will genes that came up as differentially expressed from the first time I ran Cuffdiff have the same results the second time around (ie does Cuffdiff use any random error to generate its results)? Or do I need to wait for it to finish re-running to  start my follow-up analysis?\n\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe refresh Galaxy page by clicking at \u201cGalaxy\u201d banner at the top left corner. Is the job still running?\nFour days: do you mean run time or both queue time and run time?\nThe run time depends on resources allocation for a job.\nIn my experience Cuffdiff results are consistent, as long as you use the same version of the software.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve had issues just installing galaxy because it keeps stalling. I thought I had finally got it installed but when I change the config file- it wont make my account admin even when its changed. I tried run.sh again to see if I installed it correctly and I got this warning\n*** WARNING: you are running uWSGI without its master process manager ***\n\nand it stalls at\ngalaxy.model.database_heartbeat DEBUG 2020-05-21 10:43:15,574 [p:1239,w:1,m:0] [database_heartbeart_main.web.1.thread]\nmain.web.1 is config watcher     \n\nI am trying to set up galaxy to use conda if that makes a difference. I also just upgraded my memory since before I was told it may be stalling due to lack of memory so now I know that\u2019s not the issue."
    },
    {
      "role": "system",
      "text": "Given your next related question (run.sh stalls after Toolbox index finished (@link https://help.galaxyproject.org/t/run-sh-stalls-after-toolbox-index-finished/3757)) could you please post the whole startup log, or at least couple of hundred lines from the end?"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m having trouble accessing @link http://usegalaxy.org. the problem is that in other computers it is possible to enter the service, but it simply does not load from mine"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe @link http://usegalaxy.org service is up and running.\n\n\n\n@link https://status.galaxyproject.org/\n\n\nThe Galaxy Project Status (@link https://status.galaxyproject.org/)\nWelcome to The Galaxy Project's home for real-time and historical data on system performance.\n\n\n\n\n\n\nIf you are having trouble connecting from some computers and not others, the problem likely involves your internet connection."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nUntitled1336\u00d7777 59.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/72e351eb42a8b93c639e1d9a29c71229049d3c31.png)\nHello.\nI\u2019m attaching a snapshot of the fastQC output of a sample, I\u2019m debating how to QC this and similar samples, on the one hand it look alright as is, on the other hand I sent it to bioinformatic and he did the following:\nTrimming and filtering of raw reads\nRaw reads (fastq files) were inspected for quality issues with FastQC. Following that, the reads were quality-trimmed with cutadapt, using a quality threshold of 32 for both ends, poly-G sequences (NextSeq\u2019s no signal) and adapter sequences were removed from the 3\u2019 end, and poly-T stretches were removed from the 5\u2019 end (being the reverse-complement of poly-A tails). The cutadapt parameters included using a minimal overlap of 1, allowing for read wildcards, and filtering out reads that became shorter than 15 nt. Finally low quality reads were filtered out using fastq_quality_filter, with a quality threshold of 20 at 90 percent or more of the read\u2019s positions.\nhis QC caused the lost of 16% of the reads\nI also triad using fastp on default setting which only discard 1.2% of the reads\u2026\ni will be happy to hear your inputs\nThank you\nNetanel"
    },
    {
      "role": "system",
      "text": "hi @user\nstringency of trimming depends on downstream analysis, so take settings from a recent paper with a similar analysis as yours or test different trimming setups on one sample or small number of samples. Number of reads is not really relevant without information on the project. 47M reads look like an overkill for differential gene expression in bacteria but might not be sufficient for whole genome variant calling, and excessive data might results in poor quality in some projects.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am new to Galaxy and bioinformatics so feel free to let me know if there is a better way of asking my question. The files were executed in a way that the MultiQC did not recognize the files existed. I downloaded SRA files to Galaxy. These files were transfered to a Paired-end data folder with each accession (sample) inside along with the forward and reverse fastq files. This general file format remains after running these Files through FastQC. Now, I am trying to run the files through MultiQC but the data is not recognized. Does anyone have suggestions on how to fix this?"
    },
    {
      "role": "system",
      "text": "Please see this tutorial @link https://galaxyproject.org/tutorials/ngs/#organizing-and-qcing-multiple-datasets for:\n\nHow to create fastq dataset collections\nHow to run FastQC in batch on collections\nHow to generate a MultiQC report based on those FastQC results in a collection\n\nDouble checks: Make sure the MultiQC tool form is set up correctly for this use case (all covered by the video in the tutorial):\n\nFastQC reports are in a dataset collection\n\u201cFastQC\u201d is set as the report type.\n\u201cDataset Collections\u201d are set as the input type.\nPick the \u201craw\u201d results (text report). The \u201chtml\u201d results are not appropriate.\n\nMy guess is that either item 2 or 3 was not set properly on the FastQC tool form.\nadmin edit: Cross-posted at Gitter: @link https://gitter.im/galaxyproject/Lobby?at=5c0c5f88178d7860a1a84938"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nI made xml wrappers for some gatk4 tools. They receive and/or generate bam files and also corresponding bai files. So i made separate inputs/outputs via which they can pass that bai files.\nBut today i tried to download some bam from galaxy web interface (21.05), and it showed me menu in which i can also download bai file, and this bai is not the bai generated by my tools cos it weighs about 1.5 times less. I found it by size, and it was there:\n\n/home/transgen/galaxy/database/objects/_metadata_files/8/9/c/metadata_89ca5b70-7881-4cc1-bc7a-fcc701c18b8b.dat\n\n, whereas the bam file was there:\n\n/home/transgen/galaxy/database/objects/9/4/b/dataset_94bc20e7-2eaf-4345-9b1f-a2e7c0988b5d.dat\n\nIs there a way to use this bai in tool xmls so i could pass only one noodle for each bam file?\nThanks in advance."
    },
    {
      "role": "user",
      "text": "The right answer is:\nln -s '$bam.metadata.bam_index' 'input.bam.bai'"
    }
  ],
  [
    {
      "role": "user",
      "text": "First of all, can you tell me how necessary it is to generate read counts in an RNA-Seq differential expression analysis and what it is for? I\u2019m working with the reference genome of Arabidopsis thaliana and paired-end data.\nI checked the tools FeatureCounts and HTseq counts and both of them require a GTF file, but the Arabidopsis genome is not listed there, should I do my own GTF file with the whole genome or just with some genes?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi Mario\nWithout a count table, you cannot do the differential gene expression analysis. You might want to read up on this in the training material:\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html\n\n\n\nGalaxy Training: Reference-based RNA-Seq data analysis (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html\n\n\n\nGalaxy Training: RNA-Seq reads to counts (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\n\n\n\n@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html\n\n\n\nGalaxy Training: RNA-seq counts to genes (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-genes/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nFor both htseq-count and featureCounts, you can provide your own GTF file. A possible source is here: @link http://plants.ensembl.org/info/website/ftp/index.html\nEven if you are interested in only a few genes, it is recommended to to the differential gene analysis on all genes, and select afterwards.\nI hope this helps, Hans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "We have some custom datatypes (hep.mc.root for instance) defined very simply in datatypes_conf.xml like so:\n<datatype extension=\"hep.mc.root\" type=\"galaxy.datatypes.binary:Binary\" subclass=\"true\" display_in_upload=\"true\" description=\"ROOT binary file.\"/>\n\nIn a tool we are creating we wish to have as input-data a data collection which has data elements of this type. We have this data collection (a list with X number of hep.mc.root files ) in the history.\nHowever, in the input field of the tool, only collections with type png or empty lists are possible to select (we have other tools producing png files, and thus they are in the history). png is one of the standard types from galaxy. Could it be that for Dataset Collections the new custom hep.mc.root datatype does not exist, thus can not be selected?\nThe screenshot:\nThe collections suggested in the input field are either empty or contain png files. The MC Hists item 3895 is the one I would like to be able to select, but it is not suggested in the collection list in the input field to the left as you see.\n\nScreen Shot 2021-08-25 at 13.24.221263\u00d7534 60.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/465432804cfb02f977151e13d634f6d63a5a8a17.png)\nThe tool wrapper looks like the following:\n<tool id=\"plotHistograms_maiken\" name=\"Plot Histograms MaikenTest\" profile=\"16.04\" version=\"1.0.0\">\n<description></description>\n<requirements>\n\t<requirement type=\"package\" version=\"1.0.0\">fys5555_py3</requirement>\n</requirements>\n  <command>\n    <![CDATA[\n\t     #set $origname_data = $dataFile.element_identifier\n\t     #set $orignames_mc = []\n\t     #set $galaxynames_mc = []\n\t     #for $d in $mcFileList\n\t     #set $tmpname=$d.element_identifier\n\t     #silent orignames_mc.append(\"%s\" % $tmpname)\n\t     #silent galaxynames_mc.append(\"%s\" % $d)\n\t     #end for\n\t     python '/storage/shared/software/Scripts/ATLASOpenData/13TeV/copyForMakePlots.py' '$dataFile' '$origname_data' '$galaxynames_mc' '$orignames_mc';\n\t     python '/storage/shared/software/Scripts/ATLASOpenData/13TeV/MakePlots.py' '$channel';\n    ]]> \n  </command>\n  <inputs>\n    <param name=\"channel\" type=\"select\" label=\"Channel\">\n      <option value=\"ee\" selected=\"true\">ee</option>\n      <option value=\"uu\">uu</option>\n    </param>\n    <param name=\"dataFile\" type=\"data\"  format=\"hep.data.root\"  label=\"Data root file\" />\n    <param name=\"mcFileList\" type=\"data_collection\"  label=\"MC root file(s)\" />\n  </inputs>\n  \n  <outputs>\n \t <collection name=\"output\" type=\"list\" label=\"Conventional Analysis Plots\"> \n\t\t         <discover_datasets pattern=\"(?P&lt;designation&gt;.+)\\.png\"  directory=\"Histograms/Plots\"  ext=\"png\"  recurse=\"true\" visible=\"false\" from_tool_provided_metadata=\"true\" />\n\t \n\t\t </collection>\n  </outputs>\n</tool>\n\n"
    },
    {
      "role": "user",
      "text": "Update: ?roblem solved. It was confusion related to the ext option in the discover_datasets parameter and the format option in the collection parameter in the tool that generated the list in the first place:\nWe had:\n<collection name=\"output2\" type=\"list\" format=\"hep.mc.root\"  label=\"MC Hists\">\n    <discover_datasets pattern=\"(?P&lt;designation&gt;.+.*MC.*)\\.root\" directory=\"Histograms/\"   ext=\"root\" recurse=\"true\"  />\n  </collection>\n\nAnd then changed to\n<collection name=\"output2\" type=\"list\"  label=\"MC Hists\">\n    <discover_datasets pattern=\"(?P&lt;designation&gt;.+.*MC.*)\\.root\" directory=\"Histograms/\"   ext=\"hep.mc.root\" recurse=\"true\"  />\n  </collection>\n\nSeems the format in the collection parameter is just ignored, and the ext is taken as the format. We had the wrong extention name \u201croot\u201d instead of \u201chep.mc.root\u201d.\nProblem solved!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI want to obtain 3\u2019UTR sequences from my assembled transcripts.\nWhat tool should I use?\nor is it possible to add the following tools on the online Galaxy server:\nExUTR (GitHub - huangzixia/ExUTR: ExUTR is a practical and powerful tool that enables rapid genome-wide 3'-UTR prediction from massive RNA-Seq data (@link https://github.com/huangzixia/ExUTR)) or\nCodAn (GitHub - pedronachtigall/CodAn: CDS prediction in transcripts (@link https://github.com/pedronachtigall/CodAn))."
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe have a look at TransDecoder. Activate all outputs. Try it with \u2018output only longest ORF option\u2019. It can produce GFF and BED files. GFF has annotation of UTRs, and you can get positions of UTRs from BED file, as well. Sequences can be extracted with any FASTA extraction tool, like getfastabed.\nYou probably will miss transcripts with short ORFs and some complicated cases, such as polycistronic transcripts, might be difficult to process correctly. Also, keep in mind that assembled transcripts from some tools, such as Trinity, can be in both strands, so if you use BED, make sure you consider orientation of ORF while selection 3\u2019 UTR.\nHope it does make sense.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nTrim Galore! can\u2019t actually recognize any fastq files or Cutadapt output files for Nextera Transposase trimming. Do you have any solutions?\nThanks in advance,\nBest\nBernhard"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI think it solved your problem Trim Galore! can't find fastq files - #2 by igor (@link https://help.galaxyproject.org/t/trim-galore-cant-find-fastq-files/8110/2)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI had similar problem with vcftotab on Galaxy Main (Missing information VCFtoTab-delimited (@link https://help.galaxyproject.org/t/missing-information-vcftotab-delimited/1784)) and tried to workaround this problem using vcftotab on Galaxy EU and was asked to issue this similar problem.\nUsing vcftotab on my data leaves me with lots of missing information from the INFO field. Even when using bcftools norm (accordingly to the advice in the Galaxy Tutorial \u201csomatic variant calling\u201d), cleaning out all the information on the Info field with SnpSift rmInfo and annotating it with SnpEff again (which is the annotation tool used in the just mentioned tutorial) I keep having lots of missing information.\nI tried other tools in order to query my annotated variants, such as bcftools query. However, I encounter the problem, that for information in the Info field with the same INFO Tag (gnomad_exome and gnomad_genome for Non-finish are both annotated to my variants using annovar with AF_nfe) only one value is represented in the query output.\nEventually I could manage to install a Galaxy docker and vcftotab worked perfectly on the docker. However, snpeff does not work properly on my docker (SnpEff Error java.lang.OutOfMemoryError: Java heap space (@link https://help.galaxyproject.org/t/snpeff-error-java-lang-outofmemoryerror-java-heap-space/2448)). So right know I cannot apply my workflow on Galaxy Eu or my own instance, because either way one of the tools used (snpeff or vcf2tsv) is not working.\nI am glad for any help!\nAll the best Rose."
    },
    {
      "role": "system",
      "text": "answered in Missing information VCFtoTab-delimited (@link https://help.galaxyproject.org/t/missing-information-vcftotab-delimited/1784/15)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Just to confirm: Galaxy does not need Python 2.7, right? It will work fine with 3.6/3.7?\nJust because I read this on so many places\u2026"
    },
    {
      "role": "system",
      "text": "Yes, recent Galaxy versions work fine with 3.5+ Galaxy (19.09 or newer). We still occasionally discover issues with python 3 support as more people start using Galaxy on python 3, but I would advise you to update.\nThere is currently a bug with uwsgi that prevents threading on python > 3.7.1, so your best bet now would be 3.6"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi All,\nI have a question about enabling the fractions button in FeatureCounts. The subtext seems to contain a contradiction, because it states that the \u2018NH\u2019 value ( multimapped[MM] reads) will be used for the fractions (see image) of multimapped and overlapping reads. It does not say anything about how the overlapping genes are included in the fraction counts.\nOn the biostars forum (the --fraction option of featureCounts (@link https://www.biostars.org/p/9483364/)) it is explained that the MM reads are calculated by using the NH and the overlapping reads calculated by a separate variable ( which is missing from Galaxy:FeatureCounts)\nMy question: Is the NH value the only variable used for the calculation of fractional counts on Galaxy?\n\nimage774\u00d7122 4.51 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0d2680ea5da01b6a1ee5898d7643e75c65a70a21.png)\nBest regards,\nPatrick"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe tool works as expected, but there is an error in the description, which will be fixed. Thanks for your report!\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nThis is a question for the LEfSe galaxy module: I have a question about comparing more than two classes. Is this possible in the module? I tried this but the results were a bit unexpected.\nWhen I compare, for example, class A and class B or A and C or A and B, I get several distinct differences. However, when I compare A, B, and C together, I get only one difference that never appeared in the individual comparisons? Is that an expected result? Would you have any advice on this? Thank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nThere are a few versions of Lefse in the Galaxy community. I think Lefse only produces a single comparison per run for the versions hosted at ORG, EU, and AU. Not sure about Biobakery\u2019s version. Maybe that explains your observations?"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey, My datasets are still in red and haven\u2019t changed in green since one week. I re-uploaded them but no success. What else can I do from my end."
    },
    {
      "role": "system",
      "text": "Hello, The color red means that an error occurred. You can click the bug icon to see the error message. The color yellow would mean it is still running."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,  I am going through \u201cDe novo transcriptome reconstruction with RNA-Seq\u201d  tutorial. I can not cash mm10 reference genome at GFFCompare step: \u201cUse Sequence Data\u201d :  Yes\n\u201cChoose the source for the reference list\u201d :  Locally cached\n\u201cUsing reference genome\u201d* : \u2018Mouse (Mus Musculus): mm10\u2019\nHow do I import mm10 into Galaxy?  see snapshot: \n12%20PM1028\u00d7360 18 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/8fea675f200311edf78f61e13a62263f26d9af05.png)"
    },
    {
      "role": "system",
      "text": "@quote\nGFFCompare\nWelcome, @user\nThe list of available genomes is based on the assigned \u201cdatabase\u201d of the inputs. Maybe you need to assign the \u201cmm10\u201d database to those?\nThe database can be assigned during Upload. Or, for already loaded data, click on the pencil icon to reach the Edit attributes forms. For your case, the first tab has the option of assigning the genome/build \u201cdatatype\u201d. Type in \u201cmm10\u201d to search the list. Make sure your data really is from this same exact build, and that the assigned datatype is correct, or expect problems.\nFAQs:\n\nCommon datatypes explained (@link https://galaxyproject.org/learn/datatypes/)\nThe tool I\u2019m using does not recognize any input datasets. Why? (@link https://galaxyproject.org/support/datatypes-and-tools/)\nHow do I find, adjust, and/or correct metadata? (@link https://galaxyproject.org/support/metadata/)\nMismatched Chromosome identifiers (and how to avoid them) (@link https://galaxyproject.org/support/chrom-identifiers/)\nExtended Help for Differential Expression Analysis Tools (@link https://galaxyproject.org/support/diff-expression/)\n\nThis tool is technically from a deprecated tool suite but I don\u2019t know of any specific issues with it. And I just checked it at Galaxy Main @link https://usegalaxy.org and mm10 is indexed for it.\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am analyzing five Illumina paired-end ChIP-seq datasets, and have run out of Galaxy storage needed to perform my analyses, even after deleting unused histories. Is it possible to get more storage on my account?\nThank you!\nHolly"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas indicated on Account quotas (@link https://galaxyproject.org/support/account-quotas/#i-purged-but-still-need-more-space), if your work is based on academic research and additional space is needed in the short term on @link http://usegalaxy.org, send an email to @link mailto:galaxy-bugs@lists.galaxyproject.org from your registered email address, providing some details about your project, the amount of space that you need, and how long you need it.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I was wondering if it is possible to have other versions for GROMACS production simulation. Available versions are 2018.2 and 2019.1 and I really need to use version 2021.4\nThank you"
    },
    {
      "role": "system",
      "text": "Hello @user\n@link http://UseGalaxy.eu hosts the current functional versions. Changing the tool version (@link https://training.galaxyproject.org/training-material/faqs/galaxy/tools_change_version.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Traing says .mzxml.  data is .mzml\nImage 3-10-21 at 10.10 AM691\u00d7555 89.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/8/82fceb19dffcfe58c26af009662251c8eafb3516.jpeg)"
    },
    {
      "role": "system",
      "text": "Thanks! It will be fixed. Regards."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m new to Galaxy workflow, and I have a problem with Galaxy workflow. Usually, we use GIE to customize tools to handle our data. In there, we can use the if-else branch to decide the next step based on the results in previous steps.\nMay I ask if the Galaxy workflow can achieve the same if-else function? Because I just can connect the tools when using the workflow. Thanks!"
    },
    {
      "role": "system",
      "text": "Check out the expression tools, there is a training that uses them here: @link https://training.galaxyproject.org/training-material/topics/galaxy-ui/tutorials/workflow-parameters/tutorial.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am following VGP workflow and have an issue with purging step. I am using purging mainly to decrease the number of contigs i have as when doing direct scaffolding from hifiasm i am ending up with big number of scaffolds.\nHowever, purging step appear to be removing bacterial DNA known to be present present in the sequenced insect chromosome. I also confirmed its presence in the HIFIASM assembly but it get wiped out from the purged assembly.\nKindly, any suggestion how to deal with this as i would like to maintain the bacterial DNA in the resulting purged sequences ?\nRegards"
    },
    {
      "role": "system",
      "text": "Hello,\nThe purging step may be removing bacterial DNA if the coverage is too high. You can get the purged sequences from the purgedups workflow (dataset tagged with \u201cseq_purged_p2\u201d) if you want to collect the bacterial DNA that has been purged.  In addition to using the decontamination workflow after scaffolding you should be able to recover all the data from the symbiont.\nI hope it helps,\nBests"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello -\nI keep getting an error reading as the following.\nError executing tool with id \u2018toolshed.g2.bx.psu.edu/repos/iuc/hisat2/hisat2/2.2.1+galaxy0\u2019: \u2018HistoryDatasetAssociation\u2019 object has no attribute \u2018forward\u2019\nThe server could not complete this request. Please verify your parameter settings, retry submission and contact the Galaxy Team if this error persists. A transcript of the submitted data is shown below.\n{\n\u201chistory_id\u201d: \u201c23a225085c9279cd\u201d,\n\u201ctool_id\u201d: \u201ctoolshed.g2.bx.psu.edu/repos/iuc/hisat2/hisat2/2.2.1+galaxy0\u201d,\n\u201ctool_version\u201d: \u201c2.2.1+galaxy0\u201d,\n\u201cinputs\u201d: {\n\u201creference_genome|source\u201d: \u201cindexed\u201d,\n\u201creference_genome|index\u201d: \u201chg38canon\u201d,\n\u201clibrary|type\u201d: \u201cpaired_interleaved\u201d,\n\u201clibrary|input_1\u201d: {\n\u201cvalues\u201d: [\n{\n\u201cid\u201d: \u201cf9cad7b01a472135251a6d4487ba53a9\u201d,\n\u201chid\u201d: 19,\n\u201cname\u201d: \u201cFASTQ interlacer singles from data 2 and data 1\u201d,\n\u201ctags\u201d: ,\n\u201csrc\u201d: \u201chda\u201d,\n\u201ckeep\u201d: false\n}\n],\n\u201cbatch\u201d: false\n},\n\u201clibrary|rna_strandness\u201d: \u201c\u201d,\n\u201clibrary|paired_options|paired_options_selector\u201d: \u201cdefaults\u201d,\n\u201csum|new_summary\u201d: \u201cfalse\u201d,\n\u201csum|summary_file\u201d: true,\n\u201cadv|input_options|input_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|alignment_options|alignment_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|scoring_options|scoring_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|spliced_options|spliced_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|reporting_options|reporting_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|output_options|output_options_selector\u201d: \u201cadvanced\u201d,\n\u201cadv|output_options|unaligned_file\u201d: true,\n\u201cadv|output_options|aligned_file\u201d: \u201cfalse\u201d,\n\u201cadv|sam_options|sam_options_selector\u201d: \u201cdefaults\u201d,\n\u201cadv|other_options|other_options_selector\u201d: \u201cdefaults\u201d,\n\u201c__job_resource|__job_resource__select\u201d: \u201cno\u201d\n}\n}\nOK\nCan you please help me? I\u2019m not quite sure what the issue is here and I\u2019m still new to this line of analysis. Thanks!"
    },
    {
      "role": "system",
      "text": "Hello @user\nWe replied to your email directly. This was the advice last Friday:\n\n\nHISAT2 tends to work best with non-interleaved fastq inputs\nWhen you rerun, use distinct forward and reverse read datasets\nRun QA/QC on the read data before mapping if you have not already (FastQC then quality filtering/trimming tools if needed).\n\n\nResources:\n\nPerformance status: @link https://status.galaxyproject.org/\n\nTroubleshooting strategies: Galaxy Training! (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#troubleshooting-errors)\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nThese past days I am trying to setup a AWS instance with galaxy following this tutorial (Getting started with Galaxy on the cloud) (@link https://galaxyproject.org/cloudman/getting-started).\nHowever, the CloudLaunch application (@link https://launch.usegalaxy.org/) link given in the tutorial and multiples documentations and trainings appears to be dead and not responding.\nSince there is no mention of it on galaxy help or anywhere, I suppose it is not a problem and people use a workaround. But I did not find it."
    },
    {
      "role": "system",
      "text": "Hi @user\nCurrent options are available here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use/)\nRelated prior Q&A Galaxy Cloudman or GVL? (@link https://help.galaxyproject.org/t/galaxy-cloudman-or-gvl/3651)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nAfter successfully using localhost:8080 for my local galaxy install for several months I am now receiving a localhost refused to connect error.  Nothing that I am aware of has changed in my environment, so I cleared my cache and even tried to connect with firefox\u2019s browser, but localhost\u2019s message is that it cannot receive any data.  I am using tunneling to define forward port to localhost.\nTroubleshooting questions:\nIs there any other way I can connect to Galaxy other than localhost when I have a local installation?  Does anyone have any other troubleshooting ideas?"
    },
    {
      "role": "system",
      "text": "Not sure what this exactly means.\n@quote\nI am using tunneling to define forward port to localhost.\nBut what you could do:\n\nStop galaxy\nrun the command sudo lsof -i tcp:8080\n\nKill the PID\u2019s of galaxy sudo kill 23001 (The PID of yourself, 23001 is an example)\nstart galaxy again\ntest if it solved your problem\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am new to Galaxy, and am trying to complete a differential gene expression analysis. I use two replicates of RNAseq data. Since I got the clean reads data, I started my workflow from HISAT2>>StringTie>>StringTie merge>>StringTie (using Stringtie merge as a reference genome) >> DESeq2. I found an error result of DESeq2, and then I checked my input, and I found that some of the StringTie gene counts of my data were showing 0 counts. I don\u2019t know why this happened, because the other data is good. Can anyone help me out with this problem? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe consider following any established protocol, such as\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html)\n\n\nGalaxy Training: 1: RNA-Seq reads to counts (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-reads-to-counts/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Sorry for the dumb question - I have forgotten how to log in to my local instance of galaxy and apparently, I never set up the email server for \u201cforgot my password\u201d. Can I find the password for my email so I can log in?"
    },
    {
      "role": "system",
      "text": "Passwords are not stored as plain text, so finding them in the database does not help you. Your best bet is to find Galaxy\u2019s configuration file (usually galaxy.ini) and add another admin account (@link https://galaxyproject.org/admin/#setup-admin-user), then create that new account and then you are an admin and can change people\u2019s passwords (like the one of your old account)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use Faster Download and Extract Reads in FASTQ to download single-cell RNA-seq raw reads. The specific SRR accession is SRR11821880, and the SRA run browser tells me that there are three reads per spot. The first two are technical reads (adapter: 8 bp, barcode+UMI: 28 bp), and the last one is a biological read (150 bp).\nMy settings for the tool are:\n\nSelect input type: SRR accession\nAdvanced options>Select how to split the spots: --split-files\nAdvanced options>Dump only biological reads: No\n\nCurrently, I am only getting the fastqs with the adapter sequences and the biological reads, but not the barcode+UMI. I\u2019m unsure of why this is happening, since I am able to browse and view all of the reads in the run browser. Is this tool restricted to getting only two files maximum, or am I overlooking a setting?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit seems that is not possible to get the 3 reads at all, and the ENA dump (@link https://www.ebi.ac.uk/ena/browser/view/SRR11821880?show=reads) also just contains 2 fastq files.\nIf you go to @link https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR11821880,  under data access you can get the original files, that might be the easiest way to get them.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to run a HTSEQ-COUNT function on a BAM and GTF file but it keeps failing and showing \u201cErrno 122: Disk Quota Exceeded\u201d but I\u2019ve only used 8% of my storage."
    },
    {
      "role": "system",
      "text": "Update March 24 2023\nOver the last few days, a subset of jobs failed with \u201cErrno 122: Disk Quota Exceeded\u201d and a few other related messages (examples: file not retrieved, metadata generation failed, server could not complete)\nAny jobs or operations that failed in odd ways or that produced empty green outputs should be rerun.\nFor errors produced when using the Upload tool, reloading the data is the solution.\n\nWelcome, @user\n@quote\n\u201cErrno 122: Disk Quota Exceeded\u201d\nJobs that failed with this message are due to a server issue at @link http://UseGalaxy.org.\nPlease rerun any jobs that ended this way."
    }
  ],
  [
    {
      "role": "user",
      "text": "primary factor: Treatment\n\nError in data.frame(\u2026, check.names = FALSE) :\narguments imply differing number of rows: 8578, 11447\nCalls: get_deseq_dataset \u2026 eval \u2192 eval \u2192 eval \u2192 cbind \u2192 cbind \u2192 data.frame"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit seems that your replicates have different numbers of entries. If you need additional support, please, share your Galaxy history with me. My email is Screenshot from 2022-04-03 16-52-09.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m trying to run galaxy in my local drive. However, it seems file uploading from my local drive doesn\u2019t work.\nHow can I fix it?\nthe error traceback is like this:\nTraceback (most recent call last): File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 550, in move os.rename(src, real_dst) FileNotFoundError: [Errno 2] No such file or directory: SafeStringWrapper(str:__lt__class __sq__NoneType__sq____gt__,__lt__class __sq__NotImplementedType__sq____gt__,__lt__class __sq__bool__sq____gt__,__lt__class __sq__bytearray__sq____gt__,__lt__class __sq__ellipsis__sq____gt__,__lt__class __sq__galaxy.security.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq____gt__) object at 7fbdc82aa480 on: __sq__/home/chansik/XXXX/galaxy/database/tmp/upload_params_99yg3r5a__sq__ -> '/home/chansik/\ubc14\ud0d5\ud654\uba74/galaxy/database/jobs_directory/000/26/upload_params.json' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"lib/galaxy/jobs/runners/__init__.py\", line 243, in prepare_job job_wrapper.prepare() File \"lib/galaxy/jobs/__init__.py\", line 1159, in prepare self.__prepare_upload_paramfile(tool_evaluator) File \"lib/galaxy/jobs/__init__.py\", line 1124, in __prepare_upload_paramfile shutil.move(tool_evaluator.param_dict['paramfile'], new) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 564, in move copy_function(src, real_dst) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 263, in copy2 copyfile(src, dst, follow_symlinks=follow_symlinks) File \"/home/chansik/anaconda3/envs/_galaxy_/lib/python3.6/shutil.py\", line 120, in copyfile with open(src, 'rb') as fsrc: FileNotFoundError: [Errno 2] No such file or directory: SafeStringWrapper(str:__lt__class __sq__NoneType__sq____gt__,__lt__class __sq__NotImplementedType__sq____gt__,__lt__class __sq__bool__sq____gt__,__lt__class __sq__bytearray__sq____gt__,__lt__class __sq__ellipsis__sq____gt__,__lt__class __sq__galaxy.security.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq____gt__) object at 7fbdc82aa480 on: __sq__/home/chansik/XXXX/galaxy/database/tmp/upload_params_99yg3r5a__sq__\n"
    },
    {
      "role": "system",
      "text": "which version of galaxy are you using and are you by any coincidence uploading a xlsx file?\n\n\n@link https://github.com/galaxyproject/galaxy/issues/8341\n\n\n\n\n\n\n\n\nxlsx format is not recognize (@link https://github.com/galaxyproject/galaxy/issues/8341)\n\n\n\n        opened 10:15AM - 16 Jul 19 UTC\n\n\n          closed 01:32PM - 20 Aug 19 UTC\n\n\n\n\n          FredericBGA\n         (@link https://github.com/FredericBGA)\n\n\n\n\n\n\n\non galaxy 19_05 we can't upload an xlsx file and let galaxy sniff the datatype.\n\u2026 (@link )\nit works when you set the datatype yourself.\n\nthe errors on galaxy.eu and galaxy.org are different...\nI have the same error as galaxy.org on my instance.\n\ngalaxy.eu\n> Fatal error: Exit code 1 ()\n> [Errno 2] No such file or directory: SafeStringWrapper(unicode:__lt__class __sq__galaxy.tools.wrappers.ToolParameterValueWrapper__sq____gt__,__lt__class __sq__galaxy.util.object_wrapper.SafeStringWrapper__sq____gt__,__lt__class __sq__numbers.Number__sq___\n\ngalaxy.org:\n> Traceback (most recent call last):\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 326, in <module>\n>     __main__()\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 319, in __main__\n>     metadata.append(add_file(dataset, registry, output_path))\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/tools/data_source/upload.py\", line 128, in add_file\n>     convert_spaces_to_tabs=dataset.space_to_tab,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/upload_util.py\", line 54, in handle_upload\n>     convert_spaces_to_tabs=convert_spaces_to_tabs,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 724, in handle_uploaded_dataset_file_internal\n>     auto_decompress=auto_decompress,\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 663, in handle_compressed_file\n>     sniffed_ext = run_sniffers_raw(filename, sniff_datatypes)\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 482, in run_sniffers_raw\n>     file_prefix = FilePrefix(filename_or_file_prefix)\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/datatypes/sniff.py\", line 541, in __init__\n>     compressed_format, f = compression_utils.get_fileobj_raw(filename, \"rb\")\n>   File \"/cvmfs/main.galaxyproject.org/galaxy/lib/galaxy/util/compression_utils.py\", line 50, in get_fileobj_raw\n>     with zipfile.ZipFile(filename, mode) as zh:\n>   File \"/cvmfs/main.galaxyproject.org/python/lib/python2.7/zipfile.py\", line 729, in __init__\n>     raise RuntimeError('ZipFile() requires mode \"r\", \"w\", or \"a\"')\n> RuntimeError: ZipFile() requires mode \"r\", \"w\", or \"a\"\n\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. First off, I am a complete noob, but I have managed to get Galaxy running on a M1 Mac. Unfortunately, I am not able to take advantage of the multiple cores. I know I need to edit the job_conf.xml file so that it will override the GALAXY_SLOTS default value for most of the tools, which is usually 1. I have an 10 CPU core mac with 32gb of memory and I want to try to optimise galaxy to use all of it when necessary.\nI have followed several other post and have tried to cobble together my own job_conf.xml file to have the following:\n<?xml version=\"1.0\"?>\n<job_conf>\n<plugins>\n  <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"10\"/>\n<plugin id=\"multilocal\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\" workers=\"10\"/>\n</plugins>\n<handlers>\n<handler id=\"main\"/>\n</handlers>\n\n<destinations default=\"local\">\n    <destination id=\"local\" runner=\"local\">\n    <destination id=\"local10threads\" runner=\"multilocal\">\n        <param id=\"request_cpus\">10</param>\n        <param id=\"memory\">28G</param>\n        <param id=\"embed_metadata_in_job\">False</param>\n        <param id=\"nativeSpecification\">-p Galaxy -n 10</param>\n        <env file=\"/data/galaxy/galaxy-central/galaxy_venv/bin/activate\" />\n    </destination>\n</destinations>\n<tools>\n    <tool id=\"bowtie2\" destination=\"local10threads\"/>\n    <tool id=\"bwa-mem\" destination=\"local10threads\"/>\n    <tool id=\"bwa-mem2\" destination=\"local10threads\"/>\n    <tool id=\"bowtie2\" destination=\"local10threads\"/>\n    <tool id=\"bowtie\" destination=\"local10threads\"/>\n    <tool id=\"bowtie-build\" destination=\"local10threads\"/>\n    <tool id=\"fastq_groomer_parallel\" destination=\"local10threads\"/>\n    <tool id=\"deeptools\" destination=\"local10threads\"/>\n    <tool id=\"cufflinks\" destination=\"local10threads\"/>\n    <tool id=\"cuffdiff\" destination=\"local10threads\"/>\n    <tool id=\"cuffmerge\" destination=\"local10threads\"/>\n    <tool id=\"tophat2\" destination=\"local10threads\"/>\n    <tool id=\"trimmomatic\" destination=\"local10threads\"/>\n</tools>\n</job_conf>\n\nHowever, when I run bwa-mem2 or any other tool in galaxy, it still reverts to only using one core or whatever the default value is :frowning: .\nCan someone please help me figure out what to do? I am out of ideas\nAs an example, this is what Galaxy shows is the resulting argument when I try to run bwa-mem2:\nset -o | grep -q pipefail && set -o pipefail;  ln -s '/Users/antoniolamb/galaxy/database/objects/e/1/c/dataset_e1c7b303-95f1-4185-9b51-a3e66cacde75.dat' 'localref.fa' && bwa-mem2 index 'localref.fa' &&    bwa-mem2 mem -t \"${GALAXY_SLOTS:-1}\" -v 1                 'localref.fa' '/Users/antoniolamb/galaxy/database/objects/5/d/1/dataset_5d1ed8fa-99aa-466c-b508-8fff2dfcc65b.dat' '/Users/antoniolamb/galaxy/database/objects/1/9/1/dataset_19188ad0-62b7-492c-9d7b-6263ff735bd0.dat'  | samtools sort -@${GALAXY_SLOTS:-2} -T \"${TMPDIR:-.}\" -O bam -o '/Users/antoniolamb/galaxy/database/objects/0/b/4/dataset_0b48873c-3cf7-4f80-add1-2a086f5237cb.dat'\nThis is what shows in the destination parameters. I am so confused and frustrated why it won\u2019t use all the cores available?\n\nScreenshot 2023-04-04 at 01.23.251808\u00d7656 42.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dffef635e6f83d6fbc78105f79024ed733b60473.png)\n\nScreenshot 2023-04-04 at 01.24.081840\u00d7282 54 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/5/5c397113bcd9eb25f955d716b212443c7d2705b8.png)"
    },
    {
      "role": "user",
      "text": "Hello! After almost a week of troubleshooting, I finally got it working and I now have a local installation of galaxy that uses 9/10 cores and 32 GB of memory on my macbook pro :slight_smile:. I left one for the OS so it doesn\u2019t crash. If anyone else has this problem, please use these job_conf.xml settings in your file, and change the number of cores\n(  9). It will make all tools default to using the number of cores specified.\n<?xml version=\"1.0\"?>\n<job_conf>\n    <plugins>\n        <plugin id=\"local\" type=\"runner\" load=\"galaxy.jobs.runners.local:LocalJobRunner\"/>\n    </plugins>\n    <handlers>\n        <handler id=\"main\"/>\n    </handlers>\n    <destinations default=\"multicore_destination\">\n        <destination id=\"multicore_destination\" runner=\"local\">\n            <param id=\"local_slots\">9</param>\n        </destination>\n        <tools>\n        <tool id=\"*\" destination=\"local\"/>\n    \t</tools>\n    </destinations>\n</job_conf>\n\nBelow is proof :). Trimmomatic using 46 threads running on multiple cores. I confirmed it works with BWA-mem as well.\n\nimage1768\u00d7212 30.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1521c5640f779fd24812bca08060005c5fc1c242.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello there,\nI want to use CAT bins, but it requires a CAT database. I have seen that other galaxy server have a (pre-stored) CAT database available, but not on @link http://usegalaxy.org. I could use \u201cprepare CAT database\u201d, but I have seen posts of other users that this used up their quota.\nWould it be possible to have a recent CAT database buildt for the general user available from galaxy?\nThanks a lot! Cycle"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nThe indexes between servers will be synched up later this year.\nDid the tool not work at the EU server with the built-in indexes?\nRef Database generated from CAT prepare exceed the account quota (@link https://help.galaxyproject.org/t/database-generated-from-cat-prepare-exceed-the-account-quota/7328)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Using DANTE and Protein Domains Filter I noticed that sometimes there is no exact correspondence between the coordinates of the same domain between the gff and the fasta files. What is the reason for this?\n\nImmagine_galaxy1909\u00d7871 87.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1af7274d1533017df4ce9479499b5596ea71c9e0.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nCorrect, these are not always expected to be an exact match. For this specific example, my guess is that the stop codon is being omitted from the fasta, but you could investigate that by examining the region in a genomic browser.\nQuote from the Protein Domains Filter tool form:\n\nOUTPUTS PRODUCED:\n\nFiltered GFF3 file\nTranslated protein sequences of the filtered domains regions of original DNA sequence in fasta format\n\nTranslated sequences are taken from the best alignment (Best_Hit attribute) within a domain region, however this alignment does not necessarily have to cover the whole region reported as a domain in gff file\n\nTo learn more about how this tool suite tool functions, including Galaxy usage, please see:\n\n\nRepeat Explorer documentation: @link http://repeatexplorer.org/\n\nGalaxy Repository: @link https://toolshed.g2.bx.psu.edu/view/petr-novak/dante/65a6fb89495d (includes links to the development and associated end-user resources)\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all, this morning I started to notice some problems runnings jobs. I launched three different jobs and 1 hour later all of them are still on queue\u2026I\u2019m using scanpy tools for a scRNAseq analysis and plotting some DESeq2 results with Heatmap2. Does anyone know if there has been any general problems?"
    },
    {
      "role": "system",
      "text": "Update 3/31/2023\nSee Failed local file upload with 504 timeout: Resolved at UseGalaxy.eu March 31 2023 - #6 by marek (@link https://help.galaxyproject.org/t/failed-local-file-upload-with-504-timeout-resolved-at-usegalaxy-eu-march-31-2023/9754/6)\n\nWelcome, @user\nMost jobs will queue before executing. These are shared public resources. Leaving jobs queued is important since all new jobs are added back at the end of the queue, extending the wait time.\nThe @link http://UseGalaxy.eu database did have a small issue but seems to be up now. Rerun if a job happens to fail.\n@link https://status.galaxyproject.org/"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been trying to download files from my history in @link http://usegalaxy.eu using \u2018curl\u2019, following the instructions on the following page: @link https://galaxyproject.org/support/download-data/\nThis method works fine with the main Galaxy server at @link http://usegalaxy.org, just not with the European server. I\u2019m able to download files in a web browser using http and https.\nAs an example, using the verbose parameter, I get the following output (precise URLs redacted). Any advice would be much appreciated. Downloading ginormous BAM files through a web browser is not ideal.\ninfo-sml-113-x:SSD bramblepuss$ curl -JLOv @link https://usegalaxy.eu/datasets/redacted/display?to_ext=bam\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:\u2013:-- --:\u2013:-- --:\u2013:--     0*   Trying 192.52.32.126\u2026\n\nTCP_NODELAY set\nConnected to @link http://usegalaxy.eu (192.52.32.126) port 443 (#0)\nALPN, offering h2\nALPN, offering http/1.1\nCipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\n\nsuccessfully set certificate verify locations:\nCAfile: /etc/ssl/cert.pem\nCApath: none\nTLSv1.2 (OUT), TLS handshake, Client hello (1):\n} [512 bytes data]\nTLSv1.2 (IN), TLS handshake, Server hello (2):\n{ [93 bytes data]\nTLSv1.2 (IN), TLS handshake, Certificate (11):\n{ [2636 bytes data]\nTLSv1.2 (IN), TLS handshake, Server key exchange (12):\n{ [333 bytes data]\nTLSv1.2 (IN), TLS handshake, Server finished (14):\n{ [4 bytes data]\nTLSv1.2 (OUT), TLS handshake, Client key exchange (16):\n} [70 bytes data]\nTLSv1.2 (OUT), TLS change cipher, Client hello (1):\n} [1 bytes data]\nTLSv1.2 (OUT), TLS handshake, Finished (20):\n} [16 bytes data]\nTLSv1.2 (IN), TLS change cipher, Client hello (1):\n{ [1 bytes data]\nTLSv1.2 (IN), TLS handshake, Finished (20):\n{ [16 bytes data]\nSSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256\nALPN, server did not agree to a protocol\nServer certificate:\nsubject: @link http://CN=usegalaxy.eu\n\nstart date: Dec  7 07:26:49 2018 GMT\nexpire date: Mar  7 07:26:49 2019 GMT\nsubjectAltName: host \u201c@link http://usegalaxy.eu\u201d matched cert\u2019s \u201c@link http://usegalaxy.eu\u201d\nissuer: C=US; O=Let\u2019s Encrypt; CN=Let\u2019s Encrypt Authority X3\nSSL certificate verify ok.\n\n\nGET /datasets/8fb8030d304f7807/display?to_ext=bam HTTP/1.1\nHost: @link http://usegalaxy.eu\nUser-Agent: curl/7.54.0\nAccept: /\n\n< HTTP/1.1 200 OK\n< Server: nginx/1.12.2\n< Date: Wed, 16 Jan 2019 20:16:17 GMT\n< Content-Type: text/html\n< Transfer-Encoding: chunked\n< Vary: Accept-Encoding\n< Set-Cookie: galaxysession=11ac94870d0bb33aef1bd67c23e76ec7d7d0aa26dbc781503fab4ecc9b5ac968cb576e82072afe8f; expires=Tue, 16-Apr-2019 22:16:17 GMT; httponly; Max-Age=7776000; Path=/; secure; Version=1\n< X-Content-Type-Options: nosniff\n<\n{ [4979 bytes data]\n100  4960    0  4960    0     0   9185      0 --:\u2013:-- --:\u2013:-- --:\u2013:--  9202\n\nConnection #0 to host @link http://usegalaxy.eu left intact\n"
    },
    {
      "role": "system",
      "text": "Hi - Just ran a few tests and it appears that the history and objects need to be shared before curl can be used for download at the EU server. This can just be a share link (no need to \u201cpublish\u201d as well). However, it is important to check the box that \u201calso shares objects\u201d. If the history is already shared, unshare, check the box, then reshare.\nScreenshot after clicking object share box but before clicking on the share-by-link:\n\nshare-history-objects.png1284\u00d7272 33.7 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/4181ff6b5815422f235439b178e87c6e8e7ff054.png)\nHere is an example dataset I\u2019ll leave up for a bit so you can try it: tabular dataset, one line, super small\n$ curl -o outfile_after_sharing 'https://usegalaxy.eu/datasets/531ddf9f9edb68b8/display?to_ext=txt'\n$ more outfile_after_sharing\nhello   world\n\nI\u2019ll follow-up with the EU team about this and get these details added to the general Download FAQ if \u201csharing first\u201d is an intentional pre-requirement for that server (seems so from testing).\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m using circos to create a visual representation of genomic data, but I\u2019m unable to create cytogenic bands that show up on the image. I\u2019ve managed to create the basic ideogram used by: @link https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html to depict a cancer genome but I can\u2019t seem to decipher how the band data is stored in \u2018hg18_karyotype_bands.tsv\u2019, more specifically how the seventh column is used to generate bands. I\u2019d appreciate any help. Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user, I think I can provide some help here. The 7th column is just a named color that you want to use for that band. There are a number of colors built in, here the staining colours are used.\nband\tchr1\tp36.33\tp36.33\t0\t2300000\tgneg\nband\tchr1\tp36.32\tp36.32\t2300000\t5300000\tgpos25\nband\tchr1\tp36.31\tp36.31\t5300000\t7100000\tgneg\nband\tchr1\tp36.23\tp36.23\t7100000\t9200000\tgpos25\nband\tchr1\tp36.22\tp36.22\t9200000\t12600000\tgneg\n\nyou can read more about the karyotype table format on their website @link http://circos.ca/documentation/tutorials/ideograms/karyotypes/lesson"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there a quick way to save an entire history to an external hard drive? I can go line by line and download but that would take hours\u2026"
    },
    {
      "role": "system",
      "text": "Try History Options / Export History to File"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am having issues with DESeq2. I get this error message\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nImport genomic features from the file as a GRanges object \u2026 OK\nPrepare the \u2018metadata\u2019 data frame \u2026 OK\nMake the TxDb object \u2026 OK\n\u2018select()\u2019 returned 1:many mapping between keys and columns\nNote: importing abundance.h5 is typically faster than abundance.tsv\nreading in files with read.delim (install \u2018readr\u2019 package for speed up)\n1 2 3 4 5 6\nsummarizing abundance\nsummarizing counts\nsummarizing length\nusing counts and average transcript lengths from tximport\nWarning message:\nIn .get_cds_IDX(type, phase) :\nThe \u201cphase\u201d metadata column contains non-NA values for features of type\nstop_codon. This information was ignored.\nestimating size factors\nusing \u2018avgTxLength\u2019 from assays(dds), correcting for library size\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\nError in estimateDispersionsFit(object, fitType = fitType, quiet = quiet) :\nall gene-wise dispersion estimates are within 2 orders of magnitude\nfrom the minimum value, and so the standard curve fitting techniques will not work.\nOne can instead use the gene-wise estimates as final estimates:\ndds <- estimateDispersionsGeneEst(dds)\ndispersions(dds) <- mcols(dds)$dispGeneEst\n\u2026then continue with testing using nbinomWaldTest or nbinomLRT\nCalls: DESeq \u2026 estimateDispersions -> .local -> estimateDispersionsFit\nI\u2019m not exactly sure what to do to remedy this problem."
    },
    {
      "role": "system",
      "text": "Hello,\nThere is much discussion about this error coming up at the DESee2/Bioconductor forum if you want to explore prior Q&A solutions (warning: not all will be available in Galaxy). The error would come up whether run in Galaxy or not.  Google will show the top hits for @link https://support.bioconductor.org/ with this search: \u201call gene-wise dispersion estimates are within 2 orders of magnitude\u201d\nIn summary, this is a data content-related error. It indicates that there is a data mixup upstream (the same sample processed more than once by accident) or the given samples do not really have enough expression variance to do the calculation. Biological replicates are expected; technical replicates will have the same problem, e.g. insufficient expression differences."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have a fastqsanger.gz paired-end data file with both forward and reverse reads combined into a single file. I want to use Trimmomatic to trim adapters and set a quality score. I don\u2019t see an option to input a single paired-end file. Would it be advisable to input this as a single-end file instead?\nI was originally going to use FASTQ Splitter to first split the data into two separate files for the forward and reverse reads, however my forward and reverse reads are not all the same length which is required for this tool. Also there is variable length for each read.\nI am new to this so I\u2019d really appreciate any help, thank you! :smile:"
    },
    {
      "role": "user",
      "text": "I figured I\u2019d give an update in case this helps someone else in the future.\nI reimported my data which was on NCBI using the tool \u201cNCBI using Faster Download and Extract Reads in FASTQ\u201d. This imported my pair-end data into a separate file for the forward and reverse reads and eliminated needing to use FASTQ Splitter. I was then able to use Trimmomatic with the paired-end setting with my two files."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been using Galaxy for a few months now, and lately I had to redo an old Trinity assembly and came back to the server. After setting everything i executed and the proccess has been queued up for more than one day now, I have tried setting new instances and always happens the same. Other programs seem to be working fine. Have trinity jobs been paused for now? Is there a fix for this problem?\nBest Regards\nMiguel"
    },
    {
      "role": "system",
      "text": "Hi @user\nSome tools will queue longer than others, and how long any tool queues varies daily.\nClusters than executed assembly tools, including Trinity are usually busy. Why? The tools are computationally expensive so jobs run longer and the cluster nodes turn over for new jobs slower.\nThe best advice is to queue jobs up and let them run. Avoid deleting and rerunning, as that places the new jobs back at the end of the queue, extending wait time. If done often enough, jobs may take a very long time with all the restarts!\nIf you didn\u2019t know: you can set a notification on tool forms to email yourself once jobs are done :slight_smile: Also, consider using collections and workflows. Workflows queue up tools/jobs in batch (and also has an email notification feature) and collections group similar dataset together. If you are not sure how to use those, search with the keywords here: @link https://training.galaxyproject.org/\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "i put the fastQ data, at the end pops up this message in all files:\nThis is a new dataset and not all of its data are available yet.\nformat html database ? (@link https://usegalaxy.org/datasets/bbd44e69cb8906b5c4f1eabccf816ea3/details#)\nWhat now? how to proceed?\n\nfastq sem dados1366\u00d7768 78.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/6bec149197e993b1b070a5d4bc038dce96c6444d.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\ni put the fastQ data, at the end pops up this message in all files:This is a new dataset and not all of its data are available yet.\nThose jobs are still queued.\n@quote\nformat html database\nAn HTML report is one of the outputs of this tool.\n@quote\nWhat now? how to proceed?\nLeave the jobs queued until completed. If you delete and rerun, any new jobs are placed back at the end of the queue, further extending wait time.\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_job_status.html\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I\u2019m doing an RNAseq analysis. Do you know if it\u2019s correct to use an annotation file normalized with DESeq2 (and annotated with Annotate DESeq2/DEXSeq output tables) in limma-voom or EdgeR? Since they use different normalization methods, I\u2019m not sure if it will affect my results.\nDo you think it would be enough to turn off the normalization option in EdgeR, so it doesn\u2019t get normalized twice? Or what tool would you recommend to get a differential expression table if I want to use EdgeR? I\u2019ve seen in tutorials that AnnotateMyIDs is useful, but I\u2019m not working with any of the organisms it can be used for.\nI hope you can help! Thanks!"
    },
    {
      "role": "system",
      "text": "Hi MarioG\nFor edgeR you need a count table with the raw read counts (i.e. not normalized data), See chapter 2.3 in the edgeR users guide (@link https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf)\nRegards, Hans-Rudolf"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am trying to use an .Rdata file for a mouse dataset in fgsea. The tool does not recognize the target file in my histories or in the dropdown. Thanks for any insight or solutions to running the tool!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nThe dataset might just need to have a correct datatype assigned.\n@quote\nif you are ever not sure what datatype(s) a tool accepts as input, any tool not just those that accept reads, try this:Create a new empty historyLoad up the tool formReview the input datatypes listed in the tool form\u2019s input areaNote: A tool could have more than one input area and this works for all user-supplied inputs from the history.Example\n\nEither of these resources can be search with keywords or navigated by topic.\nTutorials\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/search?query=fgsea)\n\n\nGalaxy Training: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=fgsea)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\nFAQs\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/faqs/galaxy/)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve followed the instructions to run Galaxy on my computer, however I\u2019m getting an error and it just stops working.\n\nScreen Shot 2022-08-09 at 5.09.43 PM1437\u00d71097 238 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/28d0981b19ed7e59a5d435d909094304189e1815.png)\nAny help on how to solve this?"
    },
    {
      "role": "system",
      "text": "Hello @user\nDid you resolve the problem already?\nIf not, please include more details: about your server (OS), how you sourced Galaxy (where + version), and what you have done so far. Is this a new install or are you upgrading?\nGalaxy Admin resources:\n\nTraining: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\nDocs: Galaxy Deployment & Administration \u2014 Galaxy Project 22.01.1.dev0 documentation (@link https://docs.galaxyproject.org/en/master/admin/index.html)\n\n\nI also cross-posted this to the Galaxy Admin gitter chat: galaxyproject/admins - Gitter (@link https://gitter.im/galaxyproject/admins?at=6303cbd3b16e8236e31685a7). They may reply here or there, and feel free to join the chat."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there,\nI wonder how to do md5 checksum on Galaxy to make sure files downloaded from Galaxy are original?\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\nTry this tool: Secure Hash / Message Digest on a dataset"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nWe\u2019re working on getting OIDC Okta running with galaxy and we\u2019re hitting a bug.\nWe get:\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: @link https://myco.okta.com/oauth2/.well-known/openid-configuration?client_id=\u2026\nBut our well known openid config doesn\u2019t have oauth2 in the url\n@link https://myco.okta.com/.well-known/openid-configuration?client_id=\u2026\nOn the oidc_backends.config.xml, we have:\n<redirect_uri>@link https://galaxy.myco.org/authnz/okta/callback</redirect_uri>\n<api_url>@link https://myco.okta.com/oauth2</api_url>\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf you would like to help us to develop that authentication method, please reach out to the developers. Galaxy Working Groups and Projects - Galaxy Community Hub (@link https://galaxyproject.org/community/wg/)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I started two RNA Star jobs yesterday and it has been almost 24 hours since I started the jobs and they are still showing as waiting to run.  It has been a little while since I used Galaxy for data analysis but I did not experience this type of wait times previously.  Is this normal or is there a way to submit these jobs where they would run faster."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe @link http://UseGalaxy.org server has been very busy. The best strategy is to queued jobs then let them complete.\nWe did have a small cluster issue last week but the associated backlog is mostly resolved by now. There isn\u2019t an update as of now, but if something comes up in the next few days we\u2019ll keep posting here: Fastp not running: Confirmed job delays at UseGalaxy.org 09/23/2022. Solution: allow queued jobs to process - #2 by jennaj (@link https://help.galaxyproject.org/t/8720/2)\nThe @link http://UseGalaxy.eu server has also been busy, as are most others with public access.\n@quote\nIs this normal or is there a way to submit these jobs where they would run faster.\nEveryone has equal priority when working at public Galaxy servers. Some wait time is expected, but more so for computationally expensive tools that run on our largest cluster nodes. Jobs queue, then process in the order submitted as shared resources become available appropriate for the tool. Using workflows (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/) can significantly speed things up.\nAll known Galaxy server options are covered here: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use). It is common for people to set up one account at several public sites (@link https://help.galaxyproject.org/t/7997/5) to maximize their access to the different resources available (storage, computational). Others decide to set up a private server for institutional, lab, or personal work where they can control the resources \u2013 and this can include attaching local cluster resources, cloud resources, etc."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have reached my quota limit.\nCan you please tell me for how long my history(ies) and datasets be accessible as I am submitting a manuscript and might have to revisit the dataset.\nThanks"
    },
    {
      "role": "system",
      "text": "@user\nGalaxy Europe provides a description of the current data policy at the front page. Here is the quote:\nUser data on @link http://UseGalaxy.eu (i.e. datasets, histories) will be available as long as they are not deleted by the user.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello Galaxy team,\nI am using RNA STARSolo from Galaxy which is really useful tool for people who are not familiar with the bioinformatics. I was using tutorial from the following link - Pre-processing of 10X Single-Cell RNA Datasets (@link https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/scrna-preprocessing-tenx/tutorial.html)\nMost of time, it works great. However, some of our and others dataset has R1 barcode length that are 151bp long (R2 is also 151bp), and I was wondering if there is any way to do pre-processing of those data. Help will be really appreciated.\nThank you,\nYoung"
    },
    {
      "role": "system",
      "text": "Dear @user,\nFor a different bacode-(length) you can check out the option Configure Chemistry Options and pick Custom, then you can and have to define the barcode details yourself.\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have a quite stable running local Galaxy installation running on Centos 8.1\nFor user authentication I run an apache proxy on the same machine connecting to LDAP to an domain server. Galaxy and Apache have been configured as decribed in the Galaxy Tutorials and I use Basic Authentication on Apache.\nThis setup works fine.\nMore recently I wanted to put a Sophos UTM as Web Application Firewall (reverse proxy) in front of the Galaxy-Server.\nI did this to have a more shiny form-based login to the Galaxy Server. The Sophos UTM then passes the form login data to the Galaxy-Server using Basic authentication.\nThis setup works in most of the times also nice - however, from time to time (especially after deleting browser cookies or when first-time users log in) the Galaxy server throws a error 403001 (API authentication required for this request) on the web fronted (see below).\nI looked in the logs of Apache and Galaxy and could not find anything strange. For troubleshooting: can someone explain me why and when Galaxy triggers this error? There seems to be something happening when deleting cookies or new users are logging in that pushed Galaxy on the \u201capi\u201d track.\nHappy for any ideas/suggestions.\nThanks a lot and have a nice and healthy weekend!\n\nerror691\u00d71041 31 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/4/40bc7dfa0cac024735cb0d3308335d5d357210e6.png)"
    },
    {
      "role": "user",
      "text": "Alright, took me some hours, but I found the solution.\nThe error was triggered because Galaxy did not find its session cookie in the RequestHeader that the apache proxy (proxy 2, installed on the Galaxy machine) forwarded to the Galaxy instance when GET for any HTML page was send from a client to Galaxy.\nThe reason why Galaxy got confused was that the client passed both cookies in the header: the one for the Sophos UTM WAF session (reverse proxy 1) and the one for the Galaxy session. The Sophos UTM WAF then remove its session cookie from the RequestHeader BUT, at least in some situations, left behind a \"; \" (so the end of the cookie prior to the next cookie). This leading \"; \" confused the Galaxy script that searches for the \u201cgalaxysession\u201d cookie in the header (i.e. the script did not see the cookie-string even though it was there). Therefore a 403 was triggered.\nI now remove the leading \"; \" by a command in the httpd.conf on the apache RP (proxy 2):\n\nRequestHeader edit Cookie \u201c^(?:; *)\u201d \u201c\u201d\n\nThis solves the issue.\nJust in case there are other Sophos UTM users out there\u2026\n@Mods: Maybe you want to add this (Sophos UTM WAF in front of Galaxy) to the titel of my post\u2026\nCheers"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am experiencing \u201cInternal Server Error (500)\u201d messages whenever I try to run a job with Pilon. The error appears after a few hours of the job start. The detailed error message suggest Error 500, internal server error.\nWhat should I do to avoid/fix this error.\nI\u2019m pasting the head of the error message below.\n\n{\n  \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\",\n  \"onLine\": true,\n  \"version\": \"21.09\",\n  \"xhr\": {\n    \"readyState\": 4,\n    \"responseText\": \"\",\n    \"responseJSON\": null,\n    \"status\": 500,\n    \"statusText\": \"Internal Server Error\"\n  },\n  \"options\": {\n    \"parse\": true,\n    \"filters\": {\n      \"update_time-ge\": \"2022-01-19T06:13:15.000Z\",\n      \"visible\": \"\"\n    },\n    \"remove\": false,\n    \"details\": \"bbd44e69cb8906b515e37ecb30744100\",\n    \"traditional\": true,\n    \"data\": {\n      \"details\": \"bbd44e69cb8906b515e37ecb30744100\",\n      \"order\": \"hid\",\n      \"v\": \"dev\",\n      \"q\": [\n        \"update_time-ge\",\n        \"deleted\",\n        \"purged\"\n      ],\n      \"qv\": [\n        \"2022-01-19T06:13:15.000Z\",\n        \"False\",\n        \"False\"\n      ]\n    },\n    \"emulateHTTP\": false,\n    \"emulateJSON\": false,\n    \"textStatus\": \"error\",\n    \"errorThrown\": \"Internal Server Error\"\n  },\n  \"url\": \"https://usegalaxy.org/api/histories/3d213f5e64af4795/contents?details=bbd44e69cb8906b515e37ecb30744100&order=hid&v=dev&q=update_time-ge&q=deleted&q=purged&qv=2022-01-19T06%3A13%3A15.000Z&qv=False&qv=False\",\n  \"model\": [\n    {\n      \"state\": \"running\",\n      \"deleted\": false,\n      \"purged\": false,\n      \"name\": \"FASTA from pilon on data 19 and data 7\",\n      \"accessible\": true,\n      \"data_type\": \"\",\n      \"file_ext\": \"\",\n      \"file_size\": 0,\n      \"meta_files\": [],\n      \"misc_blurb\": \"\",\n      \"misc_info\": \"\",\n      \"tags\": [],\n      \"history_id\": \"3d213f5e64af4795\",\n      \"history_content_type\": \"dataset\",\n      \"hid\": 46,\n      \"visible\": true,\n      \"model_class\": \"HistoryDatasetAssociation\",\n      \"update_time\": \"2022-01-19T03:16:37.313Z\",\n      \"id\": \"bbd44e69cb8906b515e37ecb30744100\",\n      \"type_id\": \"dataset-bbd44e69cb8906b515e37ecb30744100\",\n      \"create_time\": \"2022-01-19T03:15:50.458Z\",\n      \"extension\": \"fasta\",\n      \"type\": \"file\",\n      \"url\": \"/api/histories/3d213f5e64af4795/contents/bbd44e69cb8906b515e37ecb30744100\",\n      \"dataset_id\": \"bbd44e69cb8906b5a067e46c3375b639\",\n      \"rerunnable\": true,\n      \"creating_job\": \"bbd44e69cb8906b5dc7e571a17ea8d06\",\n      \"urls\": {\n        \"purge\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/purge_async\",\n        \"display\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/display/?preview=True\",\n        \"edit\": \"/datasets/edit?dataset_id=bbd44e69cb8906b515e37ecb30744100\",\n        \"download\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/display?to_ext=\",\n        \"report_error\": \"/dataset/errors?id=bbd44e69cb8906b515e37ecb30744100\",\n        \"rerun\": \"/tool_runner/rerun?id=bbd44e69cb8906b515e37ecb30744100\",\n        \"show_params\": \"/datasets/bbd44e69cb8906b515e37ecb30744100/details\",\n        \"visualization\": \"/visualization\",\n        \"meta_download\": \"/dataset/get_metadata_file?hda_id=bbd44e69cb8906b515e37ecb30744100&metadata_name=\"\n      }\n    },\n    {\n"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis looks like a server error that can come up when the server is restarted (and sometimes when it is really really busy) \u2013 but that should resolve with a rerun. I wouldn\u2019t expect this particular error from problematic inputs or outputs but that\u2019s possible too. Or maybe the tool has some corner-case configuration bug.\nIf a rerun hasn\u2019t worked by now, and since you were working at @link http://usegalaxy.org, I can help quicker and with more specificity if you send a bug report from one of the red error datasets. Please leave all of the inputs/outputs undeleted in the same history, plus any prior failures intact, and paste the URL to this help topic in the comments for context.\nIf you are using a workflow, consider one more rerun directly from the history instead to rule out problems with it. Or I can try that \u2013 but generate a share link to the workflow and also include that in the comments \u2013 or at least include the full name/version and I can find it that way. We want to be all reviewing the same exact processes that produced the error. This error tends to be technical when persistent.\nLet\u2019s start there, thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been running DeSeq2 for my data and an error message came up saying: \u201cUnable to run this job due to a cluster error, please retry it later\u201d. I think there is an issue with the Galaxy server at the minute, as this happened to me before, but was quickly resolved when the Galaxy server was fixed. I just thought I\u2019d give a heads up. The URL used is \u201c@link http://usegalaxy.org\u201d."
    },
    {
      "role": "system",
      "text": "Hi Joe,\nThe server is up now.\nFor anyone else that ran into the same issues, please rerun any jobs that errored. Many failed due to a cluster issue that is now resolved.\nDetails: @link https://galaxyproject.statuspage.io/\n\nApr 18, 2019\nGalaxy Main job failures (@link https://galaxyproject.statuspage.io/incidents/5kk42v9r7f68)\nResolved - Jobs run between 8:30AM and 10:30AM EDT (UTC -0500) on Galaxy Main failed due to an out-of-space condition on the primary cluster (Slurm) server. This issue has been resolved.\nApr 18, 14:48 UTC\n\nThank you for posting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Please add new Reference genomes to the DeepVariant deep learning-based variant caller:\nMycobacterium tuberculosis\nPseudomonas aeruginosa\nKlebsiella pneumonia\nAcinetobacter baumannii\nE. coli\nPneumococcus\nSARS-CoV-2\nThank you"
    },
    {
      "role": "system",
      "text": "@quote\nDeepVariant\nHi @user\nThis tool accepts a reference genome fasta file from the history using the custom genome function. If you plan on incorporating reference annotation ( gtf or gff3 ), be sure to get that at the same time and prep the files to avoid tool errors.\nPrior Q&A topics with instructions for the how-to and expected data formats. I also added some tags to this topic that find even more related topics! (sort by \u201cLatest Posts\u201d).\n@quote\nHello, \nI\u2019m new to using galaxy and RNAseq analysis in general. I wanted to perform reference based RNA-seq analysis on fastq raw read files for Pseudomonas aeruginosa, PA14. I was going through a tutorial to this and reached the mapping step using HISAT2. However, the reference genome file is unavailable in galaxy nor the UCSC bacterial genome database. Is there any way I could obtain the annotated genome fasta for the bacterium in question. Sorry if its a dumb question but I\u2019m fish out of wate\u2026\n@quote\nHi all, \nI want to add a reference genome for RNA-seq analysis to use on HISAT2 and FeatureCounts. \nGenome assembly ARS-UI_Ramb_v2.0 \n\nHow do I go about doing this and what file formats do the they need to be for each program? \nThanks in advance.\n@quote\nHow can I incorporate an annotated bacterial genome for RNA sequencing analysis using galaxy?\n@quote\nDear Admin, \nI am writing to request the addition of Plasmodium species (Plasmodium vivax, Plasmodium malaria, Plasmodium ovale and Plasmodium falciaparum) reference genome in the galaxy main webpage. Thank you.\nHope that helps :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy team,\nI would like to add rice genome as the reference genome to HISAT2, but when I load the fasta file, the page become unresponsive and snap notification appear. How can I solve this problem?\nThanks."
    },
    {
      "role": "system",
      "text": "Hello @user\nThe server was likely busy back when you were attempting to load the fasta with the Upload tool. Trying again was the solution, and hopefully that has worked for you already.\n\n\n@link https://status.galaxyproject.org/\n\n\nThe Galaxy Project Status (@link https://status.galaxyproject.org/)\nWelcome to The Galaxy Project's home for real-time and historical data on system performance.\n\n\n\n\n\nIf HISAT2 jobs happened to fail while using a fasta that large, and the format is correct (FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_working_with_fasta.html)) that might be due to the size of this plant genome versus the public computational resources available.\nYou could work at @link http://UseGalaxy.eu instead \u2013 they host a strain of this genome already built-in for HISAT2 which is far less computationally expensive to use."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am working through the Galaxy training tutorial to analyse CLIPseq data on the Galaxy / CLIP-explorer server. (CLIP-Seq data analysis from pre-processing to motif detection (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/clipseq/tutorial.html#step-7-peak-calling))\n.\nI was wondering if someone knows why the Peak caller ( PEAKachu) does not call the peak (Chr17 8,364 Mb - 8,370 Mb region) that was published in the paper on which the training set is based on.  The fold enrichment was reported to be >8 for this which is well above the threshold set in the peak caller.\nI also analyzed one complete replicate (REP2) from yeolab (original data) and the Peak caller still did not return the expected RBFOX2 peak)\n\nimage1322\u00d732 2.07 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/ce0e796ca0de7c1aa2932d4a7a20b2d328c49aff.png)\n\nimage1324\u00d7461 36.5 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/15f9b081ae2cc260e5f1dc50d405fc6ba3cab704.png)\nimage\n\nimage776\u00d7789 30.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0093cec9a48a718bcccc395d29f26f2f2cfb7caa.png)"
    },
    {
      "role": "system",
      "text": "Dear Patrick,\nThe training set is just a tiny sub-sample of the original dataset and might not give you the peak you would find in the paper.\nHowever, with the original data you should obtain the peak. Few notes, since the new version of DESeq2, it is required that you have two replicates for CLIP and control. Since the data by Van Nostrand et al. 2016 only has one replicate for the control, PEAKachu cannot calculate pvalues for the peaks and that is why you only get foldchanges. So be careful if you filter for pvalues, this might result in a false peak set. PEAKachu relies on DESeq2 to call peaks. Another point could be that the parameters are not optimize. Maybe set the MAD to 2.0 and fine tune with the two parameters minuminum cluster expression fraction and minimum block expression.\nIf you still struggle to get the required peaks than maybe try a different Peakcaller such as PureCLIP.\nI hope I could help.\nHave a good day and best wishes,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m trying to use the Infinium Human Methylation BeadChip workflow in Galaxy with idat files from an Illumina EPIC chip. It\u2019s based on Minfi, which I usually run in Rstudio but wanted to give it a try. It\u2019s ok with the idat files ant the phenotype table, but then I\u2019m asked for a GTF Genome Table (in the example wgEncodeHaibMethyl450Gm12878SitesRep1). What should this genome table be for an EPIC chip?"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nInstructions are in the tutorial associated with the workflow in this section:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/ewas-suite/tutorial.html#differentially-methylated-regions-and-positions-analysis)\n\n\nGalaxy Training: Infinium Human Methylation BeadChip (@link https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/ewas-suite/tutorial.html#differentially-methylated-regions-and-positions-analysis)\nDNA methylation is an epigenetic mechanism used by higher...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I just installed galaxy (release_19.05) on a local server (EL7) under the account galaxy and was able to install some tools.  But when I tried to download some sequences from NCBI SRA, I got problems.  The process got hung. When I checked processes of galaxy, I saw the following two processes.\ngalaxy    90075  90070  0 19:26 ?        00:00:00 prefetch -X 200G --ascp-path /usr/local/bin/ascp|/usr/local/bin/asperaweb_id_dsa.openssh SRR3541303\ngalaxy    90076  90075  0 19:26 ?        00:00:00 /usr/local/bin/ascp -i /usr/local/bin/asperaweb_id_dsa.openssh -pQTk1 -l 450m dbtest@sra-download.ncbi.nlm.nih.gov:data/sracloud/traces/sra40/SRR/003458/SRR3541303 /data/packages/galaxy/database/jobs_directory/000/21/working/sra/SRR3541303.sra.tmp.10369.tmp\n\nWhen I tried the second process on the commandline, I got prompted for Key passphrase, which I have no idea what it was. When I just pressed enter, I got a \u201cfailed to authenticate, exiting.\u201d message.  I\u2019ve extensively searched on google and here, but didn\u2019t get any info about such problems.  I didn\u2019t see any info about such issues in galaxy document (of course I could have missted it).  Any info is appreciaed."
    },
    {
      "role": "user",
      "text": "I\u2019ve found out the cause and fixed the problem.  The private key file /usr/local/bin/asperaweb_id_dsa.openssh  didn\u2019t exist.  I downloaded the aspera connect from @link https://downloads.asperasoft.com/ copied the file asperaweb_id_dsa.openssh to /usr/local/bin/."
    }
  ],
  [
    {
      "role": "user",
      "text": "I wanted to run Genes to transcript map output on my trinity assembly data, but I am not able to find that tool. Is there any alternative tool for this?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis tool is not available currently in @link http://usegalaxy.org; I opened a PR in order to install it; meanwhile, you can run your analysis in @link http://usegalaxy.eu, since in this instance this tool is available:  Genes to transcripts (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/trinity_gene_to_trans_map/trinity_gene_to_trans_map/2.8.4).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Dears, I processed my Pseudomonas Aeruginosa sequences with the reference sequence Pseudomonas Aeruginosa (str.PA7): 16720 on the Map with Bow Tie for Illumina tool.\nI need the NCBI accession number of the reference sequences Pseudomonas Aeruginosa (str.PA7): 16720 on the Galaxy Au platform. Can you help me?\nAlso, Pseudomonas Aeruginosa reference sequence is not available in Map with BWA-MEM, can you add it?"
    },
    {
      "role": "system",
      "text": "Hello @user\nThis is where the index was sourced: Index of /indexes/microbes/16720/ (@link http://datacache.galaxyproject.org/indexes/microbes/16720/)\n\nThe accession is @link http://datacache.galaxyproject.org/indexes/microbes/16720/NC_009656.tRNA.bed.\nInside the /seq directory is the fasta: @link http://datacache.galaxyproject.org/indexes/microbes/16720/seq/pseuAeru_PA7.fa\n\nAdd the fasta to a history (capture the URL and paste into the Upload tool) then use it as a custom reference genome with any tools that do not have it already indexed. How to use Custom Reference Genomes? (@link https://training.galaxyproject.org/training-material/faqs/galaxy/reference_genomes_custom_genomes.html)\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\ni have cutadapt raw output files and want to run cutadapt to trim nextera transposase sequences. Trim galore can\u2019t find any files neither my original fastq.gz files. Could you please help me?\nThanks in advance,\nBest\nBernhard"
    },
    {
      "role": "system",
      "text": "Hi @user,\nmost likely your files have \u2018fastq.gz\u2019 datatype in Galaxy, eg format: fastq.gz. Cutadapt recognizes this datatype, while Trim Galore works with datatype like fastqsanger.gz. Similar topic discussed on the forum, eg Unicycler failed - \"reads unavailable\" -- Inputs not recognized due to an incompatible datatype assignment - #3 by vanina.guernier (@link https://help.galaxyproject.org/t/unicycler-failed-reads-unavailable-inputs-not-recognized-due-to-an-incompatible-datatype-assignment/2607/3)\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am currently analysing data coming from RNA sequence. My samples come from mice. I would like to use the goseq tool. The available genome is mm10. However the alignment was done with the previous version of the genome (GRCm38.p6). I don\u2019t think that I can use the mm10 version for the goseq. Do you know if I have any other possibility or I should use external tools for the GO analysis (profiler, DAVID etc) ?\nThank you!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\n@quote\nHowever the alignment was done with the previous version of the genome (GRCm38.p6).\nGRCm38 and mm10 are the same genomic assembly. But the \u201cbuild\u201d can differ between data providers (chromosome attribute). Then, the annotation used can also differ between sources (gene + transcript attributes).\nSee mouse assemblies here for what is used in Galaxy for GRCm38/mm10 \u2192 UCSC Genome Browser Downloads (@link https://hgdownload.soe.ucsc.edu/downloads.html)\nIf your data is based on a non-UCSC source for GRCm38/mm10 you might be able to convert the chromosome labels using the tool Replace column by values which are defined in a convert file. Common convert file mappings can be obtained through the link in the down in the help section.\nMapping between annotation sources is a bit trickier since those are usually not 1-1 pairings. But, you can try using annotateMyIDs, before deciding to use an annotation that is directly supported by GOseq instead (or, before bothering to create a custom GO mapping input, see next).\n@quote\nDo you know if I have any other possibility or I should use external tools for the GO analysis (profiler, DAVID etc) ?\nThe Galaxy wrapped GOseq tool allows for custom GO mappings. Examples of the file formats are down in the tool form help.\nThe other two tools are also wrapped in Galaxy.\nIf these tools are used in Galaxy or directly, both will involve making sure that the data inputs and the reference files are based on the same genomic assembly build and a common annotation source.\nTL;DR The idea is to make sure all data is from the same assembly (GRCm37, GRCm38) and the build identifiers \u201cmatch up\u201d as a technical consideration, then the scientific algorithms are applied. Computers are literal when processing important values.\n\n\nChr1 and chr1 and 1 all mean  \u201cChromosome 1\u201d to a person, but are considered different by tools.\nSame for feature annotations: GeneA and GeneA.2 and GeneA.20 are all different to tools but could all mean \u201cGene footprint A\u201d with no particular version to a person.\n\nResources\n\n3: RNA-seq genes to pathways (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-genes-to-pathways/tutorial.html)\nData Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "How to filter SNP in file (A)as region in defined range  in file (B) in usegalaxy?\nA:\nchr1 186 187 rs378896312\nchr1 703 704 rs136740087\nchr1 833 834 rs136710007\nchr1 864 865 rs43096727\nchr1 1610 1611 rs132965558\nchr1 1611 1612 rs134909289\nchr1 1628 1629 rs136933083\nB:\nchr1  185  190\nchr1  800  840\nchr1  1600 1620\nchr1  1625  1630\noutput:\nchr1 186 187 rs378896312\nchr1 833 834 rs136710007\nchr1 1610 1611 rs132965558\nchr1 1611 1612 rs134909289\nchr1 1628 1629 rs136933083"
    },
    {
      "role": "system",
      "text": "I hope I understand your question correctly. You want to have the SNPs in file A that are to 100% in the ranges listed in your file B, and thus generating your desired output?\nThe tool you are searching for is bedtools Intersect intervals. Make sure to tick the option: Write the original A entry once if any overlaps found in B. and under Required overlap that you give Minimum overlap required as a fraction of A the value 1.0.\nA detailed explaination of the tool is given in Galaxy and here (@link https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I\u2019m trying to RNA-Seq de novo assembly with using Trinity. Australia server supports this tool. But in EU server, it is not working. It\u2019s been waiting like this since last night."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthank you so much for the report. The problem is that currently, Trinity requires to allocate 512Gb of RAM in order to work, but there are not always computer nodes that can provide those resources. We are working in order to optimize memory usage. Sorry for the inconvenience.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am new to galaxy, so this question may sound simple\u2026\nI am following the 16S analysis tutorial (@link https://galaxyproject.github.io/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html), but instead of using the Silva V4 fasta file provided ( @link https://zenodo.org/record/800651/files/silva.v4.fasta), I want to use the latest release os SILVA (138), available from Mothur (@link https://mothur.org/wiki/silva_reference_files/).\nHowever, when using align.seqs, it shows an error when using SILVA 138, although it works with the V4 FASTA available from that tutorial.\nDoes anybody know what could be going wrong? Any tips to make align.seqs work using SILVA 138?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nHere is some help from the Mothur team regarding data extraction and formatting: @link https://mothur.org/blog/2020/SILVA-v138-reference-files/\nI haven\u2019t personally tried it but that is where to start troubleshooting the data you have now versus what the tool authors had success using."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone!\nThis may seem as a odd question, but we realized that our Rad-Seq sequencing was affected by the fact that the library preparation had poor results : they used 2 enzymes (pstI and mspI), but for any reason a lot of them ended sticking to each other (eg we had sequences looking like this: pst1\u2026mst1-mst1\u2026mst1), creating chimeras. Is there a tool somewhere on Galaxy that could allow us to separate those sequences, or ultimately that could reject a sequence based on the presence of a restriction site in it?\nThank you! :slight_smile:"
    },
    {
      "role": "user",
      "text": "Hi everyone :slight_smile:\nI think I cracked it, so I\u2019ll put my solution here in case anyone have a similar issue. So in the end I chose to reject my chimeric sequences altogether, as I saw no way to \u201cdigest\u201d them with any tools. So, in case you want to get rid of them to purify your data, here\u2019s how I worked :\n\nI converted my data from fastq to fasta via the Fastq.info tool. There are others available but this one allows you to produce a quality file as well as the Fasta, so if you want to keep all nucleotidic quality you may want to use this tool or an equivalent.\nI then used the tool Filter FASTA (on the headers and/or the sequences) twice, entering the sequence of my first then second digestion enzyme (CTGCAG for pstl and CCGG for mspI). Make sure to check the Output discarded FASTA entries as the discarded entries are the ones without any traces of the restriction site (that may results in chimeras), and thus the ones we want to work with.  So, you want to perform the second filtering on the data discarded from the first filtering to end with data exempt of any of these restriction sites.\nI finally converted my data back to Fastq with the tool Make.fastq. You can either use real a quality file or just let the tool add a 100% quality indication to each nucleotide, depending on if you might need it later or not.\n\nYou should then be able to proceed with your analysis as normal ! Be aware though that this method leaves you with uncompressed fastq files, which has not been a problem for me but may be worth noting.\nHope this helps someone one day!"
    }
  ],
  [
    {
      "role": "user",
      "text": "My variant calling jobs are giving errors. I am posting the error messages as snaps. Can someone please help me out with the problem?\nIMG_20210821_1552101920\u00d71440 247 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/7b85506c91a2e890c73e0d6399fdc829271f94aa.jpeg)\n\nIMG_20210821_1549321920\u00d72560 384 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/bc638846bb97b5ec97ccaa4062d8bd1d3f5dece6.jpeg)"
    },
    {
      "role": "system",
      "text": "Hello,\nThe tool is expecting a tag that is not present in the input BAM dataset. Specifically, the \u201csample\u201d attribute.\nThis information can be added when mapping (read group options) or adjusted after mapping with a tool like AddOrReplaceReadGroups add or replaces read group information.\nMore help:\n\nSearch the BAM format specification PDF with the keyword \u201csample\u201d to learn more about the attribute: @link https://samtools.github.io/hts-specs/SAMv1.pdf\n*GATK4 workflow documentation may also help: @link https://gatk.broadinstitute.org/hc/en-us\n\nThe tool form itself has additional usage/formatting help.\nTutorials: Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been trying to upload 59 fastq.gz files, each around 500 MB.  These are going into a new history, and I uploaded them from disk with  Type = \u201cfastq.gz\u201d and Genome = \u201cS. cerevisiae\u201d.  In the upload pop-up window, the files were successfully uploaded.  However, when I close the window and view this history, only 40 out of 59 are \u201cgreen\u201d, while 19 are grey and are flagged \u201cthis job is waiting to run\u201d.\nI\u2019ve waited 6 hours, and on occasion one more file gets processed.  I\u2019m not sure what is being processed to begin with, or why it is taking so long?  I\u2019m confused as to what is even being performed, as this was just an upload (which showed completed?) - not sure what is being processed here?\nI\u2019ve tried refreshing, and resuming the jobs.  I also tried deleting the history and starting over fresh - this didn\u2019t work.\nThanks for any help!!"
    },
    {
      "role": "system",
      "text": "Hi @user!\nQueued jobs should be left in the queued state. These will eventually finish.\nThis includes Upload jobs. Metadata checks and assignments are made during this process that moves loaded data into a history. This is usually quick, and sometimes not \u2013 much depends on the overall server load and a few other technical factors.\nStarting over or rerunning jobs will only result in those new jobs being added back at the end of the queue, extending the wait time.\nThe public server Galaxy Main @link https://usegalaxy.org has had more windows of extended wait times than usual over the last few days. We are actively monitoring the ongoing rates of job processing.\nFAQ: Datasets and how jobs execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nThanks for reporting the issue."
    }
  ],
  [
    {
      "role": "user",
      "text": "What are the Galaxy tools for the de novo assembled transcriptome annotation?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit not really clear for me what you are searching for. Do you already have your assembled transcriptome? Are you searching for transcriptome annotation software?\nPlease have a look at @link https://training.galaxyproject.org especially under assembly and transcriptomics.\nCiao,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "Greetings!  I am a long-time Circos user through the command line, and I\u2019d like to be able to utilize Circos within Galaxy.\nHowever, I have been unable to get Circos to work on @link http://usegalaxy.org, for any job regardless of how basic.  Notably, one of the test files I have been using to create a basic ideogram (hg18_karyotype_withbands.txt) is taken directly from the GTN tutorial for Circos, so this input file has already been vetted in terms of proper formatting.  I\u2019ve also been testing with another custom karyotype file (karyotype.human.hg38.txt) with the same results.\nThe \u201cview or report error\u201d button does not provide any additional information for these failed jobs, and doesn\u2019t allow me to submit a bug report for some reason.\nNotably, running the same exact jobs on @link http://usegalaxy.eu is successful!  I tested both on \u201cCircos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy8)\u201d (which is not available on @link http://usegalaxy.org) and Circos visualizes data in a circular layout (Galaxy Version 0.69.8+galaxy7) (which is the newest version found on @link http://usegalaxy.org) - and in both cases for both test files, it works as intended.\nLinks to both test histories:\n@link http://usegalaxy.org history link: Galaxy | Accessible History | Circos test history (@link https://usegalaxy.org/u/rerdmann/h/circos-test-history)\n@link http://usegalaxy.eu history link: Galaxy | Europe | Accessible History | Circos test history (Europe) (@link https://usegalaxy.eu/u/robert.m.erdmann/h/circos-test-history-europe)\n(It appears this note is in the same spirit as Problems with Circos (@link https://help.galaxyproject.org/t/problems-with-circos/4988), but in this case, I am using version 7\u2026)\nThanks for taking a look."
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for reporting the problem! We were internally tracking the need to update the tool suite but I just make a public tracking ticket (@link https://github.com/galaxyproject/usegalaxy-playbook/issues/346) for that as well. Until it closes or you see the updated version on the @link http://usegalaxy.org server, please run the tool at @link http://usegalaxy.eu instead."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.  We have galaxy installed following the \u201cGalaxy Installation with Ansible\u201d tutorial (Galaxy Installation with Ansible  (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html)). Everything is working fine until we try to change the database_connection string to use our central Postgres server instead of localhost.\nWhen we change this to:\ndatabase _connection: \u201cpostgresql://galaxy:password@pg1/galaxy\u201d\nThe page fails to load with a \u201c502 Bad Gateway.\u201d  If we revert it to the original, everything works.\nWe have tested the database_connection string from the command line using \u201cpsql -Atx postgresql://galaxy:password@pg1/galaxy\u201d\nAny help or suggestions would be greatly appreciated.\nMany thanks,\nDevin"
    },
    {
      "role": "user",
      "text": "I finally fixed this by updating pg_hba.conf on the remote server to give the galaxy user access to the postgres database in addition to the galaxy database.\nIs it sufficient for galaxy to have read-only access to this database as long as it has full privs over the galaxy database?  If not, any details on exactly what it needs to access and do from the postgres database would be greatly appreciated.\nMany thanks,\nDevin"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi would it be possible to add this genome?\nAlso if I want to align using Hisat2 and use a file added to my local history in the meantime, can I just use a fasta file or do I need to generate the .ht2 files?\nthanks"
    },
    {
      "role": "system",
      "text": "d\u2019Artagnan,\nIt would be possible to add that genome to hisat2\u2019s indexes, but in the meantime, you can get away with uploading the reference genome and using that as another input by selecting the Use a genome from history option. The tool will then generate its own .ht2 files for that analysis, so you wouldn\u2019t need to do anything in addition."
    }
  ],
  [
    {
      "role": "user",
      "text": "Can I use the gencode v19 annotation gtf file or any genecode version annotation gtf file for GRCh37 assembly as the associated gene model file for splice junctions for the built-in hg19 reference genome?"
    },
    {
      "role": "system",
      "text": "Hello @user\nYes, Gencode\u2019s human annotation for build GRCh37 is a match for UCSC\u2019s hg19 version of the assembly.\nFor the how-to, please see:\nQ&A (same formatting advice applies to hg19): RNA-STAR and hg38 GTF reference annotation (@link https://help.galaxyproject.org/t/rna-star-and-hg38-gtf-reference-annotation/749/2)\nFAQ: @link https://galaxyproject.org/support/diff-expression/\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nIn untrimmed fast QC it shows me errors :\nPer sequence GC content = !\nSequence Length Distribution = !\nSequence Duplication Levels = !\nSo how i can the improve the duplication level ??\nThis one is before trimming:\n\nimage1920\u00d71080 363 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a50fc244df17df9c1c45f965b7f6fcb44a387e2c.png)\nThis one is after trrimming:\n\nimage1920\u00d71080 368 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/33f5c1c86b48ce7c29fbab5ba899acd11af39a0a.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI suggest you to remove the first 10 nucleotides form the 5\u2019 end by using PRINSEQ (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/prinseq/prinseq/0.20.4+galaxy2).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI didn\u2019t see any topic about my problem.\nI tried to follow the steps to submit with planemo a new tool on my toolshed. Then, I wanted to remove the tool but with the admin interface nothing to do that. After some search, I removed the repository located in /database/community_files/000/repo_1 and\u2026 this is the drama.\nI understood that it\u2019s not just a simple directory but also mercurial managment and sqlite database\u2026\nAnd now impossible to restart the toolshed using the  using the galaxyToolshed_start.sh\nTraceback (most recent call last):\n  File \"lib/tool_shed/webapp/buildapp.py\", line 65, in app_factory\n    app = UniverseApplication(global_conf=global_conf, **kwargs)\n  File \"lib/tool_shed/webapp/app.py\", line 95, in __init__\n    self.repository_registry = tool_shed.repository_registry.Registry(self)\n  File \"lib/tool_shed/repository_registry.py\", line 18, in __init__\n    self.certified_level_one_clause_list = self.get_certified_level_one_clause_list()\n  File \"lib/tool_shed/repository_registry.py\", line 146, in get_certified_level_one_clause_list\n    certified_level_one_tuple = self.get_certified_level_one_tuple(repository)\n  File \"lib/tool_shed/repository_registry.py\", line 164, in get_certified_level_one_tuple\n    latest_installable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(self.app, repository)\n  File \"lib/tool_shed/util/metadata_util.py\", line 93, in get_latest_downloadable_changeset_revision\n    repository_tip = repository.tip()\n  File \"lib/tool_shed/webapp/model/__init__.py\", line 331, in tip\n    repo = self.hg_repo\n  File \"lib/tool_shed/webapp/model/__init__.py\", line 219, in hg_repo\n    WEAK_HG_REPO_CACHE[self] = hg.cachedlocalrepo(hg.repository(ui.ui(), self.repo_path().encode('utf-8')))\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/hg.py\", line 231, in repository\n    createopts=createopts,\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/hg.py\", line 193, in _peerorrepo\n    ui, path, create, intents=intents, createopts=createopts\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/localrepo.py\", line 3350, in instance\n    return makelocalrepository(ui, localpath, intents=intents)\n  File \"/storage/galaxydev/aubi/venv/lib64/python3.6/site-packages/mercurial/localrepo.py\", line 546, in makelocalrepository\n    raise error.RepoError(_(b'repository %s not found') % path)\nmercurial.error.RepoError: b'repository /storage/galaxydev/aubi/galaxy21.05/database/community_files/000/repo_1 not found'\n\nI tried to re-create the folder and I add a copy of another folder repo_2 to mimic the path but now, the toolshed started but nothing happen\u2026"
    },
    {
      "role": "user",
      "text": "Finally, I reset the sqlite database and associated files before completely restart a new toolshed"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy Team,\ncurrently there are multiple jobs in my waiting list to run in RNA Star and it is not going on for more than 24h. Is there a problem with the tool?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe jobs will run when resources become available. Public servers utilized common resources shared with everyone else working at the same place; please be patient.\nRegards!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nCan some one help or tell me how I can check what goes wrong with the communication server. In my galaxy.yml I have:\n  # Galaxy real time communication server settings\n  enable_communication_server: true\n\n  # Galaxy real time communication server settings\n  communication_server_host: 0.0.0.0\n\n  # Galaxy real time communication server settings\n  communication_server_port: 7070\n\nI enabled it in preferences > Change communication settings > Enable communication\nIf I go to 0.0.0.0:7070 I see a chat screen where I can type in and send messages.\nThe problem is that the chat icon does not appear. I am currently not using a proxy.\nEDIT:\nIf I change this to my actual ip adres the icon appears but the chat window is empty (a white square)\ncommunication_server_host: 0.0.0.0\nEDIT 2:\nIt turned out that even I only see a white window if my I hover over it with my mouse the cursor changes in the \u201ctyping\u201d cursor and if I type something and press enter I see my message appear at the 0.0.0.0:7070 adress. Is it a CSS problem?\nEDIT 3:\nI solved my problem, I still hope if some one can confirm if this is the proper way.\nI simply need to add opacity: 1 !important; to base.css (I did !important because for now this is a quick fix)"
    },
    {
      "role": "system",
      "text": "Thanks for updating the post with your fix \u2013 I\u2019ll make sure this makes its way into the codebase now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, after fixing a previous problem concerning the installation of cvmfs (ERROR while trying to make CERNVM-FS key directories (@link https://help.galaxyproject.org/t/error-while-trying-to-make-cernvm-fs-key-directories/8812)), I ran into the following problem:\n*\nfatal: [192.168.123.108]: FAILED! => changed=false\n  cmd:\n  - cvmfs_config\n  - chksetup\n  delta: '0:00:00.032705'\n  end: '2022-10-20 14:46:59.249437'\n  msg: non-zero return code\n  rc: 1\n  start: '2022-10-20 14:46:59.216732'\n  stderr: |-\n    Error: failed to load cvmfs library, tried: './libcvmfs_fuse3_stub.so' '/usr/lib/libcvmfs_fuse3_stub.so' '/usr/lib64/libcvmfs_fuse3_stub.so' './libcvmfs_fuse_stub.so' '/usr/lib/libcvmfs_fuse_stub.so' '/usr/lib64/libcvmfs_fuse_stub.so'\n    ./libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    /usr/lib/libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    /usr/lib64/libcvmfs_fuse3_stub.so: cannot open shared object file: No such file or directory\n    ./libcvmfs_fuse_stub.so: cannot open shared object file: No such file or directory\n    libcrypto.so.1.0.0: cannot open shared object file: No such file or directory\n    /usr/lib64/libcvmfs_fuse_stub.so: cannot open shared object file: No such file or directory\n\n    Failed to read CernVM-FS configuration\n  stderr_lines: <omitted>\n  stdout: ''\n  stdout_lines: <omitted>\n\nI found the same error on github (Crashes when running the role for the GAT \u00b7 Issue #30 \u00b7 galaxyproject/ansible-cvmfs \u00b7 GitHub (@link https://github.com/galaxyproject/ansible-cvmfs/issues/30)), where I found out that for some reason it\u2019s written in xenial.\nxxx@xxx:~/galaxy$ cat /etc/apt/sources.list.d/cernvm.list.list\ndeb [allow-insecure=true] https://cvmrepo.web.cern.ch/cvmrepo/apt/ xenial-prod main\nxxx@xxx:~/galaxy$ grep VERSION /etc/os-release\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.1 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\n\nUnfortunately, the fix that was used there doesn\u2019t work for me. This might have to do with the fact that I have Ubuntu 22.04 installed as my OS instead of 20.04. I was wondering how to update the cvmfs role to include the 22.04 package, if this is not already done. And if so, should this be done by editing the playbook (if so where?) or by manually installing the package?\nThanks in advance!\nEdit: I also found out I already have the libcvmfs_fuse_stub.so file in the /usr/lib directory but the specific path to this file (/usr/lib/libcvmfs_fus_stub.so) is not listed under the paths where it tries to load the cvmfs library from, which might actually be causing the problem. I have been looking for where to edit/specify this path, but can\u2019t find the file. Any help regarding this is appreciated!"
    },
    {
      "role": "system",
      "text": "Morning @user, you\u2019re on an LTS/Jammy so that\u2019s good. We added support to the role for that on September 8 which is quite recent (Add jammy to list of supported Ubuntu versions \u00b7 galaxyproject/ansible-cvmfs@6b9c746 \u00b7 GitHub (@link https://github.com/galaxyproject/ansible-cvmfs/commit/6b9c7468ed269636b49c104828081735b031b587)) and it looks like we haven\u2019t released a new version of the role since then! (Whoops!)\nBy the time you read this comment the ansible role for CVMFS at version 0.2.18 should be available, if you update, that might fix it :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have built a dataset collection from existing files in my history but I missed a couple of files and now want to add these to my dataset collection. I cannot find a way to do this without making the collection again which I want to avoid as there are ~100 files and that is a lot of clicking. Does anyone know how to do this?"
    },
    {
      "role": "system",
      "text": "Collections are supposed to be immutable unless very specific conditions occur.\nHowever you can use tools for mass operations to filter and select all datasets you are interested in, and then create a collection with just these inside. See the screenshot below for an example.\nGalaxy289\u00d7650 29.4 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/13655ca5b793ef3ed8f0b882e190b12d39f95cea.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "How to filter FreeBayes output (vcf) file to specific region (SNP) bed file in usegalaxy?\njust want to look at some of the positions (SNP) in bed file.\ntx"
    },
    {
      "role": "system",
      "text": "bedtools Intersect intervals should let you do this."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have used tool String tie to prepare reference assembly and got output data  as GTF file. For structural and functional annotation I need to convert it to FASTA format. suggest me with a tool which can be used to convert GTF to FASTA."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe transcript assembly that Stringtie generates is not a full assembly that leads to a fasta output.\nFor a fasta output, you\u2019ll need to assemble the reads with a tool like Trinity.\nThe GTN Tutorials cover the how-to. Examples below. The last one is a work-in-progress but should be useful even in the current state.\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#transcriptome-assembly\n\n\n\nGalaxy Training: De novo transcriptome reconstruction with RNA-Seq (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html#transcriptome-assembly)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html\n\n\n\nGalaxy Training: Reference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/full-de-novo/tutorial.html\n\n\n\nGalaxy Training: De novo transcriptome assembly, annotation, and different... (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/full-de-novo/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n\nMore help for Trinity is available from the tool authors. The methods are described for the command-line version of the tool but the overall protocol is the same as when running the wrapped tools in Galaxy.\n\n\n\nGitHub (@link https://github.com/trinityrnaseq/trinityrnaseq/wiki)\n\n\n\n@link https://github.com/trinityrnaseq/trinityrnaseq/wiki\nTrinity RNA-Seq de novo transcriptome assembly. Contribute to trinityrnaseq/trinityrnaseq development by creating an account on GitHub.\n\n\n\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nimage930\u00d7213 5.11 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b2407bc021b2b173465af924a921b3007480c2f1.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou need to upload the reference annotation in GTF format and select it from your history.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I downloaded a custom install of galaxy. I\u2019ve tried following the example for creating a custom tool, as shown here: Adding custom tools to Galaxy (@link https://galaxyproject.org/admin/tools/add-tool-tutorial/). I\u2019ve modified the config/galaxy.yml file to point to my tool_conf.xml file. However, when I startup galaxy, I don\u2019t see my custom tool. It appears to simply be the default. What am I doing wrong?\nI\u2019ve downloaded release 21.05."
    },
    {
      "role": "user",
      "text": "I figured it out. Appears that I had capitalization discrepancies between the directory and my config file."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\ni tried running Bismark on @link http://usegalaxy.eu but it didn\u2019t work (toolshed.g2.bx.psu.edu/repos/bgruening/bismark/bismark_bowtie2/0.22.1+galaxy4). it seems the all mapping worked fine but then the files couldn\u2019t be merged (see snapshot) .i am trying to aligned 2 paired end samples to TAIR10.\nany help or advice are welcome\nthanks a lot\n\nScreenshot 2021-04-20 at 22.47.421274\u00d7784 80.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c57bd8eb5ca1ca1199180b9dd2727f6756e531f3.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthanks for reporting the error. There is a problem with the tool, could you try it tomorrow again?\nRegards.\nUpdate: the problem was solved in this PR: Bismark: Fix subprocess command by gallardoalba \u00b7 Pull Request #1110 \u00b7 bgruening/galaxytools \u00b7 GitHub (@link https://github.com/bgruening/galaxytools/pull/1110)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nSo unfortunatelly our bioinfo guy is unavailable and I need to analyze my ChIP-seq data myself. I need some guidance for the analysis with multiple biological repeats and differential binding between the treatments.\nI am checking histone marks distribution in the rat tissue at different age. I have 10 biological repeats/  per timepoint, and 4 different timepoints. What I would like to get is a list of genes marked by given histone mark at each timepoint (relatively easy), but then also observe the change of the binding of histone marks at different timepoints, so do some differential analysis.\nWhat should I do? Merge the BAM files at this stage (quite problematic, since the files are big and using quite a lot of space) and call the peaks on the merged file? How to compare then the difference in binding between the timepoints?\nShould I rather call the peaks separately for each biological repeat and then merge the BED files together at the level of MACS2? Then once again, how to compare the difference between the timepoints.\nIs there any step by step tutorial? I have seen there are some tools like diffbind or multiGPS, I am reading the papers describing them, but honestly I am quite lost (or stupid) and need some easier description of the process."
    },
    {
      "role": "system",
      "text": "Hello @user\nHopefully you have found the tutorials already, but if not please find them here. The site can be searched with keywords like datatypes and tool names, or navigated by analysis domain.\nMany tools will also have a help section with examples plus link-outs for citations/publications and author resources.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have 3 vcf files that I want to merge but one of them has \u201cchr\u201d in the chromosome identifiers and the other two don\u2019t. Is there a way to edit the identifiers so they match?"
    },
    {
      "role": "system",
      "text": "Hi @user\ntry text replacement tools. Maybe separate header and the data. For data replace values in 1st column. Concatenate header and data after replacement.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nphoto_2021-04-21_21-45-31576\u00d71280 17.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/2bb10348e122d0b4320bd6b9a8b70c1c7ea10164.jpeg)\nWhich steps should I follow to answer this question?"
    },
    {
      "role": "system",
      "text": "Hi @user, this @link https://usegalaxy.org/u/gigi-zhu/h/unnamed-history-1 provides the response."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m running RNA-STAR on 2 datasets which would normally take only <1hr to run, but now it has taken 2 days and the status is still shown in \u2018orange\u2019/running mode. The track doesn\u2019t turn red/failure either. Please let me know what\u2019s going on with these 2 jobs. Both jobs were started on Aug19, 2022.\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nIf the job is still executing, then it is usually best to let it complete. If it is too large to process or there is some input problem, the result will change to a red dataset with error logs you can troubleshoot (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#analysis).\nThe runtime for jobs is limited at most public servers, and no job will execute more than about 48-72 hours at @link http://UseGalaxy.org. Jobs may queue before that, it depends on how busy the server is.\nIf instead you already know there is some problem, and you want to run the job with different inputs/parameters, it is fine to delete + purge the current job and start up a new one with the changes. The \u201cdon\u2019t delete and rerun\u201d advice is for when there won\u2019t be any changes.\nFAQ @link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019m using galaxy for my Genomics course. We are currently working on our annotation unit. Every time I try to run gffread the job looks like it will start, but then never actually updates in my history. I do not get an error message at all. I\u2019ve been able to run other tools without any issues."
    },
    {
      "role": "system",
      "text": "You need to select one of the options that produces an output, e.g. the options under Feature File Output, or the options available if you specify a reference genome."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone\nI am working in Galaxy with miRNAs data.\nI am trying to pass my reads mapped with Bowtie to counts with the featureCounts app.\nI have not been successful, the process gives me an error because the miRBase annotation file is in gff3 format and featureCounts asks for this file in GTF file.\nHow can I solve this step?\nP.S. I have used the miRBase sequences to map my reads."
    },
    {
      "role": "system",
      "text": "Hi @user,\nI suggest you use the miRDeep2 tool for quantifying miRNAs data; it is built on Bowtie and specifically designed for working on miRNA. You can find some information about how to quantify miRNAs in this tutorial (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/mirna-target-finder/tutorial.html#mirna-data-analysis).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nIs it possible to use galaxy GOseq if I am dealing with RNA-seq analysis for bacterial specie, which is not included in the list of the Gene categories?\nI uploaded its gtf.file > Use a categories file from history, but I got an error after 3 minutes!!!\nHas anyone faced the same issue,? thanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nGOseq will not understand how to use a GTF annotation. Plus, that file type is usually doesn\u2019t contain ontology information. But if yours does, that data can be parsed into the format the tool is expecting.\nSee the help section on the tool form to learn the content/format of accepted custom inputs. The first column should match the Gene ID used in the other inputs. The second column are the GO/KEGG terms.\n\nGene categories file\nThis tool can get GO and KEGG categories for some genomes. The three GO categories are GO:MF (@link ) (Molecular Function - molecular activities of gene products), GO:CC (@link ) (Cellular Component - where gene products are active), GO:BP (@link ) (Biological Process - pathways and larger processes made up of the activities of multiple gene products). If your genome is not available, you will also need a file describing the membership of genes in categories. The category file should have two columns with an optional header row. with Gene ID in the first column and category identifier in the second column. As the mapping between categories and genes is usually many-to-many, this table will usually have multiple rows with the same Gene ID and category identifier.\nExample:\nENSG00000162526   GO:0000003\nENSG00000198648   GO:0000278\nENSG00000112312   GO:0000278\nENSG00000174442   GO:0000278\nENSG00000108953   GO:0000278\n\nHelp for data manipulations: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\nHope that helps :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "how can I run FASTAQE and how to interpret ?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI recommend you to have a look at the Quality control training:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html#assess-quality-with-fastqe----short-reads-only)\n\n\nGalaxy Training: Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html#assess-quality-with-fastqe----short-reads-only)\nAnalyses of sequences\n\n\n\n\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I\u2019m a newbie here\nI\u2019m currently working with fastq secuences from GEO and I\u2019m trying to obtain counts in Galaxy.\nIn the description of the database i\u2019m using, the researchers say \u201cReads filtering under criteria removing reads with 20% of the base quality lower than 13\u201d.\nMy question is,\nThis configuration with Trimmomatic will lead me to do what they did? or how I could do it?\nimagen_2023-06-27_151601989787\u00d7557 40.3 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d67bcc3479f0200c57a88892abce710341d5f52e.png)\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nyou are after Filter by quality (in FASTQ/FASTQ section).\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019m new to galaxy and I wanted to know why this happens:\nI made a collection of 3 items and used different tools on it. The tools worked and the output of each of the items loaded correctly and appered in green within the collection. However, the collection containing the 3 items appears to still be loading and grey in the history. How should I interpret this?"
    },
    {
      "role": "system",
      "text": "Hi @user,\ndid your collection turn green finally?\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have been trying to get a jupyter notebook running with Interactive Jupyter Notebook (Galaxy Version 0.3) to no avail. The job gets put onto my history, but never executes, and I can not find anything under my \u2018Active Interactive Tools\u2019.\nThanks in advance for any input on this matter.\n\nKazam_screenshot_000171848\u00d71053 90.4 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c530381c16684be32f5f8b998296300db19a4cae.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nAll of the interactive environments at @link http://UseGalaxy.org are problematic right now. We are working on it.\nFor now, please use these tools at @link http://UseGalaxy.eu or @link http://UseGalaxy.org.au."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI would like to know whether @link http://getgalaxy.org can be used for commercial purposes? Currently, it is listed as academic free license V3.0\nIt will be great if get the mail ID of galaxy team."
    },
    {
      "role": "system",
      "text": "Hello @user\nGalaxy is open-source. You do not need our permission to use it :slight_smile: If you would like to make more direct contact with a specific Galaxy community or project area, please see the resources here: @link https://galaxyproject.org/\nThanks!\n\n\n@link https://github.com/galaxyproject/galaxy/blob/dev/LICENSE.txt\n\n\n@link https://github.com/galaxyproject/galaxy/blob/dev/LICENSE.txt\nCopyright (c) 2005-2016 Galaxy Contributors (see CONTRIBUTORS.md)\n\nLicensed under the Academic Free License version 3.0\n\n 1) Grant of Copyright License. Licensor grants You a worldwide, royalty-free, \n    non-exclusive, sublicensable license, for the duration of the copyright, to \n    do the following:\n\n    a) to reproduce the Original Work in copies, either alone or as part of a \n       collective work;\n\n    b) to translate, adapt, alter, transform, modify, or arrange the Original \n       Work, thereby creating derivative works (\"Derivative Works\") based upon \n       the Original Work;\n\n    c) to distribute or communicate copies of the Original Work and Derivative \n       Works to the public, under any license of your choice that does not \n       contradict the terms and conditions, including Licensor's reserved \n       rights and remedies, in this Academic Free License;\n\n\n\n  This file has been truncated. show original (@link https://github.com/galaxyproject/galaxy/blob/dev/LICENSE.txt)\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m new to using galaxy and RNAseq analysis in general. I wanted to perform reference based RNA-seq analysis on fastq raw read files for Pseudomonas aeruginosa, PA14. I was going through a tutorial to this and reached the mapping step using HISAT2. However, the reference genome file is unavailable in galaxy nor the UCSC bacterial genome database. Is there any way I could obtain the annotated genome fasta for the bacterium in question. Sorry if its a dumb question but I\u2019m fish out of water in these areas. Thank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nNCBI has choices: Pseudomonas aeruginosa PA14 - NCBI - NLM (@link https://www.ncbi.nlm.nih.gov/data-hub/taxonomy/652611/)\nLoad just the genome fasta and the annotation GTF/GFF3 to Galaxy.\nThese FAQs can help with common reformatting changes that are needed. If not formatted properly, tools will fail or produce odd results.\n@quote\nYou can also review these FAQs for help addressing this and related problems that often come up together. These problems can present whether you are working in Galaxy or not.#working-with-gff-gft-gtf2-gff3-reference-annotation#working-with-very-large-fasta-datasets"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am tried to find ROSE to do super-enhancer analysis.\nAlthough this tool is very popular in this field, I cannot find this program in Galaxy.\nI wonder if it is possible to see this tool in galaxy soon.\nIt would be fantastic to do all my analysis on galaxy."
    },
    {
      "role": "system",
      "text": "Hi @user,\nI opened an @link https://github.com/galaxyproject/tools-iuc/issues/4476 about it in the IUC repository.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Run kallisto with .fasq and CDS.fa\nand I failed with this fatal message\nFatal error: Exit code 255 ()\nFATAL: container creation failed: mount /cvmfs/main.galaxyproject.org/galaxy->/cvmfs/main.galaxyproject.org/galaxy error: while mounting /cvmfs/main.galaxyproject.org/galaxy: while getting stat for /cvmfs/main.galaxyproject.org/galaxy: stat /cvmfs/main\n/corral4/main/jobs/045/461/45461829/galaxy_45461829.sh: line 129: python: command not found\nHow can i solve this?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthis error is caused by a temporal problem in the server; I suggest you to run your jobs in @link http://usegalaxy.eu until the problems have been fixed.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Galaxy Community Galaxy, we mapped sequence reads from our RNA-Seq experiment to the S. Pome genome (see for your reference  Galaxy | Accessible History | Gavin project 2 (@link https://usegalaxy.org/u/gavinwang/h/gavin-project-2)). We are trying to calculate splicing efficiency for all introns in S. pombe. I am seeking community help if anyone can guide me on how to obtain sequence reads for each intron and its flanking exons. Thank you in advance for the input and help !"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe follow any published protocol/workflow. If you are after read counts in exons, maybe look at DEXSeq tools or use tools like featureCounts without aggregation of counts. To get sequences have a look at interval tools, such as bedtools. You also can get positions of introns using bedtools, if you have 12 columns BED files.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nis there a way to open and visualize a pdb model directly in Galaxy?\nAs far as I know the only way is to build a custom chart?\nMany thanks."
    },
    {
      "role": "system",
      "text": "At least NGL (@link https://github.com/galaxyproject/galaxy/pull/3601) and @link https://biasmv.github.io/pv/ are available in Galaxy already."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Amazing eu team,\nAnswering to a direct e-mail concerning error a user has using Wallace Shiny app, I discover that he is using a local version because the IT wallace tool is not reachable on eu instance.\nAs we don\u2019t work on this IT since long time, maybe we need to make something to update it or other ? Please dont hesitate to say to me if there is issues with existing Wallace IT and / or if we have something to do on it.\nWhishing you a good week,\nBest,\nYvan"
    },
    {
      "role": "user",
      "text": "It is now sloved! \u2026 But don\u2019t know how to close the issue :wink:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there any path in galaxy to convert Tab file to Bam ?"
    },
    {
      "role": "system",
      "text": "Hi @user\nCould you explain more about what you are doing?  Share a few sample lines?\nThe tabular datatype is very general.\nOne example: If the tabular dataset contained identifiers and sequence reads:\n\nconvert tabular > fasta > fastq. actual quality scores are optional \u2013 default scores can be added in.\nconvert fastq to an unmapped bam/sam\n\n\nNot very useful and an extra step \u2026 since most tools accept reads in fastq format as an input, even if they also accept reads in an unmapped bam/sam."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello !\nI am facing a problem with stringTie\nI mapped my sequence with  HISAT2 after this I used stringTie but I\u2019m not getting results, what should I have to do\n\nimage1173\u00d7262 2.01 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b79e5c6febdd823ebda9eb1e0754002e2e524ffe.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nErrors with this tool are often due to problems with the tool settings versus the reference annotation provided at run, so check that first.\nThis is an FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_working_with_reference_annotation.html) and you can find more QA at this forum about the topic.\nThis is where to find tutorials:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nIf you cannot find the problem, then more information is needed to help. See this FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html) for how and where to investigate dataset errors plus how to include context when reporting a problem."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI attempted to download my complete history, one file being approximately 200GB and the other around 20GB. Both generated a link and the sizes were correct. However, after downloading, neither file could be extracted successfully.\nI received the following error message for both files: \u201cData error - This file is broken.\u201d\nHas anyone encountered a similar issue and found a solution? Any assistance would be greatly appreciated.\nBest,\nMin"
    },
    {
      "role": "system",
      "text": "Hi @user\nhistory export was discussed several times, for example:@quote\nHello, \nI ran Kallisto and Dsse2 to get my list of DE genes. In the results of Dseq2, I can see there are about 20000 lines - I am assuming 1 line for each gene. However, when I download the file, I only get 9000 lines. Can you please let me know why this is happening? And how can I download the entire dataset of 20000 lines?\n\nMaybe search the forum with \u2018Download history\u2019 or something similar.\nKeep in mind that some file systems support relatively small files, such as 4 Gb. Make sure your local infrastructure supports storage of large files. Your internet provider might have download limits.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I received the following error message while running edgeR, please address this issue, thanks!\nWarning message:\nIn Sys.setlocale(\u201cLC_MESSAGES\u201d, \u201cen_US.UTF-8\u201d) :\nOS reports request to set locale to \u201cen_US.UTF-8\u201d cannot be honored\nError in glmFit.default(sely, design, offset = seloffset, dispersion = 0.05,  :\nnrow(design) disagrees with ncol(y)\nCalls: estimateDisp \u2026 estimateDisp \u2192 estimateDisp.default \u2192 glmFit \u2192 glmFit.default"
    },
    {
      "role": "system",
      "text": "Hi @user\nI replied to your bug report already but for others reading:\nThis type of error message from any Bioconductor or R tool means that there is a labeling problem or design problem. The details usually have some clues \u2013 this message is stating that the tool ran into a mismatch between the rows and columns of the inputs.\n@quote\nnrow(design) disagrees with ncol(y)\nClick into the :information_source: icon for a dataset to see what was used to produce that data (error or success). This is an easier view to review tool inputs and parameters in a summary format. Often, this alone reveals what the job problems were, along with the content of the logs on that same view (stderr, stdout). And, this is the same information to share when asking a question here.\nFor your data, it looks like there is an extra column in the count matrix that is not associated with a sample name in the factor matrix. The tool Cut can be used to remove that column.\nExamples of these data are on the tool form, plus at the resources below. You might also need to:\n\nadjust headers in the factor matrix\nremove Geneid rows from the count matrix if all sample counts are 0\nfind out why the \u201cdatabase\u201d metadata is set to hg38Patch11 instead of just hg38 for both of these inputs. Only hg38 is indexed at the usegalaxy.* servers. Avoid assigning native metadata keys unless you are certain they apply. Mixups can lead to content problems that are very hard to detect.\n\nFAQs\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#analysis\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#mismatched-chromosome-identifiers-and-how-to-avoid-them\n\nGalaxy Tutorials\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nRelated Q&A at this Galaxy Forum\n\nSearch results for 'bioconductor' - Galaxy Community Help (@link https://help.galaxyproject.org/search?q=bioconductor)\nTopics tagged edger (@link https://help.galaxyproject.org/tag/edger)\n\nOther Forums \u2013 many existing public solutions will also apply to the Galaxy wrapped versions of tools. Searching with error codes is usually enough. If you can\u2019t find a solution, or need help translating a command-line function to a Galaxy tool form\u2019s options, please come back and ask your question here.\n\n@link https://www.biostars.org/\n@link https://support.bioconductor.org/\n\nThanks! :rocket:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI have been running a trinity assembly on a dataset for 4 days now. Its still waiting to run or running (no idea)? When I come in each morning there is a message that mentions somthing about the connection interruption and if it persists to to contact admin, but my assembly seems to still be waiting to run or running?"
    },
    {
      "role": "system",
      "text": "Hi @user\nPop-up message that state something about a \u201cconnection interruption\u201d mean that the server is busy with respect to the browser session (server had temporary trouble posting the webpage over your connection). Reloading the web page usually gets rid of those. It is unrelated to queued jobs.\nFor longer queued jobs (grey in color), that is expected right now. All of the usegalaxy.* servers are very busy. If the job is already executing it will be yellow in color. Allow both states to process or you will extend the wait time: lose your place in the queue, or abort an executing job and need to rerun it.\nThis assumes that you have already checked the upstream jobs/datasets that were input to the Trinity job. If you haven\u2019t yet, examine the inputs (expand the datasets) to make sure all are green and without any warnings. If you do find a problem, you\u2019ll need to correct it, then rerun Trinity with the fixed inputs."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI forgot my password for my Galaxy account and I also no longer have access to that old E-mail. Is there a way that I can transfer account to new E-mail?"
    },
    {
      "role": "system",
      "text": "Hello. If you forgot your password and you do not have access to the email address you used we have no means to verify the account is yours. Hence we cannot give you access to it. Sorry."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nHow could i filter normalized read counts which for example exclude the genes with read counts under 20.?\nBest"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can use  Filter data on any column using simple (@link https://usegalaxy.eu/root?tool_id=Filter1) for that.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI want to use Bowtie not Bowtie2. So i saw a tool named(Map with bowtie with illumina). is that the same bowtie ?"
    },
    {
      "role": "system",
      "text": "Yes, the tool Map with Bowtie for Illumina is a wrapper for bowtie 1, not 2.\nBowtie2, the one you don\u2019t want, is Bowtie2 - map reads against reference genome."
    }
  ],
  [
    {
      "role": "user",
      "text": "hi\ni was working till yesterday(29/9/21).now its showing this error\n\nThis page isn\u2019t working\n@link http://usegalaxy.org is currently unable to handle this request.\nHTTP ERROR 500"
    },
    {
      "role": "system",
      "text": "Hi @user\nYou probably noticed already, but the server is up.\nIt was out for a short time several hours ago now. Details: @link https://status.galaxyproject.org/\nThanks for reporting the problem and our apologies for the confusion!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I\u2019m kind of a new user of galaxy and I am trying to run trimmomatic to do some RNA-seq analysis. after trimmomatic, I run fastqc to check the quality of sequences but it seems adaptors are not cut. what am I doing wrong? could you please help?\nhere is the link of files after trimmomatic being run:\nhttps:/usegalaxy.org/api/datasets/f9cad7b01a472135fefff3665ea301f7/display?to_ext=fastqsanger\nlink of the following fastq:\nhttps:/usegalaxy.org/api/datasets/f9cad7b01a472135597b0ae93de397c9/display?to_ext=html"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you share your history with me? Sorry for the late response.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have audited the quality of the FASTA file in the FASTQC program on GALAXY server. The program performed the analysis, but unfortunately I can not read the graphs from the analysis. In the upper part of the page appeared a message, which I am sending below. Could you tell me, what is the reason for not displaying graphs and what can you do to display them? I opened GALAXY in the latest version of the Mozilla browser.\nThank you.\nfq_gz.gz FastQC Report @media screen { div.summary { width: 18em; position:fixed; top: 3em; margin:1em 0 0 1em; } div.main { display:block; position:absolute; overflow:auto; height:auto; width:auto; top:4.5em; bottom:2.3em; left:18em; right:0; border-left: 1px solid #CCC; padding:0 0 0 1em; background-color: white; z-index:1; } div.header { background-color: #EEE; border:0; margin:0; padding: 0.5em; font-size: 200%; font-weight: bold; position:fixed; width:100%; top:0; left:0; z-index:2; } div.footer { background-color: #EEE; border:0; margin:0; padding:0.5em; height: 1.3em; overflow:hidden; font-size: 100%; font-weight: bold; position:fixed; bottom:0; width:100%; z-index:2; } img.indented { margin-left: 3em; } } @media print { img { max-width:100% !important; page-break-inside: avoid; } h2, h3 { page-break-after: avoid; } div.header { background-color: #FFF; } } body { font-family: sans-serif; color: #000; background-color: #FFF; border: 0; margin: 0; padding: 0; } div.header { border:0; margin:0; padding: 0.5em; font-size: 200%; font-weight: bold; width:100%; } #header_title { display:inline-block; float:left; clear:left; } #header_filename { display:inline-block; float:right; clear:right; font-size: 50%; margin-right:2em; text-align: right; } div.header h3 { font-size: 50%; margin-bottom: 0; } div.summary ul { padding-left:0; list-style-type:none; } div.summary ul li img { margin-bottom:-0.5em; margin-top:0.5em; } div.main { background-color: white; } div.module { padding-bottom:1.5em; padding-top:1.5em; } div.footer { background-color: #EEE; border:0; margin:0; padding: 0.5em; font-size: 100%; font-weight: bold; width:100%; } a { color: #000080; } a:hover { color: #800000; } h2 { color: #800000; padding-bottom: 0; margin-bottom: 0; clear:left; } table { margin-left: 3em; text-align: center; } th { text-align: center; background-color: #000080; color: #FFF; padding: 0.4em; } td { font-family: monospace; text-align: left; background-color: #EEE; color: #000; padding: 0.4em; } img { padding-top: 0; margin-top: 0; border-top: 0; } p { padding-top: 0; margin-top: 0; }\nError! Filename not specified. FastQC Report"
    },
    {
      "role": "system",
      "text": "Hello,\nThis looks like a FastQC web page (HTML) output, is that correct? And you clicked on the \u201ceye\u201d icon to visualize it?\nTools with HTML display need to be checked off here (through the Galaxy web interface): Admin > Tool Management > Manage whitelist\nOnly administrators can do this.\nIf this is your own Galaxy, you can adjust the setting. The server may need to be restarted to take effect (cannot recall if needed or not but you\u2019ll be able to notice).\nIf working at the @link http://usegalaxy.org, @link http://usegalaxy.eu, or @link http://usegalaxy.org.au server (check the URL), then write back about which one. All three have admins that can be reached here to resolve the display problems. A new updated version of the tool may have been added and this step was missed.\nIf working somewhere else, contact the admins of that server and have them adjust it. Contact information is usually on the home page for public Galaxy servers and sometimes here: @link https://galaxyproject.org/use/. If at your place of work/institution, ask internally about contact info. You can share a link to this post so they\u2019ll know what to do, if don\u2019t already for some reason.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "The links to the scanpy documentation here (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_filter/scanpy_filter/1.7.1+galaxy0) on @link http://usegalaxy.eu server need to be updated to https://scanpy.readthedocs.io/en/stable/ - the current links show a 404\np.s. is this the correct way to report this or can I create a PR someplace?"
    },
    {
      "role": "system",
      "text": "Ofcourse there is no harm in reporting it here, but I think most developers are more active on the corresponding github pages. See:\n@link https://toolshed.g2.bx.psu.edu/repository?repository_id=3d319fa861b2fa1c\n\n\n@link https://github.com/galaxyproject/tools-iuc/tree/main/tools/scanpy\n\n\ntools-iuc/tools/scanpy at main \u00b7 galaxyproject/tools-iuc (@link https://github.com/galaxyproject/tools-iuc/tree/main/tools/scanpy)\n@link https://github.com/galaxyproject/tools-iuc/tree/main/tools/scanpy\nTool Shed repositories maintained by the Intergalactic Utilities Commission\n\n\n\n\n\nTo quickly find the github repository you can click the litle arrow on the top right of the tool.\n\nimage1309\u00d7197 9.69 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/68b75a023a5fb842d87ff61461e2fee03b0b4251.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have created a local galaxy server (version 22.05) by following the GetGalaxy tutorial (Get Galaxy - Galaxy Community Hub (@link https://galaxyproject.org/admin/get-galaxy/)). I would now like to automate the server by making a ansible playbook. I, however, have no idea were to start in regards to the galaxy.yml and galaxyservers.yml files.\nI have already created an ansible.cfg and a hosts file and added them to the server, but am quite stuck on what to specifically put in the galaxy.yml and galaxyserver.yml files so it wouldn\u2019t clash with already existing files with similar names or contents and would still do what it is supposed to do.\nEdit: I also saw that the GetGalaxy server uses SQLite instead of PostgreSQL and was wondering if there were other important differences between the GetGalaxy and Ansible version.\nIf anyone would have some experience or knowledge about this topic, any help would be greatly appreciated!\nThanks in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\nResources, these are our primary admin hubs with linkouts to and examples for topic-specific resources\n\nadmin documentation\n\n@link https://docs.galaxyproject.org/\n\nadmin tutorials\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\n\nGalaxy Training: Galaxy Server administration (@link https://training.galaxyproject.org/training-material/topics/admin/)\nResources related to configuration and maintenance of Gal...\n\n\n\n\n\n\nrelated prior Q&A at this forum\n\n@link https://help.galaxyproject.org/search?q=ansible\n\n@quote\nI have created a local galaxy server (version 22.05) by following the GetGalaxy tutorial (Get Galaxy - Galaxy Community Hub)\nThis guide is for a very simple install. There are other basic deployment types with more pre-configured. One example is the Docker Ansible Galaxy (@link https://galaxy.ansible.com/community/docker)\n\nGalaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use/#which-platform--platform-type-to-choose)\n\n@quote\nI have already created an ansible.cfg and a hosts file and added them to the server, but am quite stuck on what to specifically put in the galaxy.yml and galaxyserver.yml files so it wouldn\u2019t clash with already existing files with similar names or contents and would still do what it is supposed to do.\nDefinitely backup/clone what you have before making changes, unless you decide to start over and use a different deployment baseline.\n@quote\nEdit: I also saw that the GetGalaxy server uses SQLite instead of PostgreSQL and was wondering if there were other important differences between the GetGalaxy and Ansible version.\nThe database should be upgraded to Postgres for any use-case that isn\u2019t temporary. Why is explained the admin docs in the \"Administration` section, along with other strongly recommended basic configuration updates. You\u2019ll notice that the most useful advance configs are already in place (and can be custom modified) in containerized and cloud deployment choices.\nThis specific Q&A isn\u2019t summarized in a topic at this forum yet. So, let\u2019s leave this topic as a single reference specific for the \u201cansible\u201d overview of primary Galaxy options/resources. If you have another question, please start up a new topic. You can reference this topic by URL (or any other prior Q&A) for context.\nHope that helps :rocket:"
    }
  ],
  [
    {
      "role": "user",
      "text": "All of the Emboss tools are giving this error (replace tool name on the link I used extractseq as an example)\nLoading tool toolshed.g2.bx.psu.edu%2Frepos%2Fdevteam%2Femboss_5%2FEMBOSS%3A+extractseq35%2F5.0.0 failed: Could not find tool with id \u2018toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS:+extractseq35/5.0.0\u2019.\nThanks for your help"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthank you for the report; a PR (@link https://github.com/galaxyproject/usegalaxy-tools/pull/470) has been opened in order to update them.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I got this after running cutadapt to trim adapters, but the report says 0 bp were filtered. Why could this be happening?\nTotal read pairs processed:        309,147,525\n  Read 1 with adapter:                 573,790 (0.2%)\n  Read 2 with adapter:             302,963,879 (98.0%)\n\n== Read fate breakdown ==\nPairs that were too short:             708,546 (0.2%)\nPairs that were too long:          308,438,979 (99.8%)\nPairs written (passing filters):             0 (0.0%)\n\nTotal basepairs processed: 57,184,734,130 bp\n  Read 1: 10,812,785,560 bp\n  Read 2: 46,371,948,570 bp\nTotal written (filtered):              0 bp (0.0%)\n  Read 1:             0 bp\n  Read 2:             0 bp\n"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI think that the report indicates that no reads have passed the filter (all of them have been filtered).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I want to change the bed style of my genes from flybase to UCSC format in galaxy using pygenometracks. Even though I try to set it to UCSC bed style the output is always flybase. I would appreciate any help. Thanks."
    },
    {
      "role": "system",
      "text": "Hi @user\nThis could be an input content problem, a tool option problem, or a previously undetected tool bug.\nFirst, make sure you are using the most current version of the tool. Right now that is: pyGenomeTracks plot genomic data tracks (Galaxy Version 3.5)\n. If using a different version, try this one and see if that resolves the issue.\nIf the problem persists, a few more details would help. Where are you running the tool? A public Galaxy server? Which URL? Or explain if somewhere else \u2013 and maybe try at a public Galaxy server to see what happens as a comparison \u2013 if it works at a usegalaxy.* server but not your own server, that narrows down what may be going wrong.\nLet\u2019s follow up from there."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI\u2019m creating different tools with the goal of use them inside a workflow.\nI would like to know if using a list with some ids it would be possible to have a loop in the workflow that runs a tool over each id (I need their output to be separated and not inside a single table).\nThank you for the help."
    },
    {
      "role": "system",
      "text": "Hi @user,\nIt sounds like you might be able to achieve that with collections. Have a look at this tutorial: Using dataset collections (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)\nConcretely, you could split your list of IDs to a collection using this tool (@link https://usegalaxy.eu/?tool_id=split_file_to_collection). If you need the IDs as tool parameters, rather than datasets, you could use the \u2018Parse parameter value from dataset\u2019 tool (only accessible in the workflow editor).\nHope this helps, feel free to respond with any further questions.\nBest wishes,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Trying to do RNA seq data analysis denovo based but trinity giving error.\nPlease help.\nGalaxy Tool Error Report\nfrom @link https://usegalaxy.org/\nError Localization\nDataset\t79869624 (bbd44e69cb8906b5cf64175ab8c7a2f7)\nHistory\t5619023 (07bef24c7a90ef05)\nFailed Job\t121: Trinity on data 56 and data 55: Gene to transcripts map (bbd44e69cb8906b57bc5840c89cd6fe5)\nUser Provided Information\nThe user (admin redacted for privacy) provided the following information:\nNone\nDetailed Job Information\nJob environment and execution information is available at the job info page.\nJob ID\t42383847 (bbd44e69cb8906b58ccb29cce2790c64)\nTool ID\ttoolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.9.1+galaxy2\nTool Version\t2.9.1+galaxy2\nJob PID or DRM id\t42383847\nJob Tool Version\t\nJob Execution and Failure Information\nCommand Line\nif [ -z \u201c$GALAXY_MEMORY_MB\u201d ] ; then GALAXY_MEMORY_GB=1 ; else GALAXY_MEMORY_GB=$((GALAXY_MEMORY_MB / 1024)) ; fi ;  if [ ! -z \u201c$TRINITY_SCRATCH_DIR\u201d ] ; then workdir=pwd ; scratchfolder=$(mktemp -d -p \u201c$TRINITY_SCRATCH_DIR\u201d); emptyfolder=$(mktemp -d -p \u201c$TRINITY_SCRATCH_DIR\u201d); cd \u201c$scratchfolder\u201d ; fi ;  ln -s \u2018/scratch1/03166/xcgalaxy/main/staging//42383847/inputs/dataset_6d325f88-9577-4499-9b5a-bc6044005616.dat\u2019 left_input0.fastqsanger.gz && ln -s \u2018/scratch1/03166/xcgalaxy/main/staging//42383847/inputs/dataset_641bd050-dc3a-4114-8811-acf13a33f5de.dat\u2019 right_input0.fastqsanger.gz &&  Trinity --no_version_check  --left \u2018left_input0.fastqsanger.gz\u2019  --right \u2018right_input0.fastqsanger.gz\u2019  --seqType fq       --min_contig_length 200  --min_kmer_cov 1  --CPU ${GALAXY_SLOTS:-4} --max_memory ${GALAXY_MEMORY_GB:-1}G --bflyHeapSpaceMax ${GALAXY_MEMORY_GB:-1}G --bfly_opts \u2018-V 10 --stderr\u2019   &&  if [ ! -z \u201c$TRINITY_SCRATCH_DIR\u201d ] ; then mkdir -p \u201c$workdir/trinity_out_dir\u201d; cp -p trinity_out_dir/Trinity* \u201c$workdir/trinity_out_dir\u201d; cd \u201c$TRINITY_SCRATCH_DIR\u201d; rsync -a --delete \u201c$emptyfolder/\u201d \u201c$scratchfolder/\u201d; rmdir \u201c$emptyfolder\u201d \u201c$scratchfolder/\u201d; cd \u201c$workdir\u201d; fi ;\nstderr\nstdout\n ______  ____   ____  ____   ____  ______  __ __\n|      ||    \\ |    ||    \\ |    ||      ||  |  |\n|      ||  D  ) |  | |  _  | |  | |      ||  |  |\n|_|  |_||    /  |  | |  |  | |  | |_|  |_||  ~  |\n  |  |  |    \\  |  | |  |  | |  |   |  |  |___, |\n  |  |  |  .  \\ |  | |  |  | |  |   |  |  |     |\n  |__|  |__|\\_||____||__|__||____|  |__|  |____/\n\nTrinity-v2.9.1\n\nLeft read files: $VAR1 = [\n\u2018left_input0.fastqsanger.gz\u2019\n];\nRight read files: $VAR1 = [\n\u2018right_input0.fastqsanger.gz\u2019\n];\nMonday, May 2, 2022: 02:41:37\tCMD: java -Xmx64m -XX:ParallelGCThreads=2  -jar /usr/local/opt/trinity-2.9.1/util/support_scripts/ExitTester.jar 0\nPicked up _JAVA_OPTIONS:  -Xmx7g -Xms256m\nMonday, May 2, 2022: 02:41:38\tCMD: java -Xmx4g -XX:ParallelGCThreads=2  -jar /usr/local/opt/trinity-2.9.1/util/support_scripts/ExitTester.jar 1\nPicked up _JAVA_OPTIONS:  -Xmx7g -Xms256m\nMonday, May 2, 2022: 02:41:38\tCMD: mkdir -p /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir\nMonday, May 2, 2022: 02:41:38\tCMD: mkdir -p /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir/chrysalis\n\n\n-------------- Trinity Phase 1: Clustering of RNA-Seq Reads  ---------------------\n\n\n------------ In silico Read Normalization ---------------------\n\u2013 (Removing Excess Reads Beyond 200 Coverage \u2013\n\nrunning normalization on reads: $VAR1 = [\n      [\n        '/scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz'\n      ],\n      [\n        '/scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz'\n      ]\n    ];\n\n\nMonday, May 2, 2022: 02:41:38\tCMD: /usr/local/opt/trinity-2.9.1/util/insilico_read_normalization.pl --seqType fq --JM 190G  --max_cov 200 --min_cov 1 --CPU 56 --output /scratch1/03166/xcgalaxy/main/staging/42383847/working/trinity_out_dir/insilico_read_normalization --max_CV 10000  --left /scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz --right /scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz --pairs_together --PARALLEL_STATS\n-prepping seqs\nConverting input files. (both directions in parallel)CMD: seqtk-trinity seq -A -R 1  <(gunzip -c /scratch1/03166/xcgalaxy/main/staging/42383847/working/left_input0.fastqsanger.gz) >> left.fa\nCMD: seqtk-trinity seq -A -R 2  <(gunzip -c /scratch1/03166/xcgalaxy/main/staging/42383847/working/right_input0.fastqsanger.gz) >> right.fa\nCMD finished (73 seconds)\nCMD finished (74 seconds)\nCMD: touch left.fa.ok\nCMD finished (0 seconds)\nCMD: touch right.fa.ok\nCMD finished (0 seconds)\nDone converting input files.CMD: cat left.fa right.fa > both.fa\nCMD finished (11 seconds)\nCMD: touch both.fa.ok\nCMD finished (0 seconds)\n-kmer counting.\n\n----------- Jellyfish  --------------------\n\u2013 (building a k-mer catalog from reads) \u2013\nCMD: jellyfish count -t 56 -m 25 -s 27321675151  --canonical  both.fa\nCMD finished (95 seconds)\nCMD: jellyfish histo -t 56 -o jellyfish.K25.min2.kmers.fa.histo mer_counts.jf\nCMD finished (11 seconds)\nCMD: jellyfish dump -L 2 mer_counts.jf > jellyfish.K25.min2.kmers.fa\nCMD finished (31 seconds)\nCMD: touch jellyfish.K25.min2.kmers.fa.success\nCMD finished (0 seconds)\n-generating stats files\nCMD: /usr/local/opt/trinity-2.9.1/util/\u2026//Inchworm/bin/fastaToKmerCoverageStats --reads left.fa --kmers jellyfish.K25.min2.kmers.fa --kmer_size 25  --num_threads 28  --DS  > left.fa.K25.stats\nCMD: /usr/local/opt/trinity-2.9.1/util/\u2026//Inchworm/bin/fastaToKmerCoverageStats --reads right.fa --kmers jellyfish.K25.min2.kmers.fa --kmer_size 25  --num_threads 28  --DS  > right.fa.K25.stats\n-reading Kmer occurrences\u2026\n-reading Kmer occurrences\u2026\ndone parsing 81736069 Kmers, 81736069 added, taking 87 seconds.\nSequence: CCGTCTTGGACCAAACTAis smaller than 25 base pairs, skipping\nSequence: TGATGATGTASequence: GGCAAAAis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\nSequence:\nSequence: Sequence: Sequence: CTAGTGCCACAGCACAGAGCCCACGGSequence: AGGTGCATCAGTis smaller than 25 base pairs, skipping\nis smaller than is smaller than 25 base pairs, skipping25CGTGTTCTTGCTGAAGis smaller than AGGAGCCTGSequence: CTCTAGCACis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CTTAGGis smaller than\n25 base pairs, skipping\nSequence: 25Sequence: TTTCAAAATCAGTAGGCAAAAis smaller than  base pairs, skipping\nCGGCAAG25is smaller than is smaller than  base pairs, skippingSequence: CGGGGATGGATCAACACAAGCCis smaller than 25 base pairs, skipping2525\nbase pairs, skipping base pairs, skipping\nSequence: ATGATGACTGTCGAAAis smaller than 25 base pairs, skipping\nSequence: GTCCis smaller than 25 base pairs, skippingSequence:\nTAAGAAGGGTAis smaller than 25 base pairs, skipping\nSequence: TTCAAGGAAAGTGAGTCGis smaller than 25 base pairs, skippingSequence:\nSequence: GGCCACATCSequence: GTTTCATGGCTCAAis smaller than Sequence: is smaller than 25is smaller than CATCCCis smaller than  base pairs, skipping\n2525 base pairs, skipping base pairs, skipping25\nSequence: Sequence: CCACTTAGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: CTCCATTGCTCis smaller than 25 base pairs, skippingSequence: ACAAAis smaller than CTTGTTTCATGis smaller than 25 base pairs, skipping\nSequence: CCTGACTis smaller than 25Sequence: Sequence: 25 base pairs, skippingCCCGCGTCTAGTCGATTCCTTCis smaller than 25\nSequence: CAGCAGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: GTCCGAAGGGCAGTSequence: CTAGGACis smaller than 25 base pairs, skipping\nbase pairs, skippingis smaller than 25 base pairs, skippingGGCATTGGCACTGTGCis smaller than 25\nbase pairs, skippingSequence:\nSequence: CCCCGATCCAGTTCTCGCATCTGTis smaller than GCACT25is smaller than  base pairs, skipping25\nbase pairs, skippingSequence:\nCTGTCis smaller than 25 base pairs, skippingSequence:\nTTCAAATCTCAAAis smaller than 25 base pairs, skipping\nSequence: GCCCTCACAACCAis smaller than 25 base pairs, skipping\nSequence: CCGCTis smaller than 25 base pairs, skippingSequence: Sequence:\nATTGAAAGGAAATACAAATCTGATis smaller than 25is smaller than Sequence: CTGTTCG base pairs, skipping\nSequence: is smaller than 25 base pairs, skipping\nSequence: AGACCACCGTCTCCGAis smaller than 25 base pairs, skipping\nSequence: AACAGAAACAAGTAAACis smaller than 25 base pairs, skippingSequence: ACGGGGAAGAATAis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nCAGGTGACATGGTTis smaller than\nSequence: 25Sequence: GTGGCGC base pairs, skippingCAAGCATCCGAGGACis smaller than\nis smaller than 2525 base pairs, skipping base pairs, skipping\nSequence:\nSequence: CTGATCACCACGCTGTACTCGTACis smaller than 25Sequence: CTCAACTis smaller than 25 base pairs, skipping\nSequence: CGTATis smaller than 25 base pairs, skippingSequence: ATAAACTTTACCCTGCis smaller than 25 base pairs, skipping\nis smaller than Sequence: ATGAGATGTis smaller than 25 base pairs, skipping\nSequence: AAGGAATACGAATCTCAis smaller than 25 base pairs, skipping\nSequence: CGCGGTGis smaller than 25Sequence: CCTTAACATTACTTAATACCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping25\nbase pairs, skipping\nSequence: Sequence: Sequence: Sequence: TGTAATTGCTTTCACCAACAACAACGTAis smaller than 25is smaller than TGAAGGCTCTTGis smaller than  base pairs, skipping25 base pairs, skipping\nSequence:\nis smaller than 25GGCTGTCGAAGAGTCGTT25Sequence:  base pairs, skipping\nbase pairs, skippingGGCCAAis smaller than Sequence: CTCCCCCTGAATGAGis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\n25Sequence:\nbase pairs, skippingSequence: CGTTACACATGTis smaller than 25GGCGSequence: CCGGACGis smaller than  base pairs, skipping\nSequence:\n25is smaller than Sequence: CCAASequence: CACCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nCCTCAACACTTGCAGCis smaller than 25 base pairs, skippingis smaller than 25Sequence: CGGCGCGGGCAGAGGis smaller than 25 base pairs, skippingSequence: GCTACis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nSequence: ACCGAis smaller than 25\nSequence: AGGGC base pairs, skipping\nis smaller than  base pairs, skipping25 base pairs, skippingSequence: Sequence: CAGGCAGTGGTAGAGAAis smaller than 25 base pairs, skipping\nSequence: Sequence: Sequence: CGAGAGGGAGACAAis smaller than 25 base pairs, skipping\nCTACCTAATCAAis smaller than Sequence: TGTTGAGis smaller than 2525 base pairs, skipping\nGCCCGTTAGCCCTCTCTGCAA base pairs, skipping\nSequence: is smaller than 25 base pairs, skippingSequence: CCGACAAATCCATis smaller than 25 base pairs, skippingCCGGGTGTTTGACis smaller than 25\nCGCSequence:  base pairs, skippingis smaller than CCTGCCATCATASequence: TGGAGACGGCAACGAGGTGG25 base pairs, skipping\nis smaller than Sequence: is smaller than 25 base pairs, skipping25GGGGAAGTAACGCTGGTis smaller than Sequence:  base pairs, skipping\n25Sequence: CCCCis smaller than  base pairs, skippingCTCGGGTGGCGCACTTCTTGGGis smaller than 25\n25 base pairs, skipping\nSequence: Sequence: Sequence: GGCTGCCGCAAGGACCATCG base pairs, skippingAATCACTTGAGis smaller than 25\nis smaller than 25CACGTCSequence:  base pairs, skippingSequence: GCCGGTGCGGCCGATGCis smaller than 25 base pairs, skippingSequence: CGTTCCTTCACGTAis smaller than 25 base pairs, skipping\nbase pairs, skippingCCCCTCCTC\nSequence: GCTACCCTTCTTCTTCis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: CCCGCCGTCis smaller than 25 base pairs, skipping\nSequence: CTCTGCCGCAGTAGGTCTGACis smaller than 25 base pairs, skipping\nSequence: CCCACACGAACCACACCCTis smaller than 25 base pairs, skipping\nSequence: Sequence: CGCCAGATCACCGCCGAGGASequence: CGCCAAGAis smaller than 25Sequence: CCTGGATGTis smaller than 25 base pairs, skipping\nGCCis smaller than Sequence: GGACTATis smaller than 25 base pairs, skipping\nSequence: CGAis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: Sequence: ATCAATAAACATGTGis smaller than is smaller than 25 base pairs, skipping\n25CTCCAGCCGis smaller than 25Sequence: GATCTCis smaller than 25Sequence: CTCGGAis smaller than 25 base pairs, skipping\nbase pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nSequence: CCTGCGis smaller than 25Sequence: CGTGGTGGTGCTCGCCATCCTTCTis smaller than Sequence: Sequence: CGACAATATCGACAC25AGACCTTAGATTTTCATCCGis smaller than 25 base pairs, skipping\nbase pairs, skipping base pairs, skippingis smaller than\nSequence:\n25Sequence: AGAAGATAAGCGAis smaller than TCGTCCCCTTTis smaller than  base pairs, skipping25 base pairs, skipping25 base pairs, skippingSequence: GAAGTCGAAGGCCis smaller than 25 base pairs, skipping\nSequence:\nATTTTCTCis smaller than 25Sequence:  base pairs, skippingCTTCCTTGAGCTG\nSequence: is smaller than 25CACCTGCGCAGCCCTT base pairs, skippingSequence: is smaller than 25 base pairs, skippingCCGATTCTCGTTGCTCAAGGGTSequence: CCCGTAATACis smaller than 25\nSequence:  base pairs, skipping\nSequence: CTACTGis smaller than Sequence: TGGCCTTATTGCTGCAis smaller than 25CAACis smaller than 25 base pairs, skipping\n25 base pairs, skippingSequence: CGGGGAGTAAGCCGGAis smaller than\nis smaller than  base pairs, skipping\n25 base pairs, skipping\n25 base pairs, skipping\nSequence: Sequence: CTGis smaller than Sequence: GGGCGGGCAGACGCC25Sequence: GGAGis smaller than Sequence: CGTGGATGTGTCis smaller than 25 base pairs, skipping\nSequence: CTGTTGGTTCTGCTis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: Sequence: ATCCCATCCAATGAGAAATCAis smaller than CAAGSequence: AGATAAGAGTGGCGATGGCTis smaller than 25 base pairs, skipping\nCCTGC25 base pairs, skipping\nis smaller than is smaller than 25Sequence: TGTis smaller than 2525 base pairs, skipping base pairs, skipping base pairs, skipping\n25 base pairs, skippingSequence: Sequence: CTGCAGGTTCTTCTCGACAAis smaller than Sequence: CCTTis smaller than\n25AAGAGGGCGTCCAAGASequence: CGACCATATCCGTC base pairs, skippingis smaller than 25is smaller than 25 base pairs, skipping\nSequence:\n25 base pairs, skippingSequence: GCCSequence: AAGGCATAACCCCTTis smaller than 25is smaller than 25\nCACCG base pairs, skipping\nbase pairs, skippingis smaller than  base pairs, skipping\nSequence: TTCGTGTCGG25 base pairs, skipping\nis smaller than Sequence:\nSequence: CCATTGis smaller than 25ATTTTis smaller than 25 base pairs, skipping\n25Sequence: CCTCSequence: CTTGCis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skippingis smaller than\nSequence: 25 base pairs, skipping\nSequence: Sequence: AGCGGAGCTTGTTis smaller than 25Sequence: GCCTGGGTSequence: CATTAATCCCTTAAAGis smaller than 25 base pairs, skipping\nCCAAGAAGis smaller than 25 base pairs, skipping\nSequence: Sequence: ATAAAACGCGGATCis smaller than  base pairs, skipping\nACCG25 base pairs, skipping\nSequence: CGACis smaller than 25 base pairs, skipping\nis smaller than 25 base pairs, skipping\nis smaller than GTCACGATAACATACTGGAATCTis smaller than 25 base pairs, skipping\nSequence: CGACTGGAGATCis smaller than 25 base pairs, skipping\nSequence: Sequence: AAGGCATAACCCCTTATGACis smaller than 25 base pairs, skipping\nGGT25Sequence: CTCCGGAGis smaller than Sequence: CTTGCGGCAGCCT base pairs, skipping\nSequence: ACTGAis smaller than 25Sequence: TATTGCTCTis smaller than 25 base pairs, skipping\nis smaller than is smaller than 25 base pairs, skipping\n2525Sequence:  base pairs, skipping\nGTTCATCCTCC base pairs, skipping base pairs, skippingis smaller than\nSequence:\n25CAGACCG base pairs, skippingSequence: ACAAGATCACACAGCTCACGGATAis smaller than is smaller than\n2525Sequence: TCGGTAGGCTCTCGCTGC base pairs, skipping base pairs, skippingSequence: ATAGAACCGis smaller than\nis smaller than\n25Sequence: 25 base pairs, skippingCTCCATTCTCAGTCCCGGCAA base pairs, skipping\nis smaller than Sequence: CCTTGAis smaller than 25 base pairs, skipping\n25Sequence: GGAA base pairs, skipping\nis smaller than Sequence: CGCCATSequence: CAAAGCGCTACCis smaller than 25 base pairs, skipping\n25 base pairs, skippingis smaller than\n25 base pairs, skipping\nSequence: CGCTGGACTGGTis smaller than Sequence: CGTAGCTCCC25is smaller than 25 base pairs, skippingSequence:  base pairs, skipping\nCCGAAACCCTAACCSequence: TCTCTTCTC\nis smaller than 25is smaller than 25 base pairs, skipping\nSequence: CTAGGGCAGATTCTis smaller than 25 base pairs, skipping\nSequence: CTCCAis smaller than 25Sequence: GTTCGis smaller than 25 base pairs, skipping\nSequence: CCTCGCCCACTTis smaller than 25 base pairs, skipping\nSequence: CTGTTGis smaller than 25 base pairs, skipping\nbase pairs, skipping base pairs, skipping\nSequence: CTCCCCACGGAGACTTTCTTGis smaller than 25 base pairs, skipping\nSequence: TGCTCTTCCCAAGTTCCGCAAGCTis smaller than 25 base pairs, skipping\nSequence: TGGCACGGis smaller than Sequence: 25CGACCGSequence:  base pairs, skippingSequence: CGATCCTCGAis smaller than 25 base pairs, skipping\nis smaller than ACSequence: CTTCGAGCTTCTCATCCCGCAis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nis smaller than\nSequence: 25CTCAis smaller than 25 base pairs, skipping base pairs, skipping\nSequence: GTCAACGis smaller than Sequence: 25Sequence: AAGGATCTCACCAGCATG base pairs, skippingTAGAis smaller than Sequence:\nis smaller than 25ATGTCTACAATCTATGATGAis smaller than 25 base pairs, skipping base pairs, skipping\nSequence: 25 base pairs, skipping\nATGAAGCTG\nSequence: is smaller than GTTACGGT25is smaller than  base pairs, skipping25\nSequence:  base pairs, skippingGTAGGAGTT\nis smaller than 25 base pairs, skippingSequence: Sequence: Sequence: CTGGTCAAACTATACTGAACTCCGATTTATCCTTAis smaller than 25is smaller than\nis smaller than  base pairs, skipping25 base pairs, skipping\n25\nSequence:  base pairs, skippingCCCAGCTGAAACTAACAAACis smaller than\n25 base pairs, skipping\nSequence: TTGTCATCAGCTTTCAGTGATAGis smaller than 25 base pairs, skipping\nSequence: Sequence: AGACAGGCCGTTGGATTGCCCSequence: is smaller than is smaller than Sequence: CTCGATGGATCAAAGAAAGCCATTis\n\u2026\nthan 25 base pairs, skipping\nSequence: is smaller than 25GCTGGGTGSequence:  base pairs, skippingSequence: ATACGGTGTis smaller than 25 base pairs, skipping\nGTGGA\nis smaller than Sequence: Sequence: GGAAGATGG25 base pairs, skipping\nis smaller than 25 base pairs, skippingis smaller than 25 base pairs, skipping\nSequence: Sequence: GTTGATGTTGCTGAAGCCAAis smaller than 25CAAAGSequence: GGAAGACACis smaller than 25TGATG base pairs, skipping\nbase pairs, skippingis smaller than 25\nbase pairs, skipping\nis smaller than Sequence: 25 base pairs, skippingGGCCCAAACAT\nis smaller than 25 base pairs, skipping\nSequence: TGGAAGAGAASequence: is smaller than Sequence: TCCTTis smaller than ATCTAACCCA2525is smaller than  base pairs, skipping base pairs, skipping25\nbase pairs, skippingSequence:\nCTTCGAAACGGATCAis smaller than Sequence: 25Sequence: CGGAAis smaller than 25 base pairs, skipping\nSequence: Sequence: GTTCATACAis smaller than GACACGAATATGTACGGAACCTis smaller than 25 base pairs, skipping\nbase pairs, skipping\n25CAGGAAASequence: Sequence: CTTGCCCCTTis smaller than ATCGCC base pairs, skipping\n25 base pairs, skipping\nSequence: Sequence: is smaller than 25is smaller than 25 base pairs, skipping\nSequence: GTTTis smaller than 25 base pairs, skipping\nTTCCTis smaller than 25 base pairs, skipping\nbase pairs, skipping\nSequence: CACCACSequence: CCCAGis smaller than 25 base pairs, skipping\nCTAAACTTCAATCTis smaller than Sequence: Sequence: is smaller than 25CTATAAATGATTTGGCTis smaller than 25 base pairs, skipping\n25Sequence:  base pairs, skippingSequence: CGCCGis smaller than 25 base pairs, skipping\nCCACCATCAACAGCAATTGTCis smaller than  base pairs, skipping\nGAAGAGGG25\nis smaller than  base pairs, skipping\n25Sequence: CAAAGGTATGAGAAGAGGis smaller than  base pairs, skipping25\nbase pairs, skipping\nSequence: Sequence: GTGTASequence: CGGCis smaller than 25GCCAAATCGAATCGAGGAGACTTTis smaller than 25 base pairs, skippingSequence: CAAGGTTGis smaller than 25 base pairs, skipping\nSequence: GTTCAAACTTGATAAis smaller than  base pairs, skipping\nSequence: CTAGAAis smaller than 25\nis smaller than 25 base pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nSequence:\nCGTGCAGTAGCTGTGis smaller than 25 base pairs, skipping\nSequence: GGAGGAis smaller than Sequence: 25 base pairs, skipping\nSequence: Sequence: Sequence: CCCACAAACTAATAAis smaller than 25 base pairs, skipping\nCTGAAGATGGTCTCGGTCAGAGTATis smaller than is smaller than 25 base pairs, skipping25CAAGGCGGTSequence: CGCAGGTGTCCTAAGATis smaller than 25is smaller than 25 base pairs, skipping\nSequence:  base pairs, skipping\nCTCCTTTCACGCCTTGTCTTTTAT base pairs, skippingSequence: is smaller than 25 base pairs, skipping\nCCGTGCATATACTACTGCCTCTSequence: is smaller than GGCGAGGGTTCAis smaller than 2525 base pairs, skipping\nSequence: GCAACGTCis smaller than  base pairs, skippingSequence: ACGACTAGCTGCis smaller than\nSequence: AACTC25Sequence: Sequence: CCAATCAis smaller than 25 base pairs, skipping\nbase pairs, skipping\nis smaller than GCTCis smaller than 2525 base pairs, skipping base pairs, skipping\nSequence: AATCTCACAAACACATTATTCis smaller than 25 base pairs, skipping\n25Sequence: Sequence: CTCCACATGCTGATGAAACCSequence: GGCGGCCCGCAAis smaller than 25is smaller than 25 base pairs, skipping\nSequence: ATGCAAATGCis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CTAGTGTCGTCGTTCCTCCTCGis smaller than 25\nCGGCATAGTGGis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skippingSequence: CCTGTGGAAGCCCCTCCSequence: TGAAGis smaller than 25 base pairs, skipping\nSequence: GGCCGTTGGAGis smaller than\nis smaller than 2525 base pairs, skippingSequence:  base pairs, skipping\nTTGTAAGTGGCTCGTAC\nis smaller than 25 base pairs, skippingSequence: CCGCAAGCACCAis smaller than Sequence: GTTTGGAis smaller than 25 base pairs, skipping\n25Sequence: Sequence: CCCAAATTGAGTACTCGGGAAC base pairs, skipping\nSequence: GTCTCCGGTAACGCTCTCAAis smaller than 25 base pairs, skippingis smaller than AGTTAAGACT25 base pairs, skipping\nSequence: CTCGGAis smaller than 25 base pairs, skipping\nSequence:\nSequence: is smaller than 25GACCGis smaller than 25 base pairs, skipping base pairs, skipping\nCTGATATCCGis smaller than 25 base pairs, skipping\nSequence: CGTAAT\nSequence: is smaller than Sequence: GGACis smaller than 25 base pairs, skipping\n25TGTTT base pairs, skippingis smaller than\n25Sequence: CGCCCTTGACCis smaller than Sequence: CTGTGATGAGTGTGGTATCis smaller than 25Sequence: ATCTCis smaller than 25 base pairs, skipping\nSequence: CTTCGCCGCCATCTCGCTCTGGis smaller than 25Sequence: GTACAAGCTGCTATACT base pairs, skipping\nbase pairs, skipping25 base pairs, skipping base pairs, skipping\nis smaller than\nSequence:\n25TCTACATTATAGTAA base pairs, skipping\nis smaller than 25 base pairs, skipping\nSequence: Sequence: Sequence: GTTGTACGAAATACGis smaller than is smaller than is smaller than 252525 base pairs, skipping base pairs, skipping base pairs, skipping\nSequence:\nSequence: CCACGSequence: CAGGCGAAGGTGCCis smaller than TAAAAACTTCAis smaller than 25 base pairs, skipping\nSequence: Sequence: CTCCTCAGCACCCTCCTTTis smaller than 25 base pairs, skipping\nSequence: Sequence: 25 base pairs, skipping\nSequence: CCTAis smaller than 25CTAGTTTACAACTis smaller than 25 base pairs, skipping\nSequence: CCGCTis smaller than 25 base pairs, skipping\nCGCTis smaller than 25 base pairs, skippingis smaller than  base pairs, skipping\n25Sequence: GTACCGCGTTCACTGTGis smaller than 25CAAGAATGTTGCTGTCAAGGAis smaller than  base pairs, skipping\n25 base pairs, skipping base pairs, skipping\nSequence: CGAGCGATCACCGis smaller than 25 base pairs, skipping\nSequence: CTCCTTCATTCTCCTTGGCTTCCis smaller than 25 base pairs, skippingSequence: CCTCGTis smaller than Sequence: CCAACAATTAis smaller than 25 base pairs, skipping\n25Sequence:  base pairs, skippingSequence: AGCAGTTGCTTCAis smaller than GGACA\nSequence: Sequence: AAGAAAGGis smaller than 25 base pairs, skipping\n25CTGAis smaller than  base pairs, skippingis smaller than 25 base pairs, skipping25\nSequence:  base pairs, skipping\nSequence: AAAGGCTAis smaller than 25CTCAGTGCACGCCGCCAis smaller than 25 base pairs, skipping\nSequence: Sequence: GCAAAGCTCGATTTGGGSequence:  base pairs, skipping\nis smaller than Sequence: Sequence: TCCAGATACAAATTACTTATTTAis smaller than 25 base pairs, skipping\nSequence: CAGGGis smaller than 25 base pairs, skipping\nCAGTTCTCCGTGTTTCTGCAAis smaller than 25 base pairs, skipping\nCCTAGG25 base pairs, skippingCTGCTGis smaller than 25Sequence: CTGCis smaller than 25 base pairs, skipping\nSequence: is smaller than 25 base pairs, skipping\nCTCGAGGTTAGGGTSequence:  base pairs, skippingSequence: GCCTCTTCCCCCis smaller than 25 base pairs, skipping\nCGACCAGAGGAis smaller than 25 base pairs, skipping\nis smaller than 25\nSequence:\nbase pairs, skippingCGCGGACTGAATCTSequence: AACCGACTGTGis smaller than 25 base pairs, skipping\nis smaller than\nSequence: 25Sequence:  base pairs, skipping\nCCCCGAACGCCAATCCCAGTGATTCTCTGGis smaller than is smaller than 25 base pairs, skipping25 base pairs, skipping\nSequence:\nSequence: GTGGSequence: GGCTTGGAis smaller than 25is smaller than GCAGGTTTCCGTTTTTTCis smaller than 25Sequence:  base pairs, skipping\nbase pairs, skippingACACCis smaller than 25Sequence: 25\nbase pairs, skippingCAGAAACATGCCTGACis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence:\nGTTASequence: is smaller than CTCTis smaller than 25 base pairs, skipping\n25 base pairs, skipping\nSequence: TGAGis smaller than 25 base pairs, skipping\nSequence: CGAAGCCAAAGACTGTTCCCAAis smaller than 25 base pairs, skipping\nSequence: CTGAATCTis smaller than 25Sequence:  base pairs, skippingCTTTGGA\nis smaller than 25 base pairs, skipping\nSequence: CTCATGGAACATTTACCis smaller than 25 base pairs, skipping\nSequence: CAGTACCis smaller than 25 base pairs, skippingSequence:\nSequence: CGGAGAGGGCGGGGGGGAAAGSequence: CTGAGis smaller than Sequence: CCCTGATGGTTACCTGTTTGAGis smaller than 25 base pairs, skippingis smaller than 25is smaller than 2525 base pairs, skipping\nbase pairs, skippingSequence:\nbase pairs, skipping\nTGTGATATATATGTCCCTTGATCASequence: is smaller than GCCCCTTATCTCGCTCTTTCTG25is smaller than  base pairs, skipping25Sequence:  base pairs, skipping\nSequence: CATGAATACTAGTACis smaller than 25Sequence:\nCCAA base pairs, skippingTGGGACTCCAACGCis smaller than\nSequence: is smaller than 25GGC25 base pairs, skippingis smaller than  base pairs, skipping\n25Sequence:\nbase pairs, skippingGGATGATCAGTAC\nis smaller than 25 base pairs, skipping\nSequence: GTCGACGCCGAAis smaller than Sequence: 25Sequence: ATCACATTCTCCAAis smaller than Sequence: CGAAGAGTGTATTTGAAGTGCis smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: GCTATACTGCCCCAGTGTCCGACGTCTCCTGCC\n25Sequence: CTCGGAAis smaller than  base pairs, skipping\nis smaller than is smaller than 25 base pairs, skippingSequence: 25 base pairs, skipping\nCAACTTT25Sequence: CACAACATis smaller than  base pairs, skippingis smaller than Sequence: GGGAAATis smaller than 25 base pairs, skipping\nSequence:\n25Sequence: 25ATTGCCTGGCGCCis smaller than 25 base pairs, skippingSequence: CACCTCis smaller than  base pairs, skipping\nbase pairs, skippingGGATATTGGAATGGTis smaller than 25 base pairs, skipping\nSequence: GGAAGis smaller than 25 base pairs, skipping\n25\nSequence:\nSequence:  base pairs, skippingATCCCis smaller than 25Sequence: GGAGGCTis smaller than  base pairs, skipping\nAACGA\n25Sequence: GTTGCis smaller than is smaller than  base pairs, skipping25 base pairs, skipping\n25\nSequence: CCGTAGCGGTGCCTACATTGSequence: CGAACACTTCis smaller than 25is smaller than 25 base pairs, skippingSequence: CTGAACCTTATAis smaller than 25 base pairs, skipping\nbase pairs, skipping\nbase pairs, skipping\nSequence: Sequence: Sequence:\nCGCCCTTGACCCGGTATis smaller than is smaller than 25 base pairs, skipping25GAAGAATSequence: AGCCGTTCGGGGTT base pairs, skippingis smaller than 25is smaller than 25 base pairs, skipping\nbase pairs, skippingSequence: CCGCAGATATCACAAGis smaller than Sequence: CGAGGTCAGTGCAGTis smaller than 25 base pairs, skipping\nSequence: GCCTAATACGGCCGACACis smaller than 25 base pairs, skipping\n25Sequence: CGGGGis smaller than 25 base pairs, skipping\nSequence: GGCGCACACATCGATAAAACCCA base pairs, skippingis smaller than\n25Sequence: CCGAAis smaller than  base pairs, skipping\nSequence: Sequence: GCTGTGGGTCTGCCTGCTATGis smaller than 25 base pairs, skipping\nis smaller than Sequence: AGCTCGGGAAis smaller than 25Sequence: 25 base pairs, skipping\nCATTCCTGAis smaller than  base pairs, skippingSequence: 25 base pairs, skipping\nSequence: GAAGAAGis smaller than 25CTACATGCTCGGCAGCCis smaller than 25 base pairs, skipping25 base pairs, skipping\nbase pairs, skipping\nSequence: GTGGTCTis smaller than Sequence: Sequence: 25CAGGGTCGGAGACAAATCCAATGTis smaller than is smaller than 25Sequence: CTCAGTCTTGACCTGCGis smaller than 25Sequence: CCACATATis smaller than  base pairs, skipping base pairs, skipping\nbase pairs, skipping25\n25 base pairs, skipping base pairs, skipping\nSequence: Sequence: Sequence: GTTCGACCAAAAAAGCCTGAGCCCGGCGACGAGAGCCGGCTGTTACGACGGCis smaller than is smaller than is smaller than 2525 base pairs, skipping\n25 base pairs, skipping\nSequence: CATTAAGAis smaller than Sequence:  base pairs, skipping25CCACCTTAAATCCTCAAATTis smaller than Sequence:  base pairs, skipping\n25GCATCTATTGCTGATTCATis smaller than 25\nbase pairs, skipping base pairs, skippingSequence:\nCCCTGTCATCTAGTGTCGGTTGATSequence: TGGGis smaller than 25is smaller than 25 base pairs, skipping base pairs, skippingSequence: CCAATAis smaller than\nSequence: CCGGCCAAGCAAGTis smaller than Sequence: 25 base pairs, skipping\nCCGATATCTCAATTACAGCTTCCTSequence: 25is smaller than 25 base pairs, skippingGAGG base pairs, skipping\nis smaller than\n25Sequence:  base pairs, skippingCGGGATTCCTCA\nSequence: is smaller than TGAATGCTTACT25is smaller than Sequence: CCTCCCCCATCGAGTTATCAACis smaller than  base pairs, skipping25\n25 base pairs, skippingSequence: TGTAAATCTTCCGACTGSequence: ATTCis smaller than\nbase pairs, skippingis smaller than 25 base pairs, skipping\n25 base pairs, skippingSequence: ACGCCCCCGGCGCAGCCGCis smaller than 25 base pairs, skipping\nSequence: CTGCCTGCAAGTGTGGCAGTGis smaller than\nSequence: Sequence: Sequence: 25 base pairs, skippingGTCCTCGCTCTGTCCGCCGCCATTTis smaller than 25 base pairs, skippingSequence: ATCTCis smaller than 25\nGCCis smaller than 25 base pairs, skippingSequence: CCACAACCCACCGTGCCCCACTCis smaller than 25"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis message repeating means that you have many reads that are too short to assemble.\n@quote\nSequence: ACGCCCCCGGCGCAGCCGCis smaller than 25 base pairs, skipping\nTry filtering out reads that are too short to be used, then rerun. The job is seems to be running out of resources when attempting to sorting out usable read pairs from those that are not. Pre-filtering by length will address that. Use a QA tool that will remove/separate both ends of a pair if one of the paired reads fails the filter. Trinity expects intact pairs as an input.\nIf it seems like that length filter eliminates too many reads:\n\nprior QA steps may have been too strict\nyou need to still run QA\nthe reads were originally too short\nthe read quality is actually low, and the QA steps are correct\n\nThe last two would be the most concerning re is this data even usable?\nTutorials: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=quality)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI recently upgraded my galaxy instance to v19.09 from v19.05. This went fine, so I decided to try moving from python2 to python3. I removed my .venv, and created a new one with\nvirtualenv --python=/usr/bin/python3 .venv\nI had to upgrade setuptools (pip install --upgrade setuptools) and pull this change (@link https://github.com/galaxyproject/galaxy/commit/07783c058e8943af57f9c843afa112c0c1489b05#diff-3e7182e1087d0324e65dfc8d98d30d8b) into my instance before the server could make it through startup.\nThe I tried to access galaxy and I got the following:\nserving on 0.0.0.0:8080 view at http://127.0.0.1:8080\n\n10.50.135.82 - - [09/Dec/2019:09:34:47 -0700] \"GET / HTTP/1.1\" 500 - \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\"\n\nError - <class 'NameError'>: name 'unicode' is not defined\n\nURL: https://galaxy.lygos.com/\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/middleware/error.py', line 154 in __call__\n\napp_iter = self.application(environ, sr_checker)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/paste/recursive.py', line 85 in __call__\n\nreturn self.application(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/paste/httpexceptions.py', line 640 in __call__\n\nreturn self.application(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/base.py', line 143 in __call__\n\nreturn self.handle_request(environ, start_response)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/base.py', line 222 in handle_request\n\nbody = method(trans, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/galaxy/controllers/root.py', line 79 in index\n\nreturn self._bootstrapped_client(trans)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/base/controller.py', line 281 in _bootstrapped_client\n\nreturn self.template(trans, app_name, options=js_options, **kwd)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/webapps/base/controller.py', line 360 in template\n\nmasthead=masthead\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/webapp.py', line 936 in fill_template\n\nreturn self.fill_template_mako(filename, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/lib/galaxy/web/framework/webapp.py', line 949 in fill_template_mako\n\nreturn template.render(**data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/template.py', line 476 in render\n\nreturn runtime._render(self, self.callable_, args, data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 883 in _render\n\n**_kwargs_for_callable(callable_, data)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 920 in _render_context\n\n_exec_template(inherit, lclcontext, args=args, kwargs=kwargs)\n\nFile '/home/galaxy/galaxy-dist/.venv/lib/python3.5/site-packages/mako/runtime.py', line 947 in _exec_template\n\ncallable_(context, *args, **kwargs)\n\nFile '/home/galaxy/galaxy-dist/database/compiled_templates/js-app.mako.py', line 48 in render_body\n\n__M_writer(unicode( h.url_for( '/' ) ))\n\nNameError: name 'unicode' is not defined\n\nDuring Galaxy startup, I noticed a message \u2018The Galaxy client build is up to date and will not be rebuilt at this time.\u2019 I\u2019m guessing this is incorrect and I do need to rebuild, but I would love to get an opinion form someone who knows the Galaxy internals. And if I do need to rebuild, how do I kick that off?\nThanks,\n-Will"
    },
    {
      "role": "user",
      "text": "Within moments of posting I managed to solve the issue using this:\n\n\n@link https://github.com/galaxyproject/galaxy/issues/8617\n\n\n\n\n\n\n\n\nMako on Python 3 (@link https://github.com/galaxyproject/galaxy/issues/8617)\n\n\n\n        opened 04:26PM - 11 Sep 19 UTC\n\n\n          closed 08:47AM - 12 Sep 19 UTC\n\n\n\n\n          selten\n         (@link https://github.com/selten)\n\n\n\n\n\nI'm using Galaxy release 19.09 and Python 3.7.\nI'm trying to access the main page and get the following error:\nTraceback (most recent...\n\n\n\n\n\n\n\n\n\n-Will"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am currently working on my project and is using the tutorial \u201cExome sequencing data analysis for diagnosing a genetic disease\u201d. I could not find the tool called SnpEff eff and SnpEff Download. So where i can find this tool ?? Please help me out."
    },
    {
      "role": "system",
      "text": "Both tools are available on Galaxy Australia in Annotation section. Sometimes search does not pick up tools.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear All\nhow to perform differential gene expression from 4 transcriptomic library a two from control and 2 from treated which are uploaded on NCBI. So need to do the differential gene expression for fpkm/rpkm value so to check out the expression of identified element. Anybody can help.\nhighly thank ful to you"
    },
    {
      "role": "system",
      "text": "Hi @user,\nif a reference genome and a gene annotation is available, please check Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nIf the genome assembly is not available, assemble transcription with Trinity or rnaSPAdes and follow any transcriptome based protocol.\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear @system I use STAR very often and I noticed that the fasta.gz genome file was used as uncompressed file, but the GTF.gz file wasn\u2019t used as a uncompressed file resulting in a error.\nI tried to use the same fasta.gz and gtf uncompressed file directly and run without any problem.\nThere is any chance set the option gtf/gff file to be used as an uncompressed file automatically?\nHave a nice day!"
    },
    {
      "role": "system",
      "text": "@quote\nI use STAR very often and I noticed that the fasta.gz genome file was used as uncompressed file, but the GTF.gz file wasn\u2019t used as a uncompressed file resulting in a error.I tried to use the same fasta.gz and gtf uncompressed file directly and run without any problem.There is any chance set the option gtf/gff file to be used as an uncompressed file automatically?\nHi @user\nData that is in gtf.gz format is automatically uncompressed to gtf during Upload. There is no datatype for gtf.gz \u2013 so it wouldn\u2019t be possible to have a dataset with gtf.gz assigned as the datatype. This means there is nothing to implicitly convert compressed to uncompressed at runtime.\nIf you are not working at a usegalaxy.* server, maybe test out the upload of the data at one of those to better understand the default/supported behavior? Allow Galaxy to autodetect the datatype.\nOther Galaxy servers, public or private, may have different custom datatype configurations added, or be running a (much) older Galaxy release, or be running a customized version of this tool. Those admins/tool developers would be the best people to contact if you want a change that fits the custom server environment you are working in.\n@link https://galaxyproject.org/use/\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi~ I see Deseq2 (Galaxy Version 2.11.40.7+galaxy1) added the \u201csize factor\u201d option in output, rather than version 2.11.40.6, which is a big improvement for me. Further more, I wonder could we have the option to use custom defined size factor in Deseq2? because it is useful for spike-in samples."
    },
    {
      "role": "system",
      "text": "Hi @user,\nI created an issue in order to request this functionality: DESeq2: enable custom defined size factor (@link https://github.com/galaxyproject/tools-iuc/issues/4425).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I have a question related to this issue. I am using Hisat2 but in the built-in reference genome there is no mm28 only mm9 and mm10. My question Can I select mm10 during Hisat2 and in the next stingtie I use the earlier version mm39 which I uploaded?"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nCan I select mm10 during Hisat2 and in the next stingtie I use the earlier version mm39 which I uploaded\nYou will have technical and scientific problems if data based on different genome assemblies are used together in the same analysis. Choose a matching reference genome and reference annotation at the start of an analysis and use those throughout.\nPublic servers host different genome indexes. @link http://UseGalaxy.eu hosts mm39 for now.\nNote: next time, please consider asking a new question for new problems. You can reference other topics by URL for context."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am currently experiemcing some difficulies counting mapped reads: I have used featureCounts, all features show up as either Unassigned_Unmapped or Unassigned_NoFeatures.\nMapping the reads does not seem to be the issue as the reads could successfully be visualized using JBrowse.\nHowever, I think the issue might be with the GTF file I am using, as no visualization of the annotated reference (GTF from UCSC table browser,  converted to GFF3 using gffread) is possible via JBrowse. Where might I find alternative GTF files?"
    },
    {
      "role": "user",
      "text": "In case anyone else encounters this problem, here is the solution that worked for me:\nThe gtf file I had used wasn\u2019t ideal which resulted in improper assignment. This could be fixed by trying out different gtf files in JBrowse (after gffread conversion to gff3) and using the one that showed us in the visualization via JBrowse.\nAdditionally, featureCounts had to be set to look for \u201cgene\u201d instead of \u201cexon\u201d, once again due to the formatting in the gtf file."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am having trouble subsetting a VCF by desired regions. I imported the following VCF file from NCBI directly into Galaxy (vcf.gz):\nftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/NA12878_HG001/NISTv4.2.1/GRCh38/SupplementaryFiles/HG001_GRCh38_1_22_v4.2.1_all.vcf.gz\nI added format = vcf_bgzip and db=hg38\nA sample section of this VCF, skipping all # comment lines, looks like this:\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tINTEGRATION\nchr1\t10108\t.\tC\tCT\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged,lowmappabilityall\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\nchr1\t10146\t.\tAC\tA\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged,lowmappabilityall\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\nchr1\t10157\t.\tTA\tT\t5\tallfilteredbutagree\tplatforms=0;platformnames=;datasets=0;datasetnames=;callsets=0;callsetnames=;filt=CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt;difficultregion=GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set_REF_N_slop_15kb,GRCh38_AllTandemRepeats_201to10000bp_slop5,HG001.hg38.300x.bam.bilkentuniv.010920.dups,hg38.contigs.1-22.lt_500kb,hg38.segdups_sorted_merged\tGT:PS:DP:ADALL:AD:GQ\t:.:0:0,0:0,0:0\n\n\nThen, I uploaded a list of regions of interest that I want to subset from that VCF as a bed file, for example:\nchr1\t11790551\t11790551\nchr1\t11790553\t11790553\nchr1\t11790559\t11790559\nchr1\t11790560\t11790560\nchr1\t11790562\t11790562\nchr1\t11790563\t11790563\nchr1\t11790565\t11790565\n\nI used the tool VCF-BEDintersect: to intersect the original HG001_GRCh38_1_22_v4.2.1_all.vcf.gz  with the bed file containing regions of interest (around 200k). However, I could only retrieve 4 regions interesecting my bed file.\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tINTEGRATION\nchr2\t108259008\t.\tAAAACTTGGTCAGGTGATGTTAT\tA\t50\tPASS\tplatforms=4;platformnames=Illumina,PacBio,CG,10X;datasets=4;datasetnames=HiSeqPE300x,CCS15kb_20kb,CGnormal,10XChromiumLR;callsets=6;callsetnames=HiSeqPE300xGATK,CCS15kb_20kbDV,CCS15kb_20kbGATK4,CGnormal,HiSeqPE300xfreebayes,10XLRGATK;datasetsmissingcall=IonExome,SolidSE75bp;callable=CS_HiSeqPE300xGATK_callable,CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable,CS_CGnormal_callable,CS_HiSeqPE300xfreebayes_callable\tGT:PS:DP:ADALL:AD:GQ\t1/1:.:646:0,263:82,417:362\nchr11\t640056\t.\tTCCCCGGGGTCCCTGCGGCCCCGACTGTGCGCCCGCCGCGCCCAGCCTC\tT\t5\tallfilteredanddisagree\tplatforms=2;platformnames=Illumina,PacBio;datasets=2;datasetnames=HiSeqPE300x,CCS15kb_20kb;callsets=3;callsetnames=,CCS15kb_20kbDV,CCS15kb_20kbGATK4,HiSeqPE300xGATK;callable=CS_CCS15kb_20kbDV_callable;filt=CS_HiSeqPE300xGATK_filt,CS_CCS15kb_20kbDV_filt,CS_10XLRGATK_filt,CS_HiSeqPE300xfreebayes_filt;difficultregion=GRCh38_AllTandemRepeats_201to10000bp_slop5\tGT:PS:DP:ADALL:AD:GQ\t0|1:640056_TCCCCGGGGTCCCTGCGGCCCCGACTGTGCGCCCGCCGCGCCCAGCCTC_T:303:107,91:0,0:106\nchr15\t40407805\t.\tGTT\tG\t50\tPASS\tplatforms=3;platformnames=PacBio,Illumina,10X;datasets=3;datasetnames=CCS15kb_20kb,HiSeqPE300x,10XChromiumLR;callsets=5;callsetnames=CCS15kb_20kbDV,CCS15kb_20kbGATK4,HiSeqPE300xfreebayes,HiSeqPE300xGATK,10XLRGATK;datasetsmissingcall=CGnormal,IonExome,SolidSE75bp;callable=CS_CCS15kb_20kbDV_callable,CS_10XLRGATK_callable,CS_CCS15kb_20kbGATK4_callable,CS_HiSeqPE300xfreebayes_callable\tGT:PS:DP:ADALL:AD:GQ\t1/1:.:677:0,283:1,63:215\nchr22\t42126617\t.\tAG\n\nI highly doubt that there are only 4 regions of interest out of the 200k as bed I originally uploaded in the input VCF, so I suspect that there is probably something else going on but I cannot figure it. I appreciate any suggestions that you might have."
    },
    {
      "role": "system",
      "text": "Hi,\nBED files are 0 based for the start and non-inclusive for the end position. The wiki page about it is quite nice BED (file format) - Wikipedia (@link https://en.wikipedia.org/wiki/BED_(file_format))\nSo I could imagine that you need to adjust your BED file.\nCiao,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I locally installed the docker version with qiime2 (Quay (@link http://quay.io/qiime2/q2galaxy)). When I use get data \u2192 upload file \u2192 rule based and I paste remote urls (e.g. @link https://zenodo.org/api/xxx/xxx.fastqsanger.gz) I get this message: \u201cBuild rules for updating datasets - Access to this address in (sic!) not permitted by server configuration\u201d. Any suggestions? Thanks"
    },
    {
      "role": "system",
      "text": "Please see: Troubleshooting Galaxy Docker -- QIIME2 (@link https://help.galaxyproject.org/t/troubleshooting-galaxy-docker-qiime2/9836)"
    }
  ],
  [
    {
      "role": "user",
      "text": "While merging bam files (output from alignment) I got the error The tool was executed with one or more duplicate input datasets. This frequently results in tool errors due to problematic input choices.\nBut I don\u2019t have any duplicate input dataset there. 3 dataset was aligned with my account, and my teammate did another three. Then she shared her history with me. I set the read groups using AddorReplaceReadGroups tool for mine together and those of her in a second run.\nHere is the params I used-\n\nScreenshot from 2021-09-02 11-18-231532\u00d7851 142 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/c/c3edf4d91449326b2d993468524f69749b31d726.png)\nThen I tried to merge them but got the error-\n\nScreenshot from 2021-09-02 10-50-131515\u00d7747 167 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/b172909371769ebbd8d336899e6e877da0a1e8f5.png)"
    },
    {
      "role": "user",
      "text": "Got the answer submitting the bug report. It was due to different alignment files using different reference genome."
    }
  ],
  [
    {
      "role": "user",
      "text": "Nothing happens when the Upload File from your computer (@link https://usegalaxy.eu/tool_runner?tool_id=upload1) tool is clicked. It seems there is something with @link https://github.com/galaxyproject/galaxy/tree/release_21.01. Hope someone can fix this issue as soon as possible.\nimage"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit has been reported; meanwhile, you can use the Upload Data button which is under the search tools bar.\nRegads"
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been trying to read about which workflow language underlies Galaxy and I am confused.\nThis link seems to say \u201cGalaxy additionally allows design and execution of complex MapReduce-type workflows\u201d\n@link https://galaxyproject.github.io/\nHowever, this link (as well as Wikipedia), seem to suggest that Galaxy is currently implementing the use of CWL?\n@link https://www.commonwl.org/\nCould anyone clarify this to me? Which information is more up to date? Thanks"
    },
    {
      "role": "system",
      "text": "Galaxy has its own workflow specification that predates most of what is actively developed elsewhere. The specification is rich in features but not in documentation. You can download any Galaxy workflow in json format, but we also support yaml to a large extent. There is Galaxy Workflow format2 in the making which should be convertible to cwl format.\nYou can explore the following links if you want to know more.\n\n\n@link https://github.com/galaxyproject/galaxy/pull/6776\n\n\n\n\n (@link https://github.com/jmchilton)\n\nAllow API import/export of format 2 workflows. (@link https://github.com/galaxyproject/galaxy/pull/6776)\n\n\n  by @link https://github.com/jmchilton\n  on 05:43PM - 26 Sep 18 UTC (@link https://github.com/galaxyproject/galaxy/pull/6776)\n\n\n8 commits\n  changed 11 files\n  with 555 additions\n  and 416 deletions.\n\n\n\n\n\n\n\n\n\n@link https://github.com/galaxyproject/galaxy/pull/6807\n\n\n\n\n (@link https://github.com/jmchilton)\n\nSupport for Enhancements to gxformat2. (@link https://github.com/galaxyproject/galaxy/pull/6807)\n\n\n  by @link https://github.com/jmchilton\n  on 08:33PM - 30 Sep 18 UTC (@link https://github.com/galaxyproject/galaxy/pull/6807)\n\n\n4 commits\n  changed 11 files\n  with 100 additions\n  and 748 deletions.\n\n\n\n\n\n\n\n\n\n\nGitHub (@link https://github.com/jmchilton/gxformat2)\n\n\n\n@link https://github.com/jmchilton/gxformat2\nGalaxy Workflow Format 2. Contribute to jmchilton/gxformat2 development by creating an account on GitHub.\n\n\n\n\n\n\n@link https://planemo.readthedocs.io/en/latest/commands.html#workflow-convert-command"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI am using the Galaxy Europe (@link https://usegalaxy.eu/) web application and I have imported some new data and am also working with data uploaded previously. My job run times have been very long recently and now whenever I start a job I get the message \u201cThis is a new dataset and not all of its data are available yet\u201d which I have never gotten before.\nPreviously, seeing \u201cThis job is waiting to run\u201d was common but I have not seen this new message before. Is this normal?\nEdit: Sorry I forgot to mention that I am running a makeblastdb tool, a Filter Fasta tool, as well as changing the datatype on uploaded data. All have long wait times."
    },
    {
      "role": "system",
      "text": "Hi @user\nBoth of these states mean that the job is queued. The dataset will be grey in color.\n\n\n\u201cThis is a new dataset and not all of its data are available yet\u201d\n\nFirst staging state\nTechnically, the job is having its metadata interpreted\nWhy? To sort the job into the appropriate cluster queue\nThis may happen so fast that you don\u2019t see it in the UI. Or, if the server is busy, this sub-step may take longer.\n\n\n\n\u201cThis job is waiting to run\u201d\n\nSecond staging state\nTechnically, the job has been successfully added to a cluster queue\nWhy? This is a \u201cready\u201d state, meaning it is now waiting until the target cluster has an available node\nQueued time can vary (grey dataset), as can execution time (yellow dataset)\n\n\n\nIf you think the job is \u201cstuck\u201d at either step: check the server status @link https://status.galaxyproject.org/ and ask here or in a Gitter chat to let us know about a potential new problem. General chat (any server): galaxyproject/Lobby - Gitter (@link https://gitter.im/galaxyproject/Lobby) and EU server chat: usegalaxy-eu/Lobby - Gitter (@link https://gitter.im/usegalaxy-eu/Lobby). The EU admin team may also reply more here at this post.\nDifferent tools require different computational resources\n\nLower computational needs == individual jobs process quicker and as a result nodes are freed up quicker (to process the next job in their queue)\nHigher computational needs == individual jobs process longer and as a result nodes are freed up at a slower pace\n\nOther factors\n\nEach Public Galaxy server is using its own cluster resources. There is variability in the cluster number/type/size.\nHow many people are using a service at any particular time and the types of jobs they are running both impact processing performance.\nHow many jobs you are concurrently running also impacts your own processing speed. Some of your jobs will run, then some of the other people\u2019s jobs, then more of yours, repeat.\n\nIf you have work that is large or time-sensitive, the public Galaxy servers may not be appropriate.\n\n\nSome people use multiple public Galaxy servers to distribute the work/data. One account at each is fine and expected.\n\n\nSome people decide to use their own Galaxy, for short or long-term needs. When running your own Galaxy, you control how data/job quotas and job prioritization is set up, plus what resources are attached.\n\n\nWays to use Galaxy: Galaxy Platform Directory: Servers, Clouds, and Deployable Resources - Galaxy Community Hub (@link https://galaxyproject.org/use)\n\n\nThe GVL Cloudman version is a single or multi-user choice and AWS offers grants. Amazon Web Services (AWS) - Galaxy Community Hub (@link https://galaxyproject.org/use/aws/) && AWS Programs for Research and Education (@link https://aws.amazon.com/grants/)\n\n\nThe AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. AnVIL - Galaxy Community Hub (@link https://galaxyproject.org/use/anvil/)\n\n\nIf you think the job is \u201cstuck\u201d at either queued step: check the server status @link https://status.galaxyproject.org/ and ask here or in a Gitter chat to let us know about a potential new problem. General chat (any server): galaxyproject/Lobby - Gitter (@link https://gitter.im/galaxyproject/Lobby) and EU server chat: usegalaxy-eu/Lobby - Gitter (@link https://gitter.im/usegalaxy-eu/Lobby). The EU admin team may also reply more here at this post.\nI added a few tags to this topic that point to more details."
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there anyway to download a history completely ?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can download the history to a file from here:\n\nScreenshot from 2021-12-01 11-57-481922\u00d71031 151 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/7/73729f86b133bd9a16edcaee07ea2a23a957dd0a.png)\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am new to Galaxy and would like to normalize FASTA dataset which I intend to use as a reference genome. I could not locate the tool NormalizeFASTA, please any idea. Thanks."
    },
    {
      "role": "user",
      "text": "Got it. Thanks"
    }
  ],
  [
    {
      "role": "user",
      "text": "When attempting to use the \u201cFunannotate predict annotation\u201d tool in the tool parameters it requires you to provide a funannotate database. However, when attempting to do this it shows there are \u201cno options avaliable\u201d and does not show any options when clicking the drop down menu. It will not let me run this tool since there is nothing selected, but the funannotate database does not allow me to choose an option\n\nimage1062\u00d7453 34.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/9bd9c2dafca69010436edc8a3d674a24fe101beb.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe \u201cFunannotate database\u201d input for this tool has some data restrictions, and is hosted at the @link http://UseGalaxy.eu server for now. Please try running the tool there instead.\nThanks for reporting the issue!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I am using STAR to do alignment on a collection of 24 RNA-seq samples. The jobs ran smoothly for 18 of them and are finished (it was quick). However the last 6 have been running for more than 12 hours now. I have run this alignment before and have never had this happen. Is there something up on the server side or is there a limit on number of concurrent jobs that one can submit? I have tried to do the alignment 2 time and once it was stuck after 10 and now after 18. Thanks!"
    },
    {
      "role": "system",
      "text": "I see your jobs seemingly running smoothly.\nWe have no problems on our side and since they are running you have not reached any limit.\nI can suggest you delete them and launch them again if you think they have been running for too long.\nCheers"
    }
  ],
  [
    {
      "role": "user",
      "text": "Does anyone know if the Jan 1 2020 deprecation of Python 2.7 will effect locally installed Galaxy? Also, does the software upgrade to Catalina have any bugs?"
    },
    {
      "role": "system",
      "text": "Not until Apple stop supporting Python 2.7, assuming you are using macOS.\nAny way it should be pretty easy to port your local Galaxy to Python 3 by replacing the virtual environment that Galaxy uses (by default .venv) with a new one created with virtualenv -p python3 .venv ."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all - I have jobs submitted on monday that are still in waiting mode.  Is this happening to others?  Maybe I have used up all my resources?  I do have other long running jobs that are currently executing."
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nHi all - I have jobs submitted on monday that are still in waiting mode. Is this happening to others?\nTry not to delete and rerun or all the new jobs end up at the back of the queue.\n@quote\nMaybe I have used up all my resources?\nThe quota space in the account does not impact when jobs run \u2026 unless the output puts you over quota.\n@quote\nI do have other long running jobs that are currently executing.\nThis will impact run times. Some of your jobs run, some from other people, repeat."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m getting a strange permission error when running galaxy through docker.\nI\u2019m running a docker instance of galaxy locally on my centos 7 machine and it\u2019s having trouble accessing a folder I created on our NAS.\nFrom docker, I can create folders on the NAS, so I don\u2019t think the permission error is from docker.\nI thought it might be an selinux issue, but I ran sudo setenforce 0 and the permission error persisted.\nI feel like I\u2019m poking around in the dark on this error. Do galaxy permissions change when running from the docker container? Is there a way to run galaxy from the command line instead of the gui?\nThanks for your help,\nJosh\nThe traceback is here, the file is trying to create a folder:\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 393, in <module>\n    main()\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 307, in main\n    var_dict = make_results_folders(input_path, output_path)\n  File \"/export/galaxy-central/tools/dev_staging_modules/create_folders_check_data.py\", line 33, in make_results_folders\n    utils.create_folder_hierarchy(output_subdirs, output_path)\n  File \"/export/galaxy-central/tools/dev_staging_modules/utils.py\", line 411, in create_folder_hierarchy\n    os.makedirs(directory)\n  File \"/tool_deps/_conda/lib/python3.7/os.py\", line 221, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/finkbeiner/imaging/smb-robodata/Josh/tmp_galaxy_testing/GXYTMP-03312021-SW-NSCLC-cas-Output/BackgroundCorrected'\n"
    },
    {
      "role": "user",
      "text": "Hi all,\nThe problem was a permission on our NAS."
    }
  ],
  [
    {
      "role": "user",
      "text": "Will running the tutorial on: Galaxy Installation with Ansible (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/ansible-galaxy/tutorial.html) i get an error in running the step with the ansible-galaxy role: \u201cgalaxyproject.galaxy\u201d\nError is:\nERROR: Could not find a version that satisfies the requirement attmap==0.12.11 (from versions: 0.1, 0.1.1, 0.1.2, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.12.2, 0.12.3, 0.12.4, 0.12.5, 0.12.6, 0.12.7, 0.12.8, 0.12.9, 0.12.10, 0.12.11, 0.13.0)\\\\nERROR: No matching distribution found for attmap==0.12.11\n\nDoes anyone else have this same problem?"
    },
    {
      "role": "user",
      "text": "Solved by using a higher galaxy_commit_id"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m conducting an RNA-Seq analysis on galaxy and I\u2019m trying to use limma-voom to identify differential gene expression. When I input both files, I get the following error:\nError in data.frame(sample = basename(filenamesIn), filename = filenamesIn,  :\nduplicate row.names: /galaxy-repl/main/files/033/345/dataset_33345502.dat\nExecution halted\nDoes anyone know why I would get this error message? My previous data is an output from kallisto and there are two columns per file I input: the transcripts ID and the est_counts. The transcript IDs are a bit odd looking and they look like : ENST00000631435.1. Perhaps this is the problem. I\u2019m unsure which headers limma-voom accepts. I\u2019m relatively new to galaxy.and any suggestions would be helpful.\nThank you!\nK"
    },
    {
      "role": "system",
      "text": "with my experience of this, you cant DEG with limma when using kallisto\ntry Deseq2 and in \u201cchoose of input data\u201d select \u201cTPM value\u201d and then selet kallisto and put your GTF file then execute.\ntry this i hope it works"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to use AnnotatemyIDs and EGSEA to visualize the pathway enrichment for my significant genes. Is there a way to add compatibility with the Sus Scrofa species? Thank you in advance!"
    },
    {
      "role": "system",
      "text": "Hi @user\nOnly a few species are supported with built-in indexes from the source.\nUse a reference annotation GTF input from the history instead. All UCSC genome builds with a gene annotation track will have these available in their Downloads area.\n@quote\nThehg38reference annotationGTFis now best sourced from UCSC. Not from the Table Browser but from theDownloads area here. There are a few Gene tracks to choose from. You can review what each of those represents then upload the selectedGTFto Galaxy with theUploadtool. Just copy and paste in the URL and leave all other settings at the default. The correct datatype will be assigned.\nThe path to the data is similar for all genome builds: https://hgdownload.soe.ucsc.edu/goldenPath/<database>/bigZips/genes/\n\nFor pig genomes, start here UCSC Genome Browser Downloads (@link https://hgdownload.soe.ucsc.edu/downloads.html#pig). The database/dbkey used at UCSC the same as you will see as the assigned \u201cdatabase\u201d to your outputs from upstream steps (BAM) in Galaxy. Be sure not to mix different builds in the same analysis or expect weird results!\n\nFAQ:\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation)\n\n\nGalaxy Training (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation)\nCollection of tutorials developed and maintained by the w...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI submitted 12 HISAT2 jobs a few hours ago. While two of them ran successfully within 10 minutes, the rest have not started running (they are all grey and \u201cwaiting to run\u201d). No other jobs are running.\nShould I keep waiting, or is there a problem on my end?\nThank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nAllow the jobs to stay queued. The jobs will run when resources become available again. Public servers utilized common resources shared with everyone else working at the same place; some of your jobs will run, then other people\u2019s, then more of yours, repeat.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have several jobs waiting to run in RNA STAR for already for almost 2 days.\nIs there any trouble with this tool?\nI am using @link http://usegalaxy.org server and my username vngo2.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nFor your case, the RNA-STAR jobs are completed and several downstream tools are now either queued or executing as expected.\nThe server was and still is very busy. The best strategy is to queue up jobs, then wait and allow them to execute as resources become available. The tool forms have an option to send an email notification \u2013 helpful when running batches. Workflows also have that notification option.\nTry to avoid deleting and rerunning unless you need to change or adjust inputs/parameters. I added a tag to your post that links to prior Q&A + FAQs that explain how the job queues work if interested.\n:slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi.\nHow can I extract uniquely and concordantly paired-end mapped reads (aligned concordantly exactly 1 time) from HISAT2?\nThanks"
    },
    {
      "role": "system",
      "text": "I think you want samtools view:\n@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8\ne.g.:\n\nimage.png392\u00d7799 41.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/719a917ce8f75d62078d2f6becbd2b4e27965119.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. New to Galaxy here. I submitted a few jobs about 72 hours ago, and they still haven\u2019t started running. Is this typical or did I make an error somewhere?"
    },
    {
      "role": "system",
      "text": "Update\nI took a look at one of your queued jobs (Blastp) \u2013 there are input problems that will cause the job to fail once it does run. You should fix these, then rerun.\n\nIncorrect datatype \u2013 both datasets are in fasta format, but one is labeled as fastq. FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#changing-the-datatype)\n\nBoth fasta datasets includes description content on the title line. This won\u2019t always lead to errors at the mapping step, but definitely can with many downstream tools. Standardizing the format at the start of an analysis is usually best. FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-very-large-fasta-datasets)\n\n\nIn one of your other histories, there is a queued Busco job that also has an input that is in fasta format but is labeled with a fastq datatype. Fix that as well, then rerun.\nThat same history has another fasta dataset that is uncompressed and with what appears to be the correct datatype assigned. However, it seems odd that some data is uncompressed and some is compressed for similar jobs. You should confirm if any of your data is actually compressed or not, and assign the correct datatype. Running QA tools can help catch format problems, example: FastQC. Galaxy will guess the datatype when data is uploaded, or you can use the redetect function after upload/manipulations, but this tends to work best with uncompressed data for most data formats (BAM is one exception).\nHow to check for input problems (before a job is run, or after once it fails): FAQ (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages)\nHope that helps!\n\nHi @user\nThe job queue for some tools has been very busy over the last few days at the @link http://UseGalaxy.org public Galaxy server. This includes mapping and other computationally intensive tools.\nThe best strategy is to leave queued jobs queued. Avoid deleting and rerunning, as all new jobs are placed back at the end of the queue extending the wait time."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to create a clustered heat map of Cut and Run data using deeptools (computeMatrix \u2192 plotHeatmap).\nOutside of galaxy the kmeans assignment is simple in plotHeatmap, but I cannot find a way to apply this parameter in the context of galaxy.  Any suggestions?"
    },
    {
      "role": "system",
      "text": "Hi @user\nHopefully you found the option already. It is nested on the Galaxy form under the advanced options.\nScreenshot\n\nplotheatmap-kmeans1310\u00d7460 52.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d20a932f7d4c49f957248eddea959247a5e836a4.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone,\nI\u2019m currently trying to perform RNA-Seq alignment with the help of RNA STAR. I performed the alignment and I would like to export t he Reads per gene file. My question is: If I click on the reads per gene file I get 3 columns without any IDS at the top (see attached 1st picture). Hence, what exactly do these columns represent? Furthermore, should I perform additionally feature counts and then export the file (2nd picture)? I have attached the outputs for both methods attached. Thank you very much for your help!\n\nrna star343\u00d71003 11.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/dfe40dc5e642f98009ff29c70536b7cfa8050a6e.png)\n\nfeature counts356\u00d71014 10.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/28fa6f7862c8bf27633607965ee23e579fb3e7a6.png)"
    },
    {
      "role": "system",
      "text": "Hello, STAR output reads per gene gives 4 columns, they are:\n\nGene ID\nCounts for unstranded RNA-seq\nCounts for the 1st read strand aligned with RNA\nCounts for the 2nd read strand aligned with RNA\n\nI found the details here,\nReference-based RNA-Seq data analysis (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html#counting-the-number-of-reads-per-annotated-gene)\ngo to part Counting the number \u2026, on the Hands-on: Choose Your Own Tutorial, choose STAR"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello ,\nI am using @link https://usegalaxy.eu/ and I have a problem with the tool intersect. I am a new user of Galaxy and I was following the instructions in this link @link https://galaxyproject.github.io/training-material/topics/introduction/tutorials/galaxy-intro-peaks2genes/tutorial.html . I was at the step where we intersect the peak region in BED format and  the genes and I had this error : _CONDOR_JOB_PIDS=\nGALAXY_WEBAPOLLO_EXT_URL=https://usegalaxy.eu/apollo\nGALAXY_CONFIG_SERVER_NAME=handler_main_4\nSHELL=/bin/bash\nTMPDIR=/data/dnb02/galaxy_db/job_working_directory/006/018/6018455/tmp\n_CONDOR_SCRATCH_DIR=/var/lib/condor/execute/dir_2285248.\nThank you in advance"
    },
    {
      "role": "system",
      "text": "Hi and welcome,\nwe are aware of this issue, have identified the cause and fixed it upstream in the Galaxy code base. However, you will have to wait for the next update of @link http://usegalaxy.eu before things will again work on the server as advertised.\nIn the meantime, you can try to switch the version of the tool to its previous one (0.0.1 instead of 1.0.0; click on the Versions button at the top of the tool interface to select it), which should be unaffected.\nHope that helps and sorry for the inconvenience."
    }
  ],
  [
    {
      "role": "user",
      "text": "I am getting error with whatever tool i use. The data files are provided by courseera, so there\u2019s no problem with that. See the error\nimage"
    },
    {
      "role": "system",
      "text": "Update: Fixed!\nIf you or anyone else runs into this problem, the solution is to either correct the metadata directly as described below or to rerun the impacted jobs.\nThanks to @system for the quick fix!\n\n@user\nThanks for reporting the issue. We noticed it as well.\nTry clicking on the link, go to the Datatypes tab, and \u201cRedetect\u201d the datatype. You can also reassign the datatype directly. This will need to be done before inputting the data to downstream tools.\nWe are working on a correction. More feedback soon.\nThanks again!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey there\u2026 I have a bacterial genome NGS reads please help me to assemble these reads for further process"
    },
    {
      "role": "system",
      "text": "Good day Waqas. I recommend using Torsten Seeman\u2019s Shovill tool - this is a wrapper around SPAdes and does a very good job with most microbial genomes. Also note that the Microbinfie Podcast discusses Microbial Genome Assembly in episodes 19 (@link https://soundcloud.com/microbinfie/microbinfies-assemble), @link https://soundcloud.com/microbinfie/20-assembly-read-healing and @link https://soundcloud.com/microbinfie/22-assembly-after-party. All of these are worth a listen.\nFinally there are some tutorials on the Galaxy Training portal on Assembly (@link https://training.galaxyproject.org/training-material/topics/assembly/) (although I would not use Velvet as used in their examples, Shovill is much better) and Annotation (@link https://training.galaxyproject.org/training-material/topics/genome-annotation/) with e.g. Prokka."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI sometimes use FreeBayes tool and it always works well but today I am waiting for hours and it says \u201cThis job is waiting to run\u201d, all other tools that I tried, worked normally. Is something wrong with FreeBayes tool?\nThank you for your help,\nMaja"
    },
    {
      "role": "system",
      "text": "Hi Maja,\n@quote\n\u201cThis job is waiting to run\u201d\nThis message means that the job is in the cluster queue. It will execute when resources are available. Avoid deleting/rerunning unless you already know there is something wrong with the original run (inputs, parameters) \u2013 otherwise, the new job is added back to the end of the queue, extending wait time.\n@quote\nall other tools that I tried, worked normally\nTools execute on different clusters that match the resources needed to process the work. It is expected that some tools will run faster than others. The clusters that can handle computationally intensive tools, like Freebayees, will often queue for a longer time than simpler tools.\nThe best advice is to allow the job to complete. If it fails, then you can start troubleshooting the inputs.\nFAQ: *How Jobs Execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nPrior Q&A about this same topic can be found with a search at this forum like these:\n@link https://help.galaxyproject.org/tag/queued\n@link https://help.galaxyproject.org/tag/queued-gray-datasets\nI also added the \u201cqueued-gray-datasets\u201d tag to your post, so you can click on that instead.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am trying to use featureCounts and keep getting this message: \u201cExecution of this dataset\u2019s job is paused because you were over your disk quota at the time it was ready to run\u201d. I am only using 3%.\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nJust to double check that the quota usage is fully updated based on recent data additions/purges, please try this:\n\nLog into Galaxy\nGo to User \u2192 Preferences \u2192 Storage Dashboard\nClick on the \u201cRefresh\u201d button, and allow that to fully process\nReview the updated quota usage, and delete then purge more data as needed to free up space for new work. @link https://training.galaxyproject.org/training-material/faqs/galaxy/#quotas-for-datasets-and-histories\n\n\nIf that is not enough, which server are you working at? Reply with the URL for a public Galaxy or describe if other. As needed, a moderator can start a direct message to share the account\u2019s email address in. Never post your email publicly, and administrators will never need or ask for your password.\nLet\u2019s start there :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m writing to you for an issue on vcftools merge tool. I\u2019m  trying to performe the merge of two vcf files with bcftools merge, and the tool seems to work; however, when open the resulting merged vfc file, I see the genomic positions, but NO the geniotypes and info related to the specific vcf files;\n\nDiapositiva11280\u00d7720 87.2 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d82d4a00fbc7902e8ad7d32f6d3cb79e72724930.jpeg)\nHow can I solve the the problem and have a merged corrected file?\nthank you in advance for your help"
    },
    {
      "role": "system",
      "text": "Hi @user,\nsorry for that question, but you have tried to scroll over to the right in that window. Have to ask this because I do not understand what you\u2019re trying to show with that screenshot.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I am trying to download a large collection of BAM files through the command line. I have an API key, which is the last part of the download link needed to be specified; however I am lost on where to find the download link for the entire collection (links from each individual dataset are available). The \u201cdisk\u201d button does not retrieve/or is associated with a link (see screenshot)\nThanks in advanced,\nAlejandro\n\nimage572\u00d71168 106 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/2/26cdcfef5c0a0dd38fbd0d33282e018aef021bc3.jpeg)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThanks for asking about this. We knew that the functionality was dropped in the new/beta history view but weren\u2019t sure if anyone was actually using it or not. Sounds like it is!\nI created an issue ticket to add in the function. Details are here along with available workarounds: New/beta history: add functionality to enable capture of a collection's URL \u00b7 Issue #14896 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/14896)\nPlease feel free to comment/upvote that ticket plus you can follow it for updates :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I did not find methylKit(R tool ) in the galaxy. Is it present or not?"
    },
    {
      "role": "system",
      "text": "Hi @user\nI could not find this tool on usegalaxy.* servers. From a quick look, the last update of the wrapper was released in 2016. I also could not find any GTN tutorial with the tool, but I have not checked hard.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have a question about the data normalization in DESeq2.\nI have a matrix of gene counts from 30 samples belonging to the same group. I don\u2019t want to get DE genes obviously. I just need to normalize these counts. Is there a way to do this in DESeq2 without the need to specify two conditions?\nThanks\nBarbara"
    },
    {
      "role": "system",
      "text": "@quote\nIs there a way to do this in DESeq2 without the need to specify two conditions?\nThe tool needs at least two conditions each with replicates or there is nothing to compare for generated statistical tests.\nDetails direct from the original tool authors are in this post and related posts at their forum: How to perform DEseq2 on datasets without replicates or merged replicates (@link https://support.bioconductor.org/p/126795/#126851)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nWhere can i see the list of all supported data types in galaxy? I honestly searched this on plenty of galaxy resources but found exactly nothing.\nSpecifically i got \u201cWARNING:galaxy.model:Datatype class not found for extension \u2018tar.gz\u2019\u201d when i wanted to save many output files to tar.gz archive.\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can find the supported datatypes here galaxy/datatypes_conf.xml.sample at dev \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/blob/dev/lib/galaxy/config/sample/datatypes_conf.xml.sample)\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there, I have a list of compounds for docking. How do I concatenate them all together (in SMILES) in a single file? Or else I have to dock one by one? Thank you in advance !!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nHave you seen our introductory tutorial on protein-ligand docking? Protein-ligand docking (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/cheminformatics/tutorial.html)\nIt explains how to dock a list of compounds to a protein, starting from a list of SMILES, like you describe.\nTo concatenate multiple SMILES datasets, try searching for a tool like Concatenate datasets.\nBest wishes,\nSimon"
    }
  ],
  [
    {
      "role": "user",
      "text": "Have you experienced this error in galaxy?:\nFatal error: An undefined error occurred, please check your input carefully and contact your administrator.\nestimating size factors\nError in estimateSizeFactorsForMatrix(counts(object), locfunc = locfunc,  :\n  every gene contains at least one zero, cannot compute log geometric means\nCalls: DESeq ... estimateSizeFactors -> .local -> estimateSizeFactorsForMatrix\n"
    },
    {
      "role": "user",
      "text": "Every gene has at least one zero for at least one sample of your four samples. The thing is that DESeq2 calculates a geometric mean for every gene over every sample. The geometric mean will be zero if at least one entry is zero, i.e., if one of the four samples has a zero count it will be zero and cannot be used. The geometric means are later important for the DESeq2 normalisation.\nExample:\n\nID S1 S2 S3 S4\nGene_1 1 22 3 0\nGene_2 30 0 12 5\nGene_3 0 0 1 6\n\nAll Genes above are invalid and cannot be used by DESeq2 because at least one entry is zero.\nWhat you need is something like:\n\nID S1 S2 S3 S4\nGene_1 1 22 3 1\nGene_2 30 1 12 5\nGene_3 0 0 1 6\n\nThat would be fine since Gene_1 and Gene_2 can be used later for the normalisation. Gene_3 will be filtered out by DESeq2.\nSolution:\n\nYou include more data\nYou add a pseudo-count of 1 to all counts. Just add 1 to every entry. Thats technically not cheating because you treat every entry and DESeq takes the log of the data anyway. If you do then make sure you remove genes beforehand that have zeros for all samples, e.g., Remove Gene_1 in this example:\n\n\nID S1 S2 S3 S4\nGene_1 0 0 0 0\nGene_2 30 0 12 5\nGene_3 0 0 1 6\n\n\nGalaxy (DESeq) has an option for that error as well. Look under  (Optional) Method for estimateSizeFactors. These methods are more sophisticated. Choose either \u2018poscounts\u2019 or \u2018iterate\u2019. Please  read the documentation in the tool description  for both these models, before you use one of them.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "i am trying to do a simple tutorial to learn how to use galaxy but after i send the query to galaxy, nothing happens - no yellow/green. is there a usual time frame/estimate for this?"
    },
    {
      "role": "system",
      "text": "We are currently having issues with running jobs, we are working on a resolution. Sorry for the inconvenience.\nedit: I think the problem has been fixed, the jobs will start running shortly."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear specialists,\nGTN material for MaxQuant for TMT-MS does not work. MaxQuant repeatedly fails with a fatal error; MaxQuant and MSstats for the analysis of TMT data (@link https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/maxquant-msstats-tmt/tutorial.html)\nFatal error: Exit code 134 ()\nUnhandled Exception: System.Exception: Exception during execution of external process: 3803082\nat QueueingSystem.WorkDispatcher.ProcessSingleRunExternalProcess(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.DoWork(Int32 taskIndex, Int32 threadIndex)\nat QueueingSystem.WorkDispatcher.Work(Object threadIndex)\nat System.Threading.Thread.ThreadMain_ParameterizedThreadStart(Object parameter)\nat System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)\n\u2014 End of stack trace from previous location where exception was thrown \u2014\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\n/usr/local/bin/maxquant: line 4: 3795703 Aborted                 (core dumped) dotnet $DIR/MaxQuantCmd.exe $@\nI would like to kindly invite you to resolve the issue and provide feed-back so I can use the GTN material and also analyse my TMT data.\nPlease find the history link below where MaxQuant tool failed with the GTN version and its latest version;\n@link https://usegalaxy.eu/u/qcsciphi/h/maxquant-and-msstats-for-the-analysis-of-tmt-data-kmt9-ko-inhibits-a549-proliferation-pmid32095117\nI already sent a bug report from the tool with a shared link to the history with failing datasets.\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hello @user\nThe tutorial is still a work in progress.\nYou can try running through it at @link http://UseGalaxy.eu instead (where the tutorial authors are also working) but it may still have a few problems even there until it is finalized (sometime before May)."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear users,\nI\u2019m new at using Galaxy, colleagues have used it and showed it to me.\nI wanted to start with a very small and simple data set to see if it works for me.\nI have made a .fasta file from 34 NovaSeq reads and want to run it in the immune repertoire pipeline.\nHowever it keeps coming back with the same message;\n\u201c|Sample 1 of patient EVD is not a zip file so assuming fasta/fastq, using igBLASTn|\u201d\nIs anyone experiencing this same error and what do I need to change to avoid it and get the data?\nRegards, Renate LUMC Hematology"
    },
    {
      "role": "system",
      "text": "Context (worked out in a direct email):\nGalaxy Server: ARGalaxy \u2013 Erasmus MC (@link https://bioinf-galaxian.erasmusmc.nl/argalaxy)\nContact information is on the homepage of the server.\nContact information is listed also under @link https://galaxyproject.org/use at @link https://galaxyproject.org/use/argalaxy/\nReport the problem and they should be able to help.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to upload a large GTF file of 1.5G which is actually a combination of two smaller GTF files (lets call them file A and file B) which can be seperately uploaded without problems but the combined file is too large. I want to use this file in htseq-count. I was wondering if it is possible to get the same results as the combined file by running htseq count seperately for file A and file B and if not, if it is possible in another way to upload my large file to htseq count.\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi,\nplease use our FTP service to upload large files\n@link https://galaxyproject.eu/ftp\nCheers"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I am trying to use HTseq count for my data. I am using the bam file and a gff file of the human reference genome from ncbi. I have been getting this error. Any help on how to go about this?\n[bam_sort_core] merging from 64 files and 1 in-memory blocks\u2026\nError occured when processing GFF file (line 17484 of file /scratch/user/galaxy/maroon_galaxy/objects/2/5/9/dataset_259335c3-5a5c-4d34-8eef-876dfc014a1d.dat):\nunmatched quote\n[Exception type: ValueError, raised in _HTSeq.pyx:1576]"
    },
    {
      "role": "system",
      "text": "Hi @user\nit is hard to identify the issue without looking into data and the log file.\nHave you mapped reads in Galaxy? If yes, have you used \u201cbuilt-in\u201d reference genome in the mapping step? If yes, maybe try featureCounts and change Genome annotation file to featureCounts built-in. Try another annotation file, preferably from GenCode. Make sure you use the annotation file for the same version of genome assembly, as in the mapping step.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,  I am trying to convert a bedgraph file that used the mm10 genome assembly to a mm9.  I tried the liftover tool and I do not see a choice to convert to mm9.   I do not see any mmX assemblies.  I must be doing something wrong (or multiple things!).   Can someone point me in the right direction?\nThank you,\nScott"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI assume that you are using the @link http://usegalaxy.org instance but at the moment this genome assembly is not available there. However, you can find the mm9 genome assembly in the @link http://usegalaxy.eu instance."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi! I am trying to run cutadapt on paired-end reads, but the jobs stay on waiting to run for a long time (longer than 15 minutes). I was running the same files as single-end reads earlier and it worked pretty fast, so I\u2019m confused about why it\u2019s not working through paired-end reads. Any help would be greatly appreciated! Thank you!"
    },
    {
      "role": "user",
      "text": "Update: It\u2019s now working! It was most likely just having to wait for the server! :smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, when I try to use hicbuildmatrix to generate a matrix, a bed file is required (please find attached the pictures). Can I skip this step? The tutorial does not include this part.\nbed file needed1224\u00d7648 12.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/b/bd5b4e305576870fb533835ddcef7032d3f38faa.png)"
    },
    {
      "role": "system",
      "text": "Hi @user,\nyou can skip this step by selecting the tool version 3.4.0 (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicbuildmatrix/hicexplorer_hicbuildmatrix/3.4.3.0).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI run the job 11ac94870d0bb33a69a8e2a8dad79bbc (simple compute) and it doesn`t finish (yellow).\nRerunning the job finished in minutes. Can it be that there is a problem with the node used?\n\nJob Information\n\n\n\n\nGalaxy Tool ID:\ntoolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.0\n\n\n\n\nCommand Line\npython \u2018/opt/galaxy/shed_tools/toolshed.g2.bx.psu.edu/repos/devteam/column_maker/6595517c2dd8/column_maker/column_maker.py\u2019 --column-types str,int,int,str,int,int,float,float,int,int,int --file \u2018/data/jwd05e/main/059/782/59782338/configs/tmp39wdjmom\u2019 --fail-on-non-existent-columns --fail-on-non-computable \u2018/data/dnb08/galaxy_db/files/f/9/d/dataset_f9d935ca-f5ca-4a27-924e-040d951c96b5.dat\u2019 \u2018/data/jwd05e/main/059/782/59782338/outputs/galaxy_dataset_4d9ef16f-da0c-4030-899c-05ed225a86b2.dat\u2019\n\n\nTool Standard Output\nempty\n\n\nTool Standard Error\nempty\n\n\nTool Exit Code:\n\n\n\nJob API ID:\n11ac94870d0bb33a69a8e2a8dad79bbc\n\n\n\nThanks\nRalf"
    },
    {
      "role": "system",
      "text": "Welcome, @user\n@quote\nCan it be that there is a problem with the node used?\nPossibly, but glad the rerun worked!\nOther potential differences involve the input dataset and parameters. Or, just timing. The public servers run a lot of jobs, and involve many clusters and nodes. By chance some nodes might be executing more than one job at any particular time, or have different base-line resources.\nYou probably know this already, but for others reading: To visually compare two tool runs: click on the Dataset Details :information_source: to review the Tool Parameters in a summary format. Find that right below what you pasted back. That summary is sometimes clearer than reviewing a tool rerun forms :repeat: since nested parameters might become unexpanded, hiding some of the original settings.\nIf this repeats or you think more is going on, please share more details. Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)\n\n\nReferences:\n\nTool: Compute on rows (@link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.0)\n\nHow-to Tutorial: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html#computing)\n\nIn-context Tutorials: Galaxy Training! (@link https://training.galaxyproject.org/training-material/by-tool/devteam/Add_a_column1.html)\n\n\ncc @system @system"
    }
  ],
  [
    {
      "role": "user",
      "text": "Is there a way to merge xml/blastxml outputs of blastx?\nI have two sets of blast results for the same cds dataset, one against the Swissprot database and another against the NCBI nr database. (I would have done them together, but I wanted to subset the nr database so it wouldn\u2019t have such a long run time.)\nThe two outputs don\u2019t overlap. I\u2019m searching for a way to combine them into a single xml/blastxml file in which the descriptions missing from one blast output can be replaced by those in the other.\nOn Biostars, I think I found a very old solution for python and another for java, but I wondered if there was now a simpler way on Galaxy. Any ideas would be very much appreciated."
    },
    {
      "role": "user",
      "text": "In case anyone is confronted with this in the future, I found an easy solution to this.\nOn Blast2GO (or OmicsBox), I loaded my sequences and my blast results against Swissprot. Then I subset the dataframe to exclude the ones without blast results. I then selected all remaining rows and extracted them to a new dataframe. So I had a dataframe with only sequences accompanied by Swissprot hits, which I saved (.b2g or .box). I then started a new project by again loading all my sequences and the nr blast results, and saved that as well. Finally, I merged the two together by adding the sequence, GO description, and blast results for nr-blasted data to the Swissprot-blasted data if the data were missing. This procedure left me with all my sequences, their Swissprot results, and, when those were missing, the nr results, plus the sequences that had no results for either. (Later I also merged this with results from Interproscan.)\nI saw an old post that indicated that there may be a wrapper on Galaxy that merges blast xml outputs, but I haven\u2019t yet found it. Any other ideas are welcomed!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, my disk quota reports it\u2019s at 100 % capacity when I only have less than 20 GB out of the total 250 GB occupied. I have deleted files to free up more room but it has no effect (I have also made sure to \u201cpermanently delete\u201d unused files on the account storage page). Is there a way I can solve this problem and avoid it in the future?\nThanks for your help,\nJulia"
    },
    {
      "role": "system",
      "text": "If you click on your quota meter and go to the \u201cStorage Dashboard\u201d you will find a few options that can help you.  The first thing I\u2019d try is to request a quota refresh, which is the main button you see on that interface.  This will recalculate your current usage, and might resolve your issue.\nA direct link to that on @link http://usegalaxy.org is: @link https://usegalaxy.org/storage"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nPer base sequence GC content960\u00d7730 111 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/1/1a968f2e31c4eb63edf3d93d079884805259c787.jpeg)\n\nPer base sequence content885\u00d7656 76.9 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0c091a5df38da9c119efcb3ab8e0bb54e4c1cf6a.jpeg)\nDear All,\nI am new to galaxy server and trying to analyze the ChI-seq data files. I run FastQC analysis for chipped fastq file and found per base sequence content error and warning for per base sequence GC content (two peaks in the graph). I have attached both the graphs with this post.\nI wanted to understand what could be the cause for these errors and how can we resolve this?\nYour help is highly appreciated.\nBest,\nShashi"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nThe FastQC tool form has a reference link at the bottom to the tool authors website. They host a range of resources, including example graphs with descriptions about what they mean and considerations for different sequencing types.\nThis Galaxy tutorial has some parts based on that same data plus related content. Quality Control (@link https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html)\nThe GTN also hosts tutorials specific for ChIP-seq. Use the search at the very top to search with keywords to find those. Some include extra QA help.\nPlease give those a review.\nIf you need more help:\n\nIf this is raw data, try running the data through a trimming program, then rerun this tool.\nDoes that also report odd results in these or other modules?\nDo any other samples from the same source have odd results?\nIs there anything special about the data? You could just list out some features: species, source, any prior manipulations (including merging fastq files together).\n\nYou can post back more screenshots or sometimes better, post back a shared history link. A double peak indicates there are two different \u201cpools\u201d of data in the sample. Recognized contamination would show up in other sections of the report. Your job didn\u2019t fail but this FAQ explains how to review the job details and share back a history link: Troubleshooting errors (@link https://training.galaxyproject.org/training-material/faqs/galaxy/analysis_troubleshooting.html)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all,\nGalaxy server cannot be accessed in both Chrome and Firefox.\nIt is now 6 days that the problem exists.\nDoes anybody face with the same issue?\nHow can I solve this problem?\nRegards\nNazanin"
    },
    {
      "role": "system",
      "text": "I can access them. Maybe this page can also help @link https://status.galaxyproject.org/"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am receiving a server for our laboratory with 128 GB of memory to work with prokaryotes.\nI am thinking of installing a Galaxy environment in it with a SLURM queue to share with other users, with the possibility of increasing the infrastructure with the addition of new nodes in the future.\nCan I install the local GVL project, other than on a Cloud server? Could you install on an infrastructure with Rancher for your management? Is there a tutorial for this procedure?"
    },
    {
      "role": "system",
      "text": "Hey @user!\nIt is definitely possible, although fyi by default Galaxy within the GVL uses the Kubernetes Job runner rather than Slurm (although hybrid setups, running on k8s but dispatching to Slurm is definitely possible). Running the GVL locally will need some tinkering to the out-of-the box configuration which is intended for the cloud, but in theory the biggest blocker is just figuring out a proper StorageClass for the Kubernetes cluster. Traditionally for a local setup we\u2019ve used hostPath storageClass which just uses the local host storage. I haven\u2019t set it up locally in a few months, but can try it again soon and maybe add a bit of documentation for future cases.\nIf you have a lot of experience in this realm, you can likely figure it out on your own. The entry point for the boot mechanism is the @link https://github.com/CloudVE/cloudman-boot docker image which sets up Rancher and installs the base GVL infrastructure (CloudMan). If you are just starting with K8S, it might be a bit harder to find the right configuration on your own, but we could schedule a time to do an interactive session over Zoom to help you get started.\nFeel free to email at almahmoud@jhu.edu to set-up a time in the next few weeks, or send me a message on Gitter/Slack (@system) and I\u2019ll be happy to help however i can and get other people involved to help as well."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello All,\nI am relatively new to this sort of analysis. I have paired-end reads from Oncopeltus tissue. I ran these under Salmon-quant to generate TPMs. I then used these as input files into the newest version of DESEQ2 using the gff3 annotation file from the oncopeltus genome. The program runs, but returns outputs with nothing in them.\nPlease help!\nA"
    },
    {
      "role": "system",
      "text": "There is much Q&A about these tools at this forum. Please review what has worked for others. Focus on the reference data first \u2013 both content and format \u2013 since a mismatch between identifiers is the most common root issue.\nI added a few tags to your topic that will find these, plus you can also just search with tool names, datatypes, error messages, etc to get started."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi\nI am using trinity and the error message is:\n\"Fatal error: Exit code 1 ()\ncp: -r not specified; omitting directory \u2018trinity_out_dir/Trinity.tmp.fasta.salmon.idx\u2019\n/ocean/projects/mcb140028p/xcgalaxy/main/staging/50922182/.cvmfsexec/mountrepo: line 70: cd: /ocean/projects/mcb140028p/xcgalaxy/main/staging/50922182/.cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/config.d: No such file or directory\"\nI don\u2019t understand what is the problem here. The fastq files are OK and there are \u201c1\u201d and \u201c2\u201d in the names of sequences.\nPlease help."
    },
    {
      "role": "system",
      "text": "Hello @user\nThat type of stdout error can include problems with read content. There is much QA at this forum about common solutions. Please give that a review first, and if you are stuck later, share back more details and we can try to help more.\nThis post from yesterday lists out the exact information we will need to help you as well. The stderr at a minimum, a copy/paste of the full job information page report if the stderr also does not include any meaningful information. Make sure all input datasets and both logs are expanded, or we will need to ask you for that again. An error occurred when I used Trinity to analysis my RNA-seq data (clean paired-end fq.gz files) - #4 by jennaj (@link https://help.galaxyproject.org/t/an-error-occurred-when-i-used-trinity-to-analysis-my-rna-seq-data-clean-paired-end-fq-gz-files/10154/4)\nLet\u2019s start there, thanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hey everyone,\ncurrently I am struggling with aligning my seq data to my reference sequence. The annotation file hasn\u2019t any exons, just CDS, gene and region and therefore I can\u2019t use STAR aligner. Are there any possibilities to add --sjdbGTFfeatureExon to change the feature of the annotation file??\nBest,\nBenedikt"
    },
    {
      "role": "system",
      "text": "Hi @user\nmaybe consider mapping reads without gene annotation and count reads mapped to genes using featureCounts or htseq-count. Do you work with bacterial data? In this case you probably don\u2019t need gapped aligner.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nIs there a tool implemented in Galaxy or elsewhere to convert a 600Mb Fasta alignment file to Nexus?\nThank you in advance,\nAlex"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe seqret tool allows to carry out such conversion."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI was able to successfully cluster scRNAseq data from this recent paper \" Single-cell analyses define a continuum of cell state and composition changes in the malignant transformation of polyps to colorectal cancer\" in which I was able to identify several different colonic epithelial cell clusters of interest. I considered increasing the resolution of the initial clustering to get more specific epithelial subgroups, but because the annotation of clusters must be done manually, I found it difficult to label the large number of clusters that resulted from the increased resolution. Because I am only interested in further investigating the epithelial clusters and do not wish to look into smaller groups of stromal or immune cells, I was wondering if there was a way to isolate and interrogate just certain clusters from the original dataset and perform downstream subsetting on those specific clusters. Hope that makes sense!"
    },
    {
      "role": "system",
      "text": "Hi @user\nHopefully you have already seen the Galaxy scRNA-seq tutorials, but if not find them here:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\n\n\nGalaxy Training: Transcriptomics (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nThis specific tutorial might be enough\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-scanpy-pbmc3k/tutorial.html)\n\n\nGalaxy Training: Clustering 3K PBMCs with Scanpy (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-scanpy-pbmc3k/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have tried uploading several .bam datafiles, but for some reason they cannot be detected as the bam datatype (and therefore not used in my workflow). I\u2019ve tried manually setting the datatype to bam both before and after uploading the files. But it gives me the error \u2018An error occurred setting the metadata for this dataset. Set it manually or retry auto-detection\u2019. Both of these options gave a similar error. The datasets look like the image. I have uploaded .bam files before and it was never an issue. The only difference is that these bamfiles were generated from .cram files. I\u2019m quite novel to working with these datatypes, so hope I\u2019ve given all the information needed to help me with this issue. How can I make sure they are detected as proper bamfiles?\nThanks a lot for the help!\n\nimage574\u00d7730 55.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/00cf2bc319781fce0ad718acafbf5db41925c97e.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nI have uploaded .bam files before and it was never an issue. The only difference is that these bamfiles were generated from .cram files.\nTry this: check the file content before Upload to Galaxy. Try converting bam to sam format with Samtools. You could also try sorting the bam. If it fails for you locally, then it would fail in Galaxy.\nOr, try this: directly Upload the cram data to Galaxy. Most tools do not directly use cram format, but you can convert cram to bam in Galaxy with the tool Samtools view.\nFor either, try letting the Upload tool auto detect the format first. If that doesn\u2019t work, you can assign the datatype instead (during Upload \u2013 not sure if you tried that already for the bam or not).\nI\u2019m also not exactly sure how good we are at auto detecting cram format right now\u2026 but I\u2019ll test that later today or tomorrow since am now curious. If any usage deviates from the above \u2013 I\u2019ll post back.\nIt would be best to do this at the @link http://UseGalaxy.org or @link http://UseGalaxy.org.au server right now. The @link http://UseGalaxy.eu server is undergoing some admin changes (@link https://help.galaxyproject.org/t/usegalaxy-eu-unexpected-server-behavior-may-2022/8062) this week, and some odd behavior is expected until the banner clears. It is easier to troubleshoot without complicating factors.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Admin\nI would like analyse nucleolar localization sequences in my proteome dataset. May I request you to add the  \u201cNucleolar localization sequence Detector (NoD)\u201d tool in the galaxy main webpage @link https://usegalaxy.org/.\nThanking you\nSurajit"
    },
    {
      "role": "system",
      "text": "Hi @user,\na PR (@link https://github.com/galaxyproject/usegalaxy-tools/pull/547) has been opened in order to install this tool. It will be available within 24 hours.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I have 3 iterations of a library, I need to merge them. How to make?"
    },
    {
      "role": "system",
      "text": "Hi @user\nIs the goal to process all of the samples together? If yes, using dataset collections would be a good idea.\nGTN Tutorials: Search Tutorials (@link https://training.galaxyproject.org/training-material/search?query=collection)\nIf you are trying to do something else, please provide more details.\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I can\u2019t currently access @link http://usegalaxy.org because it appears that the HSTS certificate has expired. Wanted to make sure the admins know about this.\n\nScreenshot 2022-08-12 1727591920\u00d7923 137 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/fb04cfe0aaf6843ae33627b645a940f4af835e1a.jpeg)"
    },
    {
      "role": "system",
      "text": "Thanks for reporting this; the error should be resolved now."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I would like to share multiple histories with other users on my instance, what is the best way of doing that?\nI have already try Sharing/Publishing the History and moving some of datasets into a common shared data library but it is taking ages. Thanks, Kate"
    },
    {
      "role": "system",
      "text": "The best way is to publish them, you do not need to move the datasets to data library, when your users import the history no data is being hard copied. The datasets in the history just must have proper permissions so others can read it. Galaxy 19.01 (should be released in few weeks) has a convenience button to ensure all members of history are published as well."
    }
  ],
  [
    {
      "role": "user",
      "text": "is there any api for managing users in galaxy?\ncreating editing and deleting users \u2026"
    },
    {
      "role": "system",
      "text": "Yes, you can either use the Galaxy API directly or more easily with BioBlend: @link https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#module-bioblend.galaxy.users"
    }
  ],
  [
    {
      "role": "user",
      "text": "\nmacrogen pipeline704\u00d7564 67 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/0/0af09689b01a2e4e4a139888f82ad043f24c3c12.jpeg)\nCan anyone assist in recreating this here or designing a pipeline that would work? What did you use instead of the GATK toolkit?"
    },
    {
      "role": "system",
      "text": "Hi @user, try FreeBayes for variant calling. Check GTN tutorials Galaxy Training! (@link https://training.galaxyproject.org/training-material/topics/variant-analysis/)\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to use featureCounts on an unpaired collection of 9 bam files in my history. For the \u201cGene annotation file\u201d section, I selected \u201cfeatureCounts built-in\u201d. However, I can\u2019t select the genome build I want (mm10).\nHow is this possible? The menu just doesn\u2019t pop down. When I imported these BAM files, I did not specific their build at the time, in the the \u201cGenome (set all):\u201d setting. Is that why?\nThanks"
    },
    {
      "role": "system",
      "text": "If your genome is either hg38, hg19, mm10, or mm9 -> use featureCounts built-in option in Gene annotation file. You may need to set genome build for your BAM files (in the image below it is set to hg19. If it is not set you will see ?)\n06%20AM"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019m trying to use the \u2018Relabel list identifiers from contents of a file\u2019 function on a collection of paired-end reads.\nHowever when I try to do this I get the following error message:\nThe server could not complete the request. Please contact the Galaxy Team if this error persists. Error executing tool: Invalid new colleciton identifier [\u201cErthyrocytic stages (3/3) Yeoh\u201d]\n{\n\u201ctool_id\u201d: \u201cRELABEL_FROM_FILE\u201d,\n\u201ctool_version\u201d: \u201c1.0.0\u201d,\n\u201cinputs\u201d: {\n\u201cinput\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chdca\u201d,\n\u201ctags\u201d: [],\n\u201chid\u201d: 149,\n\u201cid\u201d: \u201c3bcdb03089092049\u201d,\n\u201cname\u201d: \u201cYeoh_pairedenddatalist\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201chow|how_select\u201d: \u201ctxt\u201d,\n\u201chow|labels\u201d: {\n\u201cvalues\u201d: [\n{\n\u201csrc\u201d: \u201chda\u201d,\n\u201cname\u201d: \u201cyeoh_relabel\u201d,\n\u201ctags\u201d: [],\n\u201ckeep\u201d: false,\n\u201chid\u201d: 259,\n\u201cid\u201d: \u201cbbd44e69cb8906b5b8d1bfcf6de3b443\u201d\n}\n],\n\u201cbatch\u201d: false\n},\n\u201chow|strict\u201d: \u201cfalse\u201d\n}\n}\nMy relabelling text file is simply:\n\u201cErthyrocytic stages (3/3) Yeoh\u201d\n\u201cErthyrocytic stages (2/3) Yeoh\u201d\n\u201cErthyrocytic stages (1/3) Yeoh\u201d\n\u201cMale gametocytes (3/3) Yeoh\u201d\n\u201cMale gametocytes (2/3) Yeoh\u201d\n\u201cMale gametocytes (1/3) Yeoh\u201d\n\u201cFemale gametocytes (3/3) Yeoh\u201d\n\u201cFemale gametocytes (2/3) Yeoh\u201d\n\u201cFemale gametocytes (1/3) Yeoh\u201d\nand I\u2019m using it on a list of paired data collections (length = 9). I don\u2019t understand from the error message what isn\u2019t working, why is the identifier illegal?\nHope that this is enough information to debug.\nMany thanks,\nTim."
    },
    {
      "role": "system",
      "text": "These characters are causing problems: ( ) /\nAlso, avoid whitespace (tabs, spaces).\nTry using only these characters in your new identifier:\n\nAlphanumeric: A-Z a-z 0-9\n\nUnderscores: _\n\nDashes: -\n\n\nIf you are familiar with regular expressions, this is the test that is raising the error:\nFrom lines 2781-2783 in https://github.com/galaxyproject/galaxy/blob/10c2f9cbc286477550e708d65bcd1381857dca15/lib/galaxy/tools/__init__.py\nfor key in new_elements.keys():\n        if not re.match(r\"^[\\w\\-_]+$\", key):\n            raise Exception(\"Invalid new colleciton identifier [%s]\" % key)"
    }
  ],
  [
    {
      "role": "user",
      "text": "how do i put a tool in my favorite part which GALAXY adding recently ?"
    },
    {
      "role": "system",
      "text": "Every tool should have a button next to versions/options. We will include it in the release notes once they are out.\n\nGalaxy.png916\u00d7238 29.6 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/1X/ab4970f17091bb630494f8673315ed4a1c709202.png)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I run a small private Galaxy 18.04 server. Now, I\u2019m trying to install a self-developed tool, which reads a paired collection and produces three single collections. That is, every pair of input datasets will generate three datasets on the output.\nWhat should be an xml definition for this case?\nThanks for helping this newbie Galaxy user! :slightly_smiling_face:\n======\nThis is my present xml file\n<tool id=\u201curgel_1\u201d name=\u201cURGEL 1 Cutoff\u201d version=\u201c0.1.0\u201d>\n<description>to consider genes as up-regulated, from a list of differential expression values</description>\n<command>\n<![CDATA[perl $__tool_directory__/URGEL1.pl\n#if $unicolista.choice == \u2018Par \u00danico\u2019:\n\u2018$tumoral\u2019 \u2018$controle\u2019\n#else:\n\u2018$tumconlist.forward\u2019 \u2018$tumconlist.reverse\u2019\n#end if\n]]>\n</command>\n<inputs>\n<conditional name=\u201cunicolista\u201d>\n<param name=\u201cchoice\u201d type=\u201cselect\u201d label=\u201cSelect Par \u00danico or Lista de Pares\u201d >\n<option value=\u201csingle_pair\u201d>Par \u00danico\n<option value=\u201cpaired_coll\u201d>Lista de Pares\n</param>\n<when value=\u201csingle_pair\u201d>\n<param name=\u201ctumoral\u201d type=\u201cdata\u201d format=\u201ctxt\u201d label=\u201cTumoral data file\u201d />\n<param name=\u201ccontrole\u201d type=\u201cdata\u201d format=\u201ctxt\u201d label=\u201cControl data file\u201d />\n</when>\n<when value=\u201cpaired_coll\u201d>\n<param name=\u201ctumconlist\u201d type=\u201cdata_collection\u201d collection_type=\u201cpaired\u201d format=\u201ctxt\u201d\nlabel=\u201cTumoral x Control pair list\u201d />\n</when>\n</conditional>\n</inputs>\n<outputs>\n<collection name=\u201cthrsh\u201d type=\u201clist\u201d label=\u201cthr {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"\" directory=\u201cthr\u201d visible=\u201ctrue\u201d />\n</collection>\n<collection name=\u201cdiffx\u201d type=\u201clist\u201d label=\u201cdif {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"\" directory=\u201cdif\u201d visible=\u201ctrue\u201d />\n</collection>\n<collection name=\u201cupreg\u201d type=\u201clist\u201d label=\u201cupr {tooln.name} on {on_string}\u201d >\n<discover_datasets pattern=\"*\" directory=\u201cupr\u201d visible=\u201ctrue\u201d />\n</collection>\n<data format=\u201ctxt\u201d name=\u201carqLog\u201d label=\u201cURGEL1 Log\u201d />\n</outputs>\n<help>\nOriginal: UpRegulatedGeneExtractorList.pl\nThis tool calculates the expression cutoff to consider genes as up-regulated, from a list of genes\nwith corresponding differential expression values.\n</help>\n</tool>\n======\nActually, the tool generates FOUR empty lists (one for the log file, which was intended to be a single file)."
    },
    {
      "role": "user",
      "text": "After digging the documentation (Galaxy Tool XML File), I was happy to find the solution by myself. My XML file now looks like this:\n====\n<tool id=\u201ctoolid\u201d name=\u201ctoolname\u201d version=\u201c0.1.0\u201d>\n<description>Description\u2026</description>\n<command>\n<![CDATA[perl $__tool_directory__/tool.pl\n$input.forward\n$input.reverse\n$output.outcoll1\n$output.outcoll2\n$output.outcoll3\n]]>\n</command>\n<inputs>\n<param name=\u201cinput\u201d type=\u201cdata_collection\u201d collection_type=\u201cpaired\u201d format=\u201ctxt\u201d\nlabel=\u201cPaired Collection\u201d\n</inputs>\n<outputs>\n<collection name=\u201coutput\u201d type=\u201clist\u201d label=\"(tool_name) on {on_string}\" >\n<data name=\u201coutcoll1\u201d format=\u201ctxt\u201d />\n<data name=\u201coutcoll2\u201d format=\u201ctxt\u201d />\n<data name=\u201coutcoll3\u201d format=\u201ctxt\u201d />\n</collection>\n</outputs>\n</tool>\n====\nThis way, my output is what Galaxy calls an embedded list, or a list of lists.\nThe tool now works as intended."
    }
  ],
  [
    {
      "role": "user",
      "text": "\nScreen Shot 2023-01-11 at 11.45.46 AM700\u00d7998 66.3 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/3/336c58f7a076e34fa455adba4271597180d25800.png)\nBowtie2.0 task is running for a long time. It has been 36 hours and still running. Is that normal? Thanks! I am analyzing ATAC-seq data with paired end reads.\nIt is giving me the following stats although it is still running. Can someone please tell me why? thanks\n\nScreen Shot 2023-01-11 at 11.51.26 AM1202\u00d7538 48.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/d/d37780130fed600d008d0e399b5273dda09164af.png)"
    },
    {
      "role": "system",
      "text": "Hi @user\nIt looks like the job is still completing the process of writing all the output files. Let this complete to avoid needing to rerun.\nIf you were working at @link http://UseGalaxy.org, I can let you know that the clusters were very busy when you wrote in, and are a bit less busy now.\n@quote\nIt has been 36 hours and still running. Is that normal?\nYes, sometimes, since those 3 days probably included queue + the actual job execution time. The best advice is to get jobs queued then let them complete. Using collections and workflows will significantly improve total processing time.\n\nHow jobs process: @link https://training.galaxyproject.org/training-material/faqs/galaxy/#analysis\n\nCollections and workflows: @link https://training.galaxyproject.org/training-material/topics/galaxy-interface/\n\n\nIf you think there is still a problem after a few more hours, please either reply and post back a share link to the history and note the dataset involved, or ask for a moderator to start up a direct chat to post that in privately.\n\nHow to share: @link https://training.galaxyproject.org/training-material/faqs/galaxy/#sharing-your-history\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Where are computeMatrix and plotHeatmap gone? Not showing up in tools today."
    },
    {
      "role": "system",
      "text": "Hi @user\nWe discussed this over direct email, but for others that may have run into the same problem:\n\n\nBoth tools are available as of yesterday at @link http://UseGalaxy.org. Why those appeared to be missing from the tool panel is not known.\n\n\nThe @link http://UseGalaxy.eu server had some problems this week. That could have impacted tool panel searches, and still could since the banner on the server is still up. Once the issues are resolved, the banner will be removed. Details here: UseGalaxy.eu -- unexpected server behavior May 2022 (@link https://help.galaxyproject.org/t/usegalaxy-eu-unexpected-server-behavior-may-2022/8062)\n\n\nThanks for reporting the problem!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I tried to load data to @link http://hicexplorer.galaxy.eu today and it failed. I used FileZilla. @link http://usegalaxy.org worked but not @link http://hicexplorer.galaxy.eu"
    },
    {
      "role": "system",
      "text": "@quote\nhicexplorer.galaxy.eu\nHi @user\nThe server URL is: hicexplorer.usegalaxy.eu\nThis domain-specific public Galaxy server is sub-server of the usegalaxy.eu server. Your account at both is the same.\nUpload data into your account using these FTP credentials in Filezilla. Leave other settings at default.\n\nHost: ftp.usegalaxy.eu\nUsername: your Galaxy account\u2019s email address\nPassword: your Galaxy account\u2019s password\n\nScreenshot with one test file just now loaded (to make sure it is actually working! it is :slight_smile:\nhow-to-find-FTP-upload-URL1742\u00d71224 430 KB (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/9/95e2e6aa7c18bf3ee9c5e1f87a5518193f6b42e5.png)\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am running into this error when I try to do differential seq analysis in galaxy:\nWarning message:\nIn estimateDisp.default(y = y$counts, design = design, group = group,  :\nNo residual df: setting dispersion to NA\nError in eval(ej, envir = levelsenv) :\nobject \u2018ControlvsTreated\u2019 not found\nCalls: makeContrasts \u2192 eval \u2192 eval"
    },
    {
      "role": "system",
      "text": "@user\nHave you searched the forum?\n@link https://www.biostars.org/p/212200/\nDoes it answer the question?\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, I generated tabular formatted gene abundance estimate files \u200bwhich have fpkm and tpm using StringTie.\nThen I want to calculate Correlation of variance (CV) using fpkm or tpm from multiple files.\n\uadf8\ub9bc41942\u00d71892 882 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/e/eeb31d5e87456d9c5a2505d1fc96f3a2a21ef041.jpeg)\nWhat module should I use if I want to combine the \u2018fpkm\u2019 or \u2018tpm\u2019 column only from many files?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI guess that the Multi-Join (@link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_multijoin_tool/1.1.1) tool is what you need.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello everyone\nI am trying to upload some protein fasta files into @link http://Galaxy.eu and while they load fine with no problems, but when I look in my history the box is grey and says \u201cThis job is waiting to run.\u201d and it has been like this for about an 1.5 hrs whereas before it would begin to run and go green within a few minutes.\nI have tried to restart my laptop and clear my chrome cache but with no success therefore, if I could please have some help it would be much appreciated."
    },
    {
      "role": "system",
      "text": "Hi @user\njobs can stay in queue for some time when Galaxy is busy. Usually servers have limits on concurrent jobs. I tested upload to Galaxy Europe today, and it was nearly instant for a tiny file.\nHope it all worked for you.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Normally I add an requirement in my xml file and conda will install it in a environment and the galaxy tool can use that program. Is there a proper or \u201cofficial\u201d way to install packages in the environment of the tool that are only available in pip?\nI can just use pip to install the package on my system and python can find it, but I was wondering if there is a method to install it only in the environment of that specific galaxy tool."
    },
    {
      "role": "system",
      "text": "The recommended way is to add your requirement to Bioconda (@link https://bioconda.github.io/contributing.html) or @link https://conda-forge.org/#contribute."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nHow can i delete data created with data manager?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nThere isn\u2019t an automatic way to \u201cundo\u201d content created by a Data Manager. In most cases, review and edit/remove the files directly.\nOverview Reference Genomes in Galaxy (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/reference-genomes/slides.html#1)\nEphemeris Welcome to the Ephemeris documentation! \u2014 Ephemeris 0.10.3 documentation (@link https://ephemeris.readthedocs.io/en/latest/index.html#)\nGalaxy Search \u00b7 data managers \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/search?q=data+managers)\nRelated Q&A Topics tagged data-manager (@link https://help.galaxyproject.org/tag/data-manager)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi there!\nI want to make a genome annotation using a reference fasta + 8 fastq libraries. However, the fastq libraries must be converted to fasta in order to be used as EST for Maker, and there is not enough space in the galaxy quota to do this. Does anyone know how I am supposed to do a Maker annotation with fasta files when the quota has such limited space? Thanks!"
    },
    {
      "role": "system",
      "text": "Hi @user,\nit is possible to request additional space in Galaxy.  You can find more information in Account quotas (@link https://galaxyproject.org/support/account-quotas/).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to publish our own tool to the Galaxy Tool Shed. Our tool is our own developed C++ program. Our compiled binaries may not necessarily run in Galaxy servers due to different OS platforms. How could I resolve the tool dependency of our own compiled binaries?  Thank you."
    },
    {
      "role": "system",
      "text": "Hi !\nBuild up your packages using conda-forge. There is support for compilation using c, c++ and fortran compilers. See @link https://conda-forge.org/#contribute\nThere are many examples to get started, for example parmed is now on conda-forge and it started as a PR to staged recipes @link https://github.com/conda-forge/staged-recipes/pull/8736.\nGood luck!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI have a problem with running a Workflow on a local instance of Galaxy (running on a Ubuntu server).\nIn this workflow, a Flye assembly is fed to a Medaka consensus pipeline together with a bunch of fastq files from a MinION run. This workflow runs beautifully on @link http://usegalaxy.eu but on my own server I get the follow error:\n{ \"code_desc\": \"\", \"desc\": \"Fatal error: Exit code 126 ()\", \"error_level\": 3, \"exit_code\": 126, \"type\": \"exit_code\" }\n\nand this is shown in the history:\nimage\nBecause the same workflow runs fine on @link http://usegalaxy.eu I guess it has something to do with the installation of medaka because that was not easy. Installing medaka (including the dependencies) from the admin menu always results in an error with Galaxy hanging and need a restart. This seems to be a know issue (see @link https://github.com/galaxyproject/tools-iuc/issues/4915). So I installed medaka throught the CLI with conda:\nconda create -y --quiet --override-channels --channel conda-forge --channel bioconda --channel defaults --name __medaka@1.7.2 medaka=1.7.2\n\nand then copied everything to the galaxy environment:\ncp -r ./mambaforge/envs/__medaka@1.7.2 ./galaxy/dependencies/_conda/envs/\n\nGalaxy now sees medaka tool with it\u2019s dependencies resolved but running the workflow thus gives an error.\nAny ideas?\nThanks in advance!\nErik"
    },
    {
      "role": "system",
      "text": "see also: Medaka consensus pipeline error \u00b7 Issue #5233 \u00b7 galaxyproject/tools-iuc \u00b7 GitHub (@link https://github.com/galaxyproject/tools-iuc/issues/5233)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.\nI have HIC data (.hic and.cool files) and i can\u2019t visualise them in a circular layout.\nI used \u201cCircos visualizes data in a circular layout\u201d form @link https://usegalaxy.org/ .\nAny suggestions or comments?\nThank you,"
    },
    {
      "role": "system",
      "text": "Hi @user\nCircos will require data that is structured in specific formats. This tutorial covers example manipulations with sample data in various original formats.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html)\n\n\nGalaxy Training: Visualisation with Circos (@link https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html)\nTraining material for data visualisation in Galaxy\n\n\n\n\n\nFor help with transforming your current datasets, maybe review the tools in the tool group HiCExplorer? These tutorials have more help.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/epigenetics/)\n\n\nGalaxy Training: Epigenetics (@link https://training.galaxyproject.org/training-material/topics/epigenetics/)\nDNA methylation is an epigenetic mechanism used by higher...\n\n\n\n\n\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\n\n\nGalaxy Training: Data Manipulation Olympics (@link https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html)\nGalaxy is a scientific workflow, data integration, and da...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have been trying to access my galaxy page, but it times out and gives the following error:\n\nGalaxy could not be reached\nYou are seeing this message because a request to Galaxy timed out or was refused. This may be a temporary issue which could be resolved by retrying the operation you were performing. If you receive this message repeatedly or for an extended amount of time, please check for additional information on the Galaxy status page (@link https://status.galaxyproject.org/) or the @galaxyproject Twitter feed (@link https://twitter.com/galaxyproject). If the issue is not addressed on those sources, you may report it to the support team at @link mailto:galaxy-bugs@galaxyproject.org with details on what you were trying to do and the URL in the address bar."
    },
    {
      "role": "system",
      "text": "Hi @user\nAll three of the primary usegalaxy.* servers were down for different short time windows, but all are up as of now. @link https://status.galaxyproject.org/\nNext time, please include this information wherever you are asking the question. Each server is administered independently, and the URL is needed to confirm or help.\n@quote\nwith details on what you were trying to doand the URLin the address bar"
    }
  ],
  [
    {
      "role": "user",
      "text": "At the recent ASMCUE meeting, I discussed with Mo Heydarian how it would be useful to have a tool similar to one used to get FASTQ reads from SRA, but to get FASTA genomes from NCBI Genbank or RefSeq by accession.\nMo suggested that I could add an item to a github tracker, but I wasn\u2019t sure which tracker within galaxyproject would be appropriate.\nWe talked about how this might need to be limited to \u201csmall genomes\u201d but it would be very useful to those of us who work on bacteria and bacteriophage."
    },
    {
      "role": "system",
      "text": "Hi Jim,\nI opened an issue in the Galaxyproject/IUC repo here: @link https://github.com/galaxyproject/tools-iuc/issues/2526\nPlease feel free to provide background and additional information on the feature request.\nThanks for the great suggestion!\nCheers,\nMo"
    }
  ],
  [
    {
      "role": "user",
      "text": "I just set up a fresh galaxy installation, following the instructions on: @link https://galaxyproject.org/admin/get-galaxy/ . But once I try to install a tool I get the following error:\nError cloning repository: [Errno 2] No such file or directory: \u2018hg\u2019: \u2018hg\u2019\nIt seems to always be the same error with every tool. Can someone here help me out?\nEDIT: Found it. Seems I needed to install mercurial. But it seems this is never mentioned in the installation docs?"
    },
    {
      "role": "system",
      "text": "I have added a note to our toolshed installation tutorial that should clarify this. @link https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/#connect-your-galaxy-to-a-tool-shed\nThanks for bringing it up!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear everyone!\nI am currently analysing the output of a PE sequencing of 96 samples. I have uploaded the reads and included them into a list to use the tools more efficiently. I am running a workflow using snippy, snippy core, Clustal and so. My problem is that 4/96 snippy jobs failed due to memory allocation problems and that interrupted the whole workflow. Even if I delete the failed runs, the output list is marked in red and cant resume the workflow.\nMy question is: Is there a way for me to run the failed snippy jobs and include the output in the list of the successful ones in order to continue with my workflow?\nI have thought of downloading the successful runs output, running the failed ones again, download these outputs and then after re uploading everything again make a new list but my problem is that the snippy output and the bam files are 54Gb each, way too heavy for me to do.\nI really appreciate your assistance on this :slight_smile:"
    },
    {
      "role": "system",
      "text": "Hi @user\n@quote\nMy question is: Is there a way for me to run the failed snippy jobs and include the output in the list of the successful ones in order to continue with my workflow?\nThere are two primary solutions for this situation:\n\n\nInstead of deleting failed jobs, rerun instead. The bottom of the tool form will have an extra option to resume dependencies in a way that replaces the original failed datasets.\n\n\nRemove failures during the original workflow run. @link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html#tools-that-manipulate-elements-within-a-collection\n\n\nMore workflow and collection help is here, including examples of programmatically controlled workflow execution.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\n\nGalaxy Training: Using Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Really just a minor nuisance, but if you use @link http://usegalaxy.org in a post, it gets recognized as a hyperlink target, but the same doesn\u2019t work for usegalaxy.eu or usegalaxy.org.au (presumably because .eu and .au are not recognized as top-level domains?). It would just be convenient if that could be changed."
    },
    {
      "role": "system",
      "text": "To my surprise there is actually settings for this in discourse (discourse is great!). I added the two domains there to the allowlist. Test:\n@link http://usegalaxy.eu\n@link http://usegalaxy.org.au"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like someone to explain to me what is unassigned ambiguity.\nthanks for advance"
    },
    {
      "role": "system",
      "text": "Hi @user\nconsider checking the manual by clicking at Subread link available in paragraph under Execute button:\nUnassigned Ambiguity: alignments that overlap two or more features (feature-level sum-\nmarization) or meta-features (meta-feature-level summarization).\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am running a UMI-tools extract command that works on HPC:\n(example of nextflow command)\numi_tools extract --extract-method=regex \n\u2013bc-pattern=\".+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})$\" \n-I ${sample_name}_trimmed.fastq \n-S ${sample_name}_umi_cleaned.fastq > ${sample_name}_umi_tools.log\nbut it does not run properly on Galaxy\nI need to specify the following regex so UMI-tools detect the UMI at the end:\n.+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})$\nHowever, when I run it on GA, the + and the $ seem to be dropped:\nIn the log, the pattern comes up as: .(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})\nAnd although UMI runs it fails to correctly detect the adapter and UMI at the end of the read. I end up with a few hundred reads 1-2 bp long instead of several million reads with their proper UMI extracted.\nI also tried to use backslashes :.\\+(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})\\$\nand an asterisk\n`.*(?P<discard_1>AACTGTAGGCACCATCAAT){s<=2}(?P<umi_1>.{12})``\nBut both failed\nThanks for your help"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI opened a PR (@link https://github.com/galaxyproject/tools-iuc/pull/4180) in order to fix it.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "In my galaxy.yml I have:\nuser_activation_on: true\nactivation_grace_period: 0\n\nIf you are not logged in you are still able to see the tool menu. How can I configure galaxy so only logged in users can see the tools?\nEDIT:\nI can also upload when not logged in\nThanks in advanced"
    },
    {
      "role": "system",
      "text": "# Force everyone to log in (disable anonymous access).\n#require_login: false\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI\u2019d like to remove all lines for which no gene symbol is known in order to obtain a dataset with only gene symbol annotated RNAseq expression data. Tried several options with the compute package but it either doesn\u2019t work or my syntax is wrong. Any suggestions?\nHenk"
    },
    {
      "role": "system",
      "text": "Dear @user,\nI assume your question is regarding a solution in Galaxy. Have you tried the tool Filter Tabular, choosing regex replace value in column, selecting the column of the gene names, and as a regex pattern NA.\nKind regards,\nFlorian"
    }
  ],
  [
    {
      "role": "user",
      "text": "The history window has frequent glitches when I try to scroll through it\u2013it goes back to the same spot instead of actually scrolling anywhere. Anyone having this issue? Sometimes refreshing Galaxy helps but not always."
    },
    {
      "role": "system",
      "text": "Here\u2019s an issue describing what I think you are encountering: History scrolling can stall even in small histories \u00b7 Issue #14767 \u00b7 galaxyproject/galaxy \u00b7 GitHub (@link https://github.com/galaxyproject/galaxy/issues/14767)\nWe\u2019re aware of this and are working on it \u2013 if you have any extra diagnostic feedback you could provide as to what triggers it (particular histories, contents, slow internet, \u2013 anything) that would be useful to add there.  Otherwise please subscribe to that issue to track a resolution to this."
    }
  ],
  [
    {
      "role": "user",
      "text": "I want to upload the fastQ data from ArrayExpress database at here (@link https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6830/sdrf)\nI know that there are some ways to transfer fastQ data from NCBI database using SRA accession numbers. These ways help the users to transfer data without the need to downloading and uploading the large fastQ files.\nIs there such a way to directly transfer data using the links of the fastQ files from the mentioned link from ArrayExpress? Or we have to download the fastQ files and then upload them to galaxy?"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nIn short, yes :slight_smile:\nClick into the full sample table view at that link, and you\u2019ll see the accession identifiers per sample. Those should be also available at NCBI SRA. I just confirmed this by testing the first one: @link https://www.ncbi.nlm.nih.gov/search/all/?term=ERS2519373\nThese tutorials cover how to manipulate sample tables for batch data uploads. That includes sample organization (usually worth it, so highly recommended!). But, you can also just isolate the accessions out of the table, and use a tool like Faster Download and Extract Reads in FASTQ format from NCBI SRA.\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\n\n\nGalaxy Training: Using Galaxy and Managing your Data (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/)\nA collection of microtutorials explaining various feature...\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello, I\u2019m trying to run stringtie as precursor to analysis with dexseq2. For my annotation file, I\u2019m using WBcel235 downloaded from NCBI.\nWhen I run stringtie, I get the following error:\n\" An error occurred with this dataset: format gtf database ce11 (@link https://usegalaxy.org/datasets/f9cad7b01a472135782ca0e76f796893/details#) WARNING: no reference transcripts were found for the genomic sequences where reads were mapped! Please make sure the -G annotation file uses the same naming convention for the genome sequences.\"\nI get the gist of this - Galaxy somehow doesn\u2019t like the format of my annotation file. But I am not sure entirely what is wrong with it, or how to fix this. I\u2019ve copied the first few lines of my annotation file below.\nAny help would be greatly appreciated.\n#gtf-version 2.2\n#!genome-build WBcel235\n#!genome-build-accession NCBI_Assembly:GCF_000002985.6\n#!annotation-source WormBase WS283\nNC_003279.8\tRefSeq\tgene\t3747\t3909\t.\t-\t.\tgene_id \u201cCELE_Y74C9A.6\u201d; transcript_id \u201c\u201d; db_xref \u201cGeneID:353377\u201d; gbkey \u201cGene\u201d; gene \u201cY74C9A.6\u201d; gene_biotype \u201csnoRNA\u201d; locus_tag \u201cCELE_Y74C9A.6\u201d;\nNC_003279.8\tRefSeq\ttranscript\t3747\t3909\t.\t-\t.\tgene_id \u201cCELE_Y74C9A.6\u201d; transcript_id \u201cNR_001477.2\u201d; db_x"
    },
    {
      "role": "system",
      "text": "Hi @user\nThe first thing I notice is header lines in the GTF. Many tools will complain if those are not removed.\nNext, check that the sequence IDs between that reference annotation GTF and the reference genome used for mapping all match (exactly).\n\nGalaxy indexed genomes are usually sourced from UCSC and they also host matching reference annotation. Example: Index of /goldenPath/ce11/bigZips/genes (@link https://hgdownload.soe.ucsc.edu/goldenPath/ce11/bigZips/genes/).\nIf you used a custom genome (fasta) instead, use an annotation that pairs with that version of the assembly.\n\nFAQs\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-input-error-messages\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#working-with-gff-gft-gtf2-gff3-reference-annotation\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "How often do I have to update Galaxy in a productive environment (just 2-3 users) in order to keep up with new tools?\nDoes updating Galaxy break a lot of dependencies for installed tools in general? Are there some kind of minor and mojor releases?\nIs this documented anywhere?"
    },
    {
      "role": "system",
      "text": "Updating galaxy doesn\u2019t usually break tools. The tool wrapper description doesn\u2019t really change much.\nI think Galaxy 16.01 was the last major change for Galaxy tool compatibility.\nBranches in the repo titled \u2018release_XXX\u2019 are the best way to keep up with the releases rather than using the zipped releases as they contain important bug fixes.\nGalaxy unfortunately doesn\u2019t maintain semver patch versions, you have to use the commit hash for the branch."
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear Galaxy community,\nI need to add EXAC_NFE info in VCF.\nI am thinking about useing \u201cSnpSIFT annotate\u201d to annotate the SNP ID with EXAC_NFE as INFO.\nHowever, I could not find a way to download the exac03(hg19) file and import it to Galaxy. (Tried already to find information from ANNOVAR, UCSC, and ExAC Browser, but could not find a way to get the file.)\nCan anyone help me with that?\nOr maybe I am wrong, it is not the way to add EXAC_NFE information.\nThanks a lot\nSusan"
    },
    {
      "role": "system",
      "text": "Hi Susan,\nthe simplest way to bring in these annotations would be via the GEMINI toolsuite, just that\na) you\u2019ll have to stop working with VCF format at that point because GEMINI converts VCF to an sqlite database during the annotation step and keeps using this format for all further processing and querying\nb) with the current version of GEMINI, you\u2019ll only have ExAC release 0.3 available\nc) you would have to work on @link http://usegalaxy.eu for now because GEMINI, for technical reasons, is currently not available on @link http://usegalxy.org (though hopefully this will get fixed in the not so distant future)\nIf you want to try to annotate your VCF directly (SnpSift annotate may or may not work for the task - I never tried to use it for anything but dbSNP data) the data you\u2019re looking for is available from:\n@link ftp://ftp.broadinstitute.org/pub/ExAC_release\nThe files you find there have been compressed with bgzip, but if you specify data type \u201cVCF\u201d on import Galaxy should unpack them for you."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019m trying to follow the tutorial to install a local galaxy server using ansible and when running the playbook I get the following error:\nTASK [natefoo.postgresql_objects : Create and drop users] ******************************************************************\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: AttributeError: 'NoneType' object has no attribute '__version__'\nfailed: (item={'name': 'galaxy'}) => {\"ansible_loop_var\": \"item\", \"changed\": false, \"item\": {\"name\": \"galaxy\"}, \"module_stderr\": \"Traceback (most recent call last):\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 107, in <module>\\n    _ansiballz_main()\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 99, in _ansiballz_main\\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\\n  File \\\"/var/tmp/ansible-tmp-1650470619.697677-3153329-92378227580523/AnsiballZ_postgresql_user.py\\\", line 47, in invoke_module\\n    runpy.run_module(mod_name='ansible_collections.community.postgresql.plugins.modules.postgresql_user', init_globals=dict(_module_fqn='ansible_collections.community.postgresql.plugins.modules.postgresql_user', _modlib_path=modlib_path),\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 207, in run_module\\n    return _run_module_code(code, init_globals, run_name, mod_spec)\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 97, in _run_module_code\\n    _run_code(code, mod_globals, init_globals,\\n  File \\\"/home/fyrseq/miniconda3/lib/python3.8/runpy.py\\\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/modules/postgresql_user.py\\\", line 1015, in <module>\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/modules/postgresql_user.py\\\", line 935, in main\\n  File \\\"/tmp/ansible_postgresql_user_payload_903w3p_e/ansible_postgresql_user_payload.zip/ansible_collections/community/postgresql/plugins/module_utils/postgres.py\\\", line 187, in get_conn_params\\nAttributeError: 'NoneType' object has no attribute '__version__'\\n\", \"module_stdout\": \"\", \"msg\": \"MODULE FAILURE\\nSee stdout/stderr for the exact error\", \"rc\": 1}\n\n\nI have double and triple checked that my files are setup exactly as the tutorial is so I\u2019m not sure what is going wrong on this part.\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi! That role does not support check mode. Please run your playbook without it."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello. I am using bowtie2 for my ChIP-seq analysis but not seeing the \u201ctool standard error\u201d for most of my files, though I can see that the job is successfully performed. It is coming up as \u201cempty\u201d. Consequently, I am not being able to see the alignment rate, reads aligned etc. I am new to bioinformatics tools and relying on Galaxy for all my sequencing analysis. Could you please help with this? I will be grateful.\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user,\nwhich version of the tool are you using , and on which Galaxy server are you working?\nWith toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.0+galaxy0 on @link http://usegalaxy.eu, the stderr is not empty at least.\nAlso, have you seen the option \u201cSave the bowtie2 mapping statistics to the history\u201d? This will redirect the mapping stats you are looking for from stderr to an extra dataset in your history.\nCheers,\nWolfgang"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi.\nI am going through the galaxy training material \u2019 RNA-seq genes to pathways\u2019 with the following link\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-genes-to-pathways/tutorial.html)\n\n\nGalaxy Training: 3: RNA-seq genes to pathways (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/rna-seq-genes-to-pathways/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nCan anyone help me to find the tool \u2019 Compute an expression on every row\u2019 ?\nMany thanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nthe tool was upgraded to v.2 and it is now called Compute on row. If you select an early version during the job setup (three  blocks icons at the top right corner of the middle window), you\u2019ll see the old name.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am using the main galaxy server at @link https://usegalaxy.org/\nI have a collection of paired-end RNA-seq data with 30 pairs in it. With just this collection and a few other files, my storage usage sits at 5% (although, I suspect this may be incorrect given the size of the collection?). When I try to run cutadapt on this dataset, the storage usage begins to move up, far past what I would expect it to given that it can only be making the files smaller. After only a few pairs are completed, I start getting a \u201cNo space left on device\u201d error. Even stranger, one pair will fail to run due to this error, but the next 10 will complete successfully. Then a couple more will fail due to the same error. When I let it run completely, it finishes at about 80% storage used. When run multiple times, different pairs fail each time.\nI know something must be going on, but I cannot figure it out. Earlier today I had the entire collection run through cutadapt successfully, but I purged it because it wasn\u2019t working with downstream software.\nFinally, I wanted to mention that running the data as \u201cmultiple datasets\u201d rather than \u201cdataset collection\u201d seems to function as intended. However, I lose the organization of the collection in this case, so I\u2019d like to be able to run it as it stands now.\nAny input is appreciated!"
    },
    {
      "role": "system",
      "text": "Hello, @user Thanks for reporting the issue! :bug:\nPlease try a rerun now. The problem was due to a server-side issue at @link http://usegalaxy.org, now resolved.\nFor others reading and working at this same server, any job that failed with an error message like this over the last few days should be rerun.\n\nOSError: [Errno 28] No space left on device\n\nFind the message at the end of the stderr on the Job Details report.\njob-details"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to upload a database of all bacterial nucleotide sequences (gigantic file - 107 GB). I am using FTP via FileZilla, and it got to 80.5 Gb before giving the error \u201c552 Transfer aborted. File too large\u201d. I read something about a 250Gb limit but I think I should be within that - even with files already on Galaxy, it says I am only using 2.7Gb. Not sure what to do (I am definitely no bioinformatician and it took me two weeks to figure out how to download this file -_-). If there\u2019s an alternative to use a database like this on Galaxy that is already uploaded I am all ears. Below is the full set of messages that keep iterating until I stop the queue.\n\n\n\n\nError:\nFile transfer failed after transferring 294,912 bytes in 1 second\n\n\n\n\nStatus:\nStarting upload of /Volumes/MBLab/Bacterial Nucleotide Database - October 2020.fasta\n\n\nStatus:\nRetrieving directory listing of \u201c/\u201d\u2026\n\n\nCommand:\nPASV\n\n\nResponse:\n227 Entering Passive Mode (129,114,60,56,119,69).\n\n\nCommand:\nREST 0\n\n\nResponse:\n350 Restarting at 0. Send STORE or RETRIEVE to initiate transfer\n\n\nCommand:\nMLSD\n\n\nResponse:\n150 Opening BINARY mode data connection for MLSD\n\n\nResponse:\n226 Transfer complete\n\n\nCommand:\nPASV\n\n\nResponse:\n227 Entering Passive Mode (129,114,60,56,120,177).\n\n\nCommand:\nREST 80530636800\n\n\nResponse:\n350 Restarting at 80530636800. Send STORE or RETRIEVE to initiate transfer\n\n\nCommand:\nSTOR Bacterial Nucleotide Database - October 2020.fasta\n\n\nResponse:\n150 Opening BINARY mode data connection for Bacterial Nucleotide Database - October 2020.fasta\n\n\nResponse:\n552 Transfer aborted. File too large\n\n\n\n"
    },
    {
      "role": "system",
      "text": "Discussed via email with @user\nItems that may help others:\n\nThe maximum size of an FTP file is 50 GB at public Galaxy servers. BAMs are a bit different, the maximum size is 25-35 GB.\n\n\nFAQ: @link https://galaxyproject.org/support/loading-data/\n\n\n\nSetting up a local Galaxy.\n\n\nExample step-by-step for a basic local Galaxy install with an admin account created (@link https://help.galaxyproject.org/t/example-step-by-step-for-a-basic-local-galaxy-install-with-an-admin-account-created/1451/2)\n\n\n\nAlternatives, including cloud options. Many can be easier to administrate \u2013 including the GVL of Cloudman.\n\n\nWays to use Galaxy: @link https://galaxyproject.org/ >> Use\nThe GVL version of Cloudman is one choice and AWS offers grants. @link https://aws.amazon.com/grants/\n\n\nMore help and prior Q&A can be found at this site or at the GTN tutorial site. Start by with keywords like \u201cupload\u201d or \u201ccloud\u201d or \u201cftp\u201d. If your issue remains, start a new topic and be as specific as possible for context: How using Galaxy, where using Galaxy (URL if public server), version of local/cloud Galaxy, special configurations\u2026\nBest,"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am new to Galaxy, following the tutorial for CLIP-seq data analysis, and RNA-STAR for 2 datasets has been running for 3 days now (it\u2019s colored in orange and the status is \u201cjob is running\u201d). Is this normal?\nThanks!"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nYes. The public Galaxy servers are shared resources. Everyone\u2019s jobs will run as free resources become available. Some tools queue longer than others, and that is very likely for your case since RNA-Star runs at our largest and busiest cluster. The good news is that the job is now executing (yellow/orange).\nNever interrupt a queued or executing job unless you already know it has a problem you want to fix as new jobs are always added back to the end of the queue.\nThese two FAQs explain the how the job scheduling process works, and you can browser search that whole page to find more help.\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#will-my-jobs-keep-running\n\nHope that helps :slight_smile:"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, what is causing the error message here:\nTraceback (most recent call last):\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/bin/umi_tools\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/umi_tools.py\", line 61, in main\n    module.main(sys.argv)\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/extract.py\", line 477, in main\n    reads = ReadExtractor(read1, read2)\n  File \"/usr/local/tools/_conda/envs/__umi_tools@1.1.2/lib/python3.8/site-packages/umi_tools/extract_methods.py\", line 544, in __call__\n    raise ValueError('Read sequence: %s is shorter than pattern: %s' % (\nValueError: Read sequence: ACGC is shorter than pattern: NNNXXXXNN\n"
    },
    {
      "role": "system",
      "text": "@quote\nValueError: Read sequence: ACGC is shorter than pattern: NNNXXXXNN\nHi @user,\nit seems that your FASTQ file contains reads whose length is shorter than the barcode pattern; in order to solve it, you need to filter your reads by length >=10, by using for example @link https://usegalaxy.org/root?tool_id=toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/3.5+galaxy1.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI am looking to get a file of unmapped reads after RNA star. How do I get that"
    },
    {
      "role": "system",
      "text": "Hi @user You are after any BAM filtering tool. Alternatively, consider HiSAT2 instead of RNA_STAR and in advanced options > outputs activate Write unaligned reads (in fastq format) to separate file(s).\nHope this helps.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI\u2019m running galaxy on a compute cluster with Slurm and I\u2019m having trouble running R. We have python running with the .venv, but the only way I\u2019ve been able to run R is by install R on a node, which isn\u2019t sustainable. What do you recommend for setting up R for galaxy with slurm?\nThanks!"
    },
    {
      "role": "system",
      "text": "Hi @user\nI think the solution is to run a Conda .venv, and to not use a Python .venv. From the start, so that everything else in the local configuration also uses it.\nBut you can decide. The \u201cPython\u201d way will make not just R difficult to manage but also many other dependencies troublesome. Others can correct me (and share back how) if some hybrid works for them.\nReferences:\n\n\nGalaxy Documentation \u2014 Galaxy Project 23.0.2.dev0 documentation (@link https://docs.galaxyproject.org/en/master/index.html)\n\n\nSee here first \u2192 Framework Dependencies \u2014 Galaxy Project 23.0.2.dev0 documentation (@link https://docs.galaxyproject.org/en/master/admin/framework_dependencies.html#conda)\n\n\n\n\nGalaxy Training \u2014 Admin (@link https://training.galaxyproject.org/training-material/topics/admin/)\n\n\nThen see here \u2192 Connecting Galaxy to a compute cluster (@link https://training.galaxyproject.org/training-material/topics/admin/tutorials/connect-to-compute-cluster/tutorial.html)\n\n\n\n\nHope that helps! :rocket:"
    }
  ],
  [
    {
      "role": "user",
      "text": "I would either use Ubuntu or Debian. What do you recommend in case I can choose?"
    },
    {
      "role": "system",
      "text": "Both are absolutely fine choices."
    }
  ],
  [
    {
      "role": "user",
      "text": "I have used the repeatexplorer2 tool to analyze paired-end sequencing data made from a genomic DNA library of a human cell line. The tool appears to have worked successfully and finds many satellite clusters but for rDNA it says \u201cnot found\u201d. What needs to be done differently here to analyze the rDNA? Or is it likely among the \u201cother\u201d category and just not recognized as rDNA? Thanks."
    },
    {
      "role": "system",
      "text": "First of all it looks like repeatexplorer is a fairly specific pipeline with its own galaxy server and user registration. On the website @link https://repeatexplorer-elixir.cerit-sc.cz/ I see the following text:\n\nIf you need help with RepeatExplorer or you want to report a problem and our wiki is not able to give you answers, please contact server administrator (@link mailto:regalaxy@rt.cesnet.cz).\n\nI think the server administrator can give you a better answer.\nIf you ask your question to the server administrator you may need to clarify what you mean with rDNA and if \u201cnot found\u201d is an error or an output message. I dont know the tool but I would guess that you get more output then just \u201cnot found\u201d (I can be wrong ofcourse).\nDo you mean recombinant DNA? And are it just fastq or fasta files? In addition to that if you do mean recombinant DNA I would guess it is very possible to not find satelites because mostly satelites are non-coding and recombinant DNA (created in a lab) is only coding.\nI hope for now my reply is helpfull and maybe someone with experience with this tool will also see your post."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi,\nI have generated the file using snpsift annotate (snpEff tool). This file is not recognised by Gemini load. Possibly, there is an issued in type of input files.\nWhat should be the input fie in the 2nd option?\n1.Variant input file in VCF format\n2.View dataEdit attributesDelete\nVCF File with ID field annotated (e.g. dbSNP.vcf)\nneed guidance here. Please\u2026\nThank you."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe tool is available at @link http://UseGalaxy.eu and @link http://UseGalaxy.org.au.\nAs noted on the tool form, any vcf dataset is accepted as an input but it must meet both of these criteria:\n\nBe based on the Human GRCh37 (hg19) genome build assembly\nHave hg19 assigned as the database metadata attribute.\n\n\ngemini-vcf-input808\u00d7296 27.1 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/f/f27ee0962db79e5485aa05a9b53aa6e9720c83a9.png)\nFAQs: Galaxy Training! (@link https://training.galaxyproject.org/training-material/faqs/galaxy/#analysis)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi, Today i installed the histogram tool in the graph/Display Data , and when i tried to use i found out it\u2019s written in python2 so i get syntax errors , is a solution out there or should i wait until it\u2019s upgraded by the team."
    },
    {
      "role": "system",
      "text": "I can\u2019t speak for this tool, but some legacy/unmaintained tools are unlikely to receive updates.\nYou can deal with such tools using the technique described here (@link https://gist.github.com/mvdbeek/8ca744f5ee8f61bcb74faa8e35a147ff)."
    }
  ],
  [
    {
      "role": "user",
      "text": "I would like to use the Galaxy/hutlab format to write a paper about gut microbiota.\nSo, I would like to ask four questions in order to apply to the IRB.\n\u2460If an unregistered user uploads gut microbiota data, how long does the data take to be deleted? I want to know the exact period.\n\u2461 After the data is completely deleted, will it never be restored?\n\u2462Will the data stored on your site, Galaxy/hutlab be used without the consent of the uploader?\n\u2463Will my uploaded data and analysis results are seen by anyone other than me?\nI have already browsed the following sites\uff1a\n\u30fbManaging Datasets in Galaxy - Galaxy Community Hub (@link https://galaxyproject.org/learn/managing-datasets/#delete-vs-delete-permanently)\n\u30fbDownloading and Deleting Data in Galaxy (@link https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/download-delete-data/tutorial.html#fn:1)"
    },
    {
      "role": "system",
      "text": "Hi @system\nEach public server has its own data policies including terms of service that cover your questions. Some post details on the homepage of the server or under the Help menu.\nYou are working at the huttenhower.sph.harvard.edu/galaxy public Galaxy site, correct? If so, you\u2019ll need to contact them directly for more feedback. Click into this post \u2013 it explains why and how to reach them.\n@quote\nHi@systemThe tool you are using is specific to the http://huttenhower.sph.harvard.edu/galaxy/ domain-specific public Galaxy server. \nThe tools and protocols hosted there differ significantly from the Lefse tools hosted at most other public Galaxy servers (including usegalaxy.* servers). \nThe administrators of that particular public Galaxy server do not track this forum. Instead, contact them directly, after double-checking your protocol and inputs against the help/tutorials they offer (belo\u2026\nShort answer:\n\nAny data that is under restricted privacy requirements would not be appropriate for a public Galaxy server or probably any open public resource even outside of Galaxy.\nConsider a private Galaxy choice. The AnVIL version is a single-user choice sponsored by NHGRI and is a pay-for-use Google Cloud platform. It was designed to handle restricted data. AnVIL - Galaxy Community Hub (@link https://galaxyproject.org/use/anvil/)\n\nThe Huttenhower lab hosts custom tools that might not work well on the more current releases of Galaxy, and that includes the AnVIL configuration. This is something you could ask them about.\n\nRelated Problem displaying data on UCSC Cell Browser - #3 by jennaj (@link https://help.galaxyproject.org/t/problem-displaying-data-on-ucsc-cell-browser/2148/3)\n&& Default and advanced settings for data privacy - #2 by marten (@link https://help.galaxyproject.org/t/default-and-advanced-settings-for-data-privacy/2544/2)"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have used  GROMACS - gmx, 2020.1-Ubuntu-2020.1-1. I runned this command.                                                        \u201cgmx grompp -f nvt.mdp -c em.gro -r em.gro -p topol.top -n index.ndx -o nvt.tpr\u201d\nBut I got this error while running this command.\nWARNING 1 [file topol.top, line 27620]:\nThe GROMOS force fields have been parametrized with a physically\nincorrect multiple-time-stepping scheme for a twin-range cut-off. When\nused with a single-range cut-off (or a correct Trotter\nmultiple-time-stepping scheme), physical properties, such as the density,\nmight differ from the intended values. Since there are researchers\nactively working on validating GROMOS with modern integrators we have not\nyet removed the GROMOS force fields, but you should be aware of these\nissues and check if molecules in your system are affected before\nproceeding. Further information is available at\n@link https://redmine.gromacs.org/issues/2884 , and a longer explanation of our\ndecision to remove physically incorrect algorithms can be found at\nOn The Importance of Accurate Algorithms for Reliable Molecular Dynamics Simulations | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage (@link https://doi.org/10.26434/chemrxiv.11474583.v1) .\nSetting gen_seed to 237804172\nVelocities were taken from a Maxwell distribution at 300 K\nNOTE 1 [file topol.top, line 27620]:\nThe bond in molecule-type UNK between atoms 20 C4 and 21 C3 has an\nestimated oscillational period of 1.9e-02 ps, which is less than 10 times\nthe time step of 2.0e-03 ps.\nMaybe you forgot to change the constraints mdp option.\n\nProgram:     gmx grompp, version 2020.1-Ubuntu-2020.1-1\nSource file: src/gromacs/gmxpreprocess/readir.cpp (line 2701)\nFatal error:\nGroup Protein_UNK referenced in the .mdp file was not found in the index file.\nGroup names must match either [moleculetype] names or custom index group\nnames, in which case you must supply an index file to the \u2018-n\u2019 option\nof grompp.\nFor more information and tips for troubleshooting, please check the GROMACS\nwebsite at Common Errors \u2014 GROMACS webpage https://www.gromacs.org documentation (@link http://www.gromacs.org/Documentation/Errors)"
    },
    {
      "role": "system",
      "text": "Hi @user\nThis forum is focused on running tools in the Galaxy ecosystem. @link https://galaxyproject.org\nCommand-line usage would be better asked at general bioinformatics forums or the author\u2019s resources.\n\nAbraham, M. J., Murtola, T., Schulz, R., P\u00e1ll, S., Smith, J. C., Hess, B., & Lindahl, E. (2015). GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX, 1\u20132, 19\u201325. @link https://doi.org/10.1016/j.softx.2015.06.001\n\n\nIf you want to try in Galaxy, or to just review our example data and curated information, please see:\n\n\n\nGalaxy Training Network (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/md-simulation-gromacs/tutorial.html)\n\n\nGalaxy Training: Running molecular dynamics simulations using GROMACS (@link https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/md-simulation-gromacs/tutorial.html)\nModelling, simulation and analysis of biomolecular systems\n\n\n\n\n\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello!\nWhen some job ends with error, depending jobs become paused. And when i push \u201crun again\u201d button, there is \u201cResume dependencies from this job\u201d checkmark. However when i set this mark, often no job is resumed. Moreover, if i push \u201crun again\u201d again, this checkmark disappears, so i can not resume these jobs at all. Is there a way to resume such jobs in this case, perhaps with another input? And why jobs are not resumed?\nThanks in advance."
    },
    {
      "role": "system",
      "text": "Hi @user\nA paused job usually indicates that an upstream job was in an error state. It may not be the job immediately prior but somewhere earlier in the analysis. Or, maybe an input that the tool is expecting is not connected in the workflow? You might want to open the workflow in the editor to check connections between tools (and reset those if needed).\nWorking directly in the history: If an upstream errored job is rerun, the tool form will have an extra option of the form to \u201cresume paused jobs\u201d. If successful, then downstream jobs should process.\nShould you be using collections:\n\nYou can rerun individual elements that failed. Failed elements (red datasets) in a collection will be unhidden by default. Empty elements in a collection may be harder to find/examine/rerun if they didn\u2019t technically fail.\nThere are tools to directly manipulate failed or empty elements out of collections. These tools are found under the `Collection Operations\" section of the tool panel (using default tool sections). Examples include \u201cFilter failed datasets\u201d and \u201cFilter empty datasets\u201d \u2013 those are generally more useful when the collection contains many samples and some do not process well and are to be discarded.\n\nIf that doesn\u2019t help, and it may not, please post back 1) the version of Galaxy you are running and the last update date or the changeset and 2) some screenshots of what is happening. I\u2019m assuming that you are still working in your own Galaxy \u2013 but if the behavior is at a public server please let us know which one."
    }
  ],
  [
    {
      "role": "user",
      "text": "I\u2019ve been working on a single cell RNA-seq dataset (SMARTSeq, Nextera libraries) with RNASTAR and StringTie, and have visualised the total dataset using Seurat. I\u2019d now like to look at differential expression between two groups but I\u2019m having trouble working out how to do this with the TPM data generated in StringTie. Could someone please advise which DE tools can use this as inputs? Or if there is a way to do DE analyses within the Galaxy version of Seurat? I\u2019ve tried DESeq2 a few times but it hasn\u2019t worked (and the options for input file didn\u2019t sound quite right as either raw counts or TPM from kallisto/salmon/sailfish, rather than StringTie). I haven\u2019t been able to find any tutorials that discuss DE after StringTie specifically. Any advice much appreciated!"
    },
    {
      "role": "system",
      "text": "Hi @user,  a detailed explanation of the necessary steps can be found in:\n\n\n\n@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html\n\n\n\nGalaxy Training: De novo transcriptome reconstruction with RNA-Seq (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/de-novo/tutorial.html)\nTraining material for all kinds of transcriptomics analysis.\n\n\n\n\n\nCheers."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nIt has been a couple days that I ran a command (classify.seqs) and it haven\u2019t started yet.\nI reran it twice and same, it is taking so long.\ncould you please advise?\nThanks"
    },
    {
      "role": "system",
      "text": "Hi @user\nit depends on available resources. If a job uses a lot of CPUs, it might stay in queue for some time if the server is busy. Deleting and resubmitting a job does not help, if not opposite, because resubmitted job might be at the end of the queue.\nKind regards,\nIgor"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I\u2019m running rnaSPAdes with the default parameters but searching in the manual, rnaSPAdes supports assemblying with strand-specific RNA-Seq dataset, which is different to FR and RF orientation of paired reads.\nHow can I include the flag --ss when setting the running parameters?\nThanks for your help!"
    },
    {
      "role": "system",
      "text": "Hi, the Galaxy wrapper for rnaSPAdes will be updated soon and the --ss flag will be included. Regards, Stephan"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi. I was trying to generate tracks for a microaray data file from mouse, but there is problem with the genome build for Mouse Dec 2011. (GRCm38/mm10). There is error message saying \u201cThe selected genome have not been set up with a Ensembl gene track. If you require this functionality for the selected genome, please contact the HyperBrowser team.\u201d comes up. I can\u2019t figure out a way to contact the Genomic Hyperbrowser team. If anyone knows the solutions to above problem please help."
    },
    {
      "role": "system",
      "text": "@quote\nGenomic Hyperbrowser\nSometimes public Galaxy servers include contact information on their server\u2019s home page and sometimes they include it in their Galaxy Platform Directory listing. This one has it on server home page \u2013 find it on the \u201cAbout\u201d tab.\n\n\nGalaxy Platform Directory. Search on \u201cPublic Servers\u201d tab to find listings for known public Galaxy servers: @link https://galaxyproject.org/use/genomic-hyperbrowser/\n\n\nThe Genomic Hyperbrowser listing: @link https://galaxyproject.org/use/genomic-hyperbrowser/\n\n\nThe Genomic Hyperbrowser server homepage: @link https://hyperbrowser.uio.no/\n\n\nHope that helps!"
    }
  ],
  [
    {
      "role": "user",
      "text": "When I go to @link http://usegalaxy.eu I can see that it has Viral (2019) Kraken 2 database installed. But when I go to my kraken 2 database manager, I do only see options to add Standard, Minikraken, Special, and Custom. How could I add Viral database from Kraken 2 to my Galaxy instance like @link http://usegalaxy.eu did?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nin your datamanager you can choose custom and than provide your own FASTA file.\nimage\nHope that helps,\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using galaxy for Trinity assembly data is only 3GB (1sample) 3GB (2 sample). After refreshing also, It is showing \u201cJob is waiting to run\u201d. This is insect data which came late by company. If I could not complete in this week, we will not able to catch insects for validation.\nKindly help to resolve this issue."
    },
    {
      "role": "system",
      "text": "Hi @user\nThe public Galaxy servers execute jobs as the shared computational resources become available.\nLeave queued jobs queued. Avoid deleting and rerunning, as that places new jobs back at the end of the queue.\nFAQs\n\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses\n\nUpdate: there is a new server issue. Queued jobs should not be impacted. Server issue at UseGalaxy.org September 2, 2022 -- Status = OPEN ISSUE - #2 by jennaj (@link https://help.galaxyproject.org/t/server-issue-at-usegalaxy-org-september-2-2022-status-open-issue/8591/2)"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI have been trying to run the fastq de-interlacer tool for weeks, several times, on different fastq files.\nThe tool takes too long to proceed, staying orange for days.\nIs there any known issue with the de-interlacer tool?\nThank you, have a good day."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthe problem is that this tool relies on Python code, which means that it is not very computationally efficient. I suggest you split the file into two (or more) datasets and run each one in parallel. After that, you can merge the corresponding forward and reverse reads.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello,\nI\u2019ve been waiting for >12 hours since uploading files for processing by UCHIME, but they have been stuck in the same status since yesterday. Some files are in gray and some in yellow (job currently running), but none are completed. Is this normal? I was under the impression that UCHIME is not very computationally heavy.\nThanks,\nShu"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nYes, that sounds like normal processing. When jobs queue (gray), that means they are waiting for appropriate resources (cluster nodes) to free up. The time spent executing (yellow) is the complete runtime (dispatching, processing, then results written back to Galaxy). The job details view ( :information_source: per dataset) will report exactly how long the processing itself took to run, and some servers will report even more information like estimated costs, etc.\nExample of prior Q&A here about how jobs execute Topics tagged queued-gray-datasets (@link https://help.galaxyproject.org/tag/queued-gray-datasets)\nAlso, please see our FAQs\n\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#understanding-job-statuses (@link https://faqs/galaxy/#understanding-job-statuses)\nhttps://training.galaxyproject.org/training-material/faqs/galaxy/#will-my-jobs-keep-running (@link https://faqs/galaxy/#will-my-jobs-keep-running)\n@link https://training.galaxyproject.org/training-material/faqs/galaxy/#my-jobs-arent-running\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello\nI follow Classification in Machine Learning, but\nMy history turns red when using the Generalized Linear Model tool?\nWhile I follow all the same steps of training\nI tried to use this tool in other galaxies and I came to the same conclusion\nI sent you the link of this story\n@link https://usegalaxy.eu/u/gh.maaryaam1996/h/unnamed-history"
    },
    {
      "role": "system",
      "text": "Hi @user,\ncould you try to run it again by using \u201cClass\u201d instead of \u201cclass\u201d? According to the tabular data, the first letter should be uppercase.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Dear all, Suppose to have a collection of viral reads from NGS (Illumina) technology in fastq format. After the usual pre-processing step (addressed by fastp), I need to remove the host sequences (contaminants) without having the reference genome (I cannot use bowtie2 and samtools for mapping, of course). I have ready some approaches, but I am still not sure. The goal of my project is to identify the correct taxonomy of the viral reads I have. In detail, we always know the host of our sample even if the reference genome is not available, like bat, rodents, human, or mosquito. Please, can someone suggest an appropriate strategy/starting point/approach? Thanks for your support."
    },
    {
      "role": "system",
      "text": "Could you use blast? Without reference it will difficult\u2026 In some cases if you have used specific primers you can filter on that, or on the length of the reads. Did you do some kind of amplicon sequencing?"
    }
  ],
  [
    {
      "role": "user",
      "text": "What is the difference between the P-value and the P-adj columns in the DESeq2 data table? Which of these columns is more important for assessing significant differences in gene expression?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nas probably you know, the p-value is a measure of how likely you are to get a specific gene if no real difference exists. A p-value threshold of  0.05 states that there is a 5% chance that the result is a false positive (considered differentially expressed when is not).\nWhile 5% is acceptable for one test, if we do a lot of tests on the data, then this 5% can result in a large number of false positives (e.g. a differential expression analysis of 8.000 expressed genes would include 8.000 * 0.05 = 400 false positives!) An adjusted p-value aims to overcome the problems due to multiple testing by reducing the p-value threshold from the common 0.05 to a more reasonable value.\nThen, depending on how strict you want the analysis to be, you should choose one or the other value.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have issue with Cutadapt. Sometimes it works normally, but often this tool won\u2019t even start running. Everything is gray, and it can stay like that for hours. When I try something else, like Fastqc, there is no similar problem with running. How can I resolve this problem? Is something wrong with server?\nThank you for answers! :slightly_smiling_face:"
    },
    {
      "role": "system",
      "text": "Welcome, @user\nDifferent tools use different computational resources. Some resources are busier than others.\nIt sounds like your jobs are queued \u2013 and that is normal/expected at public Galaxy servers. These are shared resources, and your jobs will execute when resources become available.\nI added a tag to your post that points to many prior Q&A topics about this. Or you can use this search link: Topics tagged queued-gray-datasets (@link https://help.galaxyproject.org/tag/queued-gray-datasets)\nFAQs: Galaxy Support (@link https://galaxyproject.org/support/#datasets-and-histories)\nStart here: Datasets and how jobs execute (@link https://galaxyproject.org/support/how-jobs-execute/)\nSelected public Galaxy resource/server status page: @link https://galaxyproject.statuspage.io/\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi everyone,\nI am new to NGS data analysis. I have done various bioinformatics work and I am also comfortable with programming in Python. May I know the smart way of learning NGS data analysis especially the data that I got from Ion Torrent PGM and Nanopore MinION."
    },
    {
      "role": "system",
      "text": "Hi @user,\nthere are a few tutorials in the Galaxy Training Network which describe the required steps for analyzing Nanopore data:\nGenome assembly\n\nChloroplast genome assembly (@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/chloroplast-assembly/tutorial.html)\n\nGenome Assembly of MRSA using Oxford Nanopore MinION Data (@link https://training.galaxyproject.org/training-material/topics/assembly/tutorials/mrsa-nanopore/tutorial.html).\n\nMetagenomics\n\n16S Microbial analysis with Nanopore data (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/nanopore-16S-metagenomics/tutorial.html)\nAntibiotic resistance detection (@link https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/plasmid-metagenomics-nanopore/tutorial.html)\n\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "trying to configure the Okta OIDC client. Getting error after authentication on Okta.\nsocial_core.exceptions.AuthTokenError: Token error: Signature verification failed\nshould the discovery URL be configured somewhere? How does it know the jwks_uri to validate tokens?\nThanks"
    },
    {
      "role": "user",
      "text": "found my issue. Documentation could use a little clarity. While it states the API URL should be:\nThis has subsequently had the \u2018/v1/authorize\u2019 removed. In productive deployments, this will likely resemble:\nhttps://{company}.okta.com/oauth2/{authServerId}/\nThe source library documentation states:\n@link https://python-social-auth.readthedocs.io/en/latest/backends/okta.html\nPlease note, do not use the  /oauth2/default  endpoint for Okta authentication:\n\u201cdefault\u201d is the API authServerID that comes with Okta. Usually oidc clients have no issues using it. So when configuring galaxy oidc_config.xml and oidc_backend_config.xml, instead of https://{company}.okta.com/oauth2/{authServerId}/ use https://$ (@link ){company}.okta.com/oauth2/\ncheers!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi Everyone.\nI followed the specific workflow which is fastq\u2013>tophats\u2013>HTseq. I ran tophat and it worked well. But now I am confused in the HTseq. The tool asks for BAM file which is from the tophats while it also asks for GTF which is I think the genome reference GTF file\u2026? Does that mean I have to load the human reference GTF file which I used for tophats (used in build index)?\nScreen Shot 2020-07-31 at 12.06.06 PM1440\u00d7900 365 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/a/a1405e57b5cbda62eef63fa38ed3eadb4852b420.png)"
    },
    {
      "role": "system",
      "text": "You need to use a GFF/GTF file (which is a feature list) that corresponds to the reference sequence you used as the reference (a sequence) you used in Tophat. So if you used the human h19 reference genome you should use a GFF/GTF file corresponding to the h19 reference. You can probably find the file you need here: @link http://useast.ensembl.org/info/data/ftp/index.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "Today I can\u2019t even reach the site and the service has not worked properly for me since May 17th. Any clues as to why that is?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nThere is a performance issue with our main storage and this is affecting the reliability of the Galaxy service. We are investigating the source of the problem."
    }
  ],
  [
    {
      "role": "user",
      "text": "Hi all,\nI am having trouble with the core gene alignment generated by Roary.\nI used Prokka to annotate 69 genomes of bacteria belonging the same genus. Then, I used the gff3 files in Roary to generate a core gene alignment.\nEverything worked out nicely expect that I am not able to use that core gene alignment because the names given to the genomes are very weird (for example \u201cdataset_00b65bd2-68c1-4486-92a6-8c88170ac99d\u201d).\nI am not able to track back those names to find out to which genomes they correspond because they are not the names of the ggf3 files nor the names of the sequence region. I don\u2019t know if I am doing something wrong or if it is a bug.\nHas someone experienced something similar?\nThanks for your help.\nBest,\nArthur"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthank you so much for your report. This problem has been fixed, but until January we won\u2019t be able to update the tool version in @link http://usegalaxy.eu. I suggest to you use @link http://usegalaxy.org, since this instance includes the correct tool.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am using a local Galaxy instance and I want to use report to generate reports of results that I have generated. I am using ireport (Galaxy version 1.1) and providing a previously generated webpage which can be used. I tried it with several different results and it keeps giving this error - \u2018base64\u2019 is not a text encoding; use codecs.encode() to handle arbitrary codecs. I saw an earlier post (IReport issue with encode base64? (@link https://help.galaxyproject.org/t/ireport-issue-with-encode-base64/5041)) that says that this is because of the difference in py2/p3 and will be rectified. How can I rectify it? any help or suggestions is highly appreciated. Thank you!"
    },
    {
      "role": "system",
      "text": "Hi @user\nWorkflow reports are now built-in to Galaxy. These are generated by default every time a workflow is executed. Release notes (see 21.01 for the original, and later releases for updates): @link https://docs.galaxyproject.org/\nGo to User \u2192 Workflow Invocations and expand the details per invocation. You\u2019ll find a link to an interactive report and a PDF version available for download.\nScreenshot\n\nworkflow-report1434\u00d7582 51.8 KB\n (@link https://global.discourse-cdn.com/business7/uploads/galaxy/original/2X/6/67d28fae265fbcec08e31a3239fb134122a1fd2a.png)\n\nThe older iReport version (@link https://toolshed.g2.bx.psu.edu/view/saskia-hiltemann/ireport/7300ed4c1481) was never updated from the original Python 2 environment, and even that may not be enough to make it useable."
    }
  ],
  [
    {
      "role": "user",
      "text": "How can I use a config file (yaml, json, etc) as an input to the workflow, to set workflow parameters and tool settings?\nFor example, set reference genome, max insert size for an aligner, min length after trimming for the adapter trimmer, etc - all in 1 file.\nWe currently use multiple workflow parameters such as ${max insert size}, etc. There is also a newer method, using Simple inputs used for workflow logic. Apparently, both of these methods require the user to type in or select 1 parameter at a time. The config file sets multiple parameters per config.\nFor many of our users, this would be more robust and faster than selecting these every time the workflow is run. The config file also provides a nice record of the most frequently changed parameters/settings, for record keeping and for debugging.\nIn most cases, the user changes only a minority of the lines (if any) in a file such as this."
    },
    {
      "role": "system",
      "text": "@quote\nSimple inputs used for workflow logic\nThese can have pre-set content. But if the users need to adjust these, then you might want to create copies/variations of the workflow for specific genomes or analysis projects.\nThere isn\u2019t a way to use a config file that I know of through the web interface, but it might be possible when running a workflow through the API. You\u2019ll need to test that out as a potential solution \u2013 IF your users are able/willing to run workflows that way. API help: @link https://docs.galaxyproject.org/en/master/api_doc.html\nIf you want to make a request to the developers about this, two choices:\n\nVet the request first at Gitter: @link https://gitter.im/galaxyproject/Lobby\n\nDirectly create an enhancement request issue: @link https://github.com/galaxyproject/galaxy\n\n\nThanks!"
    }
  ],
  [
    {
      "role": "user",
      "text": "For the Maker genome annotation tool in Galaxy, in the Repeat Masking section, there are options for choosing a repeat library source.\nThe listed options are:\nDFam (curated)\nDFam (full version)\nCustom library of repeats\nDisable repeats\nWhen using MAKER on the command line, here are the options for Repeat Masking:\nmodel_org=all #select (@link /tag/select) a model organism for RepBase masking in RepeatMasker\nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker\nrepeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner\nrm_gff= #pre-identified repeat elements from an external GFF3 file\nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no\nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)\nHow would I use a DFam database on the command line version?\nFor example, on Galaxy I use a DFam curated database for arthropoda, which I can type as the repeat source species.\nBut if I repeat this on the command line, do I run RepeatMasker separately with the database I want, and then input this into Maker?"
    },
    {
      "role": "system",
      "text": "Hi @user\nMaker runs RepeatMasker as part of the processing.\n\nFor using the tool at the command line, please see their documentation, wiki, demos, tutorials: Yandell Lab - Software - MAKER (@link http://www.yandell-lab.org/software/maker.html)\n\nExample: MAKER Tutorial 2013 - GMOD (@link http://gmod.org/wiki/MAKER_Tutorial#Details_of_What_is_Going_on_Inside_of_MAKER)\n\n\nIf you are curious about how Galaxy wrapped the tool, details are here: Galaxy | Tool Shed (@link https://toolshed.g2.bx.psu.edu/repository?repository_id=d3e7d2096672bfd2)\n\nThere is test data available at the development repository and test cases within the tool\u2019s XML file. You could load that data into a Galaxy history, then run simple jobs using the same parameters as the tests do. Galaxy will construct a command line that will be reported under the \u201cJob Details\u201d page for most results (. You wouldn\u2019t use that command line yourself \u2013 it is specific to the server \u2013 but maybe it is informative.\n"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.\nI\u2019m new in bioinformatics and in galaxy plataform.\nI would like to know about privacy of my sequence data. When I upload my shotgun data, is it available to the public?\nThank you."
    },
    {
      "role": "system",
      "text": "By default user data is accessible but unlisted (so theoretically private). You can set permissions in a way that everything including new uploads is always private to you unless you explicitly share it.\nFor details: @link https://galaxyproject.org/learn/privacy-features/"
    }
  ],
  [
    {
      "role": "user",
      "text": "Workflow don\u2019t want to start\u2026"
    },
    {
      "role": "system",
      "text": "@quote\nWorkflow don\u2019t want to start\nAll of the public servers are very very busy. The best strategy is to leave the workflow queued \u2013 it will run as resources become available. Next time you start a workflow, consider including the option to send yourself notifications (per step or as a whole). Avoid restarting \u2013 that places all of the work back in the end of the queue, further extending wait time.\nThe processing of workflow tools is similar to how tools in the history will process, but with the added advantage that tools will run serially without any more interaction from you. Some steps/tools may run quicker than others \u2013 that\u2019s due to different resource requirements per tool. @link https://training.galaxyproject.org/training-material/faqs/galaxy/datasets_job_status.html"
    }
  ],
  [
    {
      "role": "user",
      "text": "On the upper nav, there is an option to enable/disable Scratchbook. I see no documentation on what Scratchbook is and how to use it. What is it?"
    },
    {
      "role": "system",
      "text": "It is an in-page window manager. Take a @link https://usegalaxy.org/tours/core.scratchbook."
    }
  ],
  [
    {
      "role": "user",
      "text": "hello,\n.\ni am working with candida genome project.\ni am new with bioinformatics science,i want to use bwa tools to alignment genome strain to  reference genome\nthe refrence stain data easy to download, i need help to download fastaq1 and fastaq2 for the second genome,the SRA number is avilable, i found deposit data at ENA  at FASTQ files (Galaxy).\nso is that right to download the two fastaq file and used directly without fastaqc"
    },
    {
      "role": "system",
      "text": "Hi @user!\nYes this is correct. You could also use @link https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fastq_dump/2.9.1.3 to download from SRA.\nHappy research!\nBjoern"
    }
  ],
  [
    {
      "role": "user",
      "text": "I have been trying to create a Galaxy InteractoMIX account but have not succeeded. The website says to email @link mailto:interactomix.galaxy@gmail.com to create an account, but I have reached out several times and have not received a response. Is there another way to create an account?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nI suggest you write to Narcis Fernandes-Fuentes or Baldo Oliva, whose emails you can find in the Galaxy InteractoMIX paper (@link https://www.sciencedirect.com/science/article/abs/pii/S002228362030557X?via%3Dihub#!).\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "I am trying to follow this transcriptomic tutorial (@link https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/mirna-target-finder/tutorial.html?), I have a question about the reference files that have been used in the tutorial.\nstar_miRNA_seq.fasta\nmature_miRNA_AT.fasta\nmiRNA_stem-loop_seq.fasta\n\nwhere these files are publically available? Are these files from miRBase database? And what is the star file?"
    },
    {
      "role": "system",
      "text": "Hi @user,\nthose files are available on the PmiREN website (@link https://www.pmiren.com/download); all of them correspond to Arabidopsis thaliana. The star file includes the sequences that are complementary to the main miRNA sequences (originated from the same hairpin structure); the main sequences are considered the functional ones, but because there are no sufficient experimental evidence to determine if the complementary sequences are not functional, those are also included in the target identification analysis.\nRegards"
    }
  ],
  [
    {
      "role": "user",
      "text": "Hello.  Two questions\n\nDoes anyone know whether there is a gemini for hg38?\nIf there is not a gemini for hg38 (only hg19) what is the best alternative?\n\nThank you."
    },
    {
      "role": "system",
      "text": "Hello @user\n@quote\nDoes anyone know whether there is a gemini for hg38?\nThe Galaxy wrapped version of Gemini works with the hg19 genome. There are technical reasons why hg38 is not supported.\nQuote from the tool form, as hosted at @link https://usegalaxy.eu: Gemini load (@link https://usegalaxy.eu/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fgemini_load%2Fgemini_load%2F0.20.1%2Bgalaxy2)\n\nOnly build 37 (aka hg19) of the human genome is supported.\n\nGTN Tutorial that covers an example usage: @link https://galaxyproject.github.io/training-material/topics/variant-analysis/tutorials/exome-seq/tutorial.html\n@quote\nIf there is not a gemini for hg38 (only hg19) what is the best alternative?\nGemini has unique functionality. Annotation and parsing tools are all one package. But there are many other tools to annotate/parse VCF datasets. Please see this prior Q&A for some tips: GEMINI load alternatives (@link https://help.galaxyproject.org/t/gemini-load-alternatives/264)\nBest!"
    }
  ],
  [
    {
      "role": "user",
      "text": "Good Morning! I hereby inform you that my assembly has not yet started to run, as you are saying that it is still pendete to run. I would like to know if such an event is normal and how long it takes to start."
    },
    {
      "role": "system",
      "text": "@user\nIt is normal for jobs (any tool) to queue first, then execute.\nThe timing depends on the tool, how many jobs you have running, and how many other people are using the shared resources at the same public Galaxy server.\nAllow queued jobs to remain queued. If you deleted and rerun, the new job is placed back at the end of the queue again, extending wait time. The exception is if you already know that there is a problem with the tool run and need to start over anyway.\nI added a tag to your post that covers prior Q&A and FAQs about how jobs execute. Click on it to review if things are still not clear.\nThanks!"
    }
  ]
]